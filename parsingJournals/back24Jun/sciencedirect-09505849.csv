volume|issue|url|title|abstract
41|1|http://www.sciencedirect.com/science/journal/09505849/41/1|A unified characterisation for shared multimedia CSCW workspace designs|The target of this study is synchronous multimedia computer systems for co-operative work, which makes extensive use of shared workspaces, supporting both visual and audio presentations. This article develops a formal method for describing shared CSCW workspace designs through a unifying multidimensional characterisation scheme. The scheme consists of a system of taxonomic characters (TCs), and a formal notation to specify the inherent interdependencies among the design choices. This article introduces the notion, and discusses the issues of completeness and coherence of the system. The scheme is then applied to two aspects of Multimedia CSCW: shared workspaces and floor control. Particular attention is paid to provide a unified view of both the audio and visual components, by showing how they can both be treated uniformly using the same characterisaton. Finally, the notion of control shadowing for shared workspaces is presented and is described using a special notation, the notation of control whizzies.
41|1||Testing syntax and semantic coverage of Java language compilers|Software testing is a critical and important stage of the application software development life-cycle. Testing is a verification activity that affects the overall software quality. The verification of critical and dependable computer software such as real-time safety-critical software systems consumes about 50% of the project time. In this work, we consider testing compilers. Since a compiler is a highly usable software, an increased emphasis on reliability requires a careful testing before releasing the compiler. In compiler testing, the compiler implementation is verified to conform to the specified language syntax and semantic available in the standard language documentation. In this work, an algorithm is designed and a tool is developed to automate the generation of test cases to check the language syntax. In addition, we propose a heuristic approach to cover language semantics. Since Java is a relatively new programming language, we concentrate on testing the adherence of new Java compilers to the language syntax and semantics.
41|1||Improving risk management: moving from risk elimination to risk avoidance|This study has examined software reliability using a risk management framework to analyze popular risk management models with regard to their emphasis on risk avoidance. Too often, risk reduction occurs late in the software development life cycle when changes are costly and less effective. We suggest that early risk avoidance techniques, like the cleanroom software development process and its sub-process of software inspections, leads to superior products. Our approach is different from the current mainstream approach of testing to eliminate errors because we propose using risk models that emphasize preventive risk management early in development. We argue that the use of risk avoidance leads to benefits that far outweigh the cost of implementation.
41|1||Life-cycle of a dual object-oriented specification model for real-time systems1|
41|10|http://www.sciencedirect.com/science/journal/09505849/41/10|Using KBS verification techniques to demonstrate the existence of rule anomalies in ADBs|
41|10||Formalization and verification of event-driven process chains|For many companies, business processes have become the focal point of attention. As a result, many tools have been developed for business process engineering and the actual deployment of business processes. Typical examples of these tools are Business Process Reengineering (BPR) tools, Enterprise Resource Planning (ERP) systems, and Workflow Management (WFM) systems. Some of the leading products, e.g. SAP R/3 (ERP/WFM) and ARIS (BPR), use Event-driven Process Chains (EPCs) to model business processes. Although the EPCs have become a widespread process modeling technique, they suffer from a serious drawback: neither the syntax nor the semantics of an EPC are well defined. In this paper, this problem is tackled by mapping EPCs (without connectors of type ∨) onto Petri nets. The Petri nets have formal semantics and provide an abundance of analysis techniques. As a result, the approach presented in this paper gives formal semantics to EPCs. Moreover, many analysis techniques are available for EPCs. To illustrate the approach, it is shown that the correctness of an EPC can be checked in polynomial time by using Petri-net-based analysis techniques.
41|10||Control-flow semantics of use cases in UML|The control-flow for five kinds of use cases is analysed: for common use cases, variant use cases, component use cases, specialised use cases and for ordered use cases. The control-flow semantics of use cases—and of the uses-relation, the extends-relation and the precedes-relation between use cases—is described in terms of flowgraphs. Sequence diagrams of use cases are refined to capture the control-flow adequately. Guidelines are given for use case descriptions to attain a well-defined flow of control.
41|10||Look before you leap: on some fundamental issues in software engineering research|In science, clear definition and rigorous representation are of prime importance. Recently, Briand et al. (IEEE TSE 22 (1) (1996) 68–85) proposed to clarify ambiguous software quality concepts such as size and complexity through a definition based on their intuitive properties. A seemingly rigorous and generic system representation, suitable for analyzing software described with existing tools, is also advanced. We justify that their property-based definitions for these concepts are logically untenable and practically misleading. We demonstrate further that their representation is still vague and searching for a general system representation is doomed to failure and cases of refutation are provided. In addition, we prove that all the system definitions, even for the very limited practical case of programs, are flawed. Finally, we elucidate with Briand et al.'s examples that theoretic representation must be validated, set theory seems incompatible with programs, and reasoning by analogy could be misleading. Our in-depth analysis illustrates that formality is but skin-deep and we must be aware of the danger of ignoring the most important methodological rigor in software engineering research.
41|10||FHIN: an efficient storage structure and access method for object-oriented databases|While relational database technology has dominated the database field for more than a decade, object-oriented database (OODB) technology has recently gained a lot of attention in the database community. Many researchers are concerned about the performance of OODBs. This paper proposes an OODB design methodology called fragmented hash-indexed (FHIN) that is aimed at improving the operating performance of OODBs. The FHIN model's storage structure contains an Instances–Classes Table (ICT) with a two-segment data design. Query processing is done by accessing data segments through ICT with an algorithm introduced here. The FHIN model uses three access methods: hashing, indexing, or hash-indexing. The database performance of FHIN is compared to two previous access methods using 1050 simulation runs. Results indicate that the FHIN model is 43% better than either of the other models in smaller databases, 65% better in larger databases, 50% better under conditions of high updating, and 72% better under conditions of low updating. These results suggest that FHIN methodology has promise and is worthy of exploration and OODB software development.
41|11-12|http://www.sciencedirect.com/science/journal/09505849/41/11-12|Communications software engineering (CSE)|
41|11-12||A service creation environment based on scenarios|Scenarios are often constructed for illustrating example runs through reactive system. Scenarios that describe possible interactions between a system and its environment are widely used in requirement engineering, as a means for users to communicate their functional requirements. Various software development methods use scenarios to define user requirements, but often lack tool support. Existing tools are graphical editors rather than tool support for design. This paper presents a service creation environment for elicitation, integration, verification and validation of scenarios. A semi-formal language is defined for user oriented scenario representation, and a prototype tool implementing an algorithm that integrates them for formal specification generation. This specification is then used to automatically find and report inconsistencies in the scenarios.
41|11-12||A model-based authorware for the construction of distributed multimedia systems|Distributed multimedia systems typically involve a sophisticated user interaction. Further, objects are allocated on physically distributed computing systems, and multimedia data must be transferred across heterogeneous networks in a timely manner. These systems often have complex requirements on a user interaction, quality of service and temporal order among media streams. The design and implementation of these requirements are inherently complex and present an extraordinary design and programming challenge. Generally, these complex requirements cannot be adequately captured using a single model or a design notation. The challenge amounts to (i) identification of multiple, often orthogonal models, each capturing a specific aspect of the requirements, and (ii) provision of an authorware that supports the composition of these models. In this paper, we propose to capture the multimedia requirements in three different models: configuration, user control and presentation, and demonstrate how the composition of these models can be supported by an authorware using the Java and CORBA technologies. The concepts are illustrated using a real-life example based on a virtual city tour application that features distributed controls, collaborative work and multimedia presentations. Various distributed multimedia applications like video phone, video conferencing and distributed presentation have been successfully constructed using the proposed multiple models and authorware. The results are encouraging and the approach can shorten the development of multimedia applications considerably.
41|11-12||Communications software design for testability: specification transformations and testability measures|To deal with the increased complexity related to the testing of communications software, we propose the integration and application of finite state machine based specification transformations and testability measures early in the communications software development process. Based on this integration, the testability of a given design is estimated and appropriate specification transformations are defined and applied iteratively to enhance the testability of the product implementation.
41|11-12||A protocol synthesis method for fault-tolerant multipath routing|This paper proposes a new synthesis method for generating fault-tolerant multipath routing protocols. The protocol is defined as fault-tolerant if messages can be rerouted by using another path when a communication channel fails. The routing protocols obtained adopt a multipath routing function, augmented with routing table, where each table stores the next nodes for multipath routing, and updates the tables according to the network topology changes. Additionally, the routing protocol can attain flexibility by the multipath routing mechanism in the sense that only a small amount of change is needed for the change of network topology. We also briefly describe an extension of the proposed method for generating multicast routing protocols.
41|11-12||A medium access control protocol for voice and data integration in receiver-oriented DS-CDMA PCNs|Primarily used in military communications in the past, code division multiple access (CDMA) is recently found to be attractive for personal communications as well. As a large number of mobile hosts are supported within a cell and a wide range of services are provided, one of the most important issues in a CDMA personal communication network is how to control the uplink access to the shared wireless spectrum. In this paper, we address this issue in a realistic situation where the receiver-oriented transmission protocol is employed and the packet loss due to multiple access interference (MAI) cannot be ignored. A medium access control protocol for voice and data integration is proposed. It solves the problems of code assignment and MAI control at the same time. A Markov chain model is used to analyze the protocol and the analytical results are shown to be very close to simulations. Based on the modeling, the effectiveness of the protocol's MAI control is demonstrated and some system design issues are investigated.
41|11-12||Controllability and observability in distributed testing|In developing distributed systems, current trends are towards creating open distributed environments supporting interworking, interoperability, and portability, in spite of heterogeneity and autonomy of related systems. Several reference models, architectures and frameworks such as ODP, CORBA, and TINA, have already been designed and proposed. However, even though models, architectures, and frameworks, provide a good basis for developing working open distributed applications, conformance testing approaches are required for gaining confidence in final products and guaranteeing their integration and interoperability within open distributed environment. ODP provides some preliminary statements on conformance assessment in open distributed systems, but considerable work needs to be done before reaching a workable and accepted conformance testing methodology for open distributed processing. Further, ISO, ITU, OMG, and TINA-C, have recently recognized the urgent need for conformance testing. In this paper, we examine ideas gained from our experience with protocol testing, which may contribute to the design of such a framework. Our methodology is essentially guided by two features that have a great influence on all aspects of the testing process: controllability and observability.
41|11-12||A framework for the specification of test cases for real-time distributed systems|The OSI conformance testing methodology and framework (CTMF) is a well established standard which defines and regulates the conformance testing procedure for protocol implementations. Conformance testing is meant to be functional black-box testing. Besides concepts and terminology, the CTMF standardizes testing architectures and the Tree and Tabular Combined Notation (TTCN) test specification language. As more and more distributed systems such as multimedia, safety-critical and real-time systems rely on the timely availability of information, testing of real-time requirements becomes a serious issue, too. Unfortunately, testing real-time and other non-functional requirements (performance and reliability) are outside the scope of CTMF. In this paper we present an extension of CTMF which allows us to specify test cases for testing real-time requirements. The extension includes a generic testing architecture and a notation for test specification, which is called real-time TTCN.
41|11-12||Efficient checking sequences for testing finite state machines|
41|11-12||Passive testing and application to the GSM-MAP protocol|Passive testing is the process of collecting traces of messages exchanged between an operating implementation and its environment, in order to verify that these traces actually belong to the language accepted by the provided finite state machine specification. In this paper, we present an extension of the existing algorithms to consider an extended finite state machine as the specification. An algorithm is also introduced to take into account the number of transitions covered. These techniques are illustrated by the application to a real protocol, the GSM (global system for mobile communication)-MAP (mobile application part).
41|11-12||Extending EASE with new ASN.1 encoding rules|
41|11-12||Recovery of CFSM-based protocol and service design from protocol execution traces|
41|13|http://www.sciencedirect.com/science/journal/09505849/41/13|FaxWeb: accessing the WWW using the fax machine|
41|13||Semantics of recursive relationships in entity-relationship model|The recursive relationship of the ER model is used to represent a hierarchical situation in a natural way. However, the semantics of recursive relationships are quite difficult to grasp because entities carry out different roles in relationships. This article proposes four classes of entity types allowed in recursive relationships and provides a thorough analysis of all the types of recursive relationships through the four types of the recursive dependency and four classes of the entity type. By the proposed concepts of the entity type properties and the existing relationship constraints of the recursive relationships, we can more clearly capture the semantics of hierarchical structures and integrity constraints in practical databases.
41|13||Cohesion prediction using information flow: an empirical feasibility study and comparison using students as inexperienced designers|This paper describes an experimental investigation that demonstrates the feasibility of predicting weakly bound cohesive software modules [W.P. Stevens, G.J. Myers and L.L. Constantine, IBM Systems Journal (2) (1974) pp. 115–140] at the design stage of development. It compares the predictive capability of prediction systems, for weakly bound cohesive modules, based on classical regression models with systems that use binary logistic regression models. It also considers the models’ parsimony of parameters and their fit to the sampled data. Further, the external validity of the prediction systems is discussed and an appropriate mode of usage is proposed for the ‘better’ statistical models.
41|13||A workflow management system based on intelligent collaborative objects|This paper describes an architecture for workflow management systems based on Workflow Intelligent Business Objects (WIBOs). The design of WIBOs is based on principles of intelligence, autonomy, collaboration and co-operation. Using WIBOs that carry out tasks on users’ behalf, it is possible to build workflow systems that bring further improvements in process automation and dynamic management, and achieve dynamic (re)allocation of resources to Actors. A WIBO prototype architecture has been implemented using Java. A Java Remote Method Invocation (RMI) has been used to enable WIBOs to communicate over an Intranet or Internet.
41|13||Calculating upward and downward simulations of state-based specifications|
41|14|http://www.sciencedirect.com/science/journal/09505849/41/14|Perspectives on Information Technology in the New Millennium|
41|14||Software engineering education in the 21st century|
41|14||Software economics: status and prospects|
41|14||Software technologyâformal methods and scientific foundations|
41|14||How long into the 21st century will the aftermath of the millennium bug last?|While practically nobody would dispute that there is a Year 2000 (Y2K) problem with software, and by extension with computers and communications at large, there is a wide range of opinions on how critical will be that problem. Opinions vary widely and there is no way to check how reasonable they are because there is no precedent and no facts against which to gauge them.
41|14||Context management in modeling information systems (IS)|Knowledge engineering methods are shown to have an important role in addressing the challenge of building trusted contexts; namely, in providing solutions to the problems of complexity, conformity, changeability and invisibility. Current information systems (IS) design practices, however, are seen to solve only those problems pertaining to the global enterprise system, while leaving the validation issues related to user/local models unresolved. Context management mechanisms associated with the user/local models are shown to provide a basis for dynamic validation of user requirements. The proposed context management process has been developed to allow context representation and dynamic knowledge structuring at the epistemological level.
41|14||Architecture-driven component reuse|
41|14||Thirty years (and more) of databases|This paper outlines the historical development of data management systems in order to identify the key issues for successful systems. It identifies the need for data independence and the embedding of structural and behavioural semantics in the database as key issues in the development of modern systems. Hierarchical, Network, Relational, Object-oriented and Object-relational data management systems are reviewed. A short summary of related research is given. The paper concludes with some speculation on the future directions that database technology might take.
41|14||Software intensive embedded systems|
41|14||Requirements engineering for COTS based systems|In spite of the increasing use of commercial off-the-shelf (COTS) products for system development, there is little consideration on how to acquire requirements for COTS products, how to select COTS components and how to assemble them to comply to these requirements. The paper addresses the issue of the requirements engineering process for COTS components acquisition and assembly. It proposes an approach based on the notion of requirements maps and assembly strategies and demonstrates the approach with the selection of a CASE tool.
41|14||Software testingâsearching for the missing link|Software testing has been the subject of active research for approaching 30 years. In that time, there have been developments such as the invention of techniques and tools, the morphosis of these techniques and tools to deal with the adoption of new development paradigms and programming languages, and analytical and empirical comparisons of techniques to improve our understanding of their relative merits. Impressive though these developments are, there still remains the fundamental problem of trying to relate the results of testing to some objective attribute of the program (such as number of faults). It is the search for this missing link that is attracting much of the recent research in the area. Any progress made towards this will have a considerable impact on our understanding of the subject and ultimately on software itself.
41|14||An integrated web computing application for tasks related to course selection and registration|We have developed an integrated Internet application system including an OODB, which provides instant lottery selection of applicants for courses with capacity limitations, and which allows students to create their own weekly timetables and at the same time to complete their course registration to be verified/analyzed and confirmed instantly, reducing the time needed for course registration-related work before the beginning of each semester from several days to less than one hour. Information on courses held at each time period and taught by each instructor as well as the syllabus of each course can be viewed using multiple hypertext links, and no keyboard operations are needed for these.
41|14||Objects and roles: modeling based on the dualistic view|
41|14||The time of the chameleons is over?|
41|14||In the 25 years since The Mythical Man-Month what have we learned about project management?|This paper discusses Brooks’ The Mythical Man-Month, a landmark work in the software project management field, and compares the software project management advice given there with practices employed some 25 years later. To find out the state of today's practice 20 experienced software developers were interviewed regarding their impressions of factors leading to success or failure of software development projects. Their observations are compared with the points raised by Brooks in his seminal work.
41|14||Getting the best from formal methods|This paper reviews the progress made in the use of formal methods in the last 15 years and suggests some reasons for the failure of formal methods in places where they promised to be most effective. It proposes a way in which formal methods can nevertheless be infiltrated into software development activities with beneficial results.
41|15|http://www.sciencedirect.com/science/journal/09505849/41/15|Providing views and closure for the object data management group object model|The object data management group (ODMG) object model offers a standard for object-oriented database designers, while attempting to address some issues of interoperability. This research is focused on the viability of using the ODMG data model as a canonical data model in a multidatabase environment, and where weaknesses are identified we have proposed amendments to enable the model to suit the specific needs of this type of distributed database system. This paper describes our efforts to extend its relational style algebra, and to provide query closure and a viewing mechanism for object query language to construct multidatabase schemas.
41|15||ASSISTâa tool to support software inspection|Software inspection is a valuable technique for defect detection. Recent research has considered the development of computer support, with the aim of providing even greater benefits when applying inspection. This has resulted in the development of a number of prototype support systems. These suffer from some fundamental limitations, however. Asynchronous/Synchronous Software Inspection Support Tool (ASSIST) is designed to tackle these limitations and to provide a platform for rigorous investigation of the inspection process. It uses a custom-designed language known as Inspection Process Definition Language to allow support of any inspection process, and provides an open, expandable system allowing multiple document types to be catered for easily. A number of facilities designed to enhance the inspection process are also present.
41|15||Analysis of software bug causes and its prevention|In our software development group we have faced difficulties in improving software quality. In order to find the causes of this problem we have analyzed software bugs that were found in the credit authorization terminal software that our group developed, and found the real causes of the bug. As a result, it was found that about 50% of the real causes were made by the designer's carelessness. Based on this finding, guidelines were established to improve the designers’ behavior. Furthermore, we made clear the causes of software bugs, the phase in which they were made, the relation between the software bugs and their real causes. We also made the measures to prevent the software bugs and implemented them. And we refer to the effect that the substantial amount of software bugs can be decreased by the implementation.
41|15||Index|
41|15||Index|
41|2|http://www.sciencedirect.com/science/journal/09505849/41/2|Bridging objects and relations: a mediator for an OO front-end to RDBMSs|
41|2||Formalising tutoring strategy selection in multimedia tutoring systems|Variation in tutoring strategy plays an important part in tutoring systems. The potential for providing adaptive tutoring depends initially on having a range of tutoring strategies to select from. However, in order to react effectively to the student’s needs, the system not only has to be able to offer different tutoring strategies, but to choose intelligently among them and determine which is best for an individual student at a particular moment. This paper contributes a model that formalises the process of tutoring strategy selection in tutoring systems, and shows how this is deployed in a multimedia tutoring system. The evaluation of the resulting system reveals the benefits of this model in practice.
41|2||Multiple multicast groups for multimedia on the Internet|The use of large-scale multimedia applications has escalated in recent years. Distributed object frameworks such as the Common Object Request Broker Architecture are being deployed to cater this market. We argue however, that CORBA is lacking for large-scale multimedia applications requiring timely guarantees.
41|2||System analystsâ orientations and perceptions of system failure|
41|2||Heuristic principles for the design of artificial neural networks|Artificial neural networks were used to support applications across a variety of business and scientific disciplines during the past years. Artificial neural network applications are frequently viewed as black boxes which mystically determine complex patterns in data. Contrary to this popular view, neural network designers typically perform extensive knowledge engineering and incorporate a significant amount of domain knowledge into artificial neural networks. This paper details heuristics that utilize domain knowledge to produce an artificial neural network with optimal output performance. The effect of using the heuristics on neural network performance is illustrated by examining several applied artificial neural network systems. Identification of an optimal performance artificial neural network requires that a full factorial design with respect to the quantity of input nodes, hidden nodes, hidden layers, and learning algorithm be performed. The heuristic methods discussed in this paper produce optimal or near-optimal performance artificial neural networks using only a fraction of the time needed for a full factorial design.
41|3|http://www.sciencedirect.com/science/journal/09505849/41/3|Distributed implementation of the disabling operator in lotos|
41|3||Stability constraints and stability normal forms for temporal relational databases|This paper presents a new dynamic dependency and a new normal form. We give the concept of the stability constraint and establish complete axioms for stability constraints. In addition, we propose a stability normal form. A temporal third normal form can be losslessly decomposed into a sequence of stability constraints. Using the decomposition, the storage space can be reduced. The stability constraints allow to fill a suitable value into a null value. Thus, this null value therefore turns into a known value.
41|3||Comparing OPEN and UML: the two third-generation OO development approaches|
41|3||When are methods complementary?|In this article, we address the issue of when software development methods are complementary, i.e., we determine when a method is capable of a task that another method cannot perform. Our intent is to examine complementarity in order to help determine when to carry out method integration. Here, we propose some factors for method complementarity, and suggest that context-dependent criteria, such as real-world domain and non-functional development requirements, may have a significant impact on method complementarity.
41|3||Query execution scheduling in parallel object-oriented databases|Complex object-oriented queries generally consist of path expressions and explicit join operations. Since explicit join operations have been acknowledged as the most expensive operations, query executions normally start from the path expressions. Each path expression may form a sub-query. There are two existing strategies to sub-queries processing: ‘serial’ and ‘parallel’ execution scheduling strategies. Serial sub-queries execution corresponds to an execution of the sub-queries one-by-one, whereas parallel sub-queries execution corresponds to simultaneous execution of the sub-queries. When a sub-query is being processed, parallelization techniques may be applied. In this paper, we focus on the scheduling issues of the sub-queries, rather than the parallelization of the sub-queries themselves. Rules are formulated to guide the parallel query execution process. Our analysis shows that when there is no load skew, the serial scheduling strategy is preferred, otherwise the parallel scheduling strategy should be used.
41|4|http://www.sciencedirect.com/science/journal/09505849/41/4|Export database derivation in object-oriented wrappers|
41|4||The use of fuzzy cognitive maps to simulate the information systems strategic planning process|In the early 1980s articles began to focus on Strategic Planning of Information Systems (SISP) and to argue the critical importance of Information Technology (IT) in today’s organisations. Since then, a large number of models were presented in order to analyse IT from a strategic point of view and suggest new IT projects. However, researchers urge for alternative approaches to SISP, as current ones fall short in taking into consideration both the business and IT perspectives as well as they fail to tackle the complexity of the domain and suggest specific IS opportunities. This article suggests Fuzzy Cognitive Maps (FCM) as an alternative modelling approach and describes how they can be developed and used to simulate the SISP process. FCMs were successfully developed and used in several ill-structured domains, such as decision making, policy making. The proposed FCM contains 165 variables and 210 relationships from both business and IT domains. The strength of this approach lies in its capability not only to comprehensively model qualitative knowledge which dominates strategic decision making, but also to simulate and evaluate several alternative ways of using IT in order to improve organisational performance. This approach introduces computational modelling, as well as it supports scenarios development and simulation in the SISP domain.
41|4||ViewFinder: an object browser|
41|4||Object model in Java: elements and application|The Java programming language is a new object-oriented programming language that is gaining widespread popularity in the computer software industry because of its ease of learning, simplicity, generality, portability and networking capabilities. In this article, we discuss the semantic implementation in Java of each of the elements of the object model, identified by Booch [G. Booch, Object-Oriented Analysis and Design with Applications, 2nd Edition, Addison-Wesley, 1994], in addition to the mobility element. We show that Java, unlike other object-oriented languages, covers and simplifies all elements of the object model. An example illustrating those elements in Java will be drawn from a telecom software application.
41|4||From Chaos to Classes by Daniel Duffy, McGraw Hill, 1995, ISBN: 0-07-709118-3, 365pp, price: Â£29.95|
41|5|http://www.sciencedirect.com/science/journal/09505849/41/5|Multi-stage negotiation support: a conceptual framework|This article presents a multi-stage model for bilateral negotiation support. It continues from the work of Lim and Benbasat [L.H. Lim, I. Benbasat, A theoretical perspective of negotiation support systems, Journal of Management Information Systems 19(3) (1992) 27-44], in which a two-dimensional approach to negotiation support is advocated. Based on that approach, the current work identifies generic types of DSS and communication tools necessary to support the requirements and strategic analyses. An actual system built based on this conceptualisation, is then described.
41|5||Superimposition: a component adaptation technique|Several authors have identified that the only feasible way to increase productivity in software construction is to reuse existing software. To achieve this, component-based software development is one of the more promising approaches. However, traditional research in component-oriented programming often assumes that components are reused “as-is”. Practitioners have found that “as-is” reuse seldom occurs and that reusable components generally need to be adapted to match the system requirements. Existing component object models provide only limited support for component adaptation, i.e. white-box techniques such as copy–paste and inheritance, and black-box approaches such as aggregation and wrapping. These techniques suffer from problems related to reusability, efficiency, implementation overhead or the self problem. To address these problems, this article proposes superimposition, a novel black-box adaptation technique that allows one to impose predefined, but configurable types of functionality on a reusable component. Three categories of typical adaptation types are discussed, related to the component interface, component composition and component monitoring. Superimposition and the types of component adaptation are exemplified by several examples.
41|5||Transforming RDB schema into well-structured OODB schema|When transforming relational database (RDB) schema into object-oriented database(OODB) schema, much effort was put on examining key and inclusion dependency (ID) constraints to identify class and establish inheritance and association between classes. However, in order to further remove the original data redundancy and update anomaly, multi-valued dependency (MVD) should also be examined. In this paper, we discuss class structures and define well-structured classes. Based on MVDs, a theorem is given transforming a relation schema into a well-structured class. To transform RDB schema into OODB schema, a composition process simplifying the input RDB schema and an algorithm transforming the simplified RDB schema into well-structured OODB classes are developed.
41|5||Processing temporal queries in the context of object-oriented databases|
41|5||Empirical evaluation of reuse sensitiveness of complexity metrics|Measuring software products and processes is essential for improving software productivity and quality. In order to evaluate the complexity of object-oriented software, several complexity metrics have been proposed. Among them, Chidamber and Kemerer’s metrics are the most well-known for object-oriented software. Their metrics evaluate the complexity of the classes in terms of internal, inheritance, and coupling complexity. Though the reused classes of the class library usually have better quality than the newly-developed ones, their metrics deal with inheritance and coupling complexity in the same way. This article first proposes a revision of the Chidamber and Kemerer’s metrics which can be applied to software which had been constructed by reusing software components. Then, we give an analysis of data collected from the development of an object-oriented software using a GUI framework. We compare the original metrics with the revised ones by evaluating the accuracy of estimating the effort to fix faults and show the validity and usefulness of the revised metrics.
41|6|http://www.sciencedirect.com/science/journal/09505849/41/6|Groupware and social networks: will life ever be the same again?|Over the past twenty years industry and academia have been working to develop computer systems to increase work group's productivity, commonly referred to as groupware. Groupware encompasses a broad spectrum of research and development including group support systems, computer-supported collaborative work, group decision support systems, and computer mediated collaboration. Applications arising out of these efforts included concurrent multi-user authoring systems, computer conferencing, integrated computer/video meeting systems, electronic voting, brainstorming, and workflow systems. The papers in this special issue are some of the best from over 100 papers submitted to the GROUP'97 conference sponsored by the ACM Special Interest Group on Supporting Group Work. They represent work conducted by researchers on four continents from both industry and academia. As a group the authors present a blend of theory, practice, and technological innovation from the groupware research arena. This paper is intended to serve as an introduction to the area of groupware research and development. In it we explore the evolution of groupware and expose some of its effects on organizations and society.
41|6||Of maps and scripts: The status of formal constructs in cooperative work|The received understanding of the status of formal organizational constructs in cooperative work is problematic. This paper shows that the empirical evidence is not as strong as we may have believed and that there is evidence from other studies that contradicts what we may have taken for granted for years. This indicates that the role of formal constructs is more differentiated than generally taken for granted. They not only serve as `maps' but also as `scripts'.
41|6||Requirements for a virtual collocation environment|We analyze how physically collocated teams work together now and what services they require to work together across distances, focusing on real time interactions because those interactions justify collocating teams today. We explain how Integrated Product Teams (IPTs) are organized in system development programs and how their physical collocation facilitates communication, collaboration, and coordination within the team. Interactions within IPTs take two forms: scheduled meetings and opportunistic interactions. Scenarios of scheduled IPT meetings help motivate and identify requirements for supporting distributed meetings. Opportunistic interactions are far more common than scheduled meetings and more difficult to observe and analyze because they are not scheduled or predictable.
41|6||TelCoW: telework under the co-ordination of a workflow management system|Telework is considered as an innovative work organization form for new decentralized structures. The flexibility of telework in both time and location of task execution must make it possible to take advantage of this work organization for company competitiveness. Today, telework is mainly used for isolated tasks and for those with only few cross-references to others. But telework will only achieve its full potential if attention is given to the fact that work in companies is normally co-operative work. For this co-operative work, business process modeling and workflow management is accepted as a supporting methodology. On the one hand this is important for enabling a production company to distribute work to employees who operate within this organizational model, and at the same time, compared with conventional models of organizing work, to increase the productivity of task execution. The increasing effort for co-ordination and monitoring is seen to be the main obstacle of co-operative telework. The use of existing workflow management systems for the support of co-operative telework is currently not possible; there is no tool available with specific co-ordination and planning functionality for this purpose. In this paper, we will define a specific business process model which is oriented for the modeling of decentralized structures especially for telework and the direct support by a workflow management system (WFMS). Compared to traditional WFMSs, our system is extended by a module for the planning and monitoring functionality required for monitoring of teleworkers and their time management. The distribution of work is supported by means of a co-ordinator as a constituent part of the WFMS. It executes workflows which are provided by a certain method for modeling business processes. This method already considers necessary refinements in passing over business processes to workflows. The models for business processes are supplied by means of a meta-model, which is extended by aspects of telematics. The system is built using Internet technology and uses platform-independent WWW software for the user interface.
41|6||Flexible support for business processes: extending cooperative hypermedia with process support|In this paper, we present a cooperative hypermedia-based process support system focusing on flexible business processes. An analysis of the communication, coordination and cooperation requirements of business processes reveals a gap in current computer support. We propose to address these requirements by extending a cooperative hypermedia system with process support. The resulting system, called CHIPS, uses hypermedia-based activity spaces to model the structural, relational, and computational semantics of both individual tasks and processes. Application examples demonstrate that the CHIPS system retains the intuitive usability of hypertext and can support a wide range of business processes.
41|6||Toward locales: Supporting collaboration with Orbit|The WORLDS project at DSTC, the Cooperative Research Centre for Distributed Systems Technology, is experimenting with computer-based collaboration support. Our goal is to provide support for the workaday activities of distributed groups. To facilitate this we are developing, in parallel, a theory of collaborative activity (the locales framework) and a series of prototypes to test this theory. In this paper we briefly overview the theory and then describe the evolution of the three versions of our Orbit prototype. The prototypes focus on providing a range of facilities, including shared distributed objects, multiple personalizable views, user-controllable presence and awareness of user activities, and the ability to participate in multiple activities simultaneously, with varying degrees of intensity.
41|6||A company-office system âValentineâ providing informal communication and personal space based on 3D virtual space and avatars|
41|7|http://www.sciencedirect.com/science/journal/09505849/41/7|Disincentives for communicating risk: a risk paradox|Problems with the risk management in medium to large software projects have been well documented. For major software projects to be completed successfully, an open and cooperative attitude towards risk must be maintained. Despite this significant incentive, project stakeholders frequently conceal risks. This article identifies reasons in three key areas for such behaviour, and suggests approaches that reduce the motivation for this behaviour thereby providing a basis for effective risk management. Examples drawn from a study, undertaken by the authors, of a medium-sized industry project are used to illustrate many of these issues.
41|7||An improved method for the indexing of software|Many organizations are implementing free-text indexing schemes in order to build software catalogs with the aim of promoting systematic code reuse. Unfortunately, comments embedded in software systems suffer from several shortcomings, therefore it is not reasonable to pretend that the quality of the indices that can be extracted from them must be high. In the present empirical work, we implemented one such methods with the purpose of showing what could be expected when they are applied to the comments. The method we referred to uses pairs of words (called lexical affinities) as indexing units. The authors of such a method have given numerical indications (by carrying out a limited number of experiments on text-files about Unix commands) that lexical affinities provide better results than single-word schemes traditionally adopted in information retrieval. Our findings, arrived at by experimenting with such an indexing scheme over the comments of a large collection of commercial routines, account for our pessimism: only in 1.9% of the texts processed, the extracted indices are semantically representative of the purpose of the routines the comments were embedded in. A general strategy suitable to get better results is proposed in the second part of the article and evaluated against the same collection of routines.
41|7||Measurement and prediction of the verification cost of the design in a formalized methodology|This article presents a new way to enhance the prediction of software attributes in the first stages of software life cycle through the study of the relationships among the elements of models used in different development phases. A system to predict the cost of verifying the design derived from data flow diagrams (DFD) was defined using this approach in structured analysis and merise (SAM) a formalized methodology. The construction and validation of measures for featuring design attributes is based on a solid theoretical analysis of the properties that should be found in the measured object. Prediction validity is achieved as a logical consequence of development relationships between and among the involved models. Moreover, some empirical data is included to enable observation of the application of the system. The article concludes that this approach can also lead to prediction enhancement through process improvement in other methodologies like SSADM or the object-oriented (OO) ones.
41|7||Integrating structured OO approaches with formal techniques for the development of real-time systems|
41|7||Introducing software architecture specification and analysis in SAM through an example|Software architecture study has become one of the most active research areas in software engineering in the recent years. Although there have been many published results on specification and analysis method of software architectures, information on sound systematic methodology for modeling and analyzing software architectures is lacking. In this article, we present a formal systematic software architecture specification and analysis methodology called SAM and show how to apply SAM to specify a command control (C2) system and to analyze its real-time constraints.
41|8|http://www.sciencedirect.com/science/journal/09505849/41/8|Managing information system development in bureaucracies|This article examines the efficiency and effectiveness of a prescriptive systems development methodology in practice. The UK Government's mandatory Structured Systems Analysis and Design Method (SSADM) was examined to determine its value to software projects. The evidence was collected from interviews with 17 project managers, discussions with participants on three large SSADM projects and from observing 90 end users during training. The conclusions are that prescriptive information systems methodologies are unlikely to cope well with strategic uncertainty, user communication or staff development. The recommendations are to focus more on soft organisational issues and to use approaches tailored to each project.
41|8||Extending a deductive object-oriented database system with spatial data handling facilities|
41|8||Submodule construction from concurrent system specifications|
41|8||Process evolution support in concurrent software process language environment|
41|8||An extended educational system for programming and its evaluation|
41|9|http://www.sciencedirect.com/science/journal/09505849/41/9|Editorial|
41|9||Data-mining massive time series astronomical data: challenges, problems and solutions|
41|9||Interactive exploration of interesting findings in the Telecommunication Network Alarm Sequence Analyzer (TASA)|
41|9||Efficient knowledge discovery through the integration of heterogeneous data|
41|9||Evaluating data mining procedures: techniques for generating artificial data sets|
41|9||The management and mining of multiple predictive models using the predictive modeling markup language|We introduce a markup language based upon XML for working with the predictive models produced by data mining systems. The language is called the predictive model markup language (PMML) and can be used to define predictive models and ensembles of predictive models. It provides a flexible mechanism for defining schema for predictive models and supports model selection and model averaging, involving multiple predictive models. It has proved useful for applications requiring ensemble learning, partitioned learning and distributed learning. In addition, it facilitates moving predictive models across applications and systems.
41|9||Modification of belief in evidential causal networks|
41|9||Interfacing knowledge discovery algorithms to large database management systems|
42|1|http://www.sciencedirect.com/science/journal/09505849/42/1|SoftPM: a software process management system reconciling formalism with easiness|Various formal approaches to process modeling and analysis have been proposed. With the emerging importance of practicality in this field, easiness in adopting formal technology should be taken into account. In this paper, we propose a PSEE called SoftPM that is based on a high level Petri net formalism for process modeling. To overcome the difficulty in using this formalism, SoftPM exploits a multi-level modeling mechanism for the representation of software processes. SoftPM supports three different levels for process representation. They are cognitive, MAM net, and Pr/T net. Each level has different roles in SoftPM. The cognitive-level modeling provides the means of getting free from difficulties in manipulating the modeling formalism. MAM net takes the role of core modeling formalism in SoftPM. Finally, Pr/T nets support the low-level modeling formalism as an existing Petri-net class. Moreover, SoftPM offers various analysis techniques to aid managerial decision making, as well as conventional Petri-net analysis techniques. Using a Java-based thin client/server architecture, SoftPM smoothly supports execution at distributed heterogeneous platforms over the Internet.
42|1||An optimal representative set selection method|
42|1||Software process and product improvement: an empirical assessment|Despite all the attention that software process improvement (SPI) practices have received, there is no solid evidence of how extensively they are used across organizations, and their impact on quality, cost, and on-time delivery. The findings of previous studies are based on case studies, often assessing the effectiveness of a particular methodology in a large company. In our attempt to obtain a broader insight into the software process improvement practices, we conducted a survey targeted at software developers in New England. We collected 67 responses and used descriptive statistics to analyze the survey results. In addition, we examined the impact of SPI methodologies on quality factors and compared the impact to the importance of quality factors for software developers. The Spearman correlation coefficient was used to determine the degree of correlation between the two.
42|1||Distance-based software measurement: necessary and sufficient properties for software measures|Axiomatic approaches to software measurement present sets of necessary, but not sufficient measure axioms. The insufficiency of the measure axioms implies that they are useful to invalidate existing software measures, but not to validate them. In this paper, a set of measure axioms is presented whose sufficiency is guaranteed by measurement theory. The axioms referred to are the metric axioms, used in mathematics to define measures of distance. We present a constructive procedure that defines software measures satisfying these axioms. As an illustration of distance-based software measurement, a measure is defined for the aggregation coupling of object classes.
42|1||An empirical study of a software reuse reference model|In software engineering there is a need for technologies that will significantly decrease effort in developing software products, increase quality of software products and decrease time-to-markets. The software development industry can be improved by utilizing and managing software reuse with an “empirically validated reference model” that can be customized for different kinds of software development enterprises. Our research thesis is that software development based on a software reuse reference model improves the competitive edge and time-to-market of many software development enterprises. The definition and study of such a model has been carried out using four steps. First, the reference model developed here is based on the existing software reuse concepts. Second, this reference model is an empirical study which uses both legacy studies and lessons learned studies. Third, the impact of the reference model on software development effort, quality, and time-to-market is empirically derived. Fourth, an initial set of successful cases, which are based on the software reuse reference model utilization, are identified. The main contribution of this paper is a reference model for the practice of software reuse. A secondary contribution is an initial set of cases from software development enterprises which are successful in the practice of reuse in terms of decreased effort, increased quality and a high correlation in their application of our software reuse reference model activities.
42|10|http://www.sciencedirect.com/science/journal/09505849/42/10|Principles for modeling language design|Modeling languages, like programming languages, need to be designed if they are to be practical, usable, accepted, and of lasting value. We present principles for the design of modeling languages. To arrive at these principles, we consider the intended use of modeling languages. We conject that the principles are applicable to the development of new modeling languages, and for improving the design of existing modeling languages that have evolved, perhaps through a process of unification. The principles are illustrated and explained by several examples, drawing on object-oriented and mathematical modeling languages.
42|10||ROCS: an object-oriented class-level testing system based on the Relevant Observable ContextS technique|Given an algebraic specification of a class of objects, we define a fundamental pair as two equivalent terms generated by substituting all the variables on both sides of an axiom by normal forms. For any implementation error in the class, if it can be revealed by two equivalent terms in general, it can also be revealed by a fundamental pair. Hence, we need only select test cases from the set of fundamental pairs instead of equivalent pairs in general. We highlight an algorithm for selecting a finite set of fundamental pairs as test cases. Further, by using the relevant observable contexts technique, we highlight another algorithm to determine whether the objects resulting from executing a fundamental pair are observationally equivalent. If not, it reveals an implementation error.
42|10||A study of development and maintenance in Norway: assessing the efficiency of information systems support using functional maintenance|
42|10||Neuro-genetic prediction of software development effort|Prediction of resource requirements of a software project is crucial for the timely delivery of quality-assured software within a reasonable timeframe. Many conventional (model-based) and AI-oriented (model-free) resource estimators have been proposed in the recent past. This paper presents a novel genetically trained neural network (NN) predictor trained on historical data. We demonstrate substantial improvement in prediction accuracy by the neuro-genetic approach as compared to both a regression-tree-based conventional approach, as well as backpropagation-trained NN approach reported recently. The superiority of this new predictor is established using n-fold cross validation and Student's t-test on various partitions of merged Cocomo and Kemerer data sets incorporating data from 78 real-life software projects.
42|10||Assessing organisational obstacles to component-based development: a case study approach|
42|11|http://www.sciencedirect.com/science/journal/09505849/42/11|Using genetic algorithms to work out index configurations for the class-hierarchy indexing in object databases|
42|11||Proposing an information flow analysis model to measure memory load for software development|System analysis is used to develop application software. However, the conventional approach has difficulty to ensure that the software has high user-satisfaction. This research uses the user's memory load to measure the degree of satisfaction of application software. An information flow analysis model is proposed for analyzing the degree of memory load. Ten types of operations, including six internal operations and four external operations, are adopted to measure the memory load. The conventional data flow diagram (DFD) and state transition diagram (STD) are modified by adding more functional components to estimate the memory load. The information flow analysis is demonstrated by building a qualitative analysis software which requests heavier mental works but has simple software architecture.
42|11||The financial profile of the software industry between 1980 and 1994|The objective of this work is to trace the financial profile of firms in the software industry between the years 1980–1994 using data from COMPUSTAT. Our results are useful both to academics, who strive to link theory to empirical observation, and to practitioners trying to better understand the environment in which they operate. Our analysis suggests that (1) the software industry has been steadily expanding over the sample period, (2) the relative market power of the industry leaders has remained fairly stable, (3) the median firm operating in the industry has became smaller through time, (4) firms have been spending increasingly more on R&D and less in capital investment through time, (5) profitability was declining over the first half of the sample period, stabilizing in the second half, and (6) the risk of bankruptcy for the median firm has similarly declined over the sample period.
42|11||Dynamic data flow analysis for Java programs|A large portion of high-level computer programs consists of data declaration. Thus, an increased focus on testing the data flow aspects of programs should be considered. In this paper, we consider testing the data flow in Java programs dynamically. Data flow analysis has been applied for testing procedural and some object-oriented programs. We have extended the dynamic data flow analysis technique to test Java programs and show how it can be applied to detect data flow anomalies.
42|11||Query language approach based on the deductive object-oriented database paradigm|The integration of data-oriented (structural), behavioral and deductive aspects is necessary in next generation information systems. The deductive object-oriented database paradigm offers a very promising starting point for the implementation of these kinds of information systems. So far in the context of this paradigm a big problem has been the lack of a query language suitable to an ordinary end user. Typically in existing proposals for deductive object-oriented databases the user has to master well both logic-based rule formulation and object-oriented programming. In this paper we introduce a set of high-level querying primitives which increases the degree of declarativeness compared to the deductive object-oriented query languages proposed so far. In terms of these primitives it is possible to offer for end users such application-specific concepts and structures whose interpretation is obvious to users but whose specification is too demanding for them. By combining these primitives in queries the user can integrate data-oriented, behavioral and deductive aspects with each other in a concept-oriented way. Our query language approach is based on the incorporation of deductive aspects to object-orientation. Among others this means that deductive aspects of objects are inherited in the specialization/generalization hierarchy like any other properties of objects.
42|12|http://www.sciencedirect.com/science/journal/09505849/42/12|Introduction to the special issue on: model-based statistical testing of software intensive systems|
42|12||Improving software quality using statistical testing techniques|
42|12||Improving a product with usage-based testing|This paper presents the introduction and application of a usage-based testing approach for the acceptance of an aircraft traffic flow management system by the FAA. The FAA needed an approach to provide more effective testing than that typically achieved in their requirements-based acceptance testing. Testing in a manner more consistent with operational usage was desired. A testing technology that was new to the organization was introduced and applied successfully. The approach resulted in a greatly improved product being released to the field, a cost-effective approach for testing, and a commitment to apply the technology across the organization.
42|12||Applying models in your testing process|Model-based testing allows large numbers of test cases to be generated from a description of the behavior of the system under test. Given the same description and test runner, many types of scenarios can be exercised and large areas of the application under test can be covered, thus leading to a more effective and more efficient testing process.
42|12||A constraint-based approach to the representation of software usage models|
42|12||TML: a description language for Markov chain usage models|Finite-state Markov chains have proven useful as a model to characterize a population of uses of a software system. This paper presents a language (TML) for describing these models in a manner that supports development, reuse, and automated testing. TML provides a simple representation of usage models, while formalizing modeling techniques already in use informally.
42|12||Partition testing with usage models|
42|12||Stopping criteria for statistical testing|
42|12||Measuring complexity and coverage of software specifications|
42|12||Representation and optimization of software usage models with non-Markovian state transitions|We extend Markov usage models by admitting non-Markovian transitions between states. The suggested description syntax preserves the visualization capacities of Markov models and, although it is kept simple, provides the computational power of a universal programming language. It is shown that a previously introduced optimization technique for computing optimal test transition probabilities can be generalized to the presented framework. A medium-size example from a real-world application illustrates the approach.
42|12||A Markov chain model for predicting the reliability of multi-build software|In previous work we developed a method to model software testing data, including both failure events and correct behavior, as a finite-state, discrete-parameter, recurrent Markov chain. We then showed how direct computation on the Markov chain could yield various reliability related test measures. Use of the Markov chain allows us to avoid common assumptions about failure rate distributions and allows both the operational profile and test coverage of behavior to be explicitly and automatically incorporated into reliability computation.
42|13|http://www.sciencedirect.com/science/journal/09505849/42/13|Identifying relevant constraints for semantic query optimization|Semantic query optimization is the process of utilizing information implied by integrity constraints to reformulate the query into one that generates the same set of answers in a more efficient way. The difficulties of identifying relevant integrity constraints for a given query have been well recognized as have the various solutions. However, most of the previous works consider the query consisting of join(s) of base relations and the integrity constraints on base relations only. We generalize these restrictions and propose a method of identifying relevant integrity constraints for queries involving any combinations of joins and unions of base and defined relations. Our method utilizes a query graph that can be constructed dynamically during the query processing time, and, as a consequence, does not rely on heavy preprocessing or normalization. The method is extended to include the use of heuristics for generating a subset of answers.
42|13||Performance comparison of CORBA and RMI|Distributed object architectures and Java are important for building modern, scalable, web-enabled applications. This paper is focused on qualitative and quantitative comparison of two distributed object models for use with Java: CORBA and RMI. We compare both models in terms of features, ease of development and performance. We present performance results based on real world scenarios that include single client and multi-client configurations, different data types and data sizes. We evaluate multithreading strategies and analyse code in order to identify the most time-consuming methods. We compare the results and give hints and conclusions. We have found that because of its complexity CORBA is slightly slower than RMI in simple scenarios. On the other hand, CORBA handles multiple simultaneous clients and larger data amounts better and suffers from far lower performance degradation under heavy client load. The article presents a solid basis for making a decision about the underlying distributed object model.
42|13||Analyzing dependence locality for efficient construction of program dependence graph|
42|13||A survey of lead-time challenges in the development and evolution of distributed real-time systems|This paper presents a survey that identifies lead-time consumption in the development and evolution of distributed real-time systems DRTSs. Data has been collected through questionnaires, focused interviews and non-directive interviews with senior designers. Quantitative data has been analyzed using the Analytic Hierarchical Process (AHP). A trend in the 11 organizations is that there is a statistically significant shift of the main lead-time burden from programming to integration and testing, when distributing systems. From this, it is concluded that processes, tools and technologies that either reduce the need for or the time for testing have an impact on the development and evolution of lead-time of DRTSs.
42|14|http://www.sciencedirect.com/science/journal/09505849/42/14|Software project control and metrics|
42|14||From process improvement to people improvement: enabling learning in software development|The importance of people factors for the success of software development is commonly accepted, because the success of a software project is above all determined by having the right people on the right place at the right time. As software development is a knowledge intensive industry; the ‘quality’ of developers is primarily determined by their knowledge and skills. This paper presents a conceptual model of nine ‘learning enablers’ to facilitate learning in software projects. These enablers help identifying whether individual and/or organisational learning is facilitated. The main question addressed in this paper is: ‘Which factors enable learning in software projects and to what extent?’
42|14||Design of a product-focused customer-oriented process|In an increasingly dynamic world where both needs and technologies are changing rapidly, especially in applications such as e-commerce systems, there is a requirement for whole-life and measurable customer-oriented processes (CoPs) for systems and software development. In the pursuit of delivering ‘fit for purpose’ systems, the design of a CoP requires a holistic approach to understanding, engineering, managing and evolving the system and software needs and requirements. This paper summarises the CoP concepts and requirements, describes some evaluation results and proposes the way ahead.
42|14||Subjective evaluation as a tool for learning from software project success|This paper presents a method for using subjective factors to evaluate project success. The method is based on collection of subjective measures with respect to project characteristics and project success indicators. The paper introduces a new classification scheme for assessing software projects. Further, it is illustrated how the method may be used to predict software success using subjective measures of project characteristics. The classification scheme is illustrated in two case studies. The results are positive and encouraging for future development of the approach.
42|14||Integration of functional, cognitive and quality requirements. A railways case study|The paper shows a SHEL oriented requirements engineering approach, which has been applied in a case study dealing with the definition of the requirements for a new railways traffic control system. The SHEL model provides an integrated view by considering any productive process or activity performed by a combination of Hardware, Software and Liveware resources within a specific Environment. A set of SHEL oriented requirements describes the different views in a complex system. The requirements are grouped into three main classes, namely, functional, cognitive and quality requirements. The paper points out the issue of integrating different types of requirements.
42|14||Using simulation to analyse the impact of software requirement volatility on project performance|During the last decade, software process simulation has been used to address a variety of management issues and questions. These include: understanding; training and learning; planning; control and operational management; strategic management; process improvement and technology adoption.
42|14||A comparative study of two software development cost modeling techniques using multi-organizational and company-specific data|This research examined the use of the International Software Benchmarking Standards Group (ISBSG) repository for estimating effort for software projects in an organization not involved in ISBSG. The study investigates two questions: (1) What are the differences in accuracy between ordinary least-squares (OLS) regression and Analogy-based estimation? (2) Is there a difference in accuracy between estimates derived from the multi-company ISBSG data and estimates derived from company-specific data? Regarding the first question, we found that OLS regression performed as well as Analogy-based estimation when using company-specific data for model building. Using multi-company data the OLS regression model provided significantly more accurate results than Analogy-based predictions. Addressing the second question, we found in general that models based on the company-specific data resulted in significantly more accurate estimates.
42|15|http://www.sciencedirect.com/science/journal/09505849/42/15|QoS-based evaluation of file systems and distributed system services for continuous media provisioning|This paper presents a QoS-based performance analysis of file systems and distributed object services for Continuous Media (CM) provisioning, as well as the details and implementation experiences of a continuous media file system and associated CM servers. For this we have implemented QoS-driven CM servers and the Presto continuous media file system (PFS) in the context of a distributed multimedia application development environment, and validate the performance of PFS against that of the conventional Unix file system through an experimental evaluation. Using our CM server prototype, we next examine the effect of continuous media data delivery on the three different kinds of network protocols such as CORBA, UDP/IP, and TCP/IP, with respect to QoS provisioning and throughput.
42|15||Strategies and visualization tools for enhancing user auditing of spreadsheet models|Spreadsheets are very widely used at various levels of the organization. Studies have shown that errors do occur frequently during the development of spreadsheet models. Empirical studies of operational spreadsheet models show that a large percentage of them contain errors. However, the identification of errors is difficult and tedious, and errors have led to drastically wrong decisions. It is thus important to develop new strategies and auditing tools for detecting errors. A suite of new auditing visualization tools have been designed and implemented in Visual Basic for Application (VBA), as an add-in module for easy inclusion in any Excel 97 or Excel 2000 installation. Furthermore, four strategies are proposed for detecting errors. These range from an overview strategy to identify logical components of the spreadsheet model, to specific strategies targeted at specific types of error. Illustrations show how these strategies can be supported with the new visualization tools.
42|15||The development of OCPL, object conceptual prototyping language|This paper describes the development of OCPL (object conceptual prototyping language), an object–knowledge representation language. The language is based on CPL, conceptual prototyping language, developed at the Free University of Amsterdam. CPL has been extended to allow for the explicit representation of object-oriented constructs. These constructs include facilities for application system definition, generation and usage. A restricted use of the constraint model of CPL allows for systematic representation of events from which appropriate user interfaces can be generated. The paper describes OCPL and its relationship to CPL and related work. It also illustrates how the constraint model can be used to represent dynamics and provide intelligent user support.
42|15||Enhanced ER to relational mapping and interrelational normalization|This paper develops a method that maps an enhanced Entity-Relationship (ER+) schema into a relational schema and normalizes the latter into an inclusion normal form (IN-NF). Unlike classical normalization that concerns individual relations only, IN-NF takes interrelational redundancies into account and characterizes a relational database schema as a whole. The paper formalizes the sources of such interrelational redundancies in ER+ schemas and specifies the method to detect them. Also, we describe briefly a Prolog implementation of the method, developed in the context of a Computed-Aided Software Engineering shell and present a case study.
42|15||Index|
42|15||Index|
42|2|http://www.sciencedirect.com/science/journal/09505849/42/2|Special issue on constructing software engineering tools|
42|2||Issues in software engineering tool construction|A brief introduction to software engineering tools is presented, and issues involved in the construction of these tools are discussed. Some of the current issues concerning tool developers are highlighted, which include: metaCASE technology, cognitive support, evaluation and validation of tools and data interchange. Some recent developments in tool construction techniques are examined, and opportunities for further research and development in tool building are identified.
42|2||Cognitive support, UML adherence, and XMI interchange in Argo/UML|Software design is a cognitively challenging task. Most software design tools provide support for editing, viewing, storing, and transforming designs, but lack support for the essential and difficult cognitive tasks facing designers. These cognitive tasks include decision-making, decision ordering, and task-specific design understanding.
42|2||Connecting architecture reconstruction frameworks|A number of standalone tools are designed to help developers understand software systems. These tools operate at different levels of abstraction, from low level source code to software architectures. Although recent proposals have suggested how code-level frameworks can share information, little attention has been given to the problem of connecting software architecture level frameworks. In this paper, we describe the TA Exchange Format (TAXForm) exchange format for frameworks at the software architecture level. By defining mappings between TAXForm and formats that are used within existing frameworks, we show how TAXForm can be used as a “binding glue” to achieve interoperability between these frameworks without having to modify their internal structure.
42|2||Constructing component-based software engineering environments: issues and experiences|Developing software engineering tools is a difficult task, and the environments in which these tools are deployed continually evolve as software developers’ processes, tools and tool sets evolve. To more effectively develop such evolvable environments, we have been using component-based approaches to build and integrate a range of software development tools, including CASE and workflow tools, file servers and versioning systems, and a variety of reusable software agents. We describe the rationale for a component-based approach to developing such tools, the architecture and support tools we have used some resultant tools and tool facilities we have developed, and summarise the possible future research directions in this area.
42|2||MetaMOOSEâan object-oriented framework for the construction of CASE tools|
42|2||Generating tools from graph-based specifications|
42|2||A multi-layer multi-view architecture for software engineering environments|This paper presents our experience with constructing a multi-view environment for software process modeling. The environment (Spearmint) is designed to support the capture, analysis and maintenance of large, complex software process models. The environment uses multiple views to handle the inherent complexity of real software processes and to model the fact that different people within organizations have different, sometimes conflicting, views of the same process. Spearmint also supports multiple display representations for process information and addresses requirements for good maintainability, extensibility and performance. Our experience has been that a layered architecture that makes a clear separation of concerns in the application is invaluable for implementing such a multi-view tool. In this paper, we describe some of the experiences we have had with designing and implementing such an architecture.
42|3|http://www.sciencedirect.com/science/journal/09505849/42/3|The formal specification of ORN semantics|Object Relationship Notation (ORN) is a declarative scheme that permits a variety of common types of relationships to be conveniently defined to a Database Management System (DBMS), thereby allowing the DBMS to automatically enforce their semantics. Though first proposed for object DBMSs, ORN is applicable to any data model that represents binary entity-relationships or to any DBMS that implements them. In this paper, we first describe ORN semantics informally as has been done in previous papers. We then provide a formal specification of these semantics using the Z-notation. Specifying ORN semantics via formal methods gives ORN a solid mathematical foundation. The semantics are defined in the context of an abstract database of sets and relations in a recursive manner that is precise, unambiguous, and noncircular.
42|3||Designing a distributed database on a local area network: a methodology and decision support system|Local area networks (LANs) are important for an enterprise to hold a competitive edge. Many companies have therefore converted terminal-based computing systems to LAN-based distributed data processing systems. This paper proposes a design methodology for distributed databases connected by a LAN. Two primary objectives of the methodology are: (i) to allocate data files and workload among heterogeneous servers; and (ii) to determine the number of servers to satisfy the response time required for processing each transaction. The file and workload allocation decision is formulated as a nonlinear zero–one integer programming problem. This problem is proven to be NP-complete. A heuristic is developed to solve this problem effectively. A decision support system is implemented and an example is solved to illustrate the practical usefulness of the system.
42|3||An effective recovery under fuzzy checkpointing in main memory databases|
42|3||Translating update operations from relational to object-oriented databases|In migrating a legacy relational database system to the object-oriented (OO) platform, when database migration completes, application modules are to be migrated, where embedded relational database operations are mapped into their OO correspondents. In this paper we study mapping relational update operations to their OO equivalents, which include UPDATE1, INSERT and DELETE operations. Relational update operation translation from relational to OO faces the touchy problem of transformation from a value-based relationship model to a reference-based model and maintaining the relational integrity constraints. Moreover, with a relational database where inheritance is expressed as attribute value subset relationship, changing of some attribute values may lead to the change of the position of an object in the class inheritance hierarchy, which we call object migration. Considering all these aspects, algorithms are given mapping relational UPDATE, INSERT and DELETE operations to their OO correspondents. Our work emphasize in examining the differences in the representation of the source schema's semantics resulting from the translation process, as well as differences in the inherent semantics of the two models.
42|3||A business object-oriented environment for CCISs interoperability|
42|4|http://www.sciencedirect.com/science/journal/09505849/42/4|Path-based protocol verification approach|Protocol verification is one of the most challenging tasks in the design of protocols. Among the various proposed approaches, the one based on reachability analysis (or known as state enumeration approach) is of the most simple, automatic and effective. However, the state explosion problem is a principle obstacle toward successful and complete verifications of complex protocols. To overcome this problem, we proposed a new approach, the “path-based approach.” The idea is to divide a protocol into a collection of individual execution record, denoted as concurrent paths, a partial order representation recording the execution paths of individual entities. Then, the verification of the protocol is, thus, replaced by that of individual concurrent paths. Since concurrent paths can be automatically generated through Cartesian product of the execution paths of all modules, and verified independently, the memory requirement is limited to the complexity of individual concurrent path rather than the whole protocol. Thus, the state explosion problem is alleviated from such “divide and conquer” approach. Furthermore, we propose an algorithm, making the trade-off on memory requirement to generate the concurrent paths more efficiently; and utilize the technique of symmetric verification, parallel computing to improve the efficiency of verification. Eventually, our experience of verifying real protocols shows that our approach uses much less memory and time than reachability analysis.
42|4||Symbolic path-based protocol verification|
42|4||Implementation and modeling of two-phase locking concurrency controlâa performance study|Two-phase locking (2PL) is the concurrency control mechanism that is used in most commercial database systems. In 2PL, for a transaction to access a data item, it has to hold the appropriate lock (read or write) on the data item by issuing a lock request. While the way transactions set their lock requests and the way the requests are granted would certainly affect a system's performance, such aspects have not received much attention in the literature. In this paper, a general transaction-processing model is proposed. In this model, a transaction is comprised of a number of stages, and in each stage the transaction can request to lock one or more data items. Methods for granting transaction requests and scheduling policies for granting blocked transactions are also proposed. A comprehensive simulation model is developed from which the performance of 2PL with our proposals is evaluated. Results indicate that performance models in which transactions request locks on an item-by-item basis and use first-come-first-served (FCFS) scheduling in granting blocked transactions underestimate the performance of 2PL. The performance of 2PL can be greatly improved if locks are requested in stages as dictated by the application. A scheduling policy that uses global information and/or schedules blocked transactions dynamically shows a better performance than the default FCFS.
42|4||MOOV++: modular object-oriented VDM|This paper describes MOOV++, a methodology for formal object-oriented development. MOOV++ takes as its starting point an object-oriented specification, which is subsequently developed by means of an existing and well-established formal method, the Vienna Development Method (VDM). MOOV++ utilises the VDM module notation to represent a class; the paper demonstrates how all the important concepts, which together define Object-Orientation can be captured formally in VDM. It also provides a proof obligation by which the behaviour of an inherited class can be proved to be consistent with its base class.
42|4||Dynamic partitioning of complex process models|
42|5|http://www.sciencedirect.com/science/journal/09505849/42/5|Knowledge based information integration systems|Information integration systems provide facilities that support access to heterogeneous information sources in a way that isolates users from differences in the formats, locations and facilities of those sources. A number of systems have been proposed that exploit knowledge based techniques to assist with information integration, but it is not always obvious how proposals differ from each other in their scope, in the quality of integration afforded, or in the cost of exploitation. This paper presents a framework for the comparison of proposals for information integration systems, and applies the framework to a range of representative proposals. It is shown that proposals differ greatly in all of the criteria stated and that the selection of an approach is thus highly dependent on the requirements of specific applications.
42|5||A decision-making pattern for guiding the enterprise knowledge development process|During enterprise knowledge development in any organisation, developers and stakeholders are faced with situations that require them to make decisions in order to reach their intentions. To help the decision-making process, guidance is required. Enterprise Knowledge Development (EKD) is a method offering a guided knowledge development process. The guidance provided by the EKD method is based on a decision-making pattern promoting a situation and intention oriented view of enterprise knowledge development processes. The pattern is iteratively repeated through the EKD process using different types of guiding knowledge. Consequently, the EKD process is systematically guided. The presentation of the decision-making pattern is the purpose of this paper.
42|5||Knowledge based evaluation of software systems: a case study|Solving software evaluation problems is a particularly difficult software engineering process and many contradictory criteria must be considered to reach a decision. Nowadays, the way that decision support techniques are applied suffers from a number of severe problems, such as naive interpretation of sophisticated methods and generation of counter-intuitive, and therefore most probably erroneous, results. In this paper we identify some common flaws in decision support for software evaluations. Subsequently, we discuss an integrated solution through which significant improvement may be achieved, based on the Multiple Criteria Decision Aid methodology and the exploitation of packaged software evaluation expertise in the form of an intelligent system. Both common mistakes and the way they are overcome are explained through a real world example.
42|5||LODâ: A C++ extension for OODBMSs with orthogonal persistence to class hierarchies|There exist some preprocessing based language extensions for database management where persistence is orthogonal to the class hierarchy. They allow a class hierarchy to be built from both database classes and non-database classes together. Such a property is important in that classes can be reused in implementing database classes, and vice versa. In this paper, we elaborate on the orthogonality of persistence to class-hierarchies, and find that the existing method to achieve this is not satisfactory because of the side-effects of the heterogeneosity of the links in a class hierarchy; some links represent subset(IsA) relationships between database classes, while the others denote inheritance for code-reuse. Finally, we propose ∗, a C++ extension to database access, which separates the different categories of links into independent hierarchies, and supports orthogonal persistence to the class hierarchy, overcoming the limitations in the previous methods.
42|5||Towards modeling the query processing relevant shape complexity of 2D polygonal spatial objects|The shape complexity of two-dimensional (2D) polygonal spatial objects has implications for how the object can be best represented in a spatial database, and for the query-processing performance of that object. Nevertheless few useful definitions of query-processing relevant spatial complexity are available. A query-processing oriented shape complexity measure is likely to be different from a fractal measure of shape complexity that focused on compression/decompression or a shape complexity measure that would be used for pattern recognition, and should give better performance for the analysis of query processing. It could be used to classify spatial objects, cluster spatial objects in multiprocessor database systems. In a recent paper Brinkhoff et al. (T. Brinkhoff, H-P. Kriegel, R. Shneider, A. Braun, Measuring the complexity of spatial objects, Proceedings of the 3rd ACM International Workshop on Advances in Geographic Information Systems, Baltimore, MD, 1995, pp. 109–117) demonstrated the usefulness of a spatial complexity measure. They did not, however, offer much theoretical justification for their choice of parameters nor for the functional form that they used. In this paper we present a conceptual framework for discussing the query processing oriented shape complexity measures for spatial objects. It is hoped that this will lead to the development of improved measures of spatial complexity.
42|6|http://www.sciencedirect.com/science/journal/09505849/42/6|An empirical study of object-oriented system evolution|Software metrics have been used to measure software artifacts statically—measurements are taken after the artifacts are created. In this study, three metrics—System Design Instability (SDI), Class Implementation Instability (CII), and System Implementation Instability (SII)—are used for the purpose of measuring object-oriented (OO) software evolution. The metrics are used to track the evolution of an OO system in an empirical study. We found that once an OO project starts, the metrics can give good indications of project progress, e.g. how mature the design and implementation is. This information can be used to adjust the project plan in real time.
42|6||OO-CASE tools: an evaluation of Rose|Object-oriented software development utilizes new design methodologies. These methodologies can be supported by computer-aided software engineering tools, such as Rational Rose. A survey of software developers identifies the demand for various features, and reveals strengths and need for improvements in Rational Rose. Overall, respondents indicated that Rational Rose provides strong support for OO design, but could additional support for teamwork, prototyping, and improvements in ease of use.
42|6||An extensible platform for the development of synchronous groupware|The development of groupware is a complex endeavor due to several inherent features not present in single-user applications. To address this complexity many authors have presented useful platforms that permit the reutilization of code to facilitate the implementation of groupware applications. However, design reusability could be of greater value than code reusability and facilitate the use and extension of groupware features. This paper describes COCHI (Collaborative Objects for Communication and Human Interaction), an extensible pattern system for groupware applications that aims to provide reusability and extensibility of design patterns represented as COCHI subsystems and implemented as a class framework.
42|6||A data abstraction approach for query relaxation|Since a query language is used as a handy tool to obtain information from a database, users want more user-friendly and fault-tolerant query interfaces. When a query search condition does not match with the underlying database, users would rather receive approximate answers than null information by relaxing the condition. They also prefer a less rigid querying structure, one which allows for vagueness in composing queries, and want the system to understand the intent behind a query. This paper presents a data abstraction approach to facilitate the development of such a fault-tolerant and intelligent query processing system. It specifically proposes a knowledge abstraction database that adopts a multilevel knowledge representation scheme called the knowledge abstraction hierarchy. Furthermore, the knowledge abstraction database extracts semantic data relationships from the underlying database and supports query relaxation using query generalization and specialization steps. Thus, it can broaden the search scope of original queries to retrieve neighborhood information and help users to pose conceptually abstract queries. Specifically, four types of vague queries are discussed, including approximate selection, approximate join, conceptual selection and conceptual join.
42|6||Requirements engineering: making the connection between the software developer and customer|Requirements engineering are one of the most crucial steps in software development process. Without a well-written requirements specification, developer's do not know what to build, user's do not know what to expect, and there is no way to validate that the created system actually meets the original needs of the user. Much of the emphasis in the recent attention for a software engineering discipline has centered on the formalization of software specifications and their flowdown to system design and verification. Undoubtedly, the incorporation of such sound, complete, and unambiguous traceability is vital to the success of any project. However, it has been our experience through years of work (on both sides) within the government and private sector military industrial establishment that many projects fail even before they reach the formal specification stage. That is because too often the developer does not truly understand or address the real requirements of the user and his environment.
42|6||Implementation of an efficient requirements-analysis supporting system using similarity measure techniques|As software becomes more complicated and larger, the software engineer's requirements-analysis becomes an important and uneasy activity. This paper proposes a requirements-analysis supporting system that supports informal requirements-analysis. The proposed system measures the similarity between requirement sentences to identify possible redundancies and inconsistencies, and extracts the possible ambiguous requirements. The similarity measurement method combines a sliding window model and a parser model. Using these methods, the proposed system supports to trace dependency between documents and improve quality of requirement sentences. Efficiency of the proposed system and a process for requirement specification analysis using the system are presented.
42|7|http://www.sciencedirect.com/science/journal/09505849/42/7|Multi-level transaction model for semantic concurrency control in linear hash structures|In this paper, we present a version of the linear hash structure algorithm to increase concurrency using multi-level transaction model. We exploit the semantics of the linear hash operations at each level of transaction nesting to allow more concurrency. We implement each linear hash operation by a sequence of operations at lower level of abstraction. Each linear hash operation at leaf-level is a combination of search and read/write operations. We consider locks at both vertex (page) and key level (tuple) to further increase concurrency. As undo-based recovery is not possible with multi-level transactions, we use compensation-based undo to achieve atomicity. We have implemented our model using object-oriented technology and multithreading paradigm. In our implementation, linear hash operations such as find, insert, delete, split, and merge are implemented as methods and correspond to multi-level transactions.
42|7||Statistical analysis of deviation of actual cost from estimated cost using actual project data|This paper analyzes an association of a deviation of the actual cost (measured by person-month) from the estimated cost with the quality and the productivity of software development projects. Although the obtained results themselves may not be new from the academic point of view, they could provide motivation for developers to join process improvement activities in a software company and thus become a driving force for promoting the process improvement.
42|7||Translating hierarchical predicate transition nets to CC++ programs|
42|7||Evaluation of Workflow-type software products: a case study|The main objective of this paper is to propose a set of indicators for the evaluation of Workflow software-type products within the context of Information Systems. This paper is mainly based on a comprehensive bibliographical review of all topics referring to the Workflow Technology and Information Systems. Next, sets of indicators are presented for the selection of a Workflow software based on the realities of the business world, including a method of examination so as to obtain an integral evaluation on the Workflow software. Finally, the evaluation method for two types of Workflow software is applied: Lotus Domino/Notes® and Microsoft Exchange®, for the billing subsystems of a company called MANAPRO Consultants, Inc.®.
42|7||A framework and test-suite for assessing approaches to resolving heterogeneity in distributed databases|
42|8|http://www.sciencedirect.com/science/journal/09505849/42/8|Semantic-based locking technique in object-oriented databases|
42|8||Bayesian probability distributions for assessing measurement of subjective software attributes|
42|8||An operational approach to the design of workflow systems|We construct models as an aid to our thought process. A particular class of models, operational models, can be used for simulation and prototyping. The OPJ modeling language is suitable for building operational models of complex software systems.
42|8||The evaluation of access costs in object databases|Unfortunately, there is at present nothing to assist the system architect at design-time to determine whether a proposed architecture based on an object-oriented database system will perform as required. The problem is complex, the choice of suitable modelling approach difficult, and a construction of a model is often abstruse. In this paper we concentrate on a major model component: that describing the access of objects in a centralised database. We present the background for the research (modern corporate IS architectures), the choices we have made, the prototype design, and the mathematical model of the cost of object database access. We conclude the paper by describing a validation of the model and how it can be generalised. The paper has a number of objectives: first, to dispel the myth that performance modelling of object-oriented systems is an immensely difficult task; second, to show that techniques which have been in existence for some time for modelling are applicable, with some modification, to aspects of object-oriented database performance prediction; and, third, to detail a specific case study of access cost modelling which provides enough information to be replicated by other workers across a number of object-oriented database products.
42|8||A methodology for transforming inheritance relationships in an object-oriented conceptual model to relational tables|With the increasing popularity of Object-Relational technology, it becomes necessary to have a methodology that allows database designers to exploit the great modeling power of object-oriented conceptual model, and still facilitate implementation on relational database systems. This paper presents a transformation methodology from inheritance relationships to relational tables. This includes transformation of different types of inheritance, such as union inheritance, mutual exclusion inheritance, partition inheritance and multiple inheritance. Performance comparison between the proposed transformation methodology and existing methods is also carried out. From the evaluation, we conclude that the proposed transformation methodology is more efficient than the others.
42|9|http://www.sciencedirect.com/science/journal/09505849/42/9|An adaptable constrained locking protocol for high data contention environments: correctness and performance|Multiversions of data are used in database systems to increase concurrency and to provide efficient recovery. Data versions improve the concurrency by allowing the concurrent execution of “non-conflicting” read-write lock requests on different versions of data in an arbitrary fashion. A transaction that accesses a data item version, which later diagnosed to lead to an incorrect execution, is aborted. This act is reminiscent of the validation phase in the optimistic concurrency control schemes. Various performance studies suggest that these schemes perform poorly in high data contention environments where the excessive transaction aborts result due to the failed validation. We propose an adaptable constrained two-version two-phase locking (C2V2PL) scheme in which these “non-conflicting” requests are allowed only in a constrained manner. C2V2PL scheme assumes that a lock request failing to satisfy the specific constraints will lead to an incorrect execution and hence, must be either rejected or blocked. This eliminates the need for a separate validation phase. When the contention for data among the concurrent transactions is high, the C2V2PL scheduler in aggressive state rejects such lock requests. The deadlock free nature of C2V2PL scheduler adapts to the low data contention environments by accepting the lock request that have failed the specific constraints but contrary to the assumption, will not lead to an incorrect execution. Thus, C2V2PL scheme improves the potential concurrency due to reduced transaction aborts in this conservative state. We have compared performance of our scheme with other lock-based concurrency control schemes such as two phase locking, Wait-depth locking and Optimistic locking schemes. Our results show increase in throughput and reduced transaction-abort-ratio in case of C2V2PL scheme.
42|9||Component mining: a process and its pattern language|An important issue in a component-based software development process is the supply source of mature, reliable, adaptable and maintainable components. We define as component mining the deliberate, organised and automated process of extracting reusable components from an existing rich software base and present a pattern language used for mining components from programs that are typically executed as non-interactive autonomous processes. We describe the patterns in terms of intent, motivation, applicability, structure, participants, consequences and implementation. Based on the pattern language, we describe the implementation of a set of COM components that encapsulate the Unix filters and an exemplar application that uses them.
42|9||Deriving protocols for services supporting mobile users|
42|9||Natural language and message sequence chart representation of use cases|We study the relationship between natural language use cases and message sequence charts. The use cases are studied in the context of object-oriented methods. Natural language sentences and Message Sequence Charts (MSCs) are used for the representation of use cases. In this paper we establish a correspondence between three restricted classes of natural language sentences and MSC fragments.
42|9||An empirical analysis of function point adjustment factors|In function point analysis, fourteen “general systems characteristics” (GSCs) are used to construct a “value adjustment factor” (VAF), with which a basic function point count is adjusted. Although the GSCs and VAF have been criticized on both theoretical and practical grounds, they are used by many practitioners. This paper reports on an empirical investigation into their use and practical value. We conclude that recording the GSCs may be useful for understanding project cost drivers and for comparing similar projects, but the VAF should not be used: doubts about its construction are not balanced by any practical benefit. A new formulation is needed for using the GSCs to explain effort; factors identified here could guide further research.
43|1|http://www.sciencedirect.com/science/journal/09505849/43/1|PZ nets â a formal method integrating Petri nets with Z|In this paper, a formal method (called PZ nets) for specifying concurrent and distributed systems is presented. PZ nets integrate two well-known existing formal methods Petri nets and Z such that Petri nets are used to specify the overall structure, control flows, causal relation, and dynamic behavior of a system; and Z is used to define tokens, labels and constrains of the system. The essence, benefits, and problems of the integration are discussed. A set of heuristics and transformations to develop PZ nets and a technique to analyze PZ nets are proposed and demonstrated through a well-known example.
43|1||The whole-part relationship in object modelling: a definition in cOlOr|In object models built according to popular object-oriented formalisms, the commonest relationship types (excluding inheritance) are the structural relationships of association and of whole-part (often called aggregation). This last type is well known to have no accurately prescribed semantics. Here, some of the aggregation semantics frequently presented in the literature and sometimes supported in current object-oriented modelling languages, especially UML, are analysed and criticised. Because of defects, the use of a modelling notation based on these aggregation semantics is dubious and limited. Moreover, many properties are candidates for characterising the whole-part relationship provided that no redundancy and no inconsistency exist between them. A framework known as cOlOr is then offered by means of a metamodel in which the Whole-Part metatype inherits from the Structural-Relationship metatype. Defining a specific aggregation semantics leads then, first, within cOlOr, to the creation of a subtype of the Whole-Part metatype. Next, the behaviour of this last type is extended and/or restricted in using a constraint-based approach. Such a process is developed particularly for considering Composition in UML and Aggregation in OML more formally, as well as for dealing with domain-dependent aggregation semantics. Since a non-negligible feature of cOlOr is the availability of a C++ library that implements the proposed metamodel, some implementation concerns are also briefly discussed.
43|1||Data placement in a parallel DBMS with multiple disks|Various strategies have been developed to assist in determining an effective data placement for a parallel database system. However, little work has been done on assessing the relative performance obtained from different strategies. This paper studies the effects of different data placement strategies on a shared-nothing parallel relational database system when the number of disks attached to each node is varied. Three representative strategies have been used for the study and the performance of the resulting configuration has been assessed in the context of the TPC–C benchmark running on a specific parallel system. Results show an increase in sensitivity to data placement strategy with increasing number of disks per node.
43|1||Combining boolean logic and linguistic structure|Descriptor languages with Boolean operators have often been applied for Information Retrieval (IR) and Filtering (IF). In addition, linguistically motivated descriptors, such as noun phrases and index expressions, have also proven of value for IR. However, the synthesis of logic and linguistics in one descriptor language is an open issue still. In this paper, Boolean index expressions (BIEs), combining Boolean logic and linguistic structure, are proposed as a good balance between expressiveness and practical issues. BIEs provide several favourable properties as descriptor language for IR and IF: BIEs are comprehensible, expressive, compact, and tractable.
43|1||On the problem of the software cost function|The question of finding a function for software cost estimation is a long-standing issue in the software engineering field. The results of other works have shown different patterns for the unknown function, which relates software size to project cost (effort). In this work, the research about this problem has been made by using the technique of Genetic Programming (GP) for exploring the possible cost functions. Both standard regression analysis and GP have been applied and compared on several data sets. However, regardless of the method, the basic size–effort relationship does not show satisfactory results, from the predictive point of view, across all data sets. One of the results of this work is that we have not found significant deviations from the linear model in the software cost functions. This result comes from the marginal cost analysis of the equations with best predictive values.
43|10|http://www.sciencedirect.com/science/journal/09505849/43/10|Effects of shaping characteristics on the performance of nested transactions|
43|10||An approach to system design based on P/T net simulation|
43|10||Database sampling with functional dependencies|During the development of information systems, there is a need to prototype the database that the applications will use when in operation. A prototype database can be built by sampling data from an existing database. Including relevant semantic information when extracting a sample from a database is considered invaluable to support the development of data-intensive applications. Functional dependencies are an example of semantic information that could be considered when sampling a database. This paper investigates how a database relation can be sampled so that the resulting sample satisfies precisely a given set of functional dependencies (and its logical consequences), i.e. is an Armstrong relation.
43|10||Measurement program success factors revisited|Success factors for measurement programs as identified in the literature typically focus on the ‘internals’ of the measurement program; incremental implementation, support from management, a well-planned metric framework, and so on. However, for a measurement program to be successful within its larger organizational context, it has to generate value for the organization. This implies that attention should also be given to the proper mapping of some identifiable organizational problem onto the measurement program, and the translation back of measurement results to organizational actions. We have extended the well-known ‘internal’ success factors for software measurement programs with four ‘external’ success factors. These success factors are aimed at the link between the measurement program and the usage of the measurement results. In this paper, we show how this combined set of internal and external success factors explains the success or failure of five industrial measurement programs.
43|11|http://www.sciencedirect.com/science/journal/09505849/43/11|Non-functional requirements analysis: deficiencies in structured methods|This paper examines some deficiencies in structured systems development methods that have the effect of overlooking non-functional requirements. Evidence from four case studies shows that non-functional requirements are often overlooked, questioning users is insufficient, methodologies do not help in the elicitation of non-functional requirements and there is a lack of consensus about the meaning and utility of non-functional requirements. Some lessons are drawn from the domain of real time systems development. Finally a framework is advanced for taking a stakeholder approach to the conflict and organisational change issues associated with the elicitation stage of requirements analysis.
43|11||Utilizing knowledge links in the implementation of system development methodologies|Developing technical ‘know-how’ is a slow process that can become a barrier in implementing complex administrative technologies such as a software development methodology. To overcome this barrier, organizations often seek knowledge links that can enhance learning and minimize inevitable problems that are encountered in an implementation process. This paper presents the findings of an empirical study that examines the prescribed versus actual use of external consultants, universities and vendors as knowledge links during the implementation of systems development methodologies (SDM). First, the study assesses the need and value of establishing and utilizing links to external sources of expertise for successful SDM implementation. We then identify and analyze a gap that exists between what the links to external knowledge are perceived to be capable of contributing and what the links to external knowledge are actually contributing during SDM implementation. In conclusion, possible reasons for the gap are discussed.
43|11||A systematic approach for the design of post-transaction input error handling|
43|11||Software reuse in-the-small: automating group rewarding|
43|11||A survey of alternative designs for a search engine storage structure|Three information retrieval storage structures are considered to determine their suitability for a World Wide Web search engine: The Wolverhampton Web Library — The Next Generation. The structures are an inverted file, signature file and Pat tree. A number of implementations are considered for each structure. For the index of an inverted file a sorted array, B-tree, B+-tree, trie and hash table are considered. For the signature file vertical and horizontal partitioning schemes are considered and for the Pat tree a tree and array implementation are considered. A theoretical comparison of the structures is done on seven criteria that include: response time, support for results ranking, search techniques, file maintenance, efficient use of disk space (including the use of compression), scalability and extensibility. The comparison reveals that an inverted file is the most suitable structure, unlike the signature file and Pat tree, which encounter problems with very large corpora.
43|11||Erratum to âAlternative approaches to effort prediction of ERP projectsâ [Inf. Software Technol. 43 (2001) 413â423]|
43|12|http://www.sciencedirect.com/science/journal/09505849/43/12|Demonstrating the usage of single-case designs in experimental software engineering|Experimental software engineering is the subdiscipline of empirical software engineering which uses experimentation to analyze, improve, and to validate software engineering methods (concepts, techniques, models). The efficacy and efficiency of some methods have already been demonstrated by experiments. To achieve this factorial designs were used so far. Critics of software engineering experiments argue that doing an experiment would be incredibly expensive. Furthermore, for doing software engineering experiments one would need hundreds of subjects. This paper shows that this is not necessarily right. An approach is presented that shows how to conduct software engineering experiments in a cost-effective way. The approach is useful to analyze software engineering problems specific to one subject, to conduct pilot experiments that precede in-depth experiments, and to accompany technology transfer. To demonstrate the usage of a single-subject experiment the domain of reuse was chosen.
43|12||Specification of integrity-preserving operations in information systems by using a formal UML-based language|
43|12||A qualitative comparison of two processes for object-oriented software development|Two of the leading object-oriented processes are the public domain Object-oriented Process, Environment and Notation (OPEN) and the proprietary Rational Unified Process (RUP). A qualitative evaluation is performed on RUP's public domain component and on OPEN using a set of stated criteria. In particular, we focus our comparison on aspects of the process architecture and underpinning metamodel, the concepts and terminology utilized and support for project management.
43|12||Leaving inconsistency using fuzzy logic|Current software development methods do not provide adequate means to model inconsistencies and therefore force software engineers to resolve inconsistencies whenever they are detected. Certain kinds of inconsistencies, however, are desirable and should be maintained as long as possible. For instance, when multiple conflicting solutions exist for the same problem, each solution should be preserved to allow further refinements along the development process. An early resolution of inconsistencies may result in loss of information and excessive restriction of the design space. This paper aims to enhance the current methods by modeling and controlling the desired inconsistencies through the application of fuzzy logic-based techniques. It is shown that the proposed approach increases the adaptability and reusability of design models.
43|12||Corrigendum to âA tunable class hierarchy index for object-oriented databases using a multidimensional index structureâ [Information and Software Technology 43 (2001) 309â323]|
43|13|http://www.sciencedirect.com/science/journal/09505849/43/13|Supporting tailorable program visualisation through literate programming and fisheye views|This paper describes the ‘Jaba’ program editor and browser that allows users to tailor the level of abstraction at which they visualise, browse, edit and document object-oriented programs. Its design draws on concepts from literate programming, holophrasting displays, fisheye visualisation and hypertext to allow programmers to rapidly move between abstract and detailed views of Java classes. The paper focuses on the motivation for, and user interface issues surrounding, the integration of these facilities in Jaba. Limitations in the current tools and theories for programming support are identified, and modifications are proposed and demonstrated. Examples include overcoming the static post-hoc documentation support provided by Javadoc, and normalising Furnas's ‘degree of interest’ fisheye visualisation formula to avoid excessive suppression of program segments.
43|13||Managing uncertainty in project portfolio cost estimation|Although typically a software development organisation is involved in more than one project simultaneously, the available tools in the area of software cost estimation deal mostly with single software projects. In order to calculate the possible cost of the entire project portfolio, one must combine the single project estimates taking into account the uncertainty involved. In this paper, statistical simulation techniques are used to calculate confidence intervals for the effort needed for a project portfolio. The overall approach is illustrated through the adaptation of the analogy-based method for software cost estimation to cover multiple projects.
43|13||Verifying scenarios with time Petri-nets|Recently, a substantial amount of research activities has been focused on a user-oriented perspective to the development of software systems. One of the key elements in this perspective is the notion of scenarios: a description of what people do and experience as they try to make usage of computer systems and applications. A variety of applications of scenarios has been proposed, for example, to elicit user requirements, or to validate requirements specifications. As scenarios are useful for the lifecycle of requirements engineering, it is important to enable verification of these scenarios, especially, to detect any wrong information and missing information that are hidden in scenarios. However, scenarios are usually stated in an informal way, which impedes the easiness for verification. The focus of this paper is on the use of time Petri-nets (TPNs) to serve as the verification mechanism for the acquired scenarios. Use cases are used to elicit the user needs and to derive the scenarios. Each of the use cases is described from a user's perspective and depicts a specific flow of events in the system. After specifying all possible scenarios, each of them can be transformed into its correspondent time Petri-nets model. Through the analysis of these TPNs models, wrong information and missing information in scenarios can be detected. The proposed approach is illustrated by means of a course registration problem domain.
43|13||Generalized partition testing via Bayes linear methods|This paper explores the use of Bayes linear methods related to partition testing for software. If a partition of the input domain has been defined, the method works without the assumption of homogeneous (revealing) subdomains, and also includes the possibility to learn, from testing inputs in one subdomain, about inputs in other subdomains, through explicit definition of the correlations involved. To enable practical application, an exchangeability structure needs to be defined carefully, for which means the judgements of experts with relation to the software is needed. Next to presenting the basic idea of Bayes linear methods and how it can be used to generalize partition testing, some important aspects related to applications as well as for future research are discussed.
43|13||A viewpoint on software engineering and information systems: what we can learn from the construction industry?|
43|13||An investigation of coupling, reuse and maintenance in a commercial C++ application|This paper describes an investigation into the use of coupling complexity metrics to obtain early indications of various properties of a system of C++ classes. The properties of interest are: (i) the potential reusability of a class and (ii) the likelihood that a class will be affected by maintenance changes made to the overall system. The study indicates that coupling metrics can provide useful indications of both reusable classes and of classes that may have a significant influence on the effort expended during system maintenance and testing.
43|14|http://www.sciencedirect.com/science/journal/09505849/43/14|Editorial Note|
43|14||Editorial|
43|14||An overview of evolutionary algorithms: practical issues and common pitfalls|An overview of evolutionary algorithms is presented covering genetic algorithms, evolution strategies, genetic programming and evolutionary programming. The schema theorem is reviewed and critiqued. Gray codes, bit representations and real-valued representations are discussed for parameter optimization problems. Parallel Island models are also reviewed, and the evaluation of evolutionary algorithms is discussed.
43|14||Search-based software engineering|This paper claims that a new field of software engineering research and practice is emerging: search-based software engineering. The paper argues that software engineering is ideal for the application of metaheuristic search techniques, such as genetic algorithms, simulated annealing and tabu search. Such search-based techniques could provide solutions to the difficult problems of balancing competing (and some times inconsistent) constraints and may suggest ways of finding acceptable solutions in situations where perfect solutions are either theoretically impossible or practically infeasible.
43|14||Evolutionary test environment for automatic structural testing|Testing is the most significant analytic quality assurance measure for software. Systematic design of test cases is crucial for the test quality. Structure-oriented test methods, which define test cases on the basis of the internal program structures, are widely used. A promising approach for the automation of structural test case design is evolutionary testing. Evolutionary testing searches test data that fulfil a given structural test criteria by means of evolutionary computation. In this work, an evolutionary test environment has been developed that performs fully automatic test data generation for most structural test methods. The introduction of an approximation level for fitness evaluation of generated test data and the definition of an efficient test strategy for processing test goals, increases the performance of evolutionary testing considerably.
43|14||A prediction system for evolutionary testability applied to dynamic execution time analysis|Evolutionary testing (ET) is a test case generation technique based on the application of an evolutionary algorithm. It can be applied to timing analysis of real-time systems. In this instance, timing analysis is equivalent to testing. The test objective is to uncover temporal errors. This corresponds to the violation of the system's timing specification. Testability is the ability of the test technique to uncover faults. Evolutionary testability is the ability of an evolutionary algorithm to successfully generate test cases with the goal to uncover faults, in this instance violation of the timing specification. This process attempts to find the best- and worst-case execution time of a real-time system.
43|14||Can genetic programming improve software effort estimation? A comparative evaluation|Accurate software effort estimation is an important part of the software process. Originally, estimation was performed using only human expertise, but more recently, attention has turned to a variety of machine learning (ML) methods. This paper attempts to evaluate critically the potential of genetic programming (GP) in software effort estimation when compared with previously published approaches, in terms of accuracy and ease of use. The comparison is based on the well-known Desharnais data set of 81 software projects derived from a Canadian software house in the late 1980s. The input variables are restricted to those available from the specification stage and significant effort is put into the GP and all of the other solution strategies to offer a realistic and fair comparison. There is evidence that GP can offer significant improvements in accuracy but this depends on the measure and interpretation of accuracy used. GP has the potential to be a valid additional tool for software effort estimation but set up and running effort is high and interpretation difficult, as it is for any complex meta-heuristic technique.
43|14||An evolutionary approach to estimating software development projects|The use of dynamic models and simulation environments in connection with software projects paved the way for tools that allow us to simulate the behaviour of the projects. The main advantage of a Software Project Simulator (SPS) is the possibility of experimenting with different decisions to be taken at no cost. In this paper, we present a new approach based on the combination of an SPS and Evolutionary Computation. The purpose is to provide accurate decision rules in order to help the project manager to take decisions at any time in the development. The SPS generates a database from the software project, which is provided as input to the evolutionary algorithm for producing the set of management rules. These rules will help the project manager to keep the project within the cost, quality and duration targets. The set of alternatives within the decision-making framework is therefore reduced to a quality set of decisions.
43|14||The next release problem|Companies developing and maintaining complex software systems need to determine the features that should be added to their system as part of the next release. They will wish to select these features to ensure the demands of their client base are satisfied as much as possible while at the same time ensuring that they themselves have the resources to undertake the necessary development. This situation is modelled in this paper and the problem of selecting an optimal next release is shown to be NP-hard. The use of various modern heuristics to find a high quality but possibly suboptimal solution is described. Comparative studies of these heuristics are given for various test cases.
43|14||Protocols are programs too: the meta-heuristic search for security protocols|Protocol security is important. So are efficiency and cost. This paper provides an early framework for handling such aspects in a uniform way based on combinatorial optimisation techniques. The belief logic of Burrows, Abadi and Needham (BAN logic) is viewed as both a specification and proof system and as a ‘protocol programming language’. The paper shows how simulated annealing and genetic algorithms can be used to generate correct and efficient BAN protocols. It also investigates the use of parsimonious and redundant representations.
43|14||Software engineering using metaheuristic innovative algorithms: workshop report|This paper reports on the first International Workshop on Software Engineering using Metaheuristic Innovative Algorithms, which was held in Toronto on the 14th of May 2001 as a part of the IEEE International Conference on Software Engineering.
43|14||Author Index|
43|14||Keyword Index|
43|14||Volume 43 Contents|
43|15|http://www.sciencedirect.com/science/journal/09505849/43/15|Special section: Controlled Experiments in Software Engineering|
43|15||Usage-based readingâan experiment to guide reviewers with use cases|Reading methods for software inspections are used for aiding reviewers to focus on special aspects in a software artefact. Many experiments were conducted for checklist-based reading and scenario-based reading concluding that the focus is important for software reviewers. This paper describes and evaluates a reading technique called usage-based reading (UBR). UBR utilises prioritised use cases to guide reviewers through an inspection. More importantly, UBR drives the reviewers to focus on the software parts that are most important for a user. An experiment was conducted on 27 third year Bachelor's software engineering students, where one group used use cases sorted in a prioritised order and the control group used randomly ordered use cases. The main result is that reviewers in the group with prioritised use cases are significantly more efficient and effective in detecting the most critical faults from a user's point of view. Consequently, UBR has the potential to become an important reading technique. Future extensions to the reading technique are suggested and experiences gained from the experiment to support replications are provided.
43|15||Impact of effort estimates on software project work|This paper presents results from two case studies and two experiments on how effort estimates impact software project work. The studies indicate that a meaningful interpretation of effort estimation accuracy requires knowledge about the size and nature of the impact of the effort estimates on the software work. For example, we found that projects with high priority on costs and incomplete requirements specifications were prone to adjust the work to fit the estimate when the estimates were too optimistic, while too optimistic estimates led to effort overruns for projects with high priority on quality and well specified requirements.
43|15||Empirical validation of referential integrity metrics|Databases are the core of Information Systems (IS). It is, therefore, necessary to ensure the quality of the databases in order to ensure the quality of the IS. Metrics are useful mechanisms for controlling database quality. This paper presents two metrics related to referential integrity, number of foreign keys (NFK) and depth of the referential tree (DRT) for controlling the quality of a relational database. However, to ascertain the practical utility of the metrics, experimental validation is necessary. This validation can be carried out through controlled experiments or through case studies. The controlled experiments must also be replicated in order to obtain firm conclusions. With this objective in mind, we have undertaken different empirical work with metrics for relational databases. As a part of this empirical work, we have conducted a case study with some metrics for relational databases and a controlled experiment with two metrics presented in this paper. The detailed experiment described in this paper is a replication of the later one. The experiment was replicated in order to confirm the results obtained from the first experiment.
43|15||Senior executives' use of information technology|There is a paucity of literature focusing on the ingredients for effective top management, i.e. senior executives, use of Information Technology (IT). In practice, many senior executives argue that they do not see a connection between what IT does and their tasks as executives. Based on the Technology Acceptance Model (TAM), a research model was developed and tested to assess the factors that influence the use of IT by senior executives. A dedicated system supporting the task of a senior executive, an Executive Information System (EIS), was used as the IT tool under review. A large number of external variables were identified and hypothesized, influencing the core elements of TAM. To test the research model using structural equation modeling, cross-sectional data was gathered from eighty-seven senior executives drawn from twenty-one European-based multinationals. The results supported the core TAM and found only a small number of antecedent variables influencing actual use, either directly or indirectly. In addition to identifying the external factors, three of these key variables are under managerial control. They can be used to design organizational or managerial interventions that increase effective utilization of IT.
43|15||Anomaly detection in concurrent Java programs using dynamic data flow analysis|Concurrency constructs are widely used when developing complex software such as real-time, networking and multithreaded client–server applications. Consequently, testing a program, which includes concurrency constructs is a very elaborate and complex process. In this work, we first identify the different classes of synchronization anomalies that may occur in concurrent Java programs. We then consider testing concurrent Java programs against synchronization anomalies using dynamic data flow analysis techniques. Moreover, we show how the data flow analysis technique can be extended to detect such anomalies.
43|15||Call for Papers|
43|2|http://www.sciencedirect.com/science/journal/09505849/43/2|A dynamically mapped open hypermedia system framework for integrating information systems|The overall goal of this research is to design a distributed, extensible, cross-platform, collaorative, and integrated system that can supplement information systems with hypertext support. In this paper, we propose a dynamically mapped open hypermedia system (DMOHS) framework that can support information systems fully. Our framework has two axes: a logical component focus and an application requirement focus. In Axis 1, we propose a conceptual DMOHS architecture with eight logical components. In Axis 2, we define and discuss major aspects of a DMOHS that should be supported in a distributed and integrated environment. Together the two axes provide a grid for specifying the logical DHOMS functionality for supporting application requirements. Given this framework, we first evaluate five open hypermedia systems and the www, and then design our own system implemented on top of the www. This paper also contributes guidelines for building mapping routines that supplement on top of the www. Further, it contributes guidelines for building mapping routines that supplement information systems with hypertext support.
43|2||Interfaces for computer and robot assisted surgical systems|Computer and robot assisted systems for surgery imply ergonomic requirements stronger than usual, due to the fact that the typical environment of an operating room is not presently suitable for the use of a computer system. The literature does not provide many specific indications for the design of user interfaces in this particular field or to match the usability needs of the quite conservative community of surgeons. This paper tries to fill this gap giving guidelines extrapolated from the design of the present robotic systems for surgery. They can be useful to software designers to reduce the time necessary to develop a reliable and safe interface for new computer assisted systems, and to improve the usability of the existing ones.
43|2||On the maximin algorithms for test allocations in partition testing|The proportional sampling (PS) strategy is a partition testing strategy that has been proved to have a better chance than random testing to detect at least one failure. A near proportional sampling (NPS) strategy is one that approximates the PS strategy when the latter is not feasible. We have earlier proved that the (basic) maximin algorithm generates a maximin test allocation, that is, an allocation of test cases that will maximally improve the lower bound performance of the partition testing strategy, and shown that the algorithm may serve as a systematic means of approximating the PS strategy. In this paper, we derive the uniqueness and completeness conditions of generating maximin test allocations, propose the complete maximin algorithm that generates all possible maximin test allocations and demonstrate empirically that the new algorithm is consistently better than random testing as well as several other NPS strategies.
43|2||Design of data warehouses using metadata|Data warehouses have become an instant phenomenon in many large organizations that deal with massive amounts of information. Drawing on the experiences from the systems development field, we surmise that an effective design tool will enhance the success of warehouse implementations. Thus, we present a CASE tool designed to generate the SQL queries necessary to build a warehouse from a set of operational relational databases. The warehouse designer simply specifies a list of attribute names that will appear in the warehouse, conditions if any are desired, and a description of the operational databases. The tool returns the queries needed to populate the warehouse table.
43|2||Structuring requirement specifications with goals|One of the foci of the recent development in requirements engineering has been the study of conflicts and vagueness encountered in requirements. However, there is no systematic way in the existing approaches for handling the interactions among nonfunctional requirements and their impacts on the structuring of requirement specifications. In this paper, a new approach is proposed to formulate the requirement specifications based on the notion of goals along three aspects: (1) to extend use cases with goals to guide the derivation of use cases; (2) to analyze the interactions among nonfunctional requirements; and (3) to structure fuzzy object-oriented models based on the interactions found. The proposed approach is illustrated using the problem domain of a meeting scheduler system.
43|2||Testing a system specified using Statecharts and Z|A hybrid specification language Î¼SZ, in which the dynamic behaviour of a system is described using Statecharts and the data and the data transformations are described using Z, has been developed for the specification of embedded systems. This paper describes an approach to testing from a deterministic sequential specification written in Î¼SZ. By considering the Z specifications of the operations, the extended finite state machine (EFSM) defined by the Statechart can be rewritten to produce an EFSM that has a number of properties that simplify test generation. Test generation algorithms are introduced and applied to an example. While this paper considers Î¼SZ specifications, the approaches described might be applied whenever the specification is an EFSM whose states and transitions are specified using a language similar to Z.
43|2||Java garbage collection â a generic solution?|This paper first introduces the main approaches to garbage collection (GC) in modern high-level programming languages, and then focuses on how the ‘new’ language Java addresses GC. The object oriented nature of Java and the fact that it is designed to be used in networked/distributed systems poses additional GC related problems. Issues raised by these problems are discussed and suggestions made about ways in which the full advantages of GC in Java can be realized. The main conclusion is that while automatic GC in Java makes the writing of code easier, there is still the need for programmer awareness of Java memory management.
43|3|http://www.sciencedirect.com/science/journal/09505849/43/3|Guest Editorial|
43|3||Behavioural analysis of component-based systems|
43|3||Formal modeling of the Enterprise JavaBeansâ¢ component integration framework|
43|3||Controllers: reusable wrappers to adapt software components|
43|3||Understanding âvariationâ in component-based development: case findings from practice|This paper discusses the importance of understanding and capturing variation in component-based development using the context of a six-month action research project carried out within a major system solution provider. The issues raised in the empirical context show that the norms of given development disciplines need to adapt for component-based development projects and that this is neither trivial nor easy. In addressing such issues, the implications of adopting a domain-based approach to component-based development are explored alongside the means by which this may be undertaken. This exploration highlights the need to provide an explicit understanding of variation and the ramifications of providing a component with varying degrees of context independence are explored via a scenario developed in practice. The scenario shows that neither reuse nor variation should be allowed to be chaotic if component-based development is to handle flexibility in a more cost-effective and coherent manner than system maintenance currently does. The paper concludes by presenting an example of an elaborated use case to illustrate how existing techniques can be modified to provide a structured environment for understanding variation. Holistically, the discussion shows that component-based development should not be viewed as a singular discipline, but rather as an umbrella activity that requires the melding of a number of important research areas.
43|4|http://www.sciencedirect.com/science/journal/09505849/43/4|The optimal retrieval start times of media objects for the multimedia presentation|
43|4||Experience with mural in formalising Dust-Expert|
43|4||Measuring post-transaction error handling in database applications|
43|4||A method for interactive articulation of information requirements for strategic decision support|Decision support for strategic management, an extension to the traditional decision support systems, has been seen as an important area of research where the theory, methods and technologies can bring in a great deal of benefit to the whole enterprise at the executive level. The work in the paper first of all examines two paradigms of information provision for strategic decision support. The information requirements for decision-making are studied to reveal that the level of detail, granularity, format of presentation, and broad range of information type are unique for the applications at the strategic level. The provision of relevant information involves articulation of requirements based on the decision problems described by the executive manager.
43|4||On analyzing errors in a selectivity estimation method based on dynamic maintenance of data distribution|
43|4||Practitioners' views on the use of formal methods: an industrial survey by structured interview|The recognised deficiency in the level of empirical investigation of software engineering methods is particularly acute in the area of formal methods, where reports about their usefulness vary widely. We interviewed several formal methods users about the use of formal methods and their impact on various aspects of software engineering including the effects on the company, its products and its development processes as well as pragmatic issues such as scalability, understandability and tool support. The interviews are a first stage of empirical assessment. Future work will investigate some of the issues raised using formal experimentation and case studies.
43|5|http://www.sciencedirect.com/science/journal/09505849/43/5|A two-layered-class approach for the reuse of synchronization code|
43|5||Automated reusability quality analysis of OO legacy software|Software reuse increases productivity, reduces costs, and improves quality. Object-oriented (OO) software has been shown to be inherently more reusable than functionally decomposed software; however, most OO software was not specifically designed for reuse [Software Reuse Guidelines and Methods, Plenum Press, New York, 1991]. This paper describes the analysis, in terms of quality factors related to reusability, contained in an approach that aids significantly in assessing existing OO software for reusability. An automated tool implementing the approach is validated by comparing the tool's quality determinations to that of human experts. This comparison provides insight into how OO software metrics should be interpreted in relation to the quality factors they purport to measure.
43|5||A tunable class hierarchy index for object-oriented databases using a multidimensional index structure|This paper presents a tunable two-dimensional class hierarchy indexing technique (2D-CHI) for object-oriented databases. We use a two-dimensional file organization as the index structure. 2D-CHI deals with the problem of clustering objects in a two-dimensional domain space consisting of the key attribute domain and the class identifier domain. In conventional class indexing techniques using one-dimensional index structures such as the B+-tree, the clustering property is owned exclusively by one attribute. These indexing techniques do not handle the queries that address both the attribute keys and the class identifiers efficiently. 2D-CHI enhances query performance by adjusting the degree of clustering between the key value domain and the class identifier domain based on the precollected usage pattern. For performance evaluation, we first compare 2D-CHI with the conventional class indexing techniques using an analytic cost model based on the assumption of uniform object distribution, and then, verify the cost model through experiments using the multilevel grid file as the two-dimensional index. We further perform experiments with nonuniform object distributions. The experiments show that our proposed method builds optimal class index structures in terms of the total number of page accesses for given the precollected usage pattern regardless of query types and object distributions.
43|5||Safety checking in an automatic train operation system|
43|5||A conformity model of software processes|In this paper we present a new idea to compare different software process models. We propose a five-level model measuring the conformity of two process models. The approach is of practical importance for a number of applications, e.g. when a company's process model has to be checked against a generic process model required in some project, or when two companies, each working with its own process model, intend to cooperate. We illustrate our approach with a conformance study of Catalysis and the V-Model.
43|5||Calendar|
43|6|http://www.sciencedirect.com/science/journal/09505849/43/6|Encapsulating distribution by remote objects|Middleware for modern office environments and many other application areas needs to provide support for a myriad of different, highly mobile objects. At the same time, it should be able to scale to vast numbers of objects that may possibly be dispersed over a large wide-area network. The combination of flexibility and scalability requires support for object-specific solutions that is hardly addressed by current object-based systems such as CORBA. We have developed a middleware solution that seamlessly integrates traditional remote objects with physically distributed objects that can fully encapsulate their own distribution strategy. We describe how this integration takes place, and how it can be applied to existing systems such as CORBA.
43|6||The application of use case definitions in system design specification|
43|6||Ontological analysis of wholeâpart relationships in OO-models|Earlier semantic and formal analyses of whole–part (WP) relationships in object-oriented models have led to a framework, which distinguishes between primary, consequential, secondary and dependent characteristics of WP relationships. This paper interprets, validates and elaborates on that framework using an existing ontological theory and an associated formal model of objects. The revised framework confirms most of the original characteristics and suggests a number of additions and modifications. The analysis also grounds the characteristics in the framework and thereby suggests more precise definitions for some of them.
43|6||Decomposing legacy systems into objects: an eclectic approach|The identification of objects in procedural programs has long been recognised as a key to renewing legacy systems. As a consequence, several authors have proposed methods and tools that achieve, in general, some level of success, but do not always precisely identify a coherent set of objects. We show that using an eclectic approach, where a domain expert software engineer is encouraged to tailor and combine existing approaches, may overcome the limitation of the single approaches and helps to better address the particular goals of the project at hand and the unique aspects of the subject system. The eclectic approach is illustrated by reporting experiences from a case study of identifying coarse-grained, persistent objects to be used in the migration of a COBOL system to a distributed component system.
43|7|http://www.sciencedirect.com/science/journal/09505849/43/7|Alternative approaches to effort prediction of ERP projects|There exist many effort prediction systems but none specifically devised for enterprise resource planning (ERP) projects, and the empirical evidence is neither convincing nor adequate from a human user perspective. Consequently, this non-empirical evaluation contributes knowledge by investigating: (i) their applicability to ERP projects, (ii) their added value to a human user beyond making a prediction, and (iii) if they make sense. The analysis suggests that regression analysis seems to be the best choice as an ERP prediction system, and that ANGEL, ACE, CART and OSR primarily add value to a user in exploratory data analysis by their ability to identify similar projects.
43|7||OgDesk: an orthogonal graphical interface for object-oriented database systems that supports schema management, browsing and querying|
43|7||Measuring the value of information technology in technical efficiency with stochastic production frontiers|With the vast amounts of resources being invested in information technology (IT), the issue of how to measure and manage the impact of IT on organizational performance has received increased attention. Based on the production theory in microeconomics, this paper investigates the relationship between IT investments and technical efficiency in the firm's production process. The application of stochastic production frontiers to a comprehensive firm-level panel data set provides us with empirical evidence that IT has a significantly positive effect on technical efficiency and, hence, contributes to the productivity growth in organizations, claimed by some earlier studies with the same data set. The stochastic production frontiers considered include the popular Cobb–Douglas function and the more flexible translog function. Both specifications of production technology lead to the same conclusion. Managerial implications derived from the empirical results are also presented.
43|7||A pattern system for the development of collaborative applications|
43|8|http://www.sciencedirect.com/science/journal/09505849/43/8|Knowledge for network support|Computer network support is a growing area of IT activities within most organisations. Even small businesses are typically moving towards computer networks rather than separate stand-alone PCs. However, the skills and knowledge required for effective computer network support are still largely uncertain. In this paper the results of a research exercise investigating the skills and knowledge required for computer network support, based on case studies in 25 organisations are reported.
43|8||Maintenance and testing effort modeled by linear and nonlinear dynamic systems|
43|8||An integrated interactive environment for knowledge discovery from heterogeneous data resources|Discovering knowledge such as causal relations among objects in large data collections is very important in many decision-making processes. In this paper, we present our development of an integrated environment acting as a software agent for discovering correlative attributes of data objects from multiple heterogeneous resources. The environment provides necessary supporting tools and processing engines for acquiring, collecting, and extracting relevant information from multiple data resources, and then forming meaningful knowledge patterns. The agent system is featured with an interactive user interface that provides useful communication channels for human supervisors to actively engage in necessary consultation and guidance in the entire knowledge discovery processes. A cross-reference technique is employed for searching and discovering coherent set of correlative patterns from the heterogeneous data resources. A Bayesian network approach is applied as a knowledge representation scheme for recording and manipulating the discovered causal relations. The system employs common data warehousing and OLAP techniques to form integrated data repository and generate database queries over large data collections from various distinct data resources.
43|8||Wrapping legacy systems for use in heterogeneous computing environments|With the advent and widespread use of object-oriented and client–server technologies, companies expect their legacy systems, developed for the centralized environment, to take advantage of these new technologies and also cooperate with their heterogeneous environments. An alternative to migrating legacy systems from the mainframe to a user-centered, distributed object computing, and client–server platform is to wrap the legacy systems on the mainframe and expose the interfaces of the legacy systems to the remote clients. The enabling middleware technologies such as Common Object Request Broker Architecture (CORBA), Component Object Model/Distributed Component Object Model (COM/DCOM), and Java RMI make the migration of the legacy systems to a heterogeneous distributed computing environment possible. In this paper, we present an approach and practical experience for integrating the legacy systems to a heterogeneous distributed computing environment by adopting the CORBA technology. It has been reported quite often that an approach like this will improve system maintainability, portability, and interoperability. We also illustrate this approach with an industrial project. The project is viewed as a reengineering effort where a centralized reengineering system is wrapped to operate in a heterogeneous distributed computing environment by leveraging CORBA technology. The reengineering approach is a combination of redesign and simple facelift. The resulting legacy integration architecture through the application of the approach is evaluated using the equality attributes proposed by Bass, Clements, and Kazman.
43|8||User requirements for OO CASE tools|This study assesses developers' demands for OO CASE tool features and indicates the comparative strengths of various OO development features. Important OO features include support for a variety of message types, design analysis support, support for security, and concurrency and multi-thread controls. Developers are also relying on inheritance, data abstraction, and event-oriented/messaging designs in their assessment of CASE tool features. Many system developers indicate that OO CASE tool features are important; however, existing products do not address current design requirements.
43|8||Building deductive object-oriented database systems in the ODMG era|Deductive object-oriented database systems (DOODs) have been a subject of intensive research for the last 13 years, with results embodied in several research prototypes and one commercial system produced so far [1]. However, despite the considerable number of systems available, there has been surprisingly little work on organising and analysing the several system architectures and query processing strategies that have been employed in the construction of DOODs. Furthermore, with the consolidation of the ODMG specification as a standard for object databases, it is important to assess which architectural approach is better suited for building DOODs considering the ODMG framework. This paper categorises several representative DOOD systems based on their architecture and query processing approach, and provides an analysis of the issues involved in building DOOD systems compliant with the ODMG standard.
43|9|http://www.sciencedirect.com/science/journal/09505849/43/9|An interdisciplinary model of complexity in embedded intelligent real-time systems|Embedded Intelligent Real-Time Systems (EIRTS) are proliferating in many safety-critical large-scale systems where safety and reliability are important. EIRTS are often introduced in safety-critical large-scale systems to improve the reliability and safety, although, in many cases, they also increase the system's complexity. Defining, understanding, and measuring the complexity in EIRTS can aid us in designing and building more reliable and effective EIRTS, particularly in safety-critical settings.
43|9||Design units â a layered approach for design driven software development|The challenge faced by software developers is to establish a manageable relationship between design and implementation. This paper describes an integrated, traceable software development approach in the context of a use case design methodology that achieves several quality control properties. The foundation for this approach lies in partitioning the design schemata into a layered architecture of functional components called design units. Design units provide the basis for the automatic generation of modular source code, the traceability of requirements throughout the software development process and the framework for a systematic approach to change management.
43|9||Testing a distributed system: generating minimal synchronised test sequences that detect output-shifting faults|
43|9||Reuse strategies in software development: an empirical study|We report on a study of reuse strategies, as employed in performing component-based software design, that we have conducted using the Unix environment. A number of subjects were asked to develop shell scripts that would perform a set of tasks, using a design support tool that was intended to provide support for reuse of Unix processes by providing a number of recognised searching strategies to assist with identifying suitable components. We review some of the methodological problems that were posed by the study, and we also conclude that for the inexperienced subjects the early identification of reusable elements was the most consistently successful strategy to adopt.
43|9||Erratum to âOntological analysis of wholeâpart relationships in OO-modelsâ [Information and Software Technology 43 (2001) 387â389]|
44|1|http://www.sciencedirect.com/science/journal/09505849/44/1|Software development processes â an assessment|The development and maintenance of software systems has become a major challenge. Many best practices have emerged and processes have been developed which conform to such practices. This paper reviews these approaches and provides a basis for the assessment of three processes used for the development of systems.
44|1||The question of scale economies in softwareâwhy cannot researchers agree?|
44|1||The architecture of an information tool for de-mining: mine identification core module (MICM)|In this work the architecture of a mine identification core module (MICM) system is presented. This system focuses on the objects to be identified rather than the sensor technology, where most R&D efforts are concentrated. Fusing sensorial with accumulated data, it could be the base of a sensor independent decision support system forming the common core of the future multi-sensorial landmine detection/identification systems.
44|1||Translating object-oriented database transactions into relational transactions|In this paper, we present methods of translating transactions from object-oriented database (OODB) to relational database (RDB). The process involves schema mapping in data definition language and transaction translation in data manipulation language. They include scheme definition, data query and transaction operation of insert, update, and deletion. We also discuss the object-oriented features in OODB operations that are not supported by RDB, such as class hierarchy, class composition hierarchy, and set attribute, and provide a general solution to realize those mechanisms by traditional relation operations. The result of the transaction translation can be applied into adding object-oriented interface into relational database management system and to the interoperability between OODB and RDB.
44|1||Case study: factors for early prediction of software development success|Project managers can make more effective and efficient project adjustments if they detect project high-risk elements early. We analyzed 42 software development projects in order to investigate some early risk factors and their effect on software project success. Developers in our organization found the most important factors for project success to be: (1) the presence of a committed sponsor and (2) the level of confidence that the customers and users have in the project manager and development team. However, several other software project factors, which are generally recognized as important, were not considered important by our respondents.
44|10|http://www.sciencedirect.com/science/journal/09505849/44/10|Automated compression of state machines using UML statechart diagram notation|In object-oriented design, behavioral modeling aims at describing the behavior of objects using state machines. State machines can also be used in dynamic reverse engineering to capture the overall run-time behavior of the objects of interest. The unified modeling language (UML) statechart diagram notation provides powerful means to structure state machines, thus avoiding the plague of combinatorial explosion.
44|10||Design breakdowns, scenarios and rapid application development|In this paper we consider the way in which two representational forms, scenarios and design breakdowns, which have emerged in the traditions of human-centred design are relevant within the recent commercial emphasis on rapid application development (RAD). RAD is a contingent approach to interactive software development that is characterised by large amounts of user involvement, incremental prototyping and product-based project management. Scenarios have become popular as an intermediate representation within the human–computer interaction and computer supported co-operative work communities. Design breakdowns have been suggested as a useful organising device and design technique within the co-operative prototyping literature. Both these representational forms are not currently utilised within the commercial RAD tradition. In order to detail the relevance of these concepts to commercial development, we describe the ‘natural history’ of one particular RAD project and show how scenarios, breakdowns and the resolution of such breakdowns contributed to the successful implementation of an information system within a small commercial organisation. We conclude with a discussion of lessons from our work and some intended future work in this area.
44|10||Measuring software evolution at a nuclear fusion experiment site: a test case for the applicability of OO and reuse metrics in software characterization|A set of software metrics has been used to provide empirical evidence on how code organization changes when a software product evolves. A Java application for graphical data display used in experimental physics has been used as a test case. Exploiting common patterns in the way software applications evolve is desirable as it would give designers and managers a better understanding of the software process. The analysis in which framework reuse has also been considered, highlighted a limited use of the inheritance mechanism and, despite an increase in the overall complexity, a substantial invariance of the internal application organization. This fact is explained by the increasing framework usage and integration during the product's lifetime.
44|10||Understanding the use of an electronic process guide|This paper presents a case study of the installation and use of an electronic process guide within a small-to-medium software development company. The purpose of the study is to better understand how software engineers use this technology so that it can be improved and better used to support software process improvement. In the study the EPG was used to guide new processes in a software improvement programme. The use of the EPG was studied over a period of 8 months with data collected through access logs, by questionnaires and by interviews. The results show that the improvement programme was successful in improving project documentation, project management and the company's relationship with its customers. The EPG contributed to the improvement programme by providing support for the creation of templates for key project documentation, assisting with project planning and estimation and providing a forum for discussion of process and work practices. The biggest improvements that could be made to the EPG would be to provide better navigation tools including a graphical overview of the process, provide tailoring facilities, include examples and experience and link to a project management tool.
44|11|http://www.sciencedirect.com/science/journal/09505849/44/11|INSIDE FRONT COVER|
44|11||Molecule-oriented programming in Java|Molecule-oriented programming is introduced as a programming style carrying some perspective for Java. A sequence of examples is provided. Supporting the development of the molecule-oriented programming style several matters are introduced and developed: profile classes allowing the representation of class protocols as Java classes, the ‘empirical semantics’ of null, a jargon for the description of molecules, some terminology on software life-cycles related to molecule-oriented programming, and the notion of reconstruction semantics (a guiding principle behind the set of case studies).
44|11||Empirical study of exchange patterns during software peer review meetings|An observational empirical study is performed on the peer review meeting (PRM). The purpose of the study is to define a model for participant interactions based on statistical analysis of the moves that occur during the PRM. Protocol analysis is applied to the data from the observation of seven representative meetings held during a professional software development project. Lag sequential analysis is used to find significant relationships between moves, and hierarchical clustering is used to define a model for the exchange relationships. The approach illustrates the building up of communication patterns through three successive analysis iterations. Four significant types of exchanges are identified as characteristic of PRMs: cognitive synchronization, review, elaboration and refinement. A model is built to represents the qualitative and quantitative importance of the various exchanges occurring during PRMs. The central role of cognitive synchronization is illustrated.
44|11||Reconstructing a formal security model|Role-based access control (RBAC) is a flexible approach to access control, which has generated great interest in the security community. The principal motivation behind RBAC is to simplify the complexity of administrative tasks. Several formal models of RBAC have been introduced. However, there are a few works specifying RBAC in a way which system developers or software engineers can easily understand and adopt to develop role-based systems. And there still exists a demand to have a practical representation of well-known access control models for system developers who work on secure system development. In this paper we represent a well-known RBAC model with software engineering tools such as Unified Modeling Language (UML) and Object Constraints Language (OCL) to reduce a gap between security models and system developments. The UML is a general-purpose visual modeling language in which we can specify, visualize, and document the components of a software system. And OCL is part of the UML and has been used for object-oriented analysis and design as a de facto constraints specification language in software engineering arena. Our representation is based on a standard model for RBAC proposed by the National Institute of Standards and Technology. We specify this RBAC model with UML including three views: static view, functional view, and dynamic view. We also describe how OCL can specify RBAC constraints that is one of important aspects to constrain what components in RBAC are allowed to do. In addition, we briefly discuss future directions of this work.
44|11||Conceptual Modeling in the eXtreme|Conceptual Modeling-based methods and their corresponding CASE tools have traditionally had one main weak point: the use of different notations for the problem space system view (centered on the what the system is) and the solution space view (centered on the how it is to be represented). The real value of Conceptual Modeling, from a pragmatic perspective, is lost due to these different notations and the complex and often ambiguous paths required to go from one view to another. To overcome this problem, a Conceptual Schema should be a precise representation of the user requirements and should also be executable. This means that the programming tasks are really done at a higher level of abstraction, using Conceptual Modeling constructs. In this paper, we present a Conceptual Modeling in the Extreme approach for automatic software production this approach focuses the developer's efforts in the Requirements and Conceptual Modeling phases in an extreme way. This approach together with a Conceptual Model Compiler strategy produces a fully executable application.
44|12|http://www.sciencedirect.com/science/journal/09505849/44/12|INSIDE FRONT COVER|
44|12||Automatic generation of MPEG test streams from high-level grammars|
44|12||Confidence intervals for captureârecapture estimations in software inspections|
44|12||On the efficiency of domain-based COTS product selection method|Use of commercial-off-the-shelf (COTS) products is becoming a popular software development method. Current methods of selecting COTS products involve using the intuition of software developers or use a direct assessment (DA) of the products. The former approach is subjective, whereas the latter approach is expensive. This high cost is because the efficiency of the DA approach is inversely proportional to the product of the number of modules in the system to be developed and the total number of modules in the candidate COTS products. With the increase in the number of available COTS components, the time spent on choosing the appropriate COTS products could easily offset the advantages of using them. Neither of the selection approaches mentioned leads to quality results. Furthermore, inappropriately chosen COTS components may cause much greater damage to a development project than faults in software units that are developed in-house. A domain model is a generic model of the domain of an application system. It captures all of the features and characteristics of the domain. We have developed a new indirect selection approach, called the domain-based COTS product selection method, which makes use of domain models. We have successfully applied our selection method to the development of an on-line margin trading application. In this paper, we first analyze the efficiency of the domain-based COTS product selection method qualitatively. Then, we study the efficiency of the method by means of a formal approach and also through the case study of the on-line margin trading application. All of these results show that the domain-based COTS product selection method is more efficient than the DA methods.
44|13|http://www.sciencedirect.com/science/journal/09505849/44/13|INSIDE FRONT COVER|
44|13||Source code analysis and manipulation|
44|13||Characterization and automatic identification of type infeasible call chains|Many software engineering applications utilize static program analyses to gain information about programs. Some applications perform static analysis over the whole program's call graph, while others are more interested in specific call chains within a program's call graph. A particular static call chain for an object-oriented program may in fact be impossible to execute, or infeasible, such that there is no input for which the chain will be taken. Identifying infeasible static call chains can save time and resources with respect to the targeted software development tool. This paper examines type infeasibility of call chains, which may be caused by inherently polymorphic call sites and are sometimes due to imprecision in call graphs. The problem of determining whether a call chain is type infeasible is defined and exemplified, and a key property characterizing type infeasible call chains is described. An empirical study was performed on a set of Java programs, and results from examining the call graphs of these programs are presented. Finally, an algorithm that automatically determines the type infeasibility of a call chain due to object parameters is presented.
44|13||Concurrent Ada dead statements detection|
44|13||Flow insensitive points-to sets|Pointer analysis is an important part of source code analysis. Many programs that manipulate source code take points-to sets as part of their input. Points-to related data collected from 27 mid-sized C programs (ranging in size from 1168 to 87,579 lines of code) is presented. The data shows the relative sizes and the complexities of computing points-to sets. Such data is useful in improving algorithms for the computation of points-to sets as well as algorithms that make use of this information in other operations.
44|13||Analyzing cloning evolution in the Linux kernel|Identifying code duplication in large multi-platform software systems is a challenging problem. This is due to a variety of reasons including the presence of high-level programming languages and structures interleaved with hardware-dependent low-level resources and assembler code, the use of GUI-based configuration scripts generating commands to compile the system, and the extremely high number of possible different configurations.
44|13||The documentary structure of source code|Many tools designed to help programmers view and manipulate source code exploit the formal structure of the programming language. Language-based tools use information derived via linguistic analysis to offer services that are impractical for purely text-based tools. In order to be effective, however, language-based tools must be designed to account properly for the documentary structure of source code: a structure that is largely orthogonal to the linguistic but no less important. Documentary structure includes, in addition to the language text, all extra-lingual information added by programmers for the sole purpose of aiding the human reader: comments, white space, and choice of names. Largely ignored in the research literature, documentary structure occupies a central role in the practice of programming. An examination of the documentary structure of programs leads to a better understanding of requirements for tool architectures.
44|13||A cache-aware program transformation technique suitable for embedded systems|
44|13||Semantic and behavioral library transformations|While software methodology encourages the use of libraries and advocates architectures of layered libraries, in practice the composition of libraries is not always seamless and the combination of two well-designed libraries not necessarily well designed, since it could result in suboptimal call sequences, lost functionality, or avoidable overhead. In this paper we introduce Simplicissimus, a framework for rewrite-based source code transformations that allows for code replacement in a systematic and safe manner. We discuss the design and implementation of the framework and illustrate its functionality with applications in several areas.
44|13||Web application transformations based on rewrite rules|During the evolution phase, the structure (pages and links) of a Web application tends unavoidably to degrade. A solution to reverse this degradation can be restructuring the Web application, but this work may take a lot of time and effort if conducted without appropriate tools.
44|13||Source transformation in software engineering using the TXL transformation system|Many tasks in software engineering can be characterized as source to source transformations. Design recovery, software restructuring, forward engineering, language translation, platform migration, and code reuse can all be understood as transformations from one source text to another. The tree transformation language, TXL, is a programming language and rapid prototyping system specifically designed to support rule-based source to source transformation. Originally conceived as a tool for exploring programming language dialects, TXL has evolved into a general purpose source transformation system that has proven well suited to a wide range of software maintenance and reengineering tasks, including the design recovery, analysis and automated reprogramming of billions of lines of commercial Cobol, PL/I, and RPG code for the Year 2000. In this paper, we introduce the basic features of modern TXL and its use in a range of software engineering applications, with an emphasis on how each task can be achieved by source transformation.
44|14|http://www.sciencedirect.com/science/journal/09505849/44/14|INSIDE FRONT COVER|
44|14||Special Issue for the Second AsiaâPacific Conference on Quality Software|
44|14||Optimal software testing and adaptive software testing in the context of software cybernetics|Software cybernetics explores the interplay between software theory/engineering and control theory/engineering. Following the idea of software cybernetics, the controlled Markov chains (CMC) approach to software testing treats software testing as a control problem. The software under test serves as a controlled object, and the (optimal) testing strategy determined by the theory of controlled Markov chains serves as a controller. This paper analyzes the behavior of the corresponding optimal test profile determined by the CMC approach to software testing and introduces adaptive software testing. It is shown that in some cases the optimal test profile is Markovian, whereas in some other cases the optimal test profile demonstrates a different scenario. The adaptive software testing adjusts software testing strategy on-line by using testing data collected during software testing in response to changes in our understanding of the software under test. Simulation results show that the adaptive strategy of software testing is feasible and significantly reduces the number of test cases required to detect and remove a certain number of software defects in comparison with the random strategy of software testing.
44|14||Detection of dynamic execution errors in IBM system automation's rule-based expert system|We formally verify aspects of the rule-based expert system of IBM's system automation software for IBM's zSeries mainframes. Starting with a formalization of the expert system in propositional dynamic logic (PDL), we encode termination and determinism properties in PDL and its extension ÎPDL. We then translate our decision problems to propositional logic and apply advanced SAT techniques for automated proofs. In order to locate real program bugs for each failed proof attempt, we apply extra formalization steps and represent propositional error formulae in concise normal form as binary decision diagrams. In our experiments, we revealed residual non-termination bugs in a tested program version close to shipment, and, after correcting them, we formally verified the absence of this class of bugs in the production code.
44|14||A CSP and Z combined modeling of document exchange processes in e-commerce protocols|E-commerce protocols comprise a vital component of the trading infrastructure over the Internet. Effectiveness and reliability of trading transactions likely depend on the quality of such protocols. However, building a quality e-commerce protocol may be more difficult than traditional protocols, as they are deployed in an open environment and data context-dependent. Application of formal methods has been suggested by many researchers as a viable approach for improving the reliability of software systems. But, can formal methods really help with the current crop of e-commerce protocols?
44|14||Software processes for the development of electronic commerce systems|The development of electronic commerce (EC) systems is subject to different conditions than that of conventional software systems. This includes the introduction of new activities to the development process and the removal of others. An adapted process must cope with important idiosyncrasies of EC system development: EC systems typically have a high degree of interaction, which makes factors like ergonomics, didactics and psychology especially important in the development of user interfaces. Typically, they also have a high degree of integration with existing software systems such as legacy or groupware systems. Integration techniques have to be selected systematically in order not to endanger the whole software development process. This article describes the development of an EC system and it generalizes salient features of the software process used. The result is a process model which can be used for other highly integrative EC system development projects. The processes described are determined by short process lifecycles, by an orientation towards integration of legacy systems and by a strict role-based cooperation approach.
44|15|http://www.sciencedirect.com/science/journal/09505849/44/15|INSIDE FRONT COVER|
44|15||An information-leak analysis system based on program slicing|For programs using secret information such as credit card numbers, preventing information-leaks is important. Denning, for example, has proposed a mechanism to certify that a given program does not violate a security policy. Kuninobu, on the other hand, has proposed a more practical framework for calculating the secrecy level of each output value from the secrecy level set to each input value, but no implementation has been yet explored. In this paper, we propose an implementation method for information-leak analysis, and show a system we have implemented based on program slicing. We have applied this system to a credit card program. Our results show that information-leak analysis before practical use of the program is important.
44|15||Comparison of artificial neural network and regression models for estimating software development effort|Estimating the amount of effort required for developing an information system is an important project management concern. In recent years, a number of studies have used neural networks in various stages of software development. This study compares the prediction performance of multilayer perceptron and radial basis function neural networks to that of regression analysis. The results of the study indicate that when a combined third generation and fourth generation languages data set were used, the neural network produced improved performance over conventional regression analysis in terms of mean absolute percentage error.
44|15||Generating three-tier applications from relational databases: a formal and practical approach|This article describes a method for building applications with a three-tier structure (presentation, business, persistence) from an existing relational database. The method works as a transformation function that takes the relational schema as its input, producing three sets of classes (which depend on the actual system being reengineered) to represent the final application, as well as some additional auxiliary classes (which are ‘constant’ and always generated, such as an ‘About’ dialog, for example). All the classes generated are adequately placed along the three-tiers.
44|15||author index|
44|15||keyword index|
44|15||volume contents|
44|2|http://www.sciencedirect.com/science/journal/09505849/44/2|Editorial|
44|2||Bases for the development of LAST: a formal method for business software requirements specification|
44|2||A methodology for designing toolkits for specification level verification of interval-constrained information systems requirements|
44|2||Braille to print translations for Chinese|In this paper, we study Braille word segmentation and transformation of Mandarin Braille to Chinese characters. The former consists of rule, sign and knowledge bases for disambiguation and mistake correction by using adjacent constraints and bi-directional maximal matching in which segmentation precision is better than 99%. The latter can be divided into two stages: Braille to Chinese pinyin (a phonemic Romanization) and pinyin to characters. By incorporating a pinyin knowledge dictionary into the system, we have perfectly solved the problem of ambiguity in the translation from Braille to pinyin and developed a statistical language model based on the transformation of pinyin to characters. By using Viterbi search, we have built a multi-level graph and found the sequence of Chinese characters with maximal likelihood. By using an N-Best algorithm to get the N most likely character sequences and probing into the means of measurement, our correct candidates within the top-five have a further improvement of 3%. By testing on 40,000 Chinese characters for the evaluation of the system performance, our overall translation precision of Braille codes to Chinese characters for common documents arrives at 94.38%; if proper nouns are not considered, our improvement reaches 2%.
44|2||Soft-link hypertext for information retrieval|
44|2||Non-specification-based approaches to logic testing for software|Testing is a crucial part of the development of software systems. In this paper, we consider testing of an implementation that is intended to satisfy a Boolean formula. In the literature, specification-based testing has been suggested for this purpose. Typically, such methods first hypothesize a fault class and then generate tests. However, there is almost no research that justifies fault classes proposed previously. Moreover, specifications amenable to automatic test generation are not always available to testers in practice. Based on these observations, we examine the applicability of non-specification-based approaches, which need no specification in the form of a Boolean formula to create tests. We compare a specification-based approach to three non-specification-based approaches, namely, random testing, antirandom testing, and combinatorial testing. The results of an experiment show that combinatorial testing is often comparative to specification-based testing and is superior to both random testing and antirandom testing.
44|2||An efficient dynamic program slicing technique|
44|2||Referee List|
44|3|http://www.sciencedirect.com/science/journal/09505849/44/3|Temporal fault trees|Fault tree (FT) is a simple, visual, popular and standardized notation for representing relationships between a fault in a system and the associated events. FTs are widely used for supporting products and systems in diverse industries like process control, avionics, aerospace, nuclear power systems, etc. where they are used to capture specialized and experiential knowledge for diagnosis and maintenance. FTs are also used to represent safety requirements of a system, obtained during the hazard analysis phase of the system development cycle. However, a problem that prevents more analytical use of FT is their lack of rigorous semantics. Users' understanding of an FT depends on the clarity and correctness of the natural language annotations used to label and describe various parts. Moreover, it is not clear how to adapt the FT notation to represent temporal relationships between faults and events in dynamic systems. We propose to augment the FT notation by adding simple temporal gates to capture temporal dependence between events and faults. We propose techniques to perform qualitative analysis of such temporal fault trees (TFT) to detect the causes of the top event fault by matching the TFT with the trace (or log) of the system activities. We present two algorithms for depth-first traversal and cut-set computations for a given TFT that can be used for diagnosis based on TFTs.
44|3||Methodologies for developing Web applications|The Internet has had a significant impact on the process of developing information systems. However, there has been little research that has examined specifically the role of the development methodologies in this new era. Although there are many new forces driving systems development, many other issues are extensions of problems that have been there for some years. This paper identifies the main requirements of methodologies for developing e-commerce applications. A number of e-commerce application development approaches are examined and a methodology is proposed which attempts to address a number of issues identified within the literature. The Internet commerce development methodology (ICDM) considers evolutionary development of systems, provides a business and strategic focus and includes a management structure in addition to covering the engineering aspects of e-commerce application development.
44|3||Extending the ODMG standard with views|
44|3||Development of CORBA-based engineering applications from legacy Fortran programs|
44|3||L'E-Lyee: coupling L'Ecritoire and LyeeALL|
44|4|http://www.sciencedirect.com/science/journal/09505849/44/4|Objects, XML and databases|
44|4||Mutant query plans|We propose a flexible and robust framework for distributed query processing based on mutant query plans (MQP). A MQP is an XML representation of a query plan that can also include verbatim XML data, references to resource locations (URLs), or abstract resource names (URNs). Servers work using local, possibly incomplete knowledge, partially evaluate as much of the query plan as they can, incorporate the partial results into a new, mutated query plan and transfer it to some other server that can continue processing. We have implemented an initial version of this framework, and present preliminary performance results.
44|4||Institutions: integrating objects, XML and databases|A general model theory based on institutions is proposed as a formal framework for investigating typed object-oriented, XML and other data models equipped with integrity constraints. A major challenge in developing such a unified model theory is in the requirement that it must be able to handle major structural differences between the targeted models as well as the significant differences in the logic bases of their associated constraint languages. A distinctive feature of this model theory is that it is transformation-oriented. It is based on structural transformations within a particular category of models or across different categories with a fundamental requirement that the associated constraints are managed in such a way that the database integrity is preserved.
44|4||An approach to high-level language bindings to XML|Values of existing typed programming languages are increasingly generated and manipulated outside the language jurisdiction. Instead, they often occur as fragments of XML documents, where they are uniformly interpreted as labelled trees in spite of their domain-specific semantics. In particular, the values are divorced from the high-level type with which they are conveniently, safely, and efficiently manipulated within the language.
44|4||V-DOM and P-XMLâtowards a valid programming of XML-based applications|Many WWW applications generate hypertext markup language or extensible markup language (XML) documents dynamically. Current tools, however, like languages using document object model (DOM) or JAVA Server Pages do not allow to check the validity of the generated documents statically. Instead, validity has to be ‘checked’ dynamically by appropriate test runs. This paper addresses this problem by introducing a new distinct class for each element type of a document type definition (DTD). Each class extends the Element-class of the DOM. The resulting object model is called validating DOM (V-DOM). Parametric XML (P-XML) is an extension of XML allowing to insert values of the newly defined classes in places, where the corresponding element types are allowed according to the underlying DTD. Like V-DOM, P-XML guarantees the validity of all XML documents generated by using P-XML. V-DOM and P-XML are illustrated by several examples.
44|4||Using meta-data to automatically wrap bioinformatics sources|Currently there are a huge number of bioinformatics sources available over the web. Accessing these sources manually is infeasible for individual biologists. Our goal is to provide a single point of access for scientists that will retrieve data from each applicable source. One fundamental problem is automating the retrieval of data from each site. We propose a meta-data description language to delineate both the steps required to retrieve data, as well as the mechanisms necessary to access the web site that contains the data. This description will enable the automatic generation of wrappers that can extract the appropriate data. Our meta-data language is based on DARPA Agent Markup Language-S (DAML-S), extending the description to include a grounding which details the mechanics of data access.
44|4||A method for the unification of XML schemata|Extensible Markup Language (XML) is a common standard for data representation and exchange over the Web. Considering the increasing need for managing data on the Web, integration techniques are required to access heterogeneous XML sources. In this paper, we describe a unification method for heterogeneous XML schemata. The input to the unification method is a set of object-oriented-based canonical schemata that conceptually abstract local Document Type Definitions of the involved sources. The unification process applies specific algorithms and rules to the concepts of the canonical schemata to generate a preliminary ontology. Further adjustments on this preliminary ontology generate a reference ontology that acts as a front-end for user queries to the XML sources.
44|4||XML schema mappings for heterogeneous database access|
44|4||ARK: an object(ive) view of extensible minimal languages for system configuration|This paper presents ARK, a minimal, XML-based configuration language and framework for our particular problem domain: UNIX System Administration. The language allows the capture of poorly structured configuration data and code fragments written in various programming languages, while the ARK engine is responsible for interpreting the XML and allowing applications to query data and invoke code fragments.
44|4||Management of XML documents without schema in relational database systems|Many applications deal with highly flexible XML documents from different sources, which makes it difficult to define their structure by a fixed schema or a DTD. Therefore, it is necessary to explore ways to cope with such XML documents. The paper analyzes different storage and retrieval methods for schemaless XML documents using the capabilities of relational systems. We compare our experiences with the implementation of an XML-to-relational mapping with an LOB implementation in a commercial RDBMS. The paper concludes with a vision of how different storage methods could converge towards a common high-level XML-API for databases.
44|5|http://www.sciencedirect.com/science/journal/09505849/44/5|Preface|
44|5||An architecture in support of universal access to electronic commerce|In order to increase accessibility to electronic commerce applications, an overall reduction in the complexity of interactions with computerized systems is required. This paper describes an architecture that uses personalization information to customize interactions with end-users in a way that reduces interaction complexity. The storage and manipulation of personal information under the control of the end-user for the dynamic customization of interactions requires a paradigm shift from the client–server model of computing to the more general peer-to-peer model. In this paper, the existing three-tier architecture is extended with a new, human-centric layer that is configurable by domain experts who are not necessarily computer science or computer engineering professionals, allowing for a ‘technology pull’ approach to the configuration of systems. The human-centric layer will support a process-oriented peer-to-peer interaction paradigm and will use existing middleware services for access to network services and legacy systems. This paper focuses on the conceptual model and the functional framework of this new human-centric layer.
44|5||Multi-item auctions for automatic negotiation|Available resources can often be limited with regard to the number of demands. In this paper we propose an approach for solving this problem, which consists of using the mechanisms of multi-item auctions for allocating the resources to a set of software agents. We consider the resource problem as a market in which there are vendor agents and buyer agents trading on items representing the resources. These agents use multi-item auctions, which are viewed here as a process of automatic negotiation, and implemented as a network of intelligent software agents. In this negotiation, agents exhibit different acquisition capabilities that let them act differently depending on the current context or situation of the market. For example, the ‘richer’ an agent is, the more items it can buy, i.e. the more resources it can acquire. We present a model for this approach based on the English auction, then we discuss experimental evidence of such a model.
44|5||Documenting electronic commerce systems and software using the unified modeling language|
44|5||Performance testing of a negotiation platform|Accessible from all over the world, the EC became an indispensable element to our society. It allows the use of electronic systems to exchange products, services and information between the different existent users. During these exchanges, it is very important to assure a good quality of service. However, the enormous expansion of the Internet users push its resources to the maximum of its limits, which provoke, in many cases, an important degradation in its performance. Consequently, it is primordial to analyze the capacity of servers in order to handle heavy workloads that are growing considerably as a function of the number of users. It is, therefore, necessary to conduct performance tests before servers' deployment in order to detect any imperfection and predict their behavior under stress.
44|6|http://www.sciencedirect.com/science/journal/09505849/44/6|Signature caching in parallel object database systems|
44|6||A model for availability analysis of distributed software/hardware systems|System availability is a major performance concern in distributed systems design and analysis. A typical kind of application on distributed systems has a homogeneously distributed software/hardware structure. That is, identical copies of distributed application software run on the same type of computers. In this paper, the system availability for this type of system is studied. Such a study is useful when studying optimal testing time or testing resource allocation. We consider both the case of simple two-host system, and also the more general case of multi-host system. A Markov model is developed and equations are derived to obtain the steady-state availability. Both software and hardware failures are considered, assuming that software faults are constantly being identified and removed upon a failure. Although a specific model for software reliability is used for illustration, the approach is a general one. Comparisons show that system availability changes in a similar way to single-host based software/hardware systems. Sensitivity analysis is also presented. In addition, the assumptions used in this paper are discussed.
44|6||An evaluation of the impact of component-based architectures on software reusability|Component-based software development offers a promising solution to the production of complex distributed large-scale software systems. Development for reuse—the production of reusable components—and development with reuse—the production of systems with reusable components—provide the characteristics necessary to break the complexity of large-scale distributed systems. Two criteria for reuse in component-based architectures (CBAs) include inter-operability (focus on development for reuse) and integration (focus on development with reuse). Interoperability concerns how well components interact and integration defines how well components plug and play. The objective of this work is to evaluate the impact of three popular CBAs, namely, Enterprise Java Beans, Distributed interNetwork Architecture, and Object Management Architecture on reusability. A framework is introduced for a systematic and comprehensive analysis and evaluation of CBAs. The proposed framework is used to determine which of the above architectures more effectively addresses integration and interoperability. The results allow businesses to determine which CBA, of the above three, is ideal for reuse for a particular application. Further research opportunities in this area are discussed at the end.
44|6||Modeling software architectures with goals in virtual university environment|
44|6||Methodologies and website development: a survey of practice|Website development work is a growing aspect of the IT activities within many organisations. However, the manner in which website development actually takes place within organisations is still largely uncertain. In this paper, we examine the results of a research exercise involving case studies in 25 UK organisations aimed at investigating the way in which website development activities are currently carried out within UK organisations. In particular, this paper discusses the activities that are typically involved in website development projects, and the techniques and standards actually used for website development found within 25 organisations studied. One of the main findings of the research project was that in 25 organisations studied there was only limited use of formalised website design techniques (mainly hierarchy charts and storyboards). However, roughly half of the organisations studied did use some form of website layout standards. Website documentation was only produced in roughly a third of the organisations and only roughly a quarter of the organisations had any formalised website testing procedures in place.
44|6||Erratum to âBases for the development of LAST: a formal method for business software requirements specificationâ: [Information and Software Technology, 44 (2002) 65â75]|
44|7|http://www.sciencedirect.com/science/journal/09505849/44/7|A framework for evaluating the effectiveness of real-time object-oriented models|The design of a real-time system needs to incorporate methods specifically developed to represent the temporal properties of the system under consideration. Real-time systems contain time and event driven actions. Structured design methods provided a reasonable set of abstractions for design of time and event driven factors in real-time designs. As program complexity, size, and time to market pressure grows, the real-time community migrated towards object-oriented technology. Evidence suggests that object-oriented technology in non-real-time systems is most effective in abstraction, modeling, implementation, and reuse of software systems. Many design models and methods exist for object-oriented real-time designs. However, the selection process of a model for a particular application remains a tedious task. This paper introduces an analysis framework that can be applied to a design model to evaluate its effectiveness according to desired performance specifications. To illustrate our approach, we present a case study using the popular automotive cruise control example on two real-time object-oriented models.
44|7||Critical path identification in the context of a workflow|The concept of the critical path has been widely discussed in many areas of computer engineering such as parallel and distributed programs, a computer circuit, and a directed acyclic graph. The critical path analyzes in a workflow can allow us to utilize it in many workflow issues, especially workflow resource management and workflow time management. In this paper, we first describe our workflow model using the workflow queuing network. Then, we propose a method to systematically determine the critical path under the workflow model, and give an overall example that shows how our method works. In addition, several experiments are made to evaluate the accuracy of the critical path found.
44|7||The clustering property of corner transformation for spatial database applications|Spatial access methods (SAMs) are often used as clustering indexes in spatial database systems. Therefore, a SAM should have the clustering property both in the index and in the data file. In this paper, we argue that corner transformation preserves the clustering property such that objects having similar sizes and positions in the original space tend to be placed in the same region in the transform space. We then show that SAMs based on corner transformation are able to maintain clustering both in the index and in the data file for storage systems with fixed object positions and propose the MBR-MLGF as an example to implement such an index. In the storage systems with fixed object positions, the inserted objects never move during the operation of the system. Most storage systems currently available adopt this architecture. Extensive experiments comparing with the R∗-tree show that corner transformation indeed preserves the clustering property, and therefore, it can be used as a useful method for spatial query processing. This result reverses the common belief that transformation will adversely affect the clustering and shows that the transformation maintains as good clustering in the transform space as conventional techniques, such as the R∗-tree, do in the original space.
44|7||Reduction-based methods and metrics for selective regression testing|In corrective maintenance, modified software is regression tested using selected test cases in order to ensure that the modifications have not caused adverse effects. This activity of selective regression testing involves regression test selection, which refers to selecting test cases from the previously run test suite, and test-coverage identification. In this paper, we propose three test-selection methods and two coverage identification metrics. The three methods aim to reduce the number of selected test cases for retesting the modified software. The first method, referred to as modification-based reduction version 1 (MBR1), selects a reduced number of test cases based on the modification made and its effects in the software. The second method, referred to as modification-based reduction version 2 (MBR2) improves MBR1 by further omitting tests that do not cover the modification. The third method, referred to as precise reduction (PR), reduces the number of test cases selected by omitting non-modification-revealing tests from the initial test suite. The two coverage metrics are McCabe-based regression test metrics, which are referred to as the Reachability regression Test selection McCabe-based metric (RTM), and data-flow Slices regression Test McCabe-based metric (STM). These metrics aim to assist the regression tester in monitoring test-coverage adequacy, reveal any shortage or redundancy in the test suite, and assist in identifying, where additional tests may be required for regression testing.
44|8|http://www.sciencedirect.com/science/journal/09505849/44/8|A process matching approach for flexible workflow process reuse|Matching between two workflow processes is the key step of workflow process reuse. This paper presents an inexact matching approach for flexible workflow process reuse. A multi-valued process specialization relationship is defined based on the definition of activity specialization and the characteristics of workflow process. The matching degree between two workflow processes is determined by the matching degrees of their corresponding sub-processes or activities. The matching degree between two activities is determined by the activity-distance between them in the activity-ontology repository. A set of process specialization rules enables a new process matching to be derived from the existing matchings. Users are provided with an SQL-like command to retrieve the required processes in an inexact query condition from the workflow-process-ontology repository.
44|8||A general rule structure|
44|8||Architectural styles for distributed processing systems and practical selection method|The software architecture of a system has influences against various software characteristics of the system such as efficiency, reliability, maintainability, etc. For supporting to design the software architecture, we have developed architectural styles for distributed processing systems. The styles classify the architecture for distributed processing systems into nine categories based on the location of data storage and the type of processing between a client and a server. This paper describes our architectural styles and proposes a simple but practical method to select an appropriate architectural style for developing an application system. The selection method introduces the characterization of architectural styles and the characteristic charts to visualize their characteristics of architectural styles. Next, we propose a method to select an appropriate architectural style using the conformity between characteristic charts of a system and architectural styles. We have verified the applicability of this selection method using our customers' real application systems.
44|8||A methodology of testing high-level Petri nets|Petri nets have been extensively used in the modelling and analysis of concurrent and distributed systems. The verification and validation of Petri nets are of particular importance in the development of concurrent and distributed systems. As a complement to formal analysis techniques, testing has been proven to be effective in detecting system errors and is easy to apply. An open problem is how to test Petri nets systematically, effectively and efficiently. An approach to solve this problem is to develop test criteria so that test adequacy can be measured objectively and test cases can be generated efficiently, even automatically. In this paper, we present a methodology of testing high-level Petri nets based on our general theory of testing concurrent software systems. Four types of testing strategies are investigated, which include state-oriented testing, transition-oriented testing, flow-oriented testing and specification-oriented testing. For each strategy, a set of schemes to observe and record testing results and a set of coverage criteria to measure test adequacy are defined. The subsumption relationships and extraction relationships among the proposed testing methods are systematically investigated and formally proved.
44|8||Research in software engineering: an analysis of the literature|In this paper, we examine the state of software engineering (SE) research from the point of view of the following research questions:
44|9|http://www.sciencedirect.com/science/journal/09505849/44/9|Application of traditional system design techniques to web site design|After several decades of computer program construction there emerged a set of principles that provided guidance to produce more manageable programs. With the emergence of the plethora of Internet web sites one wonders if similar guidelines are followed in their construction. Since this is a new technology no apparent universally accepted methods have emerged to guide the designer in Web site construction. This paper reviews the traditional principles of structured programming and the preferred characteristics of Web sites. Finally a mapping of how the traditional guidelines may be applied to Web site construction is presented. The application of the traditional principles of structured programming to the design of a Web site can provide a more usable site for the visitors to the site. The additional benefit of using these time-honored techniques is the creation of a Web site that will be easier to maintain by the development staff.
44|9||Anatomy of the coupling query in a web warehouse|To populate a data warehouse specifically designed for Web data, i.e. web warehouse, it is imperative to harness relevant documents from the Web. In this paper, we describe a query mechanism called coupling query to glean relevant Web data in the context of our web warehousing system called Warehouse Of Web Data (WHOWEDA). Coupling query may be used for querying both HTML and XML documents. Some of the important features of our query mechanism are ability to query metadata, content, internal and external (hyperlink) structure of Web documents based on partial knowledge, ability to express constraints on tag attributes and tagless segment of data, ability to express conjunctive as well as disjunctive query conditions compactly, ability to control execution of a web query and preservation of the topological structure of hyperlinked documents in the query results. We also discuss how to formulate query graphically and in textual form using coupling graph and coupling text, respectively.
44|9||Access privilege management in protection systems|
44|9||QoS-adaptive bandwidth scheduling in continuous media streaming|
45|1|http://www.sciencedirect.com/science/journal/09505849/45/1|Editorial Board|
45|1||Introduction of a web-based submission, tracking and review system|
45|1||Fault-based testing without the need of oracles|There are two fundamental limitations in software testing, known as the reliable test set problem and the oracle problem. Fault-based testing is an attempt by Morell to alleviate the reliable test set problem. In this paper, we propose to enhance fault-based testing to alleviate the oracle problem as well. We present an integrated method that combines metamorphic testing with fault-based testing using real and symbolic inputs.
45|1||An efficient inverted index technique for XML documents using RDBMS|The inverted index is widely used in the existing information retrieval field. In order to support containment queries for structured documents such as XML, it needs to be extended. Previous work suggested an extension in storing the inverted index for XML documents and processing containment queries, and compared two implementation options: using an RDBMS and using an Information Retrieval (IR) engine. However, the previous work has two drawbacks in extending the inverted index. One is that the RDBMS implementation is generally much worse in the performance than the IR engine implementation. The other is that when a containment query is processed in an RDBMS, the number of join operations increases in proportion to the number of containment relationships in the query and a join operation always occurs between large relations. In order to solve these problems, we propose in this paper a novel approach to extend the inverted index for containment query processing, and show its effectiveness through experimental results. In particular, our performance study shows that (1) our RDBMS approach almost always outperforms the previous RDBMS and IR approaches, (2) our RDBMS approach is not far behind our IR approach with respect to performance, and (3) our approach is scalable to the number of containment relationships in queries. Therefore, our results suggest that, without having to make any modifications on the RDBMS engine, a native implementation using an RDBMS can support containment queries as efficiently as an IR implementation.
45|1||Efficient storage and querying of sequential patterns in database systemsâ|The number of patterns discovered by data mining can become tremendous, in some cases exceeding the size of the original database. Therefore, there is a requirement for querying previously generated mining results or for querying the database against discovered patters. In this paper, we focus on developing methods for the storage and querying of large collections of sequential patterns. We describe a family of algorithms, which address the problem of considering the ordering among elements, that is crucial when dealing with sequential patterns. Moreover, we take into account the fact that the distribution of elements within sequential patterns is highly skewed, to propose a novel approach for the effective encoding of patterns. Experimental results, which examine a variety of factors, illustrate the efficiency of the proposed method.
45|1||Probability graph based data hoarding for mobile environment|
45|1||Case studies to evaluate a domain specific application framework based on complexity and functionality metrics|An object-oriented framework is the reusable design or a system or subsystem implemented through a collection of concrete and abstract classes and their collaboration. It provides a generic solution to a set of similar problems in an application domain. However, it is difficult to introduce it to the organization that has been using a traditional reuse method. For effective technology transfer, the usefulness of the framework should be evaluated and shown to the organization. This paper evaluates the usefulness of a domain specific business application framework from a viewpoint of saving cost and quality of the software in a company. Here, we conducted two case studies. In the case studies, four kinds of applications are developed. Each of them is developed in two ways: using framework-based reuse and conventional module-based reuse. Then, we evaluate the difference among them using the several functionality and complexity metrics. As the results, the framework-based reuse would be more efficient than the module-based reuse in the company.
45|1||On the use of Bayesian belief networks for the prediction of software productivity|In spite of numerous methods proposed, software cost estimation remains an open issue and in most situations expert judgment is still being used. In this paper, we propose the use of Bayesian belief networks (BBNs), already applied in other software engineering areas, to support expert judgment in software cost estimation. We briefly present BBNs and their advantages for expert opinion support and we propose their use for productivity estimation. We illustrate our approach by giving two examples, one based on the COCOMO81 cost factors and a second one, dealing with productivity in ERP system localization.
45|10|http://www.sciencedirect.com/science/journal/09505849/45/10|Editorial Board|
45|10||An experiment in software component retrieval|Our research centers around exploring methodologies for developing reusable software, and developing methods and tools for building inter-enterprise information systems with reusable components. In this paper, we focus on an experiment in which different component indexing and retrieval methods were tested. The results are surprising. Earlier work had often shown that controlled vocabulary indexing and retrieval performed better than full-text indexing and retrieval [IEEE Trans. Software Engng (1994) 1, IEEE Trans. Software Engng 17 (1991) 800], but the differences in performance were often so small that some questioned whether those differences were worth the much greater cost of controlled vocabulary indexing and retrieval [Commun. Assoc. Comput. Mach. 28 (1985) 289, Commun. Assoc. Comput. Mach. 29 (1986) 648]. In our experiment, we found that full-text indexing and retrieval of software components provided comparable precision but much better recall than controlled vocabulary indexing and retrieval of components. There are a number of explanations for this somewhat counter-intuitive result, including the nature of software artifacts, and the notion of relevance that was used in our experiment. We bring to the fore some fundamental questions related to reuse repositories.
45|10||PageGen: an effective scheme for dynamic generation of web pages|We present a scheme for dynamic generation of web pages. The scheme separates presentation from content. Furthermore, by utilizing the theme metaphor, the scheme makes it easy to develop a web site with several design themes, each having its own template, graphics and style sheet characteristics. The proposed scheme relies on versatile substitution mechanisms, which nonetheless use simplified syntax. Most importantly, the scheme utilizes XML for defining custom tags that are transformed into HTML using the innovative concept of HTML patterns. The scheme was initially implemented as a COM component (PageGen) and later ported to Microsoft .NET. it has proven to be quite effective for Active Server Pages (and ASP.NET) sites used to host online books and course material. However, the scheme is general enough for use with any database-centric site or content as well as being adapted to other web application frameworks such as PHP and JSP.
45|10||A new approach to verify rule-based systems using petri nets|
45|10||Efficient management of inspections in software development projects|
45|10||Operator programs and operator processes|
45|10||MAPBOT: a Web based map information retrieval system|
45|11|http://www.sciencedirect.com/science/journal/09505849/45/11|Editorial Board|
45|11||Preface|
45|11||Distributed system requirement modeling with message sequence charts: the case of the RMTP2 protocol|This document describes a case study realized for the INTERVAL european project. The aim of this study was to test time features of MSC'2000 and the real time extensions proposed by the INTERVAL partners The starting point for this study, was an IETF document describing requirements for a multicast protocol called RMTP2. Some significant requirements could not be modeled with MSC 2000, which lead to extension proposals for MSC. The main extension proposed is the introduction of multicast communications in MSC.
45|11||Implicit integration of scenarios into a reduced timed automaton|We aim at synthesizing an executable specification for a real-time reactive system by integrating real-time scenarios into a reduced timed automaton (TA). A scenario is a part of the specification of a system behavior. The integration of scenarios into a single TA is based on its formal semantics. The TA, which results from the integration of a set of scenarios, is independent of the order in which the scenarios are added to. We also present an algorithm to reduce such resulting TA in order to prevent combinatorial explosion.
45|11||Consolidating and applying the SDL-pattern approach: a detailed case study|This paper is on design methodology for communication systems. The SDL-pattern approach proposed recently is consolidated and applied rigorously and in detail to the design of a typical communication system on two levels of abstraction. The design is decomposed into a number of steps, each of which is carried out systematically, building on well-proven, generic pieces of solutions that have proven useful in previous projects. These generic solutions—termed SDL patterns—support reuse-driven design of communication systems, raise the vocabulary of protocol engineer to a problem-oriented level, assist the discovery and exploitation of commonalities, and lead to well-justified designs. The selection and use of SDL patterns is supported by a fine-grained incremental design process, the pattern definition takes advantage of formal design languages, and a set of heuristics addresses the decomposition of communication requirements. All these elements are presented and applied in detail to the design of a simple, but functionally complete communication system.
45|11||A rigorous approach for constructing self-evolving real-time reactive systems|
45|11||Formal verification and validation for e-commerce: theory and best practices|In this paper, we describe a formal, model-driven, CORBA-based approach to developing and testing e-commerce systems. We indicate advantages and limitations of formal verification techniques using the Specification and Description Language (SDL), and relate the CORBA-based distributed object architecture to standard test methods and TTCN, the international standard test language. Finally, we enumerate industrial challenges and best practices at one of the electronic commerce software test organizations in IBM, and suggest a strategy for adapting formal test methods to the evolving industrial e-commerce test process.
45|11||Bisimulation-based non-deterministic admissible interference and its application to the analysis of cryptographic protocols|In this paper, we first define bisimulation-based non-deterministic admissible interference (BNAI), derive its process-theoretic characterisation and present a compositional verification method with respect to the main operators over communicating processes, generalising in this way the similar trace-based results obtained [J. Univ. Comput. Sci. 6 (2000) 1054] into the finer notion of observation-based bisimulation [Logic and Models of Concurrent Systems, 1985]. Like its trace-based version, BNAI admits information flow between secrecy levels only through a downgrader (e.g. a cryptosystem), but is phrased into a generalisation of observational equivalence [Communication and Concurrency, 1989]. We then describe an admissible interference-based method for the analysis of cryptographic protocols, extending, in a non-trivial way, the non-interference-based approach presented by Focardi et al. [Proceedings of DERA/RHUL Workshop on Secure Architectures and Information Flow, 2000]. Confidentiality and authentication for cryptoprotocols are defined in terms of BNAI and their respective bisimulation-based proof methods are derived. Finally, as a significant illustration of the method, we consider simple case studies: the paradigmatic examples of the Wide Mouthed Frog protocol [ACM Trans. Comput. Syst. 8 (1990) 18] and the Woo and Lam one-way authentication protocol [IEEE Comput. 25 (1992) 39]. The original idea of this methodology is to prove that the intruder may interfere with the protocol only through selected channels considered as admissible when leading to harmless interference.
45|12|http://www.sciencedirect.com/science/journal/09505849/45/12|Editorial Board|
45|12||Preface|
45|12||UIO sequence based checking sequences for distributed test architectures|
45|12||Synthesis of distributed testers from true-concurrency models of reactive systems|
45|12||Experience in developing and testing network protocol software using FDTs|This paper presents the research effort to formally specify, develop and test a complex real-life protocol for mobile network radios (MIL-STD 188-220). As a result, the team of researchers from the University of Delaware and the City College of the City University of New York, collaborating with scientists from CECOM (an R&D facility of the US Army) and the US Army Research Laboratory, have helped advance the state-of-the-art in the design, development, and testing of wireless communications protocols. Estelle is used both as the formal specification language for MIL-STD 188-220 and the source to automatically generate conformance test sequences. The formal test generation effort identified several theoretical problems for wireless communication protocols (possibly applicable to network protocols in general): (1) the timing constraint problem, (2) the controllability problem, (3) inconsistency detection and elimination problem and (4) the conflicting timers problem. Based on the collaborative research results, two software packages were written to generate conformance test sequences for MIL-STD 188-220. These packages helped generate tests for MIL-STD 188-220’s Data Link Types 1 and 4 services that were realizable without timer interruptions while providing a 200% increase in test coverage. The test cases have been delivered and are being used by a CECOM conformance testing facility.
45|12||New approaches for passive testing using an Extended Finite State Machine specification|This paper presents two new approaches for passive testing using an Extended Finite State Machine (EFSM) specification. The state of the art of passive testing shows us that all the methods for detection of errors based on EFSMs try to match the trace to the specification. Indeed, one searches a state succession in the specification machine that is able to generate the trace observed on the implementation. Using this approach, processing is performed on the specification and the trace remains in the background since no operation is applied to it. This made us realise that focusing our efforts on the trace could be beneficial and has given as result two approaches presented in this paper that extract information from the specification and then work on the trace. Thus, they take a different direction than the previous methods. We first present an approach to test traces by using invariants resulting from the specification. We formally define these invariants and we see how to extract them. We also discuss their ability to detect errors appearing in the implementation. This approach is able to test the data flow, but not in a very satisfactory way. This is the reason for a second approach seeking to apply a set of constraints to the trace. We develop in detail its principles. Both approaches are applied to a Simple Connection Protocol (SCP) and the results of preliminary experiments are presented.
45|12||Testing distributed real-time systems in the presence of inaccurate clock synchronizations|
45|12||Fault detection in Rule-based Software systems|
45|12||Formal verification of dependable distributed protocols|Dependable distributed systems often employ a hierarchy of protocols to provide timely and reliable services. Such protocols have both dependability and real-time attributes, and the verification of such composite services is a problem of growing complexity even when using formal approaches. Our intention in this paper is to exploit the modular design aspects appearing in most dependable distributed protocols to provide formal level of assurance for their correctness. We highlight the capability of our approach through a case study in formal modular specification and tool-assisted verification of a timestamp-based checkpointing protocol. Furthermore, during the process of verification, insights gained in such a stack of protocols have assisted in validating some additional properties those dealing with failure recovery.
45|13|http://www.sciencedirect.com/science/journal/09505849/45/13|Editorial Board|
45|13||Architecture layers and engineering approach for agent-based system|It is necessary to support user-centric service provision paradigm in distributed, dynamic and complex computing environment. Software agent technology is considered as one of the technologies suitable to adopt such computing environment. Many researchers have emphasized on agent-based system development, but, many agent-based systems are designed and constructed in ad hoc. In particular, they do not enough consider system organization and performance aspects. More systematic engineering approach of agent-based system is required. We propose the layered architecture and engineering approach for agent-based system design. We devise the layers necessary to design agent-based system, and methods to engineer each layer. Also we show that the devised approach can be used to design agent-based system and analyze system features. The layered architecture and engineering approach of agent-based system proposed in this paper support that engineer designs efficient agent-based system.
45|13||XDependency: maintaining relationships between XML data resources|This paper proposes an XDependency description model and language that can be used as part of a distributed integrity control system for XML data resources. The dependency description language allows relationships to be specified between XML data available from different organizations. The paper also includes a prototype agent-based system capable of enforcing distributed dependency relationships. An XDependency maintenance system should make it possible to automate the distributed integrity control for XML resources.
45|13||Collaboration and coordination in process-centered software development environments: a review of the literature|Process-centered software development environments are systems that provide automated support for software development activities.
45|13||Conceptual framework and architecture for service mediating workflow management|This paper proposes a three-layer workflow concept framework to realize workflow enactment flexibility by dynamically binding activities to their implementations at run time. A service mediating layer is added to bridge business process definition and its implementation. Based on this framework, we propose an extended workflow management systems architecture, in which service contracting layer, service binding layer and service invocation layer are extensions to support the proposed service mediating concept. According to an enactment specification, different instances of the same activity can be bound to different services to achieve enactment flexibility. The conceptual framework and architecture together provide a comprehensive approach to flexible service enactment in B2B collaborative settings.
45|14|http://www.sciencedirect.com/science/journal/09505849/45/14|Editorial Board|
45|14||Eighth International Workshop on Requirements EngineeringâFoundation for Software Quality (REFSQ'02)|
45|14||The fundamental nature of requirements engineering activities as a decision-making process|The requirements engineering (RE) process is a decision-rich complex problem solving activity. This paper examines the elements of organization-oriented macro decisions as well as process-oriented micro decisions in the RE process and illustrates how to integrate classical decision-making models with RE process models. This integration helps in formulating a common vocabulary and model to improve the manageability of the RE process, and contributes towards the learning process by validating and verifying the consistency of decision-making in RE activities.
45|14||A controlled experiment to evaluate how styles affect the understandability of requirements specifications|This paper presents a controlled experiment in which two different requirements specification styles (white-box and black-box) were compared concerning the understandability of two requirements specifications from the viewpoint of a customer. The results of the experiment confirm the common belief that black-box requirements specifications (e.g., documented with SCR) are easier to understand from a customer point of view than white-box specifications (e.g., documented with UML). Questions about particular functions and behavior of the specified system were answered faster and more correctly by the participants. This result suggests that using a black-box specification style when communicating with customers is beneficial.
45|14||Precluding incongruous behavior by aligning software requirements with security and privacy policies|Keeping sensitive information secure is increasingly important in e-commerce and web-based applications in which personally identifiable information is electronically transmitted and disseminated. This paper discusses techniques to aid in aligning security and privacy policies with system requirements. Early conflict identification between requirements and policies enables analysts to prevent incongruous behavior, misalignments and unfulfilled requirements, ensuring that security and privacy are built in rather than added on as an afterthought. Validated techniques to identify conflicts between system requirements and the governing security and privacy policies are presented. The techniques are generalizable to other domains, in which systems contain sensitive information.
45|14||Modelling access policies using roles in requirements engineering|Pressures are increasing on organisations to take an early and more systematic approach to security. A key to enforcing security is to restrict access to valuable assets. We regard access policies as security requirements that specify such restrictions. Current requirements engineering methods are generally inadequate for eliciting and analysing these types of requirements, because they do not allow complex organisational structures and procedures that underlie policies to be represented adequately. This paper discusses roles and why they are important in the analysis of security. The paper relates roles to organisational theory and how they could be employed to define access policies. A framework is presented, based on these concepts, for analysing access policies.
45|14||On the interplay between consistency, completeness, and correctness in requirements evolution|The initial expression of requirements for a computer-based system is often informal and possibly vague. Requirements engineers need to examine this often incomplete and inconsistent brief expression of needs. Based on the available knowledge and expertise, assumptions are made and conclusions are deduced to transform this ‘rough sketch’ into more complete, consistent, and hence correct requirements. This paper addresses the question of how to characterize these properties in an evolutionary framework, and what relationships link these properties to a customer's view of correctness. Moreover, we describe in rigorous terms the different kinds of validation checks that must be performed on different parts of a requirements specification in order to ensure that errors (i.e. cases of inconsistency and incompleteness) are detected and marked as such, leading to better quality requirements.
45|15|http://www.sciencedirect.com/science/journal/09505849/45/15|Editorial Board|
45|15||Special issue on modelling organisational processes|
45|15||Business processesâattempts to find a definition|Definitions of business process given in much of the literature on Business Process Management are limited in depth and their related models of business processes are correspondingly constrained. After giving a brief history of the progress of business process modeling techniques from production systems to the office environment, this paper proposes that most definitions are based on machine metaphor type explorations of a process. While these techniques are often rich and illuminating it is suggested that they are too limited to express the true nature of business processes that need to develop and adapt to today's challenging environment.
45|15||Active meta-process models: a conceptual exposition|The difficult and complex interactions between an organisation and its Information and Communications Technology (ICT) systems have presented process modelling research with a very fundamental challenge. From these concerns has emerged the concept of the active meta-process model. This sets out a conceptual basis for understanding the organisation/ICT relationship, and has the potential to influence ICT developments. The central concern of this paper is to develop the theoretical grounding of the idea of the active meta-process model. This concept has its lineage in process modelling research. Fundamentally, it is concerned with the relationship between organisations and ICT systems and, in particular, the notion that ICT systems should be viewed as providing an active canvas supporting the organisation. A conceptual exposition is developed by reference to cybernetic theory through the Viable System Model. The paper advocates an holistic and systemic approach to the comprehension and realisation of the ICT/organisational relationship which necessarily requires (a) an understanding of all of the behaviours of an organisation (including specifically the meta behaviours) and (b) a systemic and architectural view of how ICT systems should be formed in order to address the needs of organisations as systems. The outcome of this is a superior conceptual grounding for the notion of active meta-process models, and insights into how the concept might influence practice.
45|15||Using Alloy in process modelling|
45|15||Traceability in requirements through process modelling, applied to social care applications|This paper relates experiences of using a business-process approach to the determination of requirements for social care systems. A method has been developed and used successfully with a number of major research projects, most specifically PLANEC. A protocol and framework are presented that utilise the Unified Modelling Language and adopts best practice from IT and social science methods. It utilises a loose-coupled hierarchical grouping of processes as a strategic view, and more tightly coupled models such as workflows. The method, as it has evolved, has produced a clear linkage between stakeholder goals and expectations, and IT functionality expressed as UML use cases.
45|15||Bridging the gap between business models and system models|This paper discusses links that may be made between process models and Unified Modelling Language (UML) software specification techniques, working from an argument that the whole complexity of organisational activity cannot be captured by UML alone. The approach taken is to develop a set of use cases, which would be capable of providing information support to a pre-defined organisational process. The nature of the thinking, which is necessary to derive the use cases, is outlined using the pre-defined process as a case study. The grouping of transactions and state changes into Use Cases is shown to require design choices, which may vary between particular organisational contexts. Conclusions are drawn about the direction of further investigation of links between process modelling and UML.
45|15||Flexible B2B processes: the answer is in the nodes|The time and costs involved in connecting the IT systems of two companies impact the actual formation of business relationships. A flexible infrastructure for process management is instrumental for rapid and cost-effective B2B integration. One dimension of flexibility that system integrators identify as critical is node-level interaction. In this paper, we discuss the findings of the Nile project on B2B process integration. In particular, we present the methodology defined in Nile for the semi-automatic reconciliation of node-level incompatibilities.
45|15||Business process managementâthe third wave: business process modelling language (bpml) and its pi-calculus foundations|This paper introduces the ideas behind BPML, the business process modelling language published by BPMI. BPML provides a process-centric (as opposed to a datacentric) metalanguage and execution model for business systems. It is underpinned by a strong mathematical foundation, the pi-calculus. The current paper is derived from supplementary appendices to a book which describes a ‘third wave’ approach to business process management [Business Process Management: The Third Wave, 2003]. The aim is to model business processes directly in an executable form, so that the mobility and mutability inherent in business behaviour is reflected and supported in the corresponding IT systems, erasing the present IT-business divide.
45|15||Preconditions for putting processes back in the hands of their actors|Enterprises have now been crushed by ‘enterprise solutions’. The enterprise's information is back in the hands of the IT department. Will the same happen to the enterprise's processes? Or can computer support of those processes be put into the hands of the people who carry out the processes, who need to change them, combine them, separate them, and spread them? What properties must our business process management methods and technology have to make this possible? This paper identifies four.
45|15||Author Index|
45|15||keyword Index|
45|15||Volume Contents|
45|2|http://www.sciencedirect.com/science/journal/09505849/45/2|Editorial Board|
45|2||A multi-method for defining the organizational change|The assumption of the work presented in this paper is the situatedness of the change process. The Enterprise Knowledge Development-Change Management Method (EKD-CMM) provides multiple and dynamically constructed ways of working to organize and to guide the change management. The method is built on the notion of labeled graph of intentions and strategies called a road map and the associated guidelines. The EKD-CMM road map is a navigational structure that supports the dynamic selection of the intention to be achieved next and the appropriate strategy to achieve it whereas guidelines help in the operationalization of the selected intention following the selected strategy. This paper presents the EKD-CMM road map and guidelines and exemplifies their use with a real case study.
45|2||From use cases to classes: a way of building object model with UML|In a use case-driven process, classes in the class diagram need to be identified from use cases in the use case diagram. Current object modelling approaches identify classes either from use case descriptions, or using classic categories. Both ways are inefficient when use cases can be described with many scenarios in different words. This paper represents a new approach that identifies classes based on goals of use cases without descriptions. The approach produces use case-entity diagrams as a vehicle for deriving classes from use cases and to show the involvement of classes in use cases of a system.
45|2||Building XML application in rich detailed genealogical information|Genealogical information is rich and complex [2]. Numerous studies showed that XML technologies could be employed to deal with complex information systems [15]. This paper reports an investigation of building XML application in rich and complex genealogical information. XML technologies, such as XML DTD, schema, XSL and XPath, were employed to tackle this task. Three commercial software packages are used to obtain reference results for further comparison. It is found that XML application could store more specific data than those in the evaluated software; further more, it appears to offer more diverse display options compared with the evaluated software packages. Finally, it concluded that XML application is the better approach for dealing with rich detailed genealogical information and displaying them upon Internet.
45|2||Empirical studies of some hashing functions|The best hash function for a particular data set can often be found by empirical studies. The studies reported here are aimed at discovering the most appropriate function for hashing Nigerian names. Five common hash functions—the division, multiplication, midsquare, radix conversion and random methods—along with two collision—handling techniques—linear probing and chaining—were initially tried out on three data sets, each consisting of about 1000 words. The first data set consists of Nigerian names, the second of English names, and the third of words with computing associations. The major finding is that the performance of these functions with the Nigerian names is comparable to those for the other data sets. Also, the superiority of the random and division methods over others is confirmed, even though the division method will often be preferred for its ease of computation. It is also demonstrated that chaining, as a technique for collision-handling, is to be preferred. The hash methods and collision-handling methods were further tested by using much larger data sets and long multiple word strings. These further tests confirmed the previous findings.
45|3|http://www.sciencedirect.com/science/journal/09505849/45/3|Editorial Board|
45|3||A computational argumentation methodology for capturing and analyzing design rationale arising from multiple perspectives|
45|3||An effort prediction interval approach based on the empirical distribution of previous estimation accuracy|When estimating software development effort, it may be useful to describe the uncertainty of the estimate through an effort prediction interval (PI). An effort PI consists of a minimum and a maximum effort value and a confidence level. We introduce and evaluate a software development effort PI approach that is based on the assumption that the estimation accuracy of earlier software projects predicts the effort PIs of new projects. First, we demonstrate the applicability and different variants of the approach on a data set of 145 software development tasks. Then, we experimentally compare the performance of one variant of the approach with human (software professionals') judgment and regression analysis-based effort PIs on a data set of 15 development tasks. Finally, based on the experiment and analytical considerations, we discuss when to base effort PIs on human judgment, regression analysis, or our approach.
45|3||Design of a simulation environment based on software agents and the high level architecture|
45|3||The use of adapters to support interoperability of components for reusability|Interoperability of heterogeneous applications is defined as the ability for multiple software applications written in different programming languages running on different platforms with different operating systems to communicate and interact with one another over different computer networks. The emerging middleware technologies, including CORBA, COM/DCOM, and Enterprise JavaBeans offer an industrial defacto standard communication infrastructure to support the interoperability of heterogeneous applications in components. However, the implementation of a component suffers from high interaction complexities in the component that seriously degrades the application independence. Software components should be built to be independent of the context in which they are used, allowing them to be reused in many different computing environments. In this paper, we are presenting an adapter to isolate, encapsulate, and manage a component's interactions outside the component. The dynamic interface binding was designed to allow an adapter to examine the signature of the requested services at runtime such as operation names, parameters orders, parameters types, and parameters sizes. The interfaces of interconnecting components are bound at runtime. In addition, the interface language mapping allows an interface in a specific programming language to be automatically generated from an IDL interface. The use of adapters increases the reusability of components and also simplifies the integration of the components to an application.
45|3||Formal specification of design pattern combination using BPSL|Pattern users are faced with difficulties in understanding when and how to use the increasing number of available design patterns due the inherent ambiguity in the existing means (textual and graphical) of describing them. Since patterns are seldom used in isolation but are usually combined to solve complex problems, the above-mentioned difficulties have even worsen.
45|4|http://www.sciencedirect.com/science/journal/09505849/45/4|Editorial Board|
45|4||A method for module architecture verification and its application on a large component-based system|A method for module architecture verification is described, which yields support for checking on an architectural level whether the implicit module architecture of the implementation of a system is consistent with its specified module architecture, and which facilitates achieving architecture conformance by relating architectural-level violations to the code-level entities that cause them, hence making it easier to resolve them. Module architecture conformance is needed to enable implementing and maintaining the system and reasoning about it. We describe our experience having applied the proposed method to check a representative part of the module architecture of a large industrial component-based software system.
45|4||Performance and power evaluation of C++ object-oriented programming in embedded processors|The development of high-performance and lower power portable devices relies on both the underlying hardware architecture and technology as well as on the application software that executes on embedded processor cores. One way to confront the increasing complexity and decreasing time-to-market of embedded software is by means of modular and reusable code, forcing software designers to use objected oriented programming languages such as C++ [6]. However, the object-oriented approach is known to introduce a significant performance penalty compared to classical procedural programming. In this paper, the object oriented programming style is evaluated in terms of both performance and power for embedded applications. Profiling results indicate that C++ programs apart from being slower than their corresponding C versions, consume significantly more energy. Further analysis shows that this is mainly due to the increased instruction count, larger code size and increased number of accesses to the data memory for the object-oriented versions.
45|4||A multi-step approach for partial similarity search in large image data using histogram intersection|We investigate the problem of retrieving partially similar images from a large image database. The region-based image retrieval technique is a method of retrieving partially similar images and has been proposed as a way to efficiently process queries in an image database In region-based image retrieval, region matching is indispensable for computing the partial similarity between two images because the query processing is based upon regions instead of the entire image. A naive method of region matching is a pairwise comparison between regions; this causes severe overhead and deteriorates the performance of query processing. In this paper, we focus on the development of a filtering function for the reduction of overall search time in region-based image retrieval, which is of special importance in the case of retrieving partially similar images from a large image database. To prune irrelevant images in a database, we introduce a correct and efficient similarity function by using the Histogram Intersection, which is needed for a crude selection based on a lower bounding property. Subsequently the result is refined by the pairwise region comparison between the query image and selected images. We have performed extensive experiments on synthetic and real image data to evaluate our proposed method. The experimental results reveal that our proposed technique achieves a significant pruning of up to 99% of irrelevant images and is up to 22 times faster than pairwise comparison, where the number of bins is set at 100.
45|4||A comparison of âtraditionalâ and multimedia information systems development practices|As multimedia information systems begin to infiltrate organizations, there arises a need to capture and disseminate knowledge about how to develop them. Little is thus far known about the realities of multimedia systems development practice, or about how the development of multimedia systems compares to that of ‘traditional’ information systems. Herein are discussed the findings of a survey of multimedia developers in Ireland. Practitioners generally agree that systematic approaches are desirable in order to beneficially add structure to development processes, but they are predominantly using their own in-house methods rather than those prescribed in the literature.
45|5|http://www.sciencedirect.com/science/journal/09505849/45/5|Editorial Board|
45|5||A methodological approach for hypermedia security modeling|In hypermedia systems security is becoming a key requirement to preserve both confidentiality and integrity. Although some authorization models for hypermedia have been proposed, what is needed is to integrate security modeling into the whole development process, so that designers count on methods to specify all the features of their hypermedia systems, including navigation capabilities, interactivity, multimedia features as well as security. In this paper, we describe how security modeling is approached in a methodology for hypermedia systems, called Ariadne, that offers a number of conceptual and platform-independent tools to specify the features of any hyperdocument in an integrated and progressive way.
45|5||Efficient processing of regular path joins using PID|
45|5||Multiuser collaborative work in virtual environment based CASE tool|VRCASE is a virtual environment based Computer Aided Software Engineering (CASE) tool. It provides a 3D multiuser collaborative software modeling environment with automatic object-class abstraction, class diagram generation, and C++ skeleton generation facilities for assisting Object-Oriented software development. It allows multiple concurrent users to model software system collaboratively. To achieve efficient collaborative software development in VRCASE, we have proposed and implemented a Fine-grained locking and notification mechanism together with visual indicators to maintain system consistency among multiple concurrent users. The system evaluation shows that our approach can effectively support multiuser collaborative software design in VRCASE.
45|5||Communicating X-machines: a practical approach for formal and modular specification of large systems|An X-machine is a general computational machine that can model: (a) non-trivial data structures as a typed memory tuple and (b) the dynamic part of a system by employing transitions, which are not labelled with simple inputs but with functions that operate on inputs and memory values. The X-machine formal method is valuable to software engineers since it is rather intuitive, while at the same time formal descriptions of data types and functions can be written in any known mathematical notation. These differences allow the X-machines to be more expressive and flexible than a Finite State Machine. In addition, a set of X-machines can be viewed as components, which communicate with each other in order to specify larger systems. This paper describes a methodology as well as an appropriate notation, namely X-machine Description Language (XMDL), for building communicating X-machines from existing stand-alone X-machine models. The proposed methodology is accompanied by an example model of a traffic light junction, which demonstrates the use of communicating X-machines towards the incremental modelling of large-scale systems. It is suggested that through XMDL, the practical development of such complex systems can be split into two separate activities: (a) the modelling of stand-alone X-machine components and (b) the description of the communication between these components. The approach is disciplined, practical, modular and general in the sense that it subsumes the existing methods for communicating X-machines.
45|5||A viewpoint on software engineering and information systems: integrating the disciplines|
45|5||Architectural images of computer-based information systems development: A response to âA viewpoint of software engineering and information systems: integrating the disciplinesâ|
45|6|http://www.sciencedirect.com/science/journal/09505849/45/6|Editorial Board|
45|6||An expert screen design and evaluation assistant that uses knowledge-based backtracking|Current user interface builders provide only low-level assistance, because they have knowledge of neither the application, nor the principles by which interface elements are combined effectively. We describe a design tool that unites the knowledge components essential for effective dialog box layout. The knowledge base model consists of (1) style knowledge, which provides design-time constraints on interface specifications; (2) style application system, which handles the actual layout of dialog boxes based on the style knowledge base; (3) design knowledge, which provides quantitative metrics to analyse the spatial properties of the dialog box layout; and (4) design evaluation system, which uses evaluation metrics to compare and evaluate alternative designs. As our empirical study shows, the redesigns using the design tool had significant effect on the preference data.
45|6||A technique to analyze information-flow in object-oriented programs|Traditional information-flow analysis is mainly based on data-flow and control-flow equations. In object-oriented programs, because of the mechanisms such as encapsulation, inheritance, and polymorphism, information-flow analysis becomes quite complex and therefore it is not enough to analyze object-oriented information-flow using traditional techniques. Some new techniques are required in order to efficiently analyze the information-flow between basic components (such as statements, methods, classes or packages) in object-oriented programs. Based on object-oriented program slicing techniques, we discuss how to compute the amount of information-flow, the width of information-flow, the correlation coefficient between basic components as well as degree of coupling of a basic component can be computed. We also discuss some applications of the information-flow.
45|6||Data model for warehousing historical Web information|
45|6||Converting relational database into XML documents with DOM|The revolution of XML is recognized as the trend of technology on the Internet to researchers as well as practitioners. Companies need to adopt XML technology. With investment in the current relational database systems, they want to develop new XML documents while running existing relational databases on production. They need to reengineer the relational databases into XML documents with constraints preservation. In the process, schema translation must be done before data conversion. Since the existing relational databases are usually normalized, they have to be reconstructed into XML document tree structures. This can be accomplished through denormalization by joining the normalized relations into tables according to their data dependencies constraints. The joined tables are mapped into DOMs, which are then integrated into XML document trees. The user specifies an XML document root with its relevant nodes to form a partitioned XML document tree to meet their requirements. The selected XML document tree is mapped into an XML schema in the form of DTD. We then load joined tables into DOMs, integrate them into a DOM, and transform it into an XML document.
45|6||DocFlow: workflow based requirements elicitation|Use cases are the favoured technique for defining the functional requirements of a software system, but their use implies that the desired functionality of the new system is well known. The aim of this work is to present an alternative technique—and a supporting tool—to accurately define this functionality, expressed as use cases, starting from the workflows that describe the end user work. The use of hypergraphs in the proposed algorithm of transformation reinforces the generation process. In addition, the technique is independent of the development paradigm and a variation in the algorithm allows obtaining Data Flow Diagrams.
45|7|http://www.sciencedirect.com/science/journal/09505849/45/7|Editorial Board|
45|7||Introduction to software engineering with computational intelligence|
45|7||Application of fuzzy expert systems in assessing operational risk of software|Risk is the potential for realization of undesirable consequences of an event. Operational risk of software is the likelihood of untoward events occurring during operations due to software failures. NASA IV&V Facility is an independent institution which conducts Independent Assessments for various NASA projects. Its responsibilities, among others, include the assessments of operational risks of software. In this study, we investigate Independent Assessments that are conducted very early in the software development life cycle.
45|7||Software source code sizing using fuzzy logic modeling|Knowing the likely size of a software product before it has been constructed is potentially beneficial in project management: for instance, size can be an important factor in determining an appropriate development/integration schedule, and it can be a significant input in terms of the allocation of personnel and other resources. In this study we consider the applicability of fuzzy logic modeling methods to the task of software source code sizing, using a previously published data set. Our results suggest that, particularly with refinement using data and knowledge, fuzzy predictive models can outperform their traditional regression-based counterparts.
45|7||Software quality analysis with the use of computational intelligence|Quality of individual objects composing a software system is one of the important factors that determine quality of this system. Quality of objects, on the other hand, can be related to a number of attributes, such as extensibility, reusability, clarity and efficiency. These attributes do not have representations suitable for automatic processing. There is a need to find a way to support quality related activities using data gathered during quality assurance processes, which involve humans.
45|7||A formal specification for a fuzzy expert system|
45|7||Complex object comparison in a fuzzy context|The comparison concept plays a determining role in many problems related to object management in an Object-Oriented Database Model. Object comparison is appropriately managed in a crisp object-oriented context by means of the concepts of identity and value equality. However, when dealing with imprecise or imperfect objects, questions like ‘To which extent may two objects be the same one?’ or ‘How similar are two objects?’ have not a clear answer, because the equality concept becomes fuzzy. In this paper we present a set of operators that are useful when comparing objects in a fuzzy environment. In particular, we introduce a generalized resemblance degree between two fuzzy sets of imprecise objects and a generalized resemblance degree to compare complex fuzzy objects within a given class.
45|7||Modeling imprecise requirements with XML|Fuzzy theory is suitable to capture and analyze the informal requirements that are imprecise in nature, meanwhile, XML is emerging as one of the dominant data formats for data processing on the internet. In this paper, we have developed a fuzzy object-oriented modeling technique (FOOM) schema based on XML to model requirements specifications and incorporated the notion of stereotype to facilitate the modeling of imprecise requirements. FOOM schema is also transformed into a set of application programming interfaces (APIs) in an automatic manner. A schema graph is proposed to serve as an intermediate representation for the structure of FOOM schema to bridge the FOOM schema and APIs for both content validation and data access for the XML documents.
45|8|http://www.sciencedirect.com/science/journal/09505849/45/8|Editorial Board|
45|8||Computer security and operating system updates|Application and operating system errors are a continuing source of problems in computer security. As businesses increase the number of servers through distributed computing and server farms, it becomes more difficult to keep the systems up to date. A survey of security professionals reveals that most find it difficult to keep up to date with security patches. Consequently, developing more automated management tools is an important step in improving enterprise security.
45|8||A conceptual framework on the adoption of negotiation support systems|An exploratory study was conducted to identify factors affecting the intention to adopt negotiation support systems (NSS) by managers and executives. Drawing from past literature, the Theory of Planned Behavior and the Technology Acceptance Model provided basis for analyzing our results. Overall, subjective norm and perceived behavioral control emerged as strongest determinants of intention to adopt NSS. Further probing of subjective norm revealed organizational culture and industrial characteristics to play significant roles. A new conceptual framework is proposed which would be of both theoretical and practical importance.
45|8||An affiliated search system for an electronic commerce and software component architecture|This paper describes an Electronic Commerce Goods Search System (ECGSS) that has functions that increase the precision of search results through training of the search system and uses affiliated business transaction processes. The software component architecture for ECGSS also allows the effective deployment of the system on every local business site, in view of the evolving trend in information technology toward easier configuration and re-usability. A general information gathering system with the infrastructure to accept every Internet communication protocol and access control is described. In this affiliated business transaction model, we classify Internet sites into two groups: a cooperative sites group, and a non-cooperative sites group. While designing the components, we optimized their specifications with respect to the whole architecture analysis, dependencies, and interface types using a component-based software development process. Experiments on the effectiveness of the user training function for the search system and the response time for simple queries for each communication protocol are presented. Comparisons of commercial search solutions and architectural standards of several organizations are also given.
45|8||Computation of intraprocedural dynamic program slices|
45|8||Beyond productivityâproductivity and the three types of efficiencies of information technology industries|The purpose of this paper is to analyze productivity and efficiency of information technology industries of fourteen OECD countries. IT productivity has been studied for years and most papers have confirmed the IT productivity. Some even proved that IT productivity is far better than other factors. The impact of IT to the economy depends not only on the productivity but also on the size of IT production. Thus, to analyze productivity and efficiency of IT industries is important. They are the drive for the economic growth in the new economy. Although productivity has been analyzed and discussed in the information systems field for years, little research has been done in efficiency of IT.
45|8||Communication issues in requirements elicitation: a content analysis of stakeholder experiences|The gathering of stakeholder requirements comprises an early, but continuous and highly critical stage in system development. This phase in development is subject to a large degree of error, influenced by key factors rooted in communication problems. This pilot study builds upon an existing theory-based categorisation of these problems through presentation of a four-dimensional framework on communication. Its structure is validated through a content analysis of interview data, from which themes emerge, that can be assigned to the dimensional categories, highlighting any problematic areas. The paper concludes with a discussion on the utilisation of the framework for requirements elicitation exercises.
45|8||Code and data spatial complexity: two important software understandability measures|In order to maintain the software, the programmers need to understand the source code. The understandability of the source code depends upon the psychological complexity of the software, and it requires cognitive abilities to understand the source code. The individual needs to correlate the orientation and location of various entities with their processing, which requires spatial abilities. This paper presents two measures of spatial complexity, which are based on two important aspects of the program—code as well as data. The measures have been applied to 15 different software projects and results have been used to draw many conclusions. The validation of the results has been done with help of perfective maintenance data. Lower values of code as well as data spatial complexity denote better understandability of the source code.
45|9|http://www.sciencedirect.com/science/journal/09505849/45/9|Editorial Board|
45|9||Real-time disk scanning for timely retrieval of continuous media objects|
45|9||Global intelligence benevolent builder (GIBB): a system automating integration of heterogeneous classical databases and web|
45|9||Further investigations of reading techniques for object-oriented design inspection|
45|9||Evolving a model of transaction management with embedded concurrency control for mobile database systems|Transactions within a mobile database management system face many restrictions. These cannot afford unlimited delays or participate in multiple retry attempts for execution. The proposed embedded concurrency control (ECC) techniques provide support on three counts, namely—to enhance concurrency, to overcome problems due to heterogeneity, and to allocate priority to transactions that originate from mobile hosts. These proposed ECC techniques can be used to enhance the server capabilities within a mobile database management system. Adoption of the techniques can be beneficial in general, and for other special cases of transaction management in distributed real-time database management systems. The proposed model can be applied to other similar problems related to synchronization, such as the generation of a backup copy of an operational database system.
45|9||Software reuse through re-engineering the legacy systems|Software reuse is widely considered to be a way to increase the productivity and improve the quality and reliability of new software systems. Identifying, extracting and re-engineering software components that implement abstractions within existing systems is a promising cost-effective way to create reusable assets and re-engineer legacy systems. This paper summarizes our experiences with using computer-supported methods to develop a software architecture to support the re-engineering of the Janus Combat Simulation System. In this effort, we have developed an object-oriented architecture for the Janus Combat Simulation Subsystem, and validated the architecture with an executable prototype. In this paper, we propose methods to facilitate the reuse of the software components of the legacy systems by recovering the behavior of the systems using systematic methods, and illustrate their use in the context of the Janus System.
45|9||Migration of active objects in proactive|
45|9||An architecture for security-oriented perfective maintenance of legacy software|This work presents an implementation strategy which exploits the separation of concerns and reuse in a multi-tier architecture to improve the security (availability, integrity, and confidentiality) level of an existing application. Functional properties are guaranteed via wrapping of the existing software modules. Security mechanisms are handled by the business logic of the middle-tier: availability and integrity are achieved via replication of the functional modules and the confidentiality is obtained via cryptography. The technique is presented with regard to a case study application. We believe that our experience can be used as a guideline for software practitioners to solve similar problems. We thus describe the conceptual model behind the architecture, discuss implementation issues, and present technical solutions.
46|1|http://www.sciencedirect.com/science/journal/09505849/46/1|Editorial Board|
46|1||Publisher's Note|
46|1||Top-down and bottom-up expert estimation of software development effort|Expert estimation of software development effort may follow top-down or bottom-up strategies, i.e. the total effort estimate may be based on properties of the project as a whole and distributed over project activities (top-down) or calculated as the sum of the project activity estimates (bottom-up). The explorative study reported in this paper examines differences between these two strategies based on measurement and video recording of the discussions of seven estimation teams. Each estimation team applied a top-down estimation strategy on one project and a bottom-up estimation strategy on another. The main contribution of the study is the observation that the recall of very similar previously completed projects seemed to be a pre-condition for accurate top-down strategy based estimates, i.e. the abilities of the software estimators to transfer estimation experience from less similar projects was poor. This suggests that software companies should apply the bottom-up strategy unless the estimators have experience from, or access to, very similar projects.
46|1||Implementing a community web site: a scenario-based methodology|The use of the Internet to facilitate commerce promises vast benefits. An important challenge in the age of Internet business is to fine-tune the Internet business system with customers. For this alignment, this paper proposes a scenario-based object-oriented methodology for developing a community web site. The methodology consists of five phases such as customer analysis, value analysis, web design, implementation design, and construction. Scenarios are used to analyze customers' needs in a natural fashion. A meta-data scheme is proposed for supporting our methodology. A real-life community web is illustrated to demonstrate the usefulness of the methodology. Scenario thinking can help companies use the Internet to buttress and extend their values.
46|1||Evaluation of the comprehension of the dynamic modeling in UML|There is a certain degree of difficulty in developing and understanding the diagrams used for representing the dynamic behavior of a software application, specified in the Unified Modeling Language (UML). In this paper we evaluate the comprehension of the dynamic modeling in UML designs by using two split-plot factorial experiments. The metrics used for assessing the results are the time spent and the scores obtained in answering a questionnaire. In the first study three factors were controlled: the diagram type (sequence, collaboration and state), the application domain of the UML designs and the order of presentation of the documents. We observe that state diagrams provide higher semantic comprehension of dynamic modeling in UML when the domain is real-time and sequence diagrams are better in the case of a management information application. In the second study two factors were controlled: the paired combination of dynamic diagrams and the application domain. The main conclusion of the second study is that regardless of the domain, a higher semantic comprehension of the UML designs is achieved when the dynamic behavior is modeled by using the pair Sequence–State. Combining the results of both experiments we obtain the conditions which must concur to get an effective comprehension of the UML dynamic models: (a) if it is a management information application, the diagrams are sequence or the composition Sequence–State or Collaboration–State; (b) for a real-time non-reactive system the diagrams are collaboration or the pair Collaboration–State or Sequence–State; and (c) finally, when it is the design of the real-time reactive system, the best diagram is the State.
46|1||Efficient data mining for web navigation patterns|The concept of preference is proposed on the analysis of the present algorithms for mining user navigation patterns. It is based on the following hypothesis: if there are many different selections to leave a page, those selections that occur more frequently and the next page is viewed longer reveal user interest and preference. Representing user navigation interest and intention accurately by comparing relatively access ratio and the average of relatively access ratio of viewing time and selective intention, preference can be used for mining user navigation pattern instead of confidence. The higher preference, the more prefer to choose the selection. According to the conception, we propose two efficient algorithms based on the concept, UAM and PNT, which are developed for mining user preferred navigation patterns. Considering the structure of Web site, UAM can get user access preferred path by the page–page transition probabilities statistics of all users behaviours. PNT looks far into the past to correctly discriminate the different behavioral modes of the different users. Experiments show accuracy and scalability of the algorithms. It is suitable for applications in E-business, such as to optimize Web site or to design personalized service.
46|1||The productivity impact of information technology in the healthcare industry: an empirical study using a regression spline-based approach|This paper explores the productivity impact of information technology (IT) in the healthcare industry using a regression spline (RS)-based approach. Application of the RS-based approach offered additional valuable insights that contribute to our understanding of the complex relationship between investments in IT and organizational productivity. For example, the results of this study suggest that investments in the IT Stock has a positive impact on productivity only under certain conditions, and that this impact of IT is not uniform but is conditioned both by the amount invested in the IT Stock and the investments in Non-IT Capital.
46|1||Erratum to âOn the interplay between consistency, completeness, and correctness in requirements evolutionâ [Information and Software Technology 45 (2003) 993â1009]|
46|1||Reviewers List|
46|10|http://www.sciencedirect.com/science/journal/09505849/46/10|Editorial Board|
46|10||A methodological framework for generic conceptualisation: problem-sensitivity in software engineering|The first step towards developing quality software is to conceptually model the problem raised in its own context. Software engineering, however, has traditionally focused on implementation concepts, and has paid little or no attention to the problem domain. This paper presents a generic methodological framework to guide conceptual modelling, focusing on the problem within its domain. This framework is defined considering aspects related to a generic conceptualisation, and its application to software engineering—illustrated using the IFIP Case—achieves the called-for problem-sensitivity.
46|10||Cost effective management frameworks: the impact of IDS deployment technique on threat mitigation|In this paper we measure the financial benefit of an intrusion detection system (IDS) deployment. To this end, we use a standard risk analysis framework and extend it by introducing the Cascading Threat Multiplier (CTM). The idea behind the CTM is that a security compromise incurs two types of costs: (a) The direct cost of lost integrity/confidentiality/availability, and (b) the indirect cost, of the compromised component serving as a potential stepping stone for future attacks. The CTM tries to capture the second type of costs, which are typically ignored in the classic risk analysis framework. We propose new risk analysis formulas that tie the CTM concept into accurate calculation of Return on Investment (ROI), otherwise commonly known as Return on Security Investment. Finally, through a case study we demonstrate the effect of IDS deployment techniques on threat mitigation and the ROI. The result of the case can be used to support effective decision-making about which techniques are appropriate for the cost effective management of the IDS in a given environment.
46|10||Defect detection oriented lifecycle modeling in complex product development|As the complexity of today's products increases, single projects, single departments or even single companies can no longer develop total products, resulting in concurrent and distributed development. To manage the resulting organizational complexity, projects need a lifecycle that explicitly reflects the concurrent and distributed nature of the project context. This paper addresses the essence of lifecycle modeling, with emphasis on defect detection. An adequately modeled lifecycle allows the localization and recognition of defect-sensitive areas in complex product development. A case study involving real-life development projects indicates that transitions between constituent sub-projects are in particular defect-sensitive. A second case study shows that by a defect detection-driven construction of a project-specific lifecycle, fewer residual defects can be expected.
46|10||A comparison of cohesion metrics for object-oriented systems|Cohesion is the degree to which the elements of a class or object belong together. Many different object-oriented cohesion metrics have been developed; many of them are based on the notion of degree of similarity of methods. No consensus has yet arisen as to which of these metrics best measures cohesion; this is a problem for software developers since there are so many suggested metrics, it is difficult to make an informed choice. This research compares various cohesion metrics with ratings of two separate teams of experts over two software packages, to determine which of these metrics best match human-oriented views of cohesion. Additionally, the metrics are compared statistically, to determine which tend to measure the same kinds of cohesion. Differences in results for different object-oriented metrics tools are discussed.
46|10||Measurement of object-oriented software spatial complexity|One of the important activities of the maintenance phase is to understand the source-code first, and then change it. Understandability of the software gets affected by psychological complexity of the source-code and cognitive abilities are needed to understand it. The correlation between the orientation and location of various entities with their processing needs to be established by the programmers, which requires spatial abilities. These spatial abilities play an important role in object-oriented software, in which the use of data as well as the methods of the class needs to be understood in a combined way. This paper presents two measures of spatial complexity of object-oriented software, which are based on definition and usage of classes and objects. The values of proposed measures get affected suitably because of inheritance and polymorphism as well, due to change in the distances. The significance of object-oriented spatial complexity has been demonstrated with the help of 15 object-oriented projects of varied length and results have been validated with the help of reverse engineering data and perfective maintenance data.
46|11|http://www.sciencedirect.com/science/journal/09505849/46/11|Editorial Board|
46|11||Expressing and organising business rules|Business rules represent projections of organisations' constraints and ways of working on their supporting information systems. Therefore, their collection, expression, structuring and organisation should be central activities within information systems analysis. The work presented in this paper concerns the definition of a repository schema for managing business rules, taking into account the objectives (a) to facilitate the rule collection process, (b) to assist the transition from analysis to design and implementation of the information system, and (c) to support business change once the new system has been delivered. These objectives are achieved through the enhancement of the rule repository schema with information on the logistics of the collection process and references to underlying enterprise informational and behavioural knowledge models. The proposed schema and way of working are demonstrated through a number of examples, which are derived from an industrial project concerning electronic procurement in the construction sector.
46|11||A controller synthesis algorithm for building self-adaptive software|A novel approach for building self-adaptive software based on a controller synthesis algorithm is presented. Self-adaptive software is a relatively new idea aiming at producing applications that can readily adapt in the face of changing user needs, desires and environment. Self-adaptive software has multiple ways of accomplishing its purpose, enough knowledge of its construction and is capable of changing behaviour when it does not accomplish its goal or when better functionality or performance is possible. The presented approach for building self-adaptive software uses ontological models of software components, which represent the environment, the composition, the required behaviour, and the possible configurations for the self-adaptive software. Self-adaptation is based on a supervisory control algorithm that reconfigures and controls software components in order to achieve their required behaviour.
46|11||Dynamic adaptation to object state change in an information flow control model|
46|11||Multi-way R-tree joins using indirect predicates|Since spatial join processing consumes much time, several algorithms have been proposed to improve spatial join performance. Spatial join has been processed in two steps, called filter step and refinement step. The M-way R-tree join (MRJ) is a filter step join algorithm, which synchronously traverses M R-trees. In this paper, we introduce indirect predicates which do not directly come from the multi-way join conditions but are indirectly derived from them. By applying indirect predicates as well as direct predicates to MRJ, we can quickly remove the minimum bounding rectangle (MBR) combinations which do not satisfy the direct predicates or the indirect predicates at the parent level. Hence we can reduce the intermediate MBR combinations for the input to the child level processing and improve the performance of MRJ. We call such a multi-way R-tree join algorithm using indirect predicates indirect predicate filtering (IPF). Through experiments using synthetic data and real data, we show that IPF significantly improves the performance of MRJ.
46|11||The space efficiency of XML|XML is the future language for data exchange, and support for XML has been extensive. Although XML has numerous benefits including self-describing data, improved readability, and standardization, there are always tradeoffs in the introduction of new technologies that replace existing systems. The tradeoff of XML versus other data exchange languages is improved readability and descriptiveness versus space efficiency. There has been limited work on examining the space efficiency of XML. This paper compares XML to other data exchange formats. Experiments are performed to measure the overhead in XML files and determine the amount of space used for data, schema, and overhead in a typical XML document.
46|11||Erratum to âOn the interplay between consistency, completeness, and correctness in requirements evolutionâ [Information and Software Technology 45 (2003) 993â1009]|
46|11||Erratum to âOn the interplay between consistency, completeness, and correctness in requirements evolutionâ|The initial expression of requirements for a computer-based system is often informal and possibly vague. Requirements engineers need to examine this often incomplete and inconsistent brief expression of needs. Based on the available knowledge and expertise, assumptions are made and conclusions are deduced to transform this ‘rough sketch’ into more complete, consistent, and hence correct requirements. This paper addresses the question of how to characterize these properties in an evolutionary framework, and what relationships link these properties to a customer's view of correctness. Moreover, we describe in rigorous terms the different kinds of validation checks that must be performed on different parts of a requirements specification in order to ensure that errors (i.e. cases of inconsistency and incompleteness) are detected and marked as such, leading to better quality requirements.
46|12|http://www.sciencedirect.com/science/journal/09505849/46/12|Editorial Board|
46|12||Prediction of software failures through logistic regression|The quality of software has been a main concern since the inception of computer software. To be able to produce high quality software, software developers and software testers alike need continuous improvements in their developing and testing methodologies. These improvements should result in better coverage of the input domain, efficient test cases, and in spending fewer testing resources. In this paper we focus on an approach for generating efficient test cases based on the special properties of Design of Experiments and developing a logistic regression model of predicting test case outcomes. Design of Experiments will be utilized to efficiently minimize the number of test cases and the logistic regression model will be used to predict software failures. This approach, in turn, would provide the software tester with a model that reduces the number of test cases, predicts test case outcomes, reduces cost, and allows better forecast of release readiness. We demonstrate our approach using two case studies (TI Interactive software and Microsoft's Pocket PC operating system).
46|12||A formal framework for analyzing reusability complexity in component-based systems|In this paper, we present a methodology to estimate the impact of modifying a given software system design. In addition, we will be able to evaluate its reusability as well as the coupling of its components. In order to do that, the designer defines the system in terms of its components, their dependencies, the properties they fulfill, and the properties each component requires to other components. Besides, some auxiliary functions are used to define the relations between properties and the cost associated with their modification. Putting together all this information the different ways to perform a modification can be systematically generated and studied. We have applied our methodology to a medium-size system. Specifically, we dealt with an on-line intelligent tutoring system allowing users to learn the programming language Haskell.
46|12||An industrial study on building consensus around software architectures and quality attributes|When creating an architecture for a software system it is important to consider many aspects and different sides of these aspects at an early stage, lest they are misunderstood and cause problems at later stages during development. In this paper, we report from an industry study to understand and select between different architecture candidates. The company uses a method that focuses discussions of architecture candidates to where there are disagreements between the participating domain experts. The results indicate that the used method pinpoints for the company where further investigations are necessary and that the decision concerning which architecture to use is taken with more confidence as a result of the focused discussions.
46|12||A role-driven component-oriented methodology for developing collaborative commerce systems|As the commerce environment becomes more competitive, companies are compelled to adopt a collaborative commerce (c-commerce) paradigm to sustain a competitive edge over the Internet. C-commerce demands a variety of collaborative interactions among multiple stakeholders. In order to develop c-commerce systems, this paper proposes a role-driven component-oriented methodology (RCOM), which consists of four phases: collaboration analysis, component analysis, component design, and implementation. The roles can help implement well-defined business functions, while the components enable the production of reusable artifacts in a systematic fashion. Accordingly, RCOM is likely to help improve the system development process. To demonstrate the practical usefulness of this methodology, a real-life case is illustrated.
46|12||Normal forms for XML documents|This paper studies the normalization problem of XML documents with DTDs as their schemas. XML documents may contain redundant information due to a bad designed DTD which implies the similar anomaly dependencies among elements and attributes just as in relational database schema. The concepts of partial functional dependency and transitive functional dependency for XML DTD are proposed. And three XML normal forms: 1XNF, 2XNF and 3XNF, are defined based on the concepts of partial and transitive functional dependency. The concept of lossless join decomposition for DTD is defined by the relational representation of DTD. Finally, two lossless join decomposition algorithms are given for transforming a DTD into 2XNF and 3XNF, respectively.
46|12||Independent examination of software: an experiment|This paper summarises our experience in using model-checking technology to understand the behaviour of a system. A simple model of the system under test is created from the informal documentation. This model is used to create a test environment for the system. The behaviour of system combined with the test environment is then verified using the tool Verisoft. This requires a few changes to the original source code. The aim to minimize the number of changes to the original source code so that its original behaviour is not affected. The use of Verisoft to create a model, which can subsequently be used for testing is also studied. The main conclusion is that it is possible to use bounded model checking on source code with a view towards verifying key behavioural properties.
46|13|http://www.sciencedirect.com/science/journal/09505849/46/13|INSIDE FRONT COVER|
46|13||Automated rapid prototyping of TUG specifications using Prolog|A prototype is usually built quickly and cheaply to explore poorly understood user requirements in the front-end of the software development life cycle. There are several techniques to construct a prototype such as fourth generation languages, functional and logic programming languages, and simulation techniques. Despite their benefits, these techniques do not directly support formal user requirements and specifications. In this paper, a formal specification language, called TUG, is presented to support an automatic derivation of a prototype in Prolog from a specification in the language via a set of transformation rules. The work described in this paper is distinct from existing rapid prototyping techniques. There is a close correspondence between TUG and Prolog that makes the process of transformation relatively mechanical. The process also allows specifiers not to consider low-level details at implementation such as selection of data structures and algorithms due to the features of the TUG specification language and the Prolog programming language. In addition, rederivation of a prototype in Prolog from a TUG specification is also avoided whenever the specification is modified. A Change Request Script is used to update the prototype in response to the revised specification. The prototype construction and specification acquisition are integrated to handle the construction of user requirements. The formality of the TUG specification language improves the quality of the description of user requirements. Rapid prototyping from the specification via software transformations improves the understanding of user requirements in a cost effective way.
46|13||Assessing defect detection performance of interacting teams in object-oriented design inspection|Software inspection is one of the methods to ensure the quality of software by finding and repairing defect early in software development process. In a software inspection process, inspectors first review software artifacts individually and then meet in a team in order to find as many defects as possible and to eliminate false positives. However, several empirical studies suggest that inspection meeting may not be necessary since an insignificant number of new defects are found as a result of the meeting.
46|13||On the identification of categories and choices for specification-based test case generation|The category-partition method and the classification-tree method help construct test cases from specifications. In both methods, an early step is to identify a set of categories (or classifications) and choices (or classes). This is often performed in an ad hoc manner due to the absence of systematic techniques. In this paper, we report and discuss three empirical studies to investigate the common mistakes made by software testers in such an ad hoc approach. The empirical studies serve three purposes: (a) to make the knowledge of common mistakes known to other testers so that they can avoid repeating the same mistakes, (b) to facilitate researchers and practitioners develop systematic identification techniques, and (c) to provide a means of measuring the effectiveness of newly developed identification techniques. Based on the results of our studies, we also formulate a checklist to help testers detect such mistakes.
46|13||An empirical study of the effect of knowledge integration on software development performance|Although the role of integrating application domain knowledge with technical knowledge is implicitly recognized in software engineering practice, no large scale study has attempted to validate this empirically in a field setting. In this paper, a large-scale empirical study of 232 software development projects in 232 software development organizations shows that higher integration of business application domain knowledge with technical knowledge during the software development process increases software development effectiveness, reduces defect density throughout the development trajectory, lowers warranty defects, and increases software development efficiency. The findings highlight the influence of knowledge integration on various dimensions of software development performance.
46|13||A statecharts-based software development process for mobile agents|Although mobile agents and their supporting infrastructures have been extensively developed, it is still an emerging technology. A wider acceptance of mobile agents would be facilitated with the exploitation of suitable methodologies and tools which fully support their development lifecycle. This paper proposes a Statecharts-based development process for mobile agents, which allows for a seamless transition from the specification of mobile agent behaviour to its implementation and adaptation to target mobile agent systems. In particular, modelling of the mobile agent behaviour is visual and its coding is seamlessly supported by the Mobile Active Object Framework. The coded agent behaviour can be adapted to platform-specific mobile agents by means of the Mobile Agent Adaptation Framework thus enabling re-use of existing mobile agent systems.
46|14|http://www.sciencedirect.com/science/journal/09505849/46/14|INSIDE FRONT COVER|
46|14||Scalable distributed compact trie hashing (CTH*)|This last decade, a new class of data structures named Scalable Distributed Data Structures (SDDSs), is appeared completely dedicated to a distributed environment. This type of data structures opened an important axis of research, considering that the data management in a transparent manner is fundamental in a computer network. All the existing methods are mainly based on Linear hashing (LH*) and Range-partitioning (RP*). In this paper, we propose a new method with the constraints of the SDDS. Our approach is an adaptation of the well-known method Trie hashing (TH) for a distributed environment, i.e. a network of interconnected computers. The latter uses a digital tree (trie) as access function. Our major objective is the distribution of file buckets and the tree representing the hashing function. We have considered TH with the tree represented in compact form (CTH) because this option is probably more interesting for the reduction of the message size circulating on the network. Contrary to the majority of the existing methods, the proposed one provides the order of distributed files, then facilitates both the range query operations and the ordered traversal of files. Moreover, the following properties make our method a promising opening towards a new class of SDDS: (a) preservation of the order of records, (b) works without multicast (c) three bytes are sufficient to address a server, (d) the transfer of some bytes is enough for the update of the client trees. The access performances should exceed the ones of traditional files and some competitive scalable and distributed data structures.
46|14||Implementing requirements engineering processes throughout organizations: success factors and challenges|This paper aims at identifying critical factors affecting organization-wide implementation of requirements engineering (RE) processes. The paper is based on a broad literature review and three longitudinal case studies that were carried out using an action research method. The results indicate that RE process implementation is a demanding undertaking, and its success greatly depends on such human factors as motivation, commitment and enthusiasm. Therefore, it is essential that the RE process is useful for its individual users. Furthermore, the results indicate that organizations can gain benefits from RE by defining a simple RE process, by focusing on a small set of RE practices, and by supporting the systematic usage of these practices.
46|14||Experimental comparison of the comprehensibility of a Z specification and its implementation in Java|Comprehensibility is often raised as a problem with formal notations, yet formal methods practitioners dispute this. In a survey, one interviewee said ‘formal specifications are no more difficult to understand than code’. Measurement of comprehension is necessarily comparative and a useful comparison for a specification is against its implementation. Practitioners have an intuitive feel for the comprehension of code. A quantified comparison will transfer this feeling to formal specifications. We performed an experiment to compare the comprehension of a Z specification with that of its implementation in Java. The results indicate there is little difference in comprehensibility between the two.
46|14||Adaptive development and maintenance of user-centric software systems|A software system cannot be developed without considering the various facets of its environment. Stakeholders—including the users that play a central role—have their needs, expectations, and perceptions of a system. Organisational and technical aspects of the environment are constantly changing. The ability to adapt a software system and its requirements to its environment throughout its full lifecycle is of paramount importance in a constantly changing environment. The continuous involvement of users is as important as the constant evaluation of the system and the observation of evolving environments. We present a methodology for adaptive software systems development and maintenance. We draw upon a diverse range of accepted methods including participatory design, software architecture, and evolutionary design. Our focus is on user-centred software systems.
46|15|http://www.sciencedirect.com/science/journal/09505849/46/15|INSIDE FRONT COVER|
46|15||Editorial|
46|15||Optimal and adaptive testing for software reliability assessment|Optimal software testing is concerned with how to test software such that the underlying testing goal is achieved in an optimal manner. Our previous work shows that the optimal testing problem for software reliability growth can be treated as closed-loop or feedback control problem, where the software under test serves as a controlled object and the software testing strategy serves as the corresponding controller. More specifically, the software under test is modeled as controlled Markov chains (CMCs) and the control theory of Markov chains is used to synthesize the required optimal testing strategy. In this paper, we show that software reliability assessment can be treated as a feedback control problem and the CMC approach is also applicable to dealing with the optimal testing problem for software reliability assessment. In this problem, the code of the software under test is frozen and the software testing process is optimized in the sense that the variance of the software reliability estimator is minimized. An adaptive software testing strategy is proposed that uses the testing data collected on-line to estimate the required parameters and selects next test cases. Simulation results show that the proposed adaptive software testing strategy can really work in the sense that the resulting variance of the software reliability estimate is much smaller than that resulting from the random testing strategies. The work presented in this paper is a contribution to the new area of software cybernetics that explores the interplay between software and control.
46|15||Mirror adaptive random testing|
46|15||Verifying Haskell programs by combining testing, model checking and interactive theorem proving|We propose a program verification method that combines random testing, model checking and interactive theorem proving. Testing and model checking are used for debugging programs and specifications before a costly interactive proof attempt. During proof development, testing and model checking quickly eliminate false conjectures and generate counterexamples which help to correct them. With an interactive theorem prover we also ensure the correctness of the reduction of a top level problem to subproblems that can be tested or proved. We demonstrate the method using our random testing tool and binary decision diagrams-based (BDDs) tautology checker, which are added to the Agda/Alfa interactive proof assistant for dependent type theory. In particular we apply our techniques to the verification of Haskell programs. The first example verifies the BDD checker itself by testing its components. The second uses the tautology checker to verify bitonic sort together with a proof that the reduction of the problem to the checked form is correct.
46|15||Instrumenting scenarios in a model-driven development environment|SpecExplorer is an integrated environment for model-driven development of .NET software. In this paper we discuss how scenarios can be described in SpecExplorer's modeling language, Spec#, and how the SpecExplorer tool can be used to validate those scenarios by various means.
46|15||The software evaluation framework âSEFâ extended|The primary objective of this paper was to present a study on the metrics which can be applied to the Software Evaluation Framework. The paper presents the results of a preliminary study, which focuses on the measurements applied to the framework. It provides a description of the metrics used and an analysis of how they compare to each other in the measurement of the software characteristics. This objective was accomplished by empirically testing the model with quantitative techniques. An earlier qualitative study provided a list of metrics, which were then tested with the quantitative study described in this paper. The results of this study are important as it identifies the metrics, perceived by stakeholders as essential for applying the Software Evaluation Framework to software evaluation.
46|15||Call for papers|
46|15||Author Index|
46|15||Keyword Index|
46|15||Volume Contents|
46|2|http://www.sciencedirect.com/science/journal/09505849/46/2|Editorial Board|
46|2||An aspect-oriented framework for developing component-based software with the collaboration-based architectural style|Component-based development (CBD) technique for software has emerged to fulfill the demand on the reuse of existing artifacts. In comparison to traditional object-oriented techniques, CBD can provide more advanced abstraction concepts such as subsystem-level reusability, gross structure abstraction, and global control flow abstraction. Unfortunately, existing software development techniques are not mature enough to make it come true that components developed in the third party can be used in a highly flexible way. It is notable that there are certain kinds of software requirements, such as non-functional requirements, that must be implemented cross-cutting multiple classes, largely losing the modularity in object-oriented design and implementation code. Therefore, it is not easy that components are reused without consideration of their low-level implementation details.
46|2||Factors related to the difficulty of learning to program in Javaâan empirical study of non-novice programmers|Due to its relative newness and popularity, Java is being taught to numerous non-novice programmers both in industry and in academia. Claims have been made that certain background characteristics of programmers relate to ease in learning Java. In this study, background information of 135 non-novice programmers was obtained, together with data relating to their difficulty of learning several different features of Java. Results of this study could be used by software project managers contemplating the use of Java, and by academicians involved in curricular planning.
46|2||Automated elicitation of functional dependencies from source codes of database transactions|
46|2||On handling time-varying data in the relational data model|The article addresses the key issues in modeling and querying temporal data within the relational framework. These issues include representation of temporal data, temporal grouping (object) identifiers and primary keys of temporal relations, temporal integrity constraints, and fundamental operations of a temporal query language. The paper develops taxonomy for temporal relational databases and establishes criteria to evaluate various proposed extensions to the relational data model. We expect that it will benefit the researchers and guide the practitioners in choosing the right approach for managing temporal data in their applications.
46|2||Evaluating the learning effectiveness of using simulations in software project management education: results from a twice replicated experiment|The increasing demand for software project managers in industry requires strategies for the development of management-related knowledge and skills of the current and future software workforce. Although several educational approaches help to develop the necessary skills in a university setting, few empirical studies are currently available to characterise and compare their effects.
46|3|http://www.sciencedirect.com/science/journal/09505849/46/3|Editorial Board|
46|3||A distributed and interoperable object-oriented support for safe e-commerce transactions|Business via Internet is becoming popular. A number of organizations doing business in the traditional way are extending themselves to do business over the Web. Most business-to-business dealings are done through Value Added Networks but for general consumer-to-business dealings, the Internet provides a powerful base. However, customer confidence in Internet commerce needs to be further strengthened before large scale Internet purchasing and selling becomes a reality. While security standards have been established through efficient cryptographic techniques to ensure that network communications are not intercepted, the lack of trust between the endpoint parties involved in an e-commerce transaction remains an obstacle for a smooth completion of the transaction. This is because transacting parties need to trust each other before making any commitment. In this paper, we show how trust can be provided automatically through a network of Trust Service Providers (TSP). The solution we propose allows both customers and merchants to deal with each other confidently through trusted intermediaries which role is to guarantee the goods delivery to the customer and the payment to the merchant for issuing those goods. The transactions are conducted atomically and transparently. That is, a transaction process does not experience any off-line transition delay at the intermediaries and the transacting parties deal virtually directly with each other. This solution requires building a trust web based on a network of TSPs, which we implemented in the form of distributed CORBA objects.
46|3||The impact of cognitive complexity on project leadership performance|In today's complex development environments, the ability to integrate project components has been found to be a key responsibility for project leaders. This paper reports results of a preliminary experiment examining the importance of cognitive differentiation and integration (i.e. cognitive complexity) to project leadership performance. Results from this preliminary study show the importance of cognitive complexity to success in project leadership. Surprisingly, results suggest that leaders with lower levels of cognitive differentiation abilities (less information fragmentation) perform better on project definition tasks. Results also suggest that higher levels of cognitive integrative ability are associated with higher performance in project definition tasks. To this end, we suggest ways to improve complex thinking in project leaders. This study is important to IS managers responsible for identifying and training project leaders, and researchers seeking to understand factors important to successful project leadership.
46|3||Virtual visits to cultural heritage supported by web-agents|The integration between information technologies and cultural heritage can impact on everyday life, both from the point of view of institutions and of users. The cultural heritage community have recently been attracted by the chance offered by information technology, and, in particular, by the possibility of making cultural information available to a wide range of people. Museums, exhibitions and cultural institutions can now supply new services to access cultural information, and this calls for suitable infrastructures and tools. In such a context, this paper proposes a Web-based application that enables virtual visits to access cultural information tailored on the basis of user profiles and devices. The application is closely integrated within the Web; it also permits one group to build up virtual visits that can be attended by different people interested in the same subject. The application is based on an infrastructure exploiting innovative technologies such as active proxy servers and mobile agents; it grants a high degree of flexibility and is particularly suitable for an improved promulgation of cultural information.
46|3||Self-assessment of performance in software inspection processes|
46|3||A metadata approach to multimedia database federations|
46|4|http://www.sciencedirect.com/science/journal/09505849/46/4|Editorial Board|
46|4||Object-oriented conceptual modeling for commitment-based collaboration management in virtual enterprises|For the rapid progress of internet technologies in recent years, Electronic Commerce (EC) has gained attention as a major theme for enterprises to keep their competitiveness. From the perspective of effective resources utilization, it becomes now an important goal for an enterprise to promote its performance and competitiveness through integrating itself and relevant suppliers and consumers as a virtual group to achieve the so-called Business-to-Business EC. In this paper, we propose an object-oriented modeling approach that addresses the management of collaboration on the Internet between enterprises. The approach divides those required mechanisms for collaboration management into three layers: commitment, role, and activity ones. With this architecture, two enterprises may collaborate via the establishment and maintenance of commitment, the collaboration and coordination between roles, and the interaction and coordination between activities. For specification, an object-oriented model is presented for each layer that describes the working details of that layer. To illustrate, these models are applied in a typical manufacturing supply chain application among various enterprises.
46|4||M-UML: an extension to UML for the modeling of mobile agent-based software systems|The Unified Modeling Language (UML) is a language for the specification, visualization, and documentation of object-oriented software systems [The Unified Modeling Language User Guide, 1998]. However, UML cannot describe in an explicit manner the mobility requirements needed for modeling mobile agent-based software systems. In this paper, we present M-UML, our proposed extension to UML covering all aspects of mobility at the various views and diagrams of UML. The use of M-UML is illustrated using a simple mobile voting system example.
46|4||An adaptive indexing technique using spatio-temporal query workloads|Many spatio-temporal access methods, such as the HR-tree, the 3DR-tree, and the MV3R-tree, have been proposed for timestamp and interval queries. However, these access methods have the following problems: the poor performance of the 3DR-tree for timestamp queries, the huge size and the poor performance of the HR-tree for interval queries, and the large size and the high update cost of the MV3R-tree. We address these problems by proposing an adaptive partitioning technique called the Adaptive Partitioned R-tree (APR-tree) using workloads with timestamp and interval queries. The APR-tree adaptively partitions the time domain using query workloads. Since the time domain of the APR-tree is automatically fitted to query workloads, the APR-tree outperforms the other access methods for various query workloads. The size of the APR-tree is on the average 1.3 times larger than that of the 3DR-tree which has the smallest size. The update cost of the APR-tree is on the average similar to that of the 3DR-tree which has the smallest update cost.
46|4||Software release planning: an evolutionary and iterative approach|To achieve higher flexibility and to better satisfy actual customer requirements, there is an increasing tendency to develop and deliver software in an incremental fashion. In adopting this process, requirements are delivered in releases and so a decision has to be made on which requirements should be delivered in which release. Three main considerations that need to be taken account of are the technical precedences inherent in the requirements, the typically conflicting priorities as determined by the representative stakeholders, as well as the balance between required and available effort. The technical precedence constraints relate to situations where one requirement cannot be implemented until another is completed or where one requirement is implemented in the same increment as another one. Stakeholder preferences may be based on the perceived value or urgency of delivered requirements to the different stakeholders involved. The technical priorities and individual stakeholder priorities may be in conflict and difficult to reconcile. This paper provides (i) a method for optimally allocating requirements to increments; (ii) a means of assessing and optimizing the degree to which the ordering conflicts with stakeholder priorities within technical precedence constraints; (iii) a means of balancing required and available resources for all increments; and (iv) an overall method called EVOLVE aimed at the continuous planning of incremental software development. The optimization method used is iterative and essentially based on a genetic algorithm. A set of the most promising candidate solutions is generated to support the final decision. The paper evaluates the proposed approach using a sample project.
46|4||FINDIT: a fast and intelligent subspace clustering algorithm using dimension voting|The aim of this paper is to present a novel subspace clustering method named FINDIT. Clustering is the process of finding interesting patterns residing in the dataset by grouping similar data objects from dissimilar ones based on their dimensional values. Subspace clustering is a new area of clustering which achieves the clustering goal in high dimension by allowing clusters to be formed with their own correlated dimensions.
46|4||The design of an anonymous file sharing system based on group anonymity|
46|5|http://www.sciencedirect.com/science/journal/09505849/46/5|Editorial board|
46|5||Special Issue on Software Engineering: Applications, Practices and Tools from the ACM Symposium on Applied Computing 2003|
46|5||An integrated framework for formal development of open distributed systems|This paper contributes to the discussion on issues related to the formal development of open distributed systems (ODSs). Deficiencies of traditional formal notations in this setting are highlighted. We argue that there is no single formalism exhibiting all the features required to capture properties of ODSs. As a solution, we propose an integrated development framework that involves two notations: the Unified Modeling Language and the Prototype Verification System. We discuss the motivation for the choice of these notations, provide an overview of a CASE tool we have developed to support the proposed framework, and present a case study to demonstrate usability of our approach.
46|5||On formalizing UML state machines using ASMs|We present a transparent yet rigorous conceptual framework for defining the semantics of dynamic UML diagrams. We illustrate the method for UML state machines, making the “semantic variation points” of UML explicit, as well as various ambiguities and omissions in the official UML documents. This includes the event deferring and completion mechanism, the meaning of atomic and durative actions, concurrent internal activities and conflict situations which may arise through the concurrent behavior of active objects.
46|5||Adding pattern related information in structural and behavioral diagrams|Design patterns capture the distilled experience of expert designers. The compositions of design patterns may reuse design experience and solve a set of problems. Design patterns and their compositions are usually modeled using unified modeling language (UML). When a design pattern is applied or composed with other patterns, the pattern-related information may be lost because UML does not track this information. Thus, it is hard for a designer to identify a design pattern when it is applied or composed. The benefits of design patterns are compromised because the designers cannot communicate with each other in terms of the design patterns they use when the design patterns are applied or composed. In this paper, we present notations to explicitly represent the structural and behavioral aspects of each pattern in the applications and compositions of design patterns. The notations allow us to maintain pattern-related information in class and collaboration diagrams. Thus, a design pattern is identifiable and traceable from its application and composition with others in these diagrams. A case study is used to illustrate our approach.
46|5||On the composition of Java frameworks control-flows|
46|5||A method for the automatic generation of test suites from object models|
46|5||Towards model-based generation of self-priming and self-checking conformance tests for interactive systems|
46|5||How to judge testing progress|
46|5||On the testing methods used by beginning software testers|This paper describes our experiences of the methods used by novice software testers to test their own programs, as well as their perception of the classification-tree method, which is a black box testing method first introduced by Grochtmann and Grimm. We conducted two case studies involving novice software testers. The subjects in the first study possessed one-year working experience while those in the second study had a wider range of working experiences. Both studies found that white box testing methods were initially far more popular than black box methods, but the majority of the subjects were convinced of the benefits of the classification-tree method after they had learned and used it. About two-third of them indicated their preference of the classification-tree method over the methods they originally used.
46|5||A structured experiment of test-driven development|Test Driven Development (TDD) is a software development practice in which unit test cases are incrementally written prior to code implementation. We ran a set of structured experiments with 24 professional pair programmers. One group developed a small Java program using TDD while the other (control group), used a waterfall-like approach. Experimental results, subject to external validity concerns, tend to indicate that TDD programmers produce higher quality code because they passed 18% more functional black-box test cases. However, the TDD programmers took 16% more time. Statistical analysis of the results showed that a moderate statistical correlation existed between time spent and the resulting quality. Lastly, the programmers in the control group often did not write the required automated test cases after completing their code. Hence it could be perceived that waterfall-like approaches do not encourage adequate testing. This intuitive observation supports the perception that TDD has the potential for increasing the level of unit testing in the software industry.
46|5||A secure methodology for interchangeable services|
46|5||Efficient mediators with closures for handling dynamic interfaces in an imperative language|
46|6|http://www.sciencedirect.com/science/journal/09505849/46/6|Editorial Board|
46|6||Software engineering aspects of constraint-based timetablingâa case study|This paper details the stages of building a substantial, carefully specified, fully tested and fully operational university and school timetabling system. This is reported as a case study in applying Constraint Satisfaction techniques. The emphasis is on the software engineering aspects of the problem. That is, Constraint Satisfaction problems are expressed in a language more familiar to the formal software engineering community. Moreover, this language is used to formulate domain constraints and heuristic information. In addition to that, the user's needs are looked at more closely. For instance, the system supplies indications useful for relaxing or reformulating the constraints of the problem when a solution satisfying these constraints is impossible to produce. This has a value in bringing Constraint Satisfaction one-step closer to formal specification, program verification and transformation.
46|6||A document-driven agent-based approach for business processes management|Due to the development of Internet and the desire of almost all departments of business organizations to be interconnected and to make data accessible at any time and any place, more and more workflow management systems are applied to business process management. In this paper, a mobile, intelligent and document-driven agent framework is proposed to model business process management system. Each mobile agent encapsulates a single document, which includes a set of business logic. It can achieve (1) trace ability: a function that enables administrators to monitor document processes easily, (2) document life cycle: a feature using agent life cycle to manage document life cycle and concurrent processing, and (3) dynamic scheduling: a document agent can dynamically schedule its itinerary, and a document control agent can dynamically schedule its services. We also implemented an official document management system explaining our approach by Aglets.
46|6||DPEM: a decentralized software process enactment model|This paper proposes a decentralized process enactment model called DPEM, which operates on the network. It consists of multiple developer sites, a name server site, a developer coordinator site, and an event manager site. DPEM also provides an intermediate process language called DPEL. To enact a process program written in a source process language (e.g. CSPL and APPL/A), the process program is first translated into DPEL segments, in which those for developer sites are composed of activities and synchronization information. Next, DPEL segments are transferred to DPEM sites. Then, activities in developer sites are enacted concurrently, during which activities are synchronized using synchronization information transferred to the developer sites. Major features offered by DPEM are: (1) every site in DPEM can enact DPEL segments and hence no site will become a bottleneck, (2) a software product is managed by the site that accesses it frequently, which reduces network flows, and (3) DPEM can coordinate heterogeneous process-centered software engineering environments using the intermediate process language DPEL.
46|6||Network application programming interfaces (APIs) performance on commodity operating systems|Network Application Programming Interfaces (APIs) are important components of network-based applications. They play a central role in the end-to-end performance ultimately delivered by networked applications. In addition, most network architectures exploit the underlying networking APIs in their designs. We present an empirical performance evaluation on the PC platform of the most popular networking APIs, namely: Winsock/BSD, Java, and RMI.To explore the impact of the underlying operating system and the Java Virtual Machine architecture on the networking APIs, we conducted performance tests on four widely used operating systems namely, Windows NT 4.0, Windows 2000, Linux, and Solaris 8. We found that RMI latency is 1.7 times higher over Java. Latency over Java is around two to three times higher than over native Windows or BSD sockets. Moreover, native sockets yield around 1.8 and 3.5 times higher throughput over Java and RMI, respectively. We hope that our results will be useful to application designers and developers and help them better optimize the end-to-end performance of their applications with a knowledge of the performance of the underlying networking APIs.
46|6||Reengineering windows software applications into reusable CORBA objects|CORBA is becoming one of the most important middleware for supporting object-oriented and client/server paradigms in distributed computing systems. However, application systems based on CORBA are still scarce to date. One reason is that only few CORBA object services have been developed. To have a new CORBA application, a programmer must make the effort to design a program with a CORBA interface. In our previous work [Proceedings of the Sixth IEEE Computer Society Workshop on Future Trends of Distributed Computing Systems (1997) 2], a re-engineering approach was proposed to convert RPC-based programs into CORBA objects. This has successfully increased the development of CORBA applications. However, the source code is required in this approach. In many cases, software designers cannot acquire the source code. This prevents adapting existing PC software applications, particularly for Windows applications. Our study addresses this problem. A graphic factory temperature monitor system, which integrates MS-Excel under MS-Windows, was implemented to demonstrate the feasibility of our approach.
46|6||Structuring professional cooperation|
46|6||A peer to peer (P2P) architecture for dynamic workflow management|This paper presents the architecture of a novel Peer to Peer (P2P) workflow management system. The proposed P2P architecture is based on concepts such as a Web Workflow Peers Directory (WWPD) and Web Workflow Peer (WWP). The WWPD is an active directory system that maintains a list of all peers (WWPs) that are available to participate in Web workflow processes. Similar to P2P systems such as Napster and Gnutella, it allows peers to register with the system and offer their services and resources to other peers over the Internet. Furthermore, the architecture supports a novel notification mechanism to facilitate distributed workflow administration and management.
46|7|http://www.sciencedirect.com/science/journal/09505849/46/7|Inside Front Cover|
46|7||An object-oriented analysis method for customer relationship management information systems|For the advances of Internet technologies in recent years, Electronic Commerce (EC) has gained many attentions as a major theme for enterprises to keep their competitiveness. Amongst all possibly desired endeavors for the EC, research has shown that effective management of customer relationships is a major source for keeping competitive differentiation. Therefore, it is commonly recognized as an important goal for an enterprise to promote its management of customer relationships through a prospect information system on the Internet to achieve the so-called Business-to-Customer EC. In this paper, we propose an object-oriented analysis method for the development of such a Customer Relationship Management Information System (CRMIS). The approach starts from the identification of prospect customers and their desired behaviors under preferable execution environments, and ends with the specification of system—internal objects/entities that collaborate to satisfy these behaviors and environments. The method is a use case driven approach with UML utilized and extended as its tool. To illustrate, the method is applied to an exemplified CRMTS for house agency.
46|7||A distributed execution environment for shared java objects|This paper discusses the implementation of a distributed execution environment, DJO, which supports the use of shared Java objects for parallel and distributed applications and provides the Java programmer with the illusion of a network-wide shared object space on loosely coupled distributed systems. DJO supports shared objects through an implementation of multiple reader/single writer write-invalidate DSM protocol in software, providing the shared memory abstraction at object granularity. Object distribution and sharing are implemented through the replication mechanism, transparently to application. The system enforces mutual consistency among replicas of an object. The main benefits of DJO are enhanced availability and performance due to the replicated object model and easier application design, as the underlying software takes care of distribution and memory consistency issues.
46|7||Migrating legacy codes to distributed computing environments: a CORBA approach|
46|7||Safer language subsets: an overview and a case history, MISRA C|This paper gives an overview of safer language subsets in general and considers one widely-used one, MISRA C, in particular.
46|7||Communication and co-ordination practices in software engineering projects|In this paper we report on the investigation, description and analysis of communication and co-ordination practices in software engineering projects. We argue that existing models of the software process do not adequately address the situated, day-to-day practices in which software engineers collectively engage, yet it is through these practices that effective co-ordination is achieved. Drawing on concepts from organizational theory, we describe an approach for studying co-ordination activity in software engineering and the application of this approach to two real-world software projects. We describe key co-ordination issues in these projects and discuss their implications for software engineering practice.
46|7||Object versioning and information management|It is already widely accepted that the use of data abstraction in object oriented modelling enables real world objects to be well represented in information systems. However, the issue of how to deal with the continuity problem during gradual or sudden changes of objects continues to pose conceptual and technical challenges. This paper investigates the use of object versioning techniques to examine the continuity and pattern of changes of objects over time. In adopting this the authors assess not only attributes changes to homogenous objects, but also behaviour changes that lead to transforming or destroying existing objects and creating new ones.
46|8|http://www.sciencedirect.com/science/journal/09505849/46/8|Inside Front Cover|
46|8||Toward reuse of object-oriented software design models|In software reuse, which is an important approach to improving the practice of software engineering, many factors may hinder reusing software artifacts. Among those factors are the availability of software artifacts at a different level of abstraction and a method to classify and retrieve them. This paper proposes an approach based on faceted classification scheme for the classification and retrieval of software design artifacts, namely Object-Oriented Design Models, thus facilitating their reuse. Six facets, Domain, Abstractions, Responsibilities, Collaborations, Design View, and Asset Type have been defined to constitute the classification and the retrieval attributes. Each of the facets describes one aspect of an Object-Oriented design model. It contains a number of predefined terms chosen through the analysis of various software systems specifications. The selected terms of each facet are arranged on a conceptual graph to aid the retrieval process. A design artifact is classified by associating with it a software descriptor through the selection of one or more terms from each facet. The role of a descriptor is to emphasize the important structural and behavioral properties of a design artifact and also to document the artifacts associated with the design model. The associated similarity-based retrieval mechanism helps users to search for candidate design artifacts that best match their target specification. The similarity analysis is based on the estimation of the conceptual distance between the terms in a query descriptor and the terms in the specified descriptors of various design models in a software repository. A case study is presented to illustrate the classification and the retrieval process.
46|8||Prediction of software development faults in PL/SQL files using neural network models|Database application constitutes one of the largest and most important software domains in the world. Some classes or modules in such applications are responsible for database operations. Structured Query Language (SQL) is used to communicate with database middleware in these classes or modules. It can be issued interactively or embedded in a host language. This paper aims to predict the software development faults in PL/SQL files using SQL metrics. Based on actual project defect data, the SQL metrics are empirically validated by analyzing their relationship with the probability of fault detection across PL/SQL files. SQL metrics were extracted from Oracle PL/SQL code of a warehouse management database application system. The faults were collected from the journal files that contain the documentation of all changes in source files. The result demonstrates that these measures may be useful in predicting the fault concerning with database accesses. In our study, General Regression Neural Network and Ward Neural Network are used to evaluate the capability of this set of SQL metrics in predicting the number of faults in database applications.
46|8||Comparison of fault classes in specification-based testing|Our results extending Kuhn's fault class hierarchy provide a justification for the focus of fault-based testing strategies on detecting particular faults and ignoring others. We develop a novel analytical technique which allows us to elegantly prove that the hierarchy applies to arbitrary expressions, not just those in disjunctive normal form. We also use the technique to extend the hierarchy to a wider range of fault classes. To demonstrate broad applicability, we compare faults in practical situations and analyze previous results. In particular, using our technique, we show that the basic meaningful impact strategy of Weyuker et al. tests for stuck-at faults, not just variable negation faults.
46|8||Component documentationâa key issue in software product lines|
46|8||Data fusion application from evidential databases as a support for decision making|We present in this paper, a data fusion [Third International Conference in Information Fusion, Paris, 2000] algorithm from evidential databases [IEEE SMC’02, 2002]. This algorithm has the specificity of using a hybrid operator avoiding the overwhelming trace problem. Besides, it uses a specific scale for computing reliabilities of information's sources. We apply then data fusion to a company in order to improve the decision-making process.
46|9|http://www.sciencedirect.com/science/journal/09505849/46/9|Editorial Board|
46|9||The design, implementation, and performance of the V2 temporal document database system|It is now feasible to store previous versions of documents, and not only the most recent version which has been the traditional approach. This is of interest in a number of application, both temporal document databases as well as web archiving systems and temporal XML warehouses. In this paper, we describe the architecture and the implementation of V2, a temporal document database system that supports storage, retrieval, and querying of temporal documents. We also give some performance results from a mini-benchmark run on the V2 prototype.
46|9||An aspect-based approach to modeling access control concerns|
46|9||Size and effort estimation for applications written in Java|The paper presents a methodology for estimation of software size and effort at early stages of software development. The research concentrates on the size estimation problem, which seems to be the weakest element of cost estimation. The methodology concerns the object-oriented technology and the Java language.
46|9||How useâoriented development can take place|Usability is still a problem for software development. As the introduced software changes the use context, use qualities cannot be fully anticipated. Close co-operation between users and developers during development has been proposed as a remedy. Others fear such involvement of users as it might jeopardize planning and control. Based on the observation of an industrial project, we show how user participation and control can be achieved at the same time. The present article discusses the specific measures that allowed for co-operation between users and developers in an industrial context. It indicates measures to improve software development by focusing on use-orientation, i.e. allowing for user–developer co-operation.
46|9||Method configuration: adapting to situational characteristics while creating reusable assets|The world of systems engineering methods is changing as rigorous ‘off-the-shelf’ methods gain popularity. The need for configuration of such methods is increasing accordingly. In this paper, method configuration is treated as a kind of method engineering, focusing on adaptation of a base method. A meta-method based on the concepts of Configuration Packages and Configuration Templates is proposed. Configuration Packages are pre-made reusable configurations of a base method suitable for a specific characteristic of a development situation. Configuration Templates with different characteristics can be related to different Configuration Packages and used as a base for reaching a situational method efficiently. The paper presents experiences from two empirical studies in which the Method for Method Configuration was developed and validated. These studies indicate that this meta-method eases the burden of the method engineer in configuring a method for particular project characteristics. Specifically it helped in deciding what in the base method to omit and to make sure that omissions made were congruent with the overall situational method.
47|1|http://www.sciencedirect.com/science/journal/09505849/47/1|INSIDE FRONT COVER|
47|1||Systematic Reviews in Evidence-based Software Technology and Software Engineering|
47|1||Assessing effort estimation models for corrective maintenance through empirical studies|We present an empirical assessment and improvement of the effort estimation model for corrective maintenance adopted in a major international software enterprise. Our study was composed of two phases. In the first phase we used multiple linear regression analysis to construct effort estimation models validated against real data collected from five corrective maintenance projects. The model previously adopted by the subject company used as predictors the size of the system being maintained and the number of maintenance tasks. While this model was not linear, we show that a linear model including the same variables achieved better performances. Also we show that greater improvements in the model performances can be achieved if the types of the different maintenance tasks is taken into account. In the second phase we performed a replicated assessment of the effort prediction models built in the previous phase on a new corrective maintenance project conducted by the subject company on a software system of the same type as the systems of the previous maintenance projects. The data available for the new project were finer grained, according to the indications devised in the first study. This allowed to improve the confidence in our previous empirical analysis by confirming most of the hypotheses made. The new data also provided other useful indications to better understand the maintenance process of the company in a quantitative way.
47|1||Software productivity and effort prediction with ordinal regression|In the area of software cost estimation, various methods have been proposed to predict the effort or the productivity of a software project. Although most of the proposed methods produce point estimates, in practice it is more realistic and useful for a method to provide interval predictions. In this paper, we explore the possibility of using such a method, known as ordinal regression to model the probability of correctly classifying a new project to a cost category. The proposed method is applied to three data sets and is validated with respect to its fitting and predictive accuracy.
47|1||Adaptive fuzzy logic-based framework for software development effort prediction|Algorithmic effort prediction models are limited by their inability to cope with uncertainties and imprecision present in software projects early in the development life cycle. In this paper, we present an adaptive fuzzy logic framework for software effort prediction. The training and adaptation algorithms implemented in the framework tolerates imprecision, explains prediction rationale through rules, incorporates experts knowledge, offers transparency in the prediction system, and could adapt to new environments as new data becomes available. Our validation experiment was carried out on artificial datasets as well as the COCOMO public database. We also present an experimental validation of the training procedure employed in the framework.
47|1||A comparison of four process metamodels and the creation of a new generic standard|Software development processes and methodologies to date have frequently been described purely textually. However, more recently, a number of metamodels have been constructed to both underpin and begin to formalize these methodologies. We have critically examined four of these: the Object Management Group's Software Process Engineering Metamodel (SPEM), the OPEN Process Framework (OPF), the OOSPICE metamodel for capability assessment and the LiveNet approach for computer-supported collaborative work (CSCW). Based on this analysis, a new, combined metamodel, named Standard Metamodel for Software Development Methodologies (SMSDM) has been constructed which supports not only process but also products and capability assessment in the contexts of both software development and CSCW. As a proof of concept we conclude with a partial example to show how the SMSDM metamodel (and by inference the other metamodels) are used in practice by creating a simple yet usable methodology.
47|10|http://www.sciencedirect.com/science/journal/09505849/47/10|Combating architectural degeneration: a survey|As software systems evolve over time, they invariably undergo changes that can lead to a degeneration of the architecture. Left unchecked, degeneration may reach a level where a complete redesign is necessary, a task that requires significant effort. In this paper, we present a survey of technologies developed by researchers that can be used to combat degeneration, that is, technologies that can be employed in identifying, treating and researching degeneration. We also discuss the various causes of degeneration and how it can be prevented.
47|10||Parallel testing of distributed software|The paper presents the experience of the use of parallel computing technologies to accelerate the testing of a complex distributed programming system such as Orbix 3, which is IONA's implementation of the CORBA 2.1 standard. The design and implementation of the parallel testing system are described in detail. Experimental results proving the high efficiency of the system are given.
47|10||A theoretical foundation of variability in component-based developmentâ|Component-Based Development (CBD) is revolutionizing the process of building applications by assembling pre-built reusable components. Components should be designed more for inter-organizational reuse, rather than intra-organization reuse through domain analysis which captures the commonality of the target domain. Moreover, the minor variations within the commonality should also be modeled and reflected in the design of components so that family members can effectively customize the components for their own purpose. To carry out domain analysis effectively and design widely reusable components, precise definitions of variability-related terms and a classification of variability types must be made. In this paper, we identify the fundamental difference between conventional variability and component variability, and present five types of variability and three kinds of variability scopes. Each type of variability is precisely defined for its applicable situations and guidelines. Having a formal view on variability, not only the domain analysis but also component customization can be effectively carried out in a precise manner.
47|10||Quality and comprehension of UML interaction diagrams-an experimental comparison|UML (Unified Modeling Language) is a collection of somewhat overlapping modeling techniques, thus creating a difficulty in establishing practical guidelines for selecting the most suitable techniques for modeling OO artifacts. This is true mainly with respect to two types of interaction diagrams: Sequence and collaboration. Attempts have been made to evaluate the comprehensibility of these diagram types for various types of applications, but they did not address the issue of quality of diagrams created by analysts. This article reports the findings from a controlled experiment where both the comprehensibility and quality of the interaction diagrams were investigated in two application domains: management information systems (MIS) and real-time (RT) systems.
47|10||A survey of component based system quality assurance and assessment|Component Based Software Development (CBSD) is focused on assembling existing components to build a software system, with a potential benefit of delivering quality systems by using quality components. It departs from the conventional software development process in that it is integration centric as opposed to development centric. The quality of a component based system using high quality components does not therefore necessarily guarantee a system of high quality, but depends on the quality of its components, and a framework and integration process used. Hence, techniques and methods for quality assurance and assessment of a component based system would be different from those of the traditional software engineering methodology. It is essential to quantify factors that contribute to the overall quality, for instances, the trade off between cost and quality of a component, analytical techniques and formal methods, and quality attribute definitions and measurements. This paper presents a literature survey of component based system quality assurance and assessment; the areas surveyed include formalism, cost estimation, and assessment and measurement techniques for the following quality attributes: performance, reliability, maintainability and testability. The aim of this survey is to help provide a better understanding of CBSD in these aspects in order to facilitate the realisation of its potential benefits of delivering quality systems.
47|11|http://www.sciencedirect.com/science/journal/09505849/47/11|Communicating bugs: global bug knowledge distribution|Unfortunately, software-component libraries shared on a global scale contain bugs. Members of the library user community often report bugs, workarounds, and fixes. This bug knowledge, however, generally remain undiscovered on library web site or in open bug databases.
47|11||On the specification and implementation of distributed systems using NMDS and LIPS|This paper describes a graphical notation called NMDS and an implementation language called LIPS for producing distributed systems. NMDS is a set of notations based on Role Activity Diagrams with features for expressing concurrency, dataflow and communication and it lends itself to elicitation and verification while expressing concurrency unambiguously in a concise manner. It also fits in with the syntax and semantics of LIPS. LIPS is a distributed message passing language that promotes the separation of communication from computation by using the concept of guarded processes. One of the major advantages of using NMDS with LIPS is that reverse engineering of LIPS programs can easily be achieved.
47|11||A study on managing the performance requirements of a distributed service delivery software system|Like other non-functional requirements (NFR), performance requirements can have global impact on a system. This article presents a preliminary study on the management of performance requirements of a distributed service delivery (DSD) system using the ‘Performance Requirement Framework (PeRF)’ developed by Nixon [B.A. Nixon, Management of performance requirements for information systems, IEEE Transactions on Software Engineering 20 (12), (2000)]. Various steps in the framework are explained in the context of the DSD system, and the effects of some design decisions on the performance have been evaluated. PeRF was originally developed for information systems. The article shows how the framework can be adapted to a distributed system. However, additional studies on a number of distributed systems need to be carried out in order to develop a full framework for performance requirements for distributed systems, or perhaps for different categories of such systems.
47|11||A framework for evaluating a software bidding model|This paper discusses the issues involved in evaluating a software bidding model. We found it difficult to assess the appropriateness of any model evaluation activities without a baseline or standard against which to assess them. This paper describes our attempt to construct such a baseline. We reviewed evaluation criteria used to assess cost models and an evaluation framework that was intended to assess the quality of requirements models. We developed an extended evaluation framework and an associated evaluation process that will be used to evaluate our bidding model. Furthermore, we suggest the evaluation framework might be suitable for evaluating other models derived from expert-opinion based influence diagrams.
47|11||Experiences of using an evaluation framework|
47|12|http://www.sciencedirect.com/science/journal/09505849/47/12|An approach to ontology for institutional facts in the semantic web|Refinement in software engineering allows a specification to be developed in stages, with design decisions taken at earlier stages constraining the design at later stages. Refinement in complex data models is difficult due to lack of a way of defining constraints, which can be progressively maintained over increasingly detailed refinements. Category theory provides a way of stating wide scale constraints. These constraints lead to a set of design guidelines, which maintain the wide scale constraints under increasing detail. Previous methods of refinement are essentially local, and the proposed method does not interfere very much with these local methods. The result is particularly applicable to semantic web applications, where ontologies provide systems of more or less abstract constraints on systems, which must be implemented and therefore refined by participating systems. With the approach of this paper, the concept of committing to an ontology carries much more force.
47|12||Multi-way spatial join selectivity for the ring join graph|
47|12||Enhancing class commutability in the deployment of design patterns|A design pattern provides a structure to facilitate program changes with respect to a design concern. For example, the State pattern manages object behaviour in different internal states of objects. It allows new internal states of an object to be supported with the reuse of the object context. The deployment of a design pattern in a software program comprises a set of classes following the structure of the pattern. Within the set, classes that implement the managed concern of the pattern are commuted to new ones when changes related to the concern occur. However, commutation efforts can be tedious if these classes are accessed arbitrarily throughout the software. To confine the commutation efforts, these classes should be properly encapsulated. This paper proposes design restrictions in pattern deployments to achieve proper encapsulation. The approach is illustrated by a pattern-based program that supports appointment scheduling of multiple users. Preliminary experiments show that our approach facilitates program changes subject to multiple design concerns.
47|12||Computing dynamic slices of concurrent object-oriented programs|
47|12||A formal framework for database sampling|Database sampling is commonly used in applications like data mining and approximate query evaluation in order to achieve a compromise between the accuracy of the results and the computational cost of the process. The authors have recently proposed the use of database sampling in the context of populating a prototype database, that is, a database used to support the development of data-intensive applications. Existing methods for constructing prototype databases commonly populate the resulting database with synthetic data values. A more realistic approach is to sample a database so that the resulting sample satisfies a predefined set of integrity constraints. The resulting database, with domain-relevant data values and semantics, is expected to better support the software development process. This paper presents a formal study of database sampling. A Denotational Semantics description of database sampling is first discussed. Then the paper characterises the types of integrity constraints that must be considered during sampling. Lastly, the sampling strategy presented here is applied to improve the data quality of a (legacy) database. In this context, database sampling is used to incrementally identify the set of tuples which are the cause of inconsistencies in the database, and therefore should be the ones to be addressed by the data cleaning process.
47|13|http://www.sciencedirect.com/science/journal/09505849/47/13|Efficient discovery of multilevel spatial association rules using partitions|Spatial data mining has been identified as an important task for understanding and use of spatial data- and knowledge-bases. In this paper, we present a new approach to discover strong multilevel spatial association rules in spatial databases based on partitioning the set of rows with respect to the spatial relations denoted as relation table R. Meanwhile, the introduction of the equivalence partition tree makes the discovery of multilevel spatial association rules easy and efficient. Experiments show that the new algorithm is efficient.
47|13||XML-based requirements engineering for an electronic clearinghouse|We present methods and tools to support XML-based requirements engineering for an electronic clearinghouse that connects trading partners in the telecommunications area. The original semi-structured requirements, locally known as business rules, were written as message specifications in a non-standardized and error-prone format using MS Word. To remedy the resulting software failures and faults, we first formalized the requirements by designing an W3C XML Schema for the precise definition of the requirements structure. The schema allows a highly structured representation of the essential information in eXtensible Markup Language (XML). Second, to offer the requirements engineers the ability to edit the XML documents in a friendly way while preserving their information structure, we developed a custom editor called XLEdit. Third, by developing a converter from MS Word to the target XML format, we helped the requirements engineers to migrate the existing business rules. Fourth, we developed translators from the structured requirements to schema languages, which enabled automated generation of message-validation code. The increase in customer satisfaction and clearinghouse-service efficiency are primary gains from the investment in the technology for structured requirements editing and validation.
47|13||Measuring design testability of a UML class diagram|Design-for-testability is a very important issue in software engineering. It becomes crucial in the case of OO designs where control flows are generally not hierarchical, but are diffuse and distributed over the whole architecture. In this paper, we concentrate on detecting, pinpointing and suppressing potential testability weaknesses of a UML class diagram. The attribute significant from design testability is called ‘class interaction’ and is generalized in the notion of testability anti-pattern: it appears when potentially concurrent client/supplier relationships between classes exist in the system. These interactions point out parts of the design that need to be improved, driving structural modifications or constraints specifications, to reduce the final testing effort. In this paper, the testability measurement we propose counts the number and the complexity of interactions that must be covered during testing. The approach is illustrated on application examples.
47|13||Automated software size estimation based on function points using UML models|A systematic approach to software size estimation is important for accurate project planning. In this paper, we will propose the unified mapping of UML models into function points. The mapping is formally described to enable the automation of the counting procedure. Three estimation levels are defined that correspond to the different abstraction levels of the software system. The level of abstraction influences an estimate's accuracy. Our research, based on a small data set, proved that accuracy increases with each subsequent abstraction level. Changes to the FPA complexity tables for transactional functions will also be proposed in order to better quantify the characteristics of object-oriented software.
47|14|http://www.sciencedirect.com/science/journal/09505849/47/14|A roadmap of problem frames research|It has been a decade since Michael Jackson introduced problem frames to the software engineering community. Since then, he has published further work addressing problem frames as well as presenting several keynote addresses. Other authors have researched problem frames, have written about their experiences and have expressed their opinions. It was not until 2004 that an opportunity presented itself for researchers in the field to gather as a community. The first International Workshop on Advances and Applications of Problem Frames (IWAAPF'04) was held at the International Conference on Software Engineering in Edinburgh on 24th May 2004. This event attracted over 30 participants: Jackson delivered a keynote address, researchers presented their work and an expert panel discussed the challenges of problem frames.
47|14||Problem frames and software engineering|A general account is given of the problem frames approach to the development of software-intensive systems, assuming that the reader is already familiar with its basic ideas. The approach is considered in the light of the long-standing aspiration of software developers to merit a place among practitioners of the established branches of engineering. Some of its principles are examined, and some comments offered on the range of its applicability. A view of the approach is suggested by an important account of engineering in the aeronautical industry: in particular, the problem classes captured by elementary problem frames are likened to those solved in established engineering branches by normal, rather than radical, design. The relative lack of specialisation in software development is identified as an important factor holding back the evolution of normal design practice in some areas.
47|14||An approach to formal automated analysis of problem-frame concerns|
47|14||A UML-based approach for problem frame oriented software development|We propose a software development approach that combines the use of the structuring concepts provided by problem frames, the use of the UML notation, together with our methodological approach for well-founded methods. Problem frames are used to provide a first idea of the main elements of the problem under study. Then we provide ad hoc UML based development methods for some of the most relevant problem frames together with precise guidelines for the users. The general idea of our method is that, for each frame, several artifacts have to be produced, each one corresponding to a part of the frame. The description level may range from informal and sketchy, to formal and precise, while this approach is drawn from experience in formal specifications. Thus we show how problem frames may be used upstream of a development method to yield an improved and more efficient method equipped with the problem frames structuring concepts.
47|15|http://www.sciencedirect.com/science/journal/09505849/47/15|Most cited journal articles in software engineering|
47|15||An analysis of the most cited articles in software engineering journalsâ1999|Citations and related work are crucial in any research to position the work and to build on the work of others. A high citation count is an indication of the influence of specific articles. The importance of citations means that it is interesting to analyze which articles are cited the most. Such an analysis has been conducted using the ISI Web of Science to identify the most cited software engineering journal articles published in 1999. The objective of the analysis is to identify and list the articles that have influenced others the most as measured by citation count. An understanding of which research is viewed as most valuable to build upon may provide valuable insights into what research to focus on now and in the future. Based on the analysis, a list of the 20 most cited articles is presented here. The intention of the analysis is twofold. First, to actually show the most cited articles, and second, to invite the authors of the most cited articles in 1999 to contribute to a special issue of Information and Software Technology. Five invited authors have accepted the invitation and their articles are appearing in this special issue. Moreover, the research topics and methods of the most cited articles in 1999 are compared with those from the most cited articles in 1994 to provide a picture of similarities and differences between the years.
47|15||Engineering a software tool for gene structure prediction in higher organisms|The research area now commonly called ‘bioinformatics’ has brought together biologists, computer scientists, statisticians, and scientists of many other fields of expertise to work on computational solutions to biological problems. A large number of algorithms and software packages are freely available for many specific tasks, such as sequence alignment, molecular phylogeny reconstruction, or protein structure determination. Rapidly changing needs and demands on data handling capacity challenge the application providers to consistently keep pace. In practice, this has led to many incremental advances and re-writing of code that present the user community with confusing options and a large overhead from non-standardized implementations that need to be integrated into existing work flows. This situation gives much scope for contributions by software engineers. In this article, we describe an example of engineering a software tool for a specific bioinformatics task known as spliced alignment. The problem was motivated by disabling limitations in an original, ad hoc, and yet widely popular implementation by one of the authors. The present collaboration has led to a robust, highly versatile, and extensible tool (named GenomeThreader) that not only overcomes the limitations of the earlier implementation but greatly improves space and time requirements.
47|15||A measurement framework for object-oriented software testability|Testing is an expensive activity in the development process of any software system. Measuring and assessing the testability of software would help in planning testing activities and allocating required resources. More importantly, measuring software testability early in the development process, during analysis or design stages, can yield the highest payoff as design refactoring can be used to improve testability before the implementation starts.
47|15||BP's multi-enterprise asset management system|BP is one of the largest energy companies in the world with 2003 revenues of $233 billion. In this paper, we analyse its use of an innovative ‘multi-enterprise asset management system’ that supports and enables the asset management strategy of BP's exploration and production division on the UK continental shelf (UKCS). The analysis focuses on how BP connects its business processes with over 1500 suppliers to co-ordinate the maintenance, operation and repair of specialised exploration and production equipment. The systems strategy is novel because it takes the enterprise computing concept and implements it across organisational boundaries—hence the term ‘multi-enterprise system’. This use of a shared system with all of its suppliers is distinctive from the most common way of connecting with economic partners which is to use shared data systems based on common data standards and communication technologies such as EDI and more recently XML-based systems within vertical industries such as RosettaNet. The design of the multi-enterprise system is based on a sophisticated business process management system called Maximo and this is used to illustrate the systems design aspect of the overall information system in the broader contexts of business strategy and information technology infrastructure.
47|15||Software project management using PROMPT: A hybrid metrics, modeling and utility framework|In this paper, we present a ‘forward-looking’ decision support framework that integrates up-to-date metrics data with simulation models of the software development process in order to support the software project management control function. This forward-looking approach (called the PROMPT method) provides predictions of project performance and the impact of various management decisions. Tradeoffs among performance measures are accomplished using outcome based control limits (OBCLs) and are augmented using multi-criteria utility functions and financial measures of performance to evaluate various process alternatives. The decision support framework enables the program manager to plan, manage and track current software development activities in the short term and to take corrective action as necessary to bring the project back on track. The model provides insight on potential performance impacts of the proposed corrective actions. A real world example utilizing a software process simulation model is presented.
47|15||Simulating families of studies to build confidence in defect hypotheses|While it is clear that there are many sources of variation from one development context to another, it is not clear a priori what specific variables will influence the effectiveness of a process in a given context. For this reason, we argue that knowledge about software process must be built from families of studies, in which related studies are run within similar contexts as well as very different ones. Previous papers have discussed how to design related studies so as to document as precisely as possible the values of likely context variables and be able to compare with those observed in new studies. While such a planned approach is important, we argue that an opportunistic approach is also practical. The approach would combine results from multiple individual studies after the fact, enabling recommendations to be made about process effectiveness in context.
47|15||Referree List (see INFSOF 44/2 for template)|
47|15||Author Index|
47|15||Keyword Index|
47|15||Volume Contents|
47|2|http://www.sciencedirect.com/science/journal/09505849/47/2|INSIDE FRONT COVER|
47|2||Business-oriented process improvement: practices and experiences at Thales Naval The Netherlands (TNNL)|
47|2||Designing an adaptable heterogeneous abstract machine by means of reflection|The concepts of abstract and virtual machines have been used for many different purposes to obtain diverse benefits such as code portability, compiler simplification, interoperability, distribution and direct support of specific paradigms. Despite of these benefits, the main drawback of virtual machines has always been execution performance. Consequently, there has been considerable research aimed at improving the performance of virtual machine's application execution compared to its native counterparts. Techniques like adaptive Just In Time compilation or efficient and complex garbage collection algorithms have reached such a point that Microsoft and Sun Microsystems identify this kind of platforms as appropriate to implement commercial applications.
47|2||A documentation infrastructure for the management of data mining projects|Effective project management is a key factor for successful Knowledge Discovery in Databases (KDD) projects. The systematic documentation of previous knowledge, experiments, data and results is a helpful means of keeping track of the project current status. Despite its value, documentation is most often perceived as an overhead. We propose a documentation infrastructure composed of a documentation model and a supporting environment that allows the capture, storage and retrieval of KDD process-related information and artifacts. The paper describes this infrastructure, and reports preliminary experiences on its use. Preliminary results reveal generalized satisfaction with regard to infrastructure expressiveness and functionality, and highlight the contributions of the documentation produced for improving project management, project execution and team communication. The role of documentation in learning and reuse was also identified.
47|2||An exploratory study into the use of qualitative research methods in descriptive process modelling|The paper describes an exploratory study that investigated two descriptive software process models derived from the same process data using two different techniques. To set the context, the paper describes qualitative methods, particularly grounded theory and its techniques, and then explores the nature of the differences in the two models produced. It suggests ways in which constant comparison may contribute to the process-modelling task. As far as we are aware, it also serves as the first exploratory research on the application of this method in the software engineering process research domain. Based on data analysis using the technique of constant comparison often used in grounded theory research, a naive process modeller derived one of the models. An experienced process engineer relying heavily on experience and skill using an ad hoc approach derived the second model. The aim of the study was to explore differences in the models derived and to use this comparison as a basis for reflection on the method conventionally used in descriptive process modelling in contrast with the use of more formal qualitative analysis. The results show that (1) data analysis using the technique of constant comparison could be successfully applied to analyse process data, (2) the person with little experience in process modelling could produce a process model based on the data analysis using constant comparison and (3) the process model produced by the naive modeller was not equivalent to that produced by an experienced process engineer.
47|2||Fast mining of frequent tree structures by hashing and indexing|Hierarchical semistructured data arise frequently in the Web, or in biological information processing applications. Semistructured objects describing the same type of information have similar but not identical structure. Usually they share some common ‘schema’. Finding the common schema of a collection of semistructured objects is a very important task and due to the huge amount of such data encountered, data mining techniques have been employed.
47|3|http://www.sciencedirect.com/science/journal/09505849/47/3|INSIDE FRONT COVER|
47|3||Evaluating the ordering of the SPICE capability levels: an empirical study|
47|3||Î¼cROSE: automated measurement of COSMIC-FFP for Rational Rose RealTime|During the last 10 years, many organizations have invested resources and energy in order to be rated at the highest level as possible according to some maturity models for software development. Since measures play an important role in these models, it is essential that CASE tools offer facilities to automatically measure the sizes of various documents produced using them. This paper introduces a tool, called Î¼cROSE, that automatically measures the functional software size, as defined by the COSMIC-FFP method, for Rational Rose RealTime models. Î¼cROSE streamlines the measurement process, ensuring repeatability and consistency in measurement while reducing measurement cost. It is the first tool to address automatic measurement of COSMIC-FFP and it can be integrated into the Rational Rose RealTime toolset.
47|3||DMC: a more precise cohesion measure for classes|
47|3||ECâa measurement based safer subset of ISO C suitable for embedded system development|With the explosive growth of embedded systems, there is a major need for a standardised code of practice in the use of C. Although this area has been explored before, progress has been ad hoc and most importantly, carried out in the general absence of any measurement support. This paper attempts to define a relatively small number of rules which avoid known fault modes in the language which have published occurrence rates. It will studiously avoid any rules for which no measurement support is available. It is anticipated that the base subset may be extended as time goes by as further data becomes available. Much of the subset is equally relevant to ISO C++, although very little data exists to guide similar initiatives for the considerable part of C++, which lies outside C.
47|3||NCDS: data mining for discovering interesting network characteristics|This paper presents an approach to observe network characteristics based on data mining framework. Consequently, such observations may be expressed in structured patterns to support the process of network planning. The underlying system monitors the network protocol tables that describe each network connection or host session in order to discover interesting patterns. To achieve this purpose a data abstraction procedure is applied to learn rules that may express the behavior of network characteristics. Thus, the system is capable to discover various operational patterns, provide sensible advices, and support the network planning activity.
47|3||An address mapping approach for test data generation of dynamic linked structures|Software testing is an important technique to assure the correctness of the software. One of the essential prerequisite tasks of software testing is test data generation. This paper proposes an approach to generate test data specifically for dynamic pointer structures. In our context, a pointer is considered and handled as a location in memory, represented by a dynamic linear array that expands and shrinks during execution. As such, pointer test data can be directly generated from this linear array. The proposed technique can also support any dynamic structures, as well as homogeneous and heterogeneous recursive structures.
47|4|http://www.sciencedirect.com/science/journal/09505849/47/4|An information extraction approach to reorganizing and summarizing specifications|
47|4||Replicating software engineering experiments: a poisoned chalice or the Holy Grail|Recently, software engineering has witnessed a great increase in the amount of work with an empirical component; however, this work has often little or no established empirical framework within the topic to draw upon. Frequently, researchers use frameworks from other disciplines in an attempt to alleviate this deficiency. A common underpinning in these frameworks is that experimental replication is available as the cornerstone of knowledge discovery within the discipline. This paper investigates the issues involved in accepting this premise as a fundamental building block with empirical software engineering and recommends extending the traditional view of replication to improve the effectiveness of this essential process within our domain.
47|4||A unified classification system for research in the computing disciplines|The field of computing is made up of several disciplines of which Computer Science, Software Engineering, and Information Systems are arguably three of the primary ones. Despite the fact that each discipline has a specific focus, there is also considerable overlap. Knowledge sharing, however, is becoming increasingly difficult as the body of knowledge in each discipline increases and specialization results. For effective knowledge sharing, it is therefore important to have a unified classification system by means of which the bodies of knowledge that constitute the field may be compared and contrasted. This paper presents a multi-faceted system based on five research-focused characteristics: topic, approach, method, unit of analysis, and reference discipline. The classification system was designed based on the requirements for effective classification systems, and was then used to investigate these five characteristics of research in the computing field.
47|4||Software development productivity on a new platform: an industrial case study|The high non-functional requirements on mobile telecommunication applications call for new solutions. An example of such a solution can be a software platform that provides high performance and availability. The introduction of such a platform may, however, affect the development productivity. In this study, we present experiences from research carried out at Ericsson. The purpose of the research was productivity improvement and assessment when using the new platform. In this study, we quantify and evaluate the current productivity level by comparing it with UNIX development. The comparison is based on two large, commercially, available systems. We reveal a factor of four differences in productivity. Later, we decompose the problem into two issues: code writing speed and average amount of code necessary to deliver a certain functionality. We assess the impact of both these issues. We describe the nature of the problem by identifying factors that affect productivity and estimating their importance. To the issues identified we suggest a number of remedies. The main methods used in the study are interviews and historical data research.
47|4||Comprehension and quality of analysis specificationsâa comparison of FOOM and OPM methodologies|
47|5|http://www.sciencedirect.com/science/journal/09505849/47/5|Change of Editors|
47|5||Postmortem reviews: purpose and approaches in software engineering|Conducting postmortems is a simple and practical method for organisational learning. Yet, not many companies have implemented such practices, and in a survey, few expressed satisfaction with how postmortems were conducted. In this article, we discuss the importance of postmortem reviews as a method for knowledge sharing in software projects, and give an overview of known such processes in the field of software engineering. In particular, we present three lightweight methods for conducting postmortems found in the literature, and discuss what criteria companies should use in defining their way of conducting postmortems.
47|5||Virtual workgroups in offshore systems development|The market for offshore systems development, motivated by lower costs in developing countries, is expected to increase and reach about $15 billion in the year 2007. Virtual workgroups supported by computer and communication technologies enable offshore systems development. This article discusses the limitations of using virtual work in offshore systems development, and describes development processes and management procedures amenable to virtual work in offshore development projects. It also describes a framework to use virtual work selectively, while offshore developing various types of information systems.
47|5||Deriving requirements from process models via the problem frames approach|Jackson's problem frames is an approach to describing a recurring software problem. It is presumed that some knowledge of the application domain and context has been gathered so that an appropriate problem frame can be determined. However, the identification of aspects of the problem, and its appropriate ‘framing’ is recognised as a difficult task. One way to describe a software problem context is through process modelling. Once contextual information has been elicited, and explicitly described, an understanding of what problems need to be solved should emerge. However, this use of process models to inform requirements is often rather ad hoc; the traceability from business process to software requirement is not always as straightforward as it ought to be. Hence, this paper proposes an approach for deriving and contextualising software requirements through use of the problem frames approach from business process models. We apply the approach on a live industrial e-business project in which we assess the relevance and usefulness of problem frames as a means of describing the requirements context. We found that the software problem did not always match easily with Jackson's five existing frames. Where no frame was identified, however, we found that Jackson's problem diagrams did couch the requirements in their right context, and thus application of the problem frames approach was useful. This implies a need for further work in adapting a problem frames approach to the context of e-business systems.
47|5||Supporting cartoon animation techniques in direct manipulation graphical user interfaces|If judiciously applied, the techniques of cartoon animation can enhance the illusion of direct manipulation that many human computer interfaces strive to present. In particular, animation can convey a feeling of substance in the objects that a user manipulates, strengthening the sense that real work is being done. This paper describes algorithms and implementation issues to support cartoon style graphical object distortion effects for direct manipulation user interfaces. Our approach is based on suggesting a range of animation effects by distorting the view of the manipulated object. To explore the idea, we added a warping transformation capability to the InterViews user interface toolkit.
47|5||Implementation of fuzzy classification in relational databases using conventional SQL querying|In this paper, a framework for implementing fuzzy classifications in information systems using conventional SQL querying is presented. The fuzzy classification and use of conventional SQL queries provide easy-to-use functionality for data extraction similar to the conventional non-fuzzy classification and SQL querying. The developed framework can be used as data mining tool in large information systems and easily integrated with conventional relational databases. The benefits of using the presented approach include more flexible data analysis and improvement of information presentation at the report generation phase. To confirm the theory, a prototype was developed based on the stored procedures and database extensions of Microsoft SQL Server 2000.
47|6|http://www.sciencedirect.com/science/journal/09505849/47/6|A novel approach for component-based fault-tolerant software development|With the recent advancements in component-based software engineering, there is an increasing trend in developing applications for highly reliable and critical systems using pre-validated and reusable software components. As these applications are inherently complex and component-interactions are not straightforward, there is an immediate need for a methodology that could aid in composition of these reusable components ensuring the correctness of the composed software system. In this paper, we illustrate how the concepts of category theory can be utilized to develop component-based fault-tolerant software systems that encompass software components capable of tolerating particular types of faults. Our proposed framework for the development of a composite fault-tolerant program and verification of its overall correctness has been realized through a mechanized formal tool.
47|6||Does UML make the grade? Insights from the software development community|The Unified Modeling Language (UML) has become the de facto standard for systems development and has been promoted as a technology that will help solve some of the longstanding problems in the software industry. However, there is still little empirical evidence supporting the claim that UML is an effective approach to modeling software systems. Indeed, there is much anecdotal evidence suggesting the contrary, i.e. that UML is overly complex, inconsistent, incomplete and difficult to learn. This paper describes an investigation into the adoption and use of UML in the software development community. A web-based survey was conducted eliciting responses from users of UML worldwide. Results indicate a wide diversity of opinion regarding UML, reflecting the relative immaturity of the technology as well as the controversy over its effectiveness. This paper discusses the results of the survey and charts of the course for future research in UML usage.
47|6||Exploring defect causes in products developed by virtual teams|This paper explores the effects of virtual development on product quality, from the viewpoint of ‘conformance to specifications’. Virtual Development refers to the development of products by teams distributed across space, time, and organization boundaries (hence virtual teams). Specifically, causes of defect injection and non- or late-detection are explored. Because of the practical difficulties of obtaining hard project-specific defect data, an approach was taken that relied upon accumulated expert knowledge. The accumulated expert knowledge based approach was found to be a practical alternative to an in-depth defect causal analysis on a per-project basis. Defect injection causes are concentrated in the Requirements Specification phases. Thus defect dispersion is likely to increase, as requirements specifications are input for derived requirements specifications in multiple, related sub-projects. Similarly, a concentration of causes for the non- or late-detection of defects was found in the Integration Test phases. Virtual development increases the likelihood of defects in the end product because of the increased likelihood of defect dispersion, because of new virtual development related defect causes, and because causes already existing in co-located development are more likely to occur. The findings are important for virtual development environments and (1) allow further research focusing on a framework for lowering residual defects, and (2) give insights that can be used immediately by practitioners to devise strategies for lowering residual defects.
47|6||Resource conscious development of middleware for control environments: a case of CORBA-based middleware for the CAN bus systems|While it is imperative to exploit middleware technologies in developing software for distributed embedded control systems, it is also necessary to tailor them to meet the stringent resource constraints and performance requirements of embedded control systems. In this paper, we propose a CORBA-based middleware for Controller Area Network (CAN) bus systems. Our design goals are to reduce the memory footprint and remote method invocation overhead of the middleware and make it support group communication that is often needed in embedded control systems. To achieve these, we develop a transport protocol on the CAN and a group communication scheme based on the publisher/subscriber model by realizing subject-based addressing that utilizes the message filtering mechanism of the CAN. We also customize the method invocation and message passing protocol of CORBA so that CORBA method invocations are efficiently serviced on a low-bandwidth network such as the CAN. This customization includes packed data encoding and variable-length integer encoding for compact representation of IDL data types.
47|6||Stepwise deployment methodology of a service oriented architecture for business communities|This paper describes the deployment of a Service Oriented Architecture in the specific context of the ‘Business Communities’, i.e. Communities of heterogeneous actors that cooperate in the same business area. The architecture is based on XML and Web Services technologies.
47|7|http://www.sciencedirect.com/science/journal/09505849/47/7|Translating relational schema into XML schema definition with data semantic preservation and XSD graph|
47|7||Designing secure databases|Security is an important issue that must be considered as a fundamental requirement in information systems development, and particularly in database design. Therefore security, as a further quality property of software, must be tackled at all stages of the development. The most extended secure database model is the multilevel model, which permits the classification of information according to its confidentiality, and considers mandatory access control. Nevertheless, the problem is that no database design methodologies that consider security (and therefore secure database models) across the entire life cycle, particularly at the earliest stages currently exist. Therefore it is not possible to design secure databases appropriately. Our aim is to solve this problem by proposing a methodology for the design of secure databases. In addition to this methodology, we have defined some models that allow us to include security information in the database model, and a constraint language to define security constraints. As a result, we can specify a fine-grained classification of the information, defining with a high degree of accuracy which properties each user has to own in order to be able to access each piece of information. The methodology consists of four stages: requirements gathering; database analysis; multilevel relational logical design; and specific logical design. The first three stages define activities to analyze and design a secure database, thus producing a general secure database model. The last stage is made up of activities that adapt the general secure data model to one of the most popular secure database management systems: Oracle9i Label Security. This methodology has been used in a genuine case by the Data Processing Center of Provincial Government. In order to support the methodology, we have implemented an extension of Rational Rose, including and managing security information and constraints in the first stages of the methodology.
47|7||Requirements engineering for organizational transformation|
47|7||Automatic implementation of constraints in component based applications|Component-based software architectures have become one of the predominant solutions in the software technologies scenario. As well, constraints have been assuming an ever more relevant role in modeling distributed systems as long as business rules implementation, design-by-contract practice, and fault-tolerance requirements are concerned. Nevertheless, component developers are not sufficiently supported by existing tools to implement these features.
47|8|http://www.sciencedirect.com/science/journal/09505849/47/8|From requirements negotiation to software architecture decisions|Architecture design and requirements negotiations are conceptually tightly related but often performed separately in real-world software development projects. As our prior case studies have revealed, this separation causes uncertainty in requirements negotiation that hinders progress, limits the success of architecture design, and often leads to wasted effort and substantial re-work later in the development life-cycle. Explicit requirements elicitation and negotiation is needed to be able to appropriately consider and evaluate architecture alternatives and the architecture alternatives need be understood during requirements negotiation. This paper propose the WinCBAM framework, extending an architecture design method, called cost benefit analysis method (CBAM) framework to include an explicit requirements negotiation component based on the WinWin methodology. We then provide a retrospective case study that demonstrates the use of the WinCBAM. We show that the integrated method is substantially more powerful than the WinWin and CBAM methods performed separately. The integrated method can assist stakeholders to elicit, explore, evaluate, negotiate, and agree upon software architecture alternatives based on each of their requirement Win conditions. By understanding the architectural implication of requirements they can be negotiated more successfully: potential requirements conflicts can be discovered or alleviated relatively early in the development life-cycle.
47|8||A lightweight approach for migrating web frameworks|Web application development frameworks, like the Java Server Pages framework (JSP), provide web applications with essential functions such as maintaining state information across the application and access control. In the fast paced world of web applications, new frameworks are introduced and old ones are updated frequently. A framework is chosen during the initial phases of the project. Hence, changing it to match the new requirements and demands is a cumbersome task.
47|8||Deriving objects from use cases in real-time embedded systems|In recent years, a number of use case-driven processes have emerged for the development of real-time embedded systems. In these processes, once requirements have been defined by use cases, the next step is usually to identify from that use cases, the central objects in the system and describing how they interact with one another. However, identifying objects/classes from the requirements is both a critical and hard task. This is mainly due to the lack of pragmatic technique that steers such a task. In this article, we present a systematic approach to identify objects from the use case model for the real-time embedded systems. After hierarchically decomposing the system into its parts, we first transform the use case structured-text style into an activity diagram, which may be reused in the next development activities. Second, we use the derived activity diagram for identifying objects. With the behavioural model, an object model can be viewed as a first cut at a design model, and is thus an essential input when the system is shaped in design and design implementation.
47|8||The impacts of quality and productivity perceptions on the use of software process improvement innovations|Numerous software process improvement (SPI) innovations have been proposed to improve software development productivity and system quality; however, their diffusion in practice has been disappointing. This research investigates the adoption of the Personal Software Process on industrial software projects. Quantitative and qualitative analyses reveal that perceived increases in software quality and development productivity, project management benefits, and innovation fit to development tasks, enhance the usefulness of the innovation to developers. Results underscore the need to enrich current technology acceptance models with these constructs, and serve to encourage project managers to adopt formal SPI methods if developers perceive the methods will have positive impacts on their productivity and system quality.
47|8||Embedded System Paranoia: a tool for testing embedded system arithmetic|The quality of arithmetic implementation is of concern to all who work with or depend on the results of numerical computations. Embedded systems have become enormously complicated and widespread in most if not all consumer devices in recent years so there is a clear need to measure the quality of the arithmetic in the same way that conventional systems have been measured for some time using programs such as the well-known paranoia. A new version of paranoia has been introduced specifically to extend the domain of testable systems to embedded control systems. This paper describes the development of ESP (Embedded System Paranoia) and gives example outputs and free download sites. The example outputs indicate that even today, the quality of arithmetic implementations cannot be taken for granted with numerous implementation problems being reported in those embedded environments tried so far.
47|8||iJob: an Internet-based job execution environment using asynchronous messaging|Various toolkits exist today for the distributed execution of computational algorithms on clusters of machines. These toolkits are often referred to by the terms ‘Grid Toolkits’, ‘Job Execution Environments’, and ‘Problem Solving Environments (PSEs)’. Here, we introduce iJob—an Internet-based job execution environment that sets out to meet many of the goals of PSEs, such as providing facilities and services to solve a class of problems. In addition, the iJob software allows execution of computational algorithms utilizing standard Internet technologies such as Java, XML, and asynchronous communication protocols. The goals of this project include: (1) deploying the toolkit easily to multiple platforms using the Java technologies; (2) running multiple types of algorithms and supporting multiple users simultaneously; (3) providing a web-based GUI for monitoring and controlling the status of jobs; and (4) providing security at both the user-level and at the network-level. The toolkit has been tested using several simulation codes on pools of Windows 2000 and Solaris systems.
47|9|http://www.sciencedirect.com/science/journal/09505849/47/9|From timetabling to train regulationâa new train operation model|
47|9||How to steer an embedded software project: tactics for selecting the software process model|Modern large new product developments (NPD) are typically characterized by many uncertainties and frequent changes. Often the embedded software development projects working on such products face many problems compared to traditional, placid project environments. One of the major project management decisions is then the selection of the project's software process model. An appropriate process model helps coping with the challenges, and prevents many potential project problems. On the other hand, an unsuitable process choice causes additional problems. This paper investigates the software process model selection in the context of large market-driven embedded software product development for new telecommunications equipment. Based on a quasi-formal comparison of publicly known software process models including modern agile methodologies, we propose a process model selection frame, which the project manager can use as a systematic guide for (re)choosing the project's process model. A novel feature of this comparative selection model is that we make the comparison against typical software project problem issues. Some real-life project case examples are examined against this model. The selection matrix expresses how different process models answer to different questions, and indeed there is not a single process model that would answer all the questions. On the contrary, some of the seeds to the project problems are in the process models themselves. However, being conscious of these problems and pitfalls when steering a project enables the project manager to master the situation.
47|9||Web cache management based on the expected cost of web objects|
47|9||The application of use cases in systems analysis and design specification|This paper begins by reviewing the application of use cases in the analysis and design phases of software development. At present, a use case derived in analysis is generally mapped into design through the synthesis of object behaviour for all scenarios associated with the use case. Hence the use case level of abstraction is not directly used in this process and a semantic gap exists between analysis and design. With informal textually based use case definitions this is to be expected, however, if the use cases themselves are given a more concrete structure, for example in the form of a statechart, then their direct use becomes more feasible.
48|1|http://www.sciencedirect.com/science/journal/09505849/48/1|Modelling software development across time zones|Economic factors and the World Wide Web are turning software usage and its development into global activities. Many benefits accrue from global development not least from the opportunity to reduce time-to-market through ‘around the clock’ working.
48|1||XQuery speedup by deploying structural redundancy in mapping XML into relations|In designing a relational schema, we often consider that an attribute of a table is replicated into other table to reduce the join cost. Maybe such a possible redundancy will be grasped through E/R model (i.e. semantic analysis). Similarly, in mapping XML into relations, we can consider some redundancies to enhance query performance and they can be grasped through the structural traits of DTD (or XML schema). Several practical structural redundancies are formulated in this paper. If given XML data and queries are very large and complex, finding essential replications may also be difficult, and two efficient search methods are introduced for helping the search. Since the search problem is NP-hard, the methods are heuristically designed. Finally, read and update query costs arising by employing the structural redundancy are analyzed experimentally and the efficiency of two search methods is analyzed. They showed that the replication strategy can be very useful.
48|1||Testing web applications|Traditional testing techniques are not adequate for web-based applications, since they miss their additional features such as their multi-tier nature, hyperlink-based structure, and event-driven feature. Limited work has been done on testing web applications. In this paper, we propose new techniques for white box testing of web applications developed in the .NET environment with emphasis on their event-driven feature. We extend recent work on modeling of web applications by enhancing previous dependence graphs and proposing an event-based dependence graph model. We apply data flow testing techniques to these dependence graphs and propose an event flow testing technique. Also, we present a few coverage testing approaches for web applications. Further, we propose mutation testing operators for evaluating the adequacy of web application tests.
48|1||Supporting use case based requirements engineering|Use cases that describe possible interactions involving a system and its environment are increasingly being accepted as effective means for functional requirements elicitation and analysis. In the current practice, informal definitions of use cases are used and the analysis process is manual. In this paper, we present an approach supported by a tool for use cases based requirements engineering. Our approach includes use cases formalization, a restricted form of natural language for use cases description, and the derivation of an executable specification as well as a simulation environment from use cases.
48|1||An application of Bayesian network for predicting object-oriented software maintainability|As the number of object-oriented software systems increases, it becomes more important for organizations to maintain those systems effectively. However, currently only a small number of maintainability prediction models are available for object-oriented systems. This paper presents a Bayesian network maintainability prediction model for an object-oriented software system. The model is constructed using object-oriented metric data in Li and Henry's datasets, which were collected from two different object-oriented systems. Prediction accuracy of the model is evaluated and compared with commonly used regression-based models. The results suggest that the Bayesian network model can predict maintainability more accurately than the regression-based models for one system, and almost as accurately as the best regression-based model for the other system.
48|10|http://www.sciencedirect.com/science/journal/09505849/48/10|Guest co-editorâs comments|
48|10||A quest for appropriate software fault models: Case studies on fault detection effectiveness of model-based test generation techniques|Model-based test generation (MBTG) is becoming an area of active research. These techniques differ in terms of (1) modeling notations used, and (2) the adequacy criteria used for test generation. This paper (1) reviews different classes of MBTG techniques at a conceptual level, and (2) reports results of three case studies comparing various techniques in terms of their fault detection effectiveness. Our results indicate that MBTG technique which employs mutation and explicitly generates state verification sequences has better fault detection effectiveness than those based on boundary values, and predicate coverage criteria for transitions. Instead of a default adequacy criteria, certain techniques allow the user to specify test objectives in addition to the model. Our experience indicates that the task of defining appropriate test objectives is not intuitive. Furthermore, notations provided to describe such test objectives may have inadequate expressive power. We posit the need for a suitable fault modeling notation which also treats domain invariants as first class entities.
48|10||Prioritized interaction testing for pair-wise coverage with seeding and constraints|Interaction testing is widely used in screening for faults. In software testing, it provides a natural mechanism for testing systems to be deployed on a variety of hardware and software configurations. In many applications where interaction testing is needed, the entire test suite is not run as a result of time or budget constraints. In these situations, it is essential to prioritize the tests. Here, we adapt a “one-test-at-a-time” greedy method to take importance of pairs into account. The method can be used to generate a set of tests in order, so that when run to completion all pair-wise interactions are tested, but when terminated after any intermediate number of tests, those deemed most important are tested. In addition, practical concerns of seeding and avoids are addressed. Computational results are reported.
48|10||Environment behavior models for automation of testing and assessment of system safety|This paper presents an approach to automatic scenario generation from environment behavior models for testing of real-time reactive systems. The model of behavior is defined as a set of events (event trace) with two basic relations: precedence and inclusion. The attributed event grammar (AEG) specifies possible event traces and provides a uniform approach for automatically generating and executing test cases. The environment model includes a description of hazardous states in which the system may arrive and makes it possible to gather statistics for system safety assessment. The approach is supported by a generator that creates test cases from the AEG models. We demonstrate the approach with a case study of a software prototype of the computer-assisted resuscitation algorithm for a safety-critical casualty intravenous fluid infusion pump.
48|10||Extended state identification and verification using a model checker|This article presents a method for the application of model checking, i.e., verifying a finite state system against a given temporal specification, on the problem of generating test inputs. The generated test inputs allow state characterization, i.e., the identification of internal states of the software under test by observation of the input/output behavior only. A test model is derived semi-automatically from a given state-based specification and the testing goal is specified in terms of temporal logic. On the basis of these inputs, a model checking tool performs the testing input generation automatically. In consequence, the complexity of our approach is strongly depending on the input model, the testing goal, and the model checking algorithm, which is implemented in the used tool. The presented approach can be adapted with small changes to other model checking tools. It is a capable test generation method, whenever a finite state model of the software under test exists. Furthermore, it provides a descriptive view on state-based testing, which may be beneficial in other contexts, e.g., education and program comprehension.
48|11|http://www.sciencedirect.com/science/journal/09505849/48/11|A longitudinal study of development and maintenance in Norway: Report from the 2003 investigation|
48|11||Using an experimental study to develop group awareness support for real-time distributed collaborative writing|Supporting group awareness is vital for the success of real-time, distributed, collaborative writing systems. Many awareness mechanisms have been introduced, but highly effective solutions are few. The research presented in this paper focuses on the development of awareness mechanisms using an experimental study of synchronous distributed collaborative writing. Our study has made two major contributions to research on group awareness.
48|11||Identifying knowledge brokers that yield software engineering knowledge in OSS projects|Much research on open source software development concentrates on developer lists and other software repositories to investigate what motivates professional software developers to participate in open source software projects. Little attention has been paid to individuals who spend valuable time in lists helping participants on some mundane yet vital project activities. Using three Debian lists as a case study we investigate the impact of knowledge brokers and their associated activities in open source projects. Social network analysis was used to visualize how participants are affiliated with the lists. The network topology reveals substantial community participation. The consequence of collaborating in mundane activities for the success of open source software projects is discussed. The direct beneficiaries of this research are in the identification of knowledge experts in open source software projects.
48|11||Optimization of analogy weights by genetic algorithm for software effort estimation|A reliable and accurate estimate of software development effort has always been a challenge for both the software industry and academia. Analogy is a widely adopted problem solving technique that has been evaluated and confirmed in software effort or cost estimation domains. Similarity measures between pairs of effort drivers play a central role in analogy-based estimation models. However, hardly any research has addressed the issue of how to decide on suitable weighted similarity measures for software effort drivers. The present paper investigates the effect on estimation accuracy of the adoption of genetic algorithm (GA) to determine the appropriate weighted similarity measures of effort drivers in analogy-based software effort estimation models. Three weighted analogy methods, namely, the unequally weighted, the linearly weighted and the nonlinearly weighted methods are investigated in the present paper. We illustrate our approaches with data obtained from the International Software Benchmarking Standards Group (ISBSG) repository and the IBM DP services database. The experimental results show that applying GA to determine suitable weighted similarity measures of software effort drivers in analogy-based software effort estimation models is a feasible approach to improving the accuracy of software effort estimates. It also demonstrates that the nonlinearly weighted analogy method presents better estimate accuracy over the results obtained using the other methods.
48|11||Empirical assessment of the impact of structural properties on the changeability of object-oriented software|The changeability of software can be viewed as the quality of being capable of change, which among others implies that the task of changing the software requires little effort. It is hypothesized that structural properties of the software affect changeability, in which case measures of such properties can be used as changeability indicators.
48|11||Bayesian statistical effort prediction models for data-centred 4GL software development|
48|11||An empirical study of relationships among extreme programming engineering activities|Extreme programming (XP) is an agile software process that promotes early and quick production of working code. In this paper, we investigated the relationship among three XP engineering activities: new design, refactoring, and error fix. We found that the more the new design performed to the system the less refactoring and error fix were performed. However, the refactoring and error fix efforts did not seem to be related. We also found that the error fix effort is related to number of days spent on each story, while new design is not. The relationship between the refactoring effort and number of days spent on each story was not conclusive.
48|11||On using cache conscious clustering for improving OODBMS performance|
48|11||Test diversity|This paper describes a novel method for measuring the degree to which a set of test cases executes a given program in diverse ways with respect to the two fundamental programming concepts: control and data. Test diversity is a method for measuring the variety of software control flow and data flow, comprising of four new measures: conditional diversity, data diversity, standard deviation of diversity, and test orthogonality. These closely related measures could be used to evaluate the test effectiveness and the test-effort distribution of a test suite.
48|11||Modelling Spatial WholeâPart relationships using an ISO-TC211 conformant approach|
48|11||VBP: An approach to modelling process variety and best practice|The concept of best practice is both attractive and highly problematic. Whilst organisations can learn from the practices of others there is also a danger that local variety may be squeezed out and that “one size fits all” solutions may stifle local context-specific innovations. This paper outlines an approach to modelling process specialisation hierarchies and best practice patterns with the Unified Modelling Language (UML). The process Variety and Best Practice approach, VBP, is applied to a recent e-government project that explored variety and best practice in citizens’ access portals for four UK local authorities.
48|11||An analysis of web services support for dynamic business process outsourcing|Outsourcing of business processes is crucial for organizations to be effective, efficient and flexible. In fast changing markets, dynamic outsourcing is required, in which business relationships are established and enacted on-the-fly in an adaptive, fine-grained way. This requires automated means for the establishment of outsourcing relationships and for the enactment of services performed in these relationships. Due to wide industry support and their model of loose coupling, Web Services have become the mechanism of choice to interconnect organizations. This paper analyzes Web Services support for the dynamic process outsourcing paradigm. We discuss contract-based outsourcing to define requirements, introduce the Web Services framework and investigate the match between the two. We observe that the framework requires further support for cross-organizational business processes and mechanisms for contracting, QoS management and transaction management. We suggest an approach to fill these gaps based on a business process support application layer implemented on Web Service technology.
48|12|http://www.sciencedirect.com/science/journal/09505849/48/12|Assuring quality of web-based applications|
48|12||Data flow analysis and testing of JSP-based Web applications|Web applications often rely on server-side scripts to handle HTTP requests, to generate dynamic contents, and to interact with other components. The server-side scripts are usually not checked by any compiler and, hence, can be error-prone. In this paper, we adapt traditional data flow testing techniques into the context of Java Server Pages (JSP), a very popular server-side script for developing Web applications with Java Technology. We point out that the JSP implicit objects and action tags can introduce several unique data flow test artifacts which need to be addressed. A test model is presented to capture the data flow information of JSP pages with considerations of various implicit objects and action tags. Based on the test model, we describe an approach to compute the intraprocedural, interprocedural, and sessional data flow test paths for uncovering the data anomalies of JSP pages.
48|12||Binary analysis and automated hot patching for Web-based applications|Patching technologies are commonly applied to improve the dependability of software after release. This paper reports the design of an automated hot patching (AHP) framework that fully automates reasoning for the causes of failures and patching the binary code of Web-based applications. AHP admits the hardness for rooting out all faults before product release, and autonomously patches problems of application programs. By directly operating on binary code, AHP is universal to virtually all applications. A promising application of AHP is to shortcut a function of the remote maintenance center (RMC) and hence to reduce the turn around time for patches.
48|12||An agent-based data-flow testing approach for Web applications|In recent years, Web applications (WAs) have grown so quickly that they have already become crucial to the success of businesses. However, since they are built on Internet and open standard technologies, WAs possess their own unique features, such as dynamic behaviors, heterogeneous representations, and novel data handling mechanisms. These features provide concrete support to the success of WAs, but they bring new challenges to researchers and developers, especially in regard to testing WAs and ensuring their quality. Testing approaches for non-WAs have to be extended to handle these features before they are used in WA testing. This paper presents an agent-based approach to perform data-flow testing of WAs. More precisely, the data-flow testing will be performed by autonomous test agents at the method level, object level, and object cluster level, from low abstraction level to high abstraction level. In the process of the recommended data-flow testing, an agent-based WA testing system (WAT) will automatically generate and coordinate test agents to decompose the task of testing an entire WA into a set of subtasks that can be accomplished by test agents. The test agents, rooted in the Belief–Desire–Intention (BDI) model, cooperate with each other to complete the testing of a WA. An example is used to show the feasibility of the proposed approach.
48|12||Testing Web-based applications: The state of the art and future trends|Software testing is a difficult task and testing Web-based applications may be even more difficult, due to the peculiarities of such applications. In the last years, several problems in the field of Web-based applications testing have been addressed by research work, and several methods and techniques have been defined and used to test Web-based applications effectively. This paper will present the main differences between Web-based applications and traditional ones, how these differences impact the testing of the former ones, and some relevant contributions in the field of Web application testing developed in recent years. The focus is mainly on testing the functionality of a Web-based application, even if some discussion about the testing of non-functional requirements is provided too. Some indications about future trends in Web application testing are also outlined in the paper.
48|12||Code-coverage guided prioritized test generation|Most automatic test generation research focuses on generation of test data from pre-selected program paths or input domains or program specifications. This paper presents a methodology for a full solution to code-coverage-based test case generation, which includes code coverage-based path selection, test data generation and actual test case representation in program’s original languages. We implemented this method in an automatic testing framework, eXVantage. Experimental results and industrial trials show that the framework is able to generate tests to achieve program line coverage from 20% to 98% with reduced overall testing effort. Our major contributions include an innovative coverage-based program prioritization algorithm, a novel path selection algorithm that takes into consideration program priority and functional calling relationship, and a constraint solver for test data generation that derives constraints from bytecode and solves complex constraints involving strings and dynamic objects.
48|12||Referree List|
48|12||Author Index|
48|12||Keyword Index|
48|12||Volume Contents|
48|2|http://www.sciencedirect.com/science/journal/09505849/48/2|Bridging patterns: An approach to bridge gaps between SE and HCI|Adding usability improving solutions during late stage development is to some extent restricted by the software architecture. However, few software engineers and human–computer interaction engineers are aware of this important constraint and as a result avoidable rework is frequently necessary. In this paper we present a new type of pattern called a bridging pattern. Bridging patterns extend interaction design patterns by adding information on how to generally implement this pattern. Bridging patterns can be used for architectural analysis: when the generic implementation is known, software architects can assess what it means in their context and can decide whether they need to modify the software architecture to support these patterns. This may prevent part of the high costs incurred by adaptive maintenance activities once the system has been implemented and leads to architectures with better support for usability.
48|2||Compacting XML documents|
48|2||MAPIS, a multi-agent system for information personalization|In the domain of multi-user and agent-oriented information systems, personalized information systems aim to give specific and customized responses to individual user requests. In addition to the ability to analyze user needs and to retrieve, understand and act on distributed data that is offered by any agent-oriented system, multi-agent systems also offer interesting possibilities for interaction, particularly with regard to information sharing and task coordination. Our approach exploits these interactive possibilities in order to make the system capable of personalizing information. In addition, reusable models at both the social and individual levels were chosen for this approach in order to facilitate subsequent information system design. With these two ideas in mind, several models of agent interaction (social) and the internal activity cycles (individual) have been proposed with the aim of creating a multi-agent system for information personalization.
48|2||Partial rollback in object-oriented/object-relational database management systems with dual buffer|
48|3|http://www.sciencedirect.com/science/journal/09505849/48/3|Genetic algorithm based software integration with minimum software risk|This paper investigates an approach of integrating software with a minimum risk using Genetic Algorithms (GA). The problem was initially proposed by the need of sharing common software components among various departments within a same organization. Two significant contributions have been made in this study: (1) an assimilation exchange based software integration approach is proposed; (2) the software integration problem is formulated as a search problem and solved by using a GA. A case study is based on an on-going software integration project carried out in the Derbyshire Fire Rescue Service, and used to illustrate the application of the approach.
48|3||The role of cultural diversity and leadership in computer-supported collaborative learning: a content analysis|Computer-mediated communication (CMC) technologies are increasingly being used to support collaborative learning in groups. Its potential to shift the traditional pedagogical paradigm triggers considerable amount of research. However, very few of the research studies focus on the social interactions and their influences on the learning process, which are crucial to understanding computer-supported collaborative learning (CSCL). This paper reports on a laboratory experiment with a 2×2 factorial design, conducted to investigate the influences of cultural diversity and leadership availability on the CSCL process using a content analysis approach. With the mediation of CMC systems, cultural diversity is found to engender more informational influences but reduce normative influences. Leadership has a positive effect on both normative and informational influences. Taking into account the learning outcomes, it is evident that the influences of the interaction process are closely related to CSCL effectiveness.
48|3||Dynamic graphical UML views from formal B specifications|This paper addresses the graphical representation of the behaviour of B specifications, using state transition diagrams. These diagrams can help understand the specification for stakeholders who are not familiar with the B method, such as customers or certification authorities. The paper first discusses the principles of the graphical representation on a deterministic example, featuring a small set of states. It then discusses the representation of specifications which feature a large or infinite set of states, or which are non-deterministic. Abstraction techniques are used to overcome these difficulties. They result in a variety of possible representations. Finally, three techniques, based on animation and proof, are presented to help construct the diagrams.
48|3||Dreamer: A resource management architecture for Jini federation|
48|3||Model for measuring quality of software in DVRS using the gap concept and fuzzy schemes with GA|A model of software quality is proposed to measure the quality of the software in a digital video recorder system (DVRS) during its development stage. The characteristics and metrics of this model are adopted from ISO/IEC 9126 and 14598. The model incorporates a Î»-fuzzy measure, a genetic algorithm and a hierarchical Choquet integral. It is based on the gap concept between perceive performance by the developers and satisfaction by the end-users, acquirers and evaluators of third parties in software development stage. A checklist about of the software quality is used to reduce the gap between the quality of the DVRS software quality as assessed by the developers and that as assessed by the end-users, acquirers and the evaluators of third parties.
48|3||Testing spreadsheet accuracy theory|Electronic spreadsheets are used extensively to support financial analysis and problem solving processes; however, research has revealed that experienced professionals and students make many errors when developing spreadsheets. Practitioners recognize the importance of accuracy and have published many techniques for improving the accuracy of their spreadsheets. Their prescriptions and results of research are consistent and together these works form the basis for spreadsheet accuracy theory. Three propositions describe how the three constructs influence spreadsheet accuracy in a laboratory experiment. The results of this study indicate that the Spreadsheet Accuracy Theory developed three aids that significantly improve development of accurate spreadsheets.
48|3||The relative influence of management control and userâIS personnel interaction on project performance|Research has failed to establish a conclusive link between levels of user involvement and information system project success. Communication and control theories indicate that the quality of interactions between users and inofrmation personnel may serve to better the coordinaton in a project and lead to greater success. A model is developed that directly relates management control to the quality of interaction and project success, with interaction quality as a potential intermediary. These variables provide a more distinct relationship to success as interactions are more structurally defined and controlled. A survey of 196 IS professionals provides evidence that management control techniques improve the quality of user–IS personnel interactions and eventual project success. These formal structures provide guidelines for managers in controlling the critical relations between users and IS personnel.
48|4|http://www.sciencedirect.com/science/journal/09505849/48/4|DEVSpecL: DEVS specification language for modeling, simulation and analysis of discrete event systems|
48|4||Group cohesion in organizational innovation: An empirical examination of ERP implementation|Enterprise Resource Planning systems present unique difficulties in implementation in that they typically involve changes to the entire organization and are a novel application for the organization. These characteristics add to the importance of making groups more cohesive in their goals, commitment, and ability to work toward completion of the new system project. Such cohesiveness is built partly through the willingness of the team members to participate and commitment to learning the new system. To determine if these relationships hold, a survey of users and managers in Taiwan was conducted to test a model derived from social capital theory. The data support the positive relationships between group cohesion and both willingness to participate and commitment to learning. Group cohesion is likewise positively related to meeting management goals. Resources within an organization should support the climate of learning and the building of team participation.
48|4||Mapping DTDs to relational schemas with semantic constraints|
48|4||From a B formal specification to an executable code: application to the relational database domain|This paper presents a formal approach for the development of trustworthy database applications. This approach consists of three complementary steps. Designers start by modeling applications using UML diagrams dedicated to database applications domain. These diagrams are then automatically translated into B specifications suitable not only for reasoning about data integrity checking but also for the derivation of trustworthy implementations. In this paper, we present a process based on the B refinement technique for the derivation of a SQL relational implementation, embedded in the JAVA language (JAVA/SQL), from a B specification obtained by the first translation phase.
48|4||An interactive service customization model|Mass customization has become one of the key strategies for a service provider to differentiate itself from its competitors in a highly segmented global service market. This paper proposes an interactive service customization model to support individual service offering for customers. In this model, not only that the content of an activity is customizable, but the process model can also be constructed dynamically according to the customer's requirements. Based on goal ontology, the on-demand customer requirements are transformed into a high-level service process model. Process components, which are building blocks for reusable standardized service processes, are designed to support on-demand process composition. The customer can incrementally define the customized service process through a series of operations, including activation of goal decomposition, reusable component selection, and process composition. In this paper, we first discuss the key requirements of the service customization problem. We then present in detail a knowledge-based customizable service process model and the accompanying customization method. Finally we demonstrate the feasibility of the our approach through a case study of the well-known travel planning problem and present a prototype system that enables users to interactively organize a satisfying travel plan.
48|4||How large are software cost overruns? A review of the 1994 CHAOS report|The Standish Group reported in their 1994 CHAOS report that the average cost overrun of software projects was as high as 189%. This figure for cost overrun is referred to frequently by scientific researchers, software process improvement consultants, and government advisors. In this paper, we review the validity of the Standish Group's 1994 cost overrun results. Our review is based on a comparison of the 189% cost overrun figure with the cost overrun figures reported in other cost estimation surveys, and an examination of the Standish Group's survey design and analysis methods. We find that the figure reported by the Standish Group is much higher than those reported in similar estimation surveys and that there may be severe problems with the survey design and methods of analysis, e.g. the population sampling method may be strongly biased towards ‘failure projects’. We conclude that the figure of 189% for cost overruns is probably much too high to represent typical software projects in the 1990s and that a continued use of that figure as a reference point for estimation accuracy may lead to poor decision making and hinder progress in estimation practices.
48|4||Software effort estimation terminology: The tower of Babel|It is well documented that the software industry suffers from frequent cost overruns. A contributing factor is, we believe, the imprecise estimation terminology in use. A lack of clarity and precision in the use of estimation terms reduces the interpretability of estimation accuracy results, makes the communication of estimates difficult, and lowers the learning possibilities. This paper reports on a structured review of typical software effort estimation terminology in software engineering textbooks and software estimation research papers. The review provides evidence that the term ‘effort estimate’ is frequently used without sufficient clarification of its meaning, and that estimation accuracy is often evaluated without ensuring that the estimated and the actual effort are comparable. Guidelines are suggested on how to reduce this lack of clarity and precision in terminology.
48|5|http://www.sciencedirect.com/science/journal/09505849/48/5|Evaluation and Assessment in Software Engineering (EASE 05)|
48|5||Assessing multiview framework (MF) comprehensibility and efficiency: A replicated experiment|Goal oriented quality models have become an important means for assessing and improving software quality. In previous papers, the authors have proposed an approach called multiview framework, for guiding quality managers in designing and managing a goal oriented quality model. This approach has been validated through a controlled experiment carried out with university students.
48|5||Process improvement for small firms: An evaluation of the RAPID assessment-based method|With increasing interest by the software development community in software process improvement (SPI), it is vital that SPI programs are evaluated and the reports of lessons learned disseminated. This paper presents an evaluation of a program in which low-rigour, one-day SPI assessments were offered at no cost to 22 small Australian software development firms. The assessment model was based on ISO/IEC 15504 (SPICE). About 12 months after the assessment, the firms were contacted to arrange a follow-up meeting to determine the extent to which they had implemented the recommendations from the assessment.
48|5||A preliminary study on the impact of a pair design phase on pair programming and solo programming|The drawback of pair programming is the nearly doubled personnel cost. The extra cost of pair programming originates from the strict rule of extreme programming where every line of code should be developed by a pair of developers. Is this rule not a waste of resources? Is it not possible to gain a large portion of the benefits of pair programming by only a small fraction of the meeting time of a pair programming session? We conducted a preliminary study to answer this question by splitting the pair programming process into a pair design and a pair implementation phase. The pair implementation phase is compared to a solo implementation phase, which in turn was preceeded by a pair design phase, as well. The study is preliminary as its major goal was to identify an appropriate sample size for subsequent experiments. The data from this study suggest that there is no difference in terms of development cost between a pair and a solo implementation phase if the cost for developing programs of similar level of correctness is concerned.
48|5||Trust in software outsourcing relationships: An empirical investigation of Indian software companies|This paper investigates trust in software outsourcing relationships. The study is based on an empirical investigation of eighteen high maturity software vendor companies based in India. Our analysis of the literature suggests that trust has received a lot of attention in all kinds of business relationships. This includes inter-company relationships, whether cooperative ventures or subcontracting relationships, and relationship among different parts of a single company. However, trust has been relatively under-explored in software outsourcing relationships. In this paper, we present a detailed empirical investigation of trust in commercial software outsourcing relationships. The investigation presents what vendor companies perceive about getting trust from client companies in outsourcing relationships. We present the results in two parts—(1) achieving trust initially in outsourcing relationships and (2) maintaining trust in ongoing outsourcing relationships. Our findings confirm that the critical factors to achieving trust initially in an outsourcing relationship include previous clients' reference and experience of vendor in outsourcing engagements. Critical factors identified for maintaining trust in an established outsourcing relationship include transparency, demonstrability, honesty, process followed and commitment. Our findings also suggest that trust is considered to be very fragile in outsourcing relationships.
48|6|http://www.sciencedirect.com/science/journal/09505849/48/6|Special Issue Editorial: WAMIS 2005 Workshop|
48|6||Efficient mining and prediction of user behavior patterns in mobile web systems|The development of wireless and web technologies has allowed the mobile users to request various kinds of services by mobile devices at anytime and anywhere. Helping the users obtain needed information effectively is an important issue in the mobile web systems. Discovery of user behavior can highly benefit the enhancements on system performance and quality of services. Obviously, the mobile user's behavior patterns, in which the location and the service are inherently coexistent, become more complex than those of the traditional web systems. In this paper, we propose a novel data mining method, namely SMAP-Mine that can efficiently discover mobile users' sequential movement patterns associated with requested services. Moreover, the corresponding prediction strategies are also proposed. Through empirical evaluation under various simulation conditions, SMAP-Mine is shown to deliver excellent performance in terms of accuracy, execution efficiency and scalability. Meanwhile, the proposed prediction strategies are also verified to be effective in measurements of precision, hit ratio and applicability.
48|6||Object-relational complex structures for XML storage|XML data can be stored in various database repositories, including Object-Relational Database (ORDB). Using an ORDB, we get the benefit of the relational maturity and the richness of Object-Oriented modeling, including various complex data types. These data types resemble the true nature of XML data and therefore, the conceptual semantic of XML data can be preserved. However, very often when the data is stored in an ORDB repository, they are treated as purely flat tables. Not only do we not fully utilize the facilities in current ORDB, but also we do not preserve the conceptual semantic of the XML data.
48|6||FCVW agent framework|The rising cost and growing complexity of software development is a triggering force for the development of frameworks. Frameworks provide reusability of components and they have been developed for many domains. However, very few attempts have been made to develop agent frameworks for Collaborative Virtual Environments (CVEs). This paper presents processes used in developing an agent framework for Federated Collaborative Virtual Workspace (FCVW) based on Agent Oriented Software Engineering (AOSE) techniques. FCVW is an extension of MITRE's Collaborative Virtual Workspace (CVW). The main objective of this framework is to allow FCVW users to create software agents more easily.
48|6||Performance evaluation of e-commerce requests in wireless cellular networks|
48|6||Design and evaluation of a panoramic visualization environment on Semantic Web|
48|6||Reaching consensus: A moderated fuzzy web services discovery method|Web services are used for developing and integrating highly distributed and heterogeneous systems in different domains such as e-business, grid services, and e-government systems. Web services discovery is a key to dynamically locating desired web services across the Internet. Prevailing research trend is to dynamically discover and compose web services in order to develop composite services that provide enhanced functionality. Existing discovery techniques do not take into account the diverse preferences and expectations of service consumers and providers which are generally used for searching or advertising web services. This paper presents a moderated fuzzy web service discovery approach to model subjective and fuzzy opinions, and to assist service consumers and providers in reaching a consensus. The method achieves a common consensus on the distinct opinions and expectations of service consumers and providers. This process is iterative such that further fuzzy opinions and preferences can be added to improve the precision of web service discovery. The proposed method is implemented as a prototype system and is tested through various experiments. Experimental results demonstrate the effectiveness of the proposed method.
48|6||The implementation of a secure and pervasive multimodal Web system architecture|While most users currently access Web applications from Web browser interfaces, pervasive computing is emerging and offering new ways of accessing Internet applications from any device at any location, by utilizing various modes of interfaces to interact with their end users. The PC and its back-end servers remain important in a pervasive system, and the technology could involve new ways of interfacing with a PC and/or various types of gateways to back-end servers. In this research, cellular phone was used as the pervasive device for accessing an Internet application prototype, a multimodal Web system (MWS), through voice user interface technology.
48|7|http://www.sciencedirect.com/science/journal/09505849/48/7|On the relationship between two control-flow coverage criteria: all JJ-paths and MCDC|Coverage criteria may be used to assess the adequacy of software test data. Improved test data, that takes account of any inadequacies identified by lack of coverage, may then be developed. It is natural to seek ways of comparing different criteria and the ‘subsumes’ relationship is one such way: one criterion subsumes another, if satisfying the first always implies satisfaction of the second. This paper considers two criteria: ‘all jump-to-jump paths’ (all JJ-paths) and ‘modified condition/decision coverage’ (MCDC). It might be anticipated that there would be a relationship between these criteria since both are based on advanced control-flow concepts. MCDC has particular importance since it is involved in the DO-178B standard for avionics software. However, it is shown that ‘all JJ-paths’ and MCDC are, in general, incomparable, but for programs written under certain specific constraints ‘all JJ-paths’ subsumes MCDC.
48|7||Towards an ontology-based approach for specifying and securing Web services|With the increasing popularity of Web services and increasing complexity of satisfying needs of users, there has been a renewed interest in Web services composition. Composition addresses the case of a user request that cannot be satisfied by any available Web service, whereas a composite service obtained by integrating Web services might be used. Because Web services originate from different providers, their composition faces the obstacle of the context heterogeneity of Web services. An unawareness or poor consideration of this heterogeneity during Web services composition and execution result in a lack of the quality and relevancy of information that permits tracking the composition, monitoring the execution, and handling exceptions. This paper presents an ontology-based approach for context reconciliation. The approach also focuses on the security breaches that threaten the integrity of the context of Web services, and proposes appropriate means to achieve this integrity.
48|7||A framework for classifying and developing extensible architectural views|
48|7||An event-driven framework for inter-user communication applications|
48|7||Comparison of software architecture reverse engineering methods|Problems related to interactions between components is a sign of problems with the software architecture of the system and are often costly to fix. Thus it is very desirable to identify potential architectural problems and track them across releases to see whether some relationships between components are repeatedly change-prone.
48|7||Applying Model-Driven Architecture to achieve distribution transparencies|This paper proposes a principled methodology for the realization of distribution transparencies. The proposed methodology is placed within the general context of Model-Driven Architecture (MDA) development. Specifically, it consists of a UML-based representation for the specification of platform independent models of a system. Moreover, it comprises an automated aspect-oriented method for the refinement of platform independent models into platform specific ones (i.e. models describing the realization of the system's distribution transparency requirements, based on a standard middleware platform like CORBA, J2EE, COM+, etc.). Finally, the proposed methodology includes an aspect-oriented method for the generation of platform specific code from platform specific models.
48|7||Definition of a problem-sensitive conceptual modelling language: foundations and application to software engineering|A conceptual modelling language should provide constructors that can be used to represent the conceptualisation of a problem considering the problem domain. However, software engineering has traditionally focused on implementation concepts.
48|7||A technique for expressing IT security objectives|At the specification phase, the developer of an IT security product identifies and documents applicable security objectives. Specifications are often intuitive and hard to assess and while being syntactically correct may still fail to appropriately capture the security problem addressed. A technique is proposed for expressing Common Criteria compliant security environments and security objectives for high assurance IT security products. The technique is validated by an analysis of the security specification for a device computing digital signatures within the European Union PKI framework. Modifications to the specification are proposed and the possibility of extending the CC treatment of security objectives is discussed.
48|7||On coordinating personalized composite web services|
48|7||Theory and algorithms for slicing unstructured programs|
48|7||The use and effects of an electronic process guide and experience repository: a longitudinal study|This paper presents a consolidated view of two evaluations on the use of an electronic process guide and experience repository within a small software development company. The use and effects of the tool were studied over a period of one and a half years, first for 6 months and then 1 year after its installation, for another 5 months. The tool was used regularly and in a consistent manner in both studies but declining usage was observed in the second study. The repository remained used to retrieve mostly examples and templates but the number of retrievals of anecdotal experiences, such as lessons learned had noticeably increased. Similar benefits such as time saving and improved documentation quality were observed in both studies, with additional benefits in the second study like improved project planning and cost estimation, and easier negotiation and traceability of altered or new system requirements with clients. The initial load that users experienced in learning to use the tool was not observed in the second study. The results show that tangible benefits can be realised quickly and continued to be experienced, leading to users having higher morale and more confidence in executing their tasks.
48|7||Dynamic model for the system testing process|The approach for estimating and controlling the software testing effort presented in this paper is based on the theory of dynamical systems. The system testing process is modeled by a dynamical system to better understand its behaviour and to assist project and test managers in planning and tracking effort needs.
48|7||Automatic test data generation using genetic algorithm and program dependence graphs|The complexity of software systems has been increasing dramatically in the past decade, and software testing as a labor-intensive component is becoming more and more expensive. Testing costs often account for up to 50% of the total expense of software development; hence any techniques leading to the automatic generation of test data will have great potential to considerably reduce costs. Existing approaches of automatic test data generation have achieved some success by using evolutionary computation algorithms, but they are unable to deal with Boolean variables or enumerated types and they need to be improved in many other aspects. This paper presents a new approach utilizing program dependence analysis techniques and genetic algorithms (GAs) to generate test data. A set of experiments using the new approach is reported to show its effectiveness and efficiency based upon established criterion.
48|7||Finding frequent itemsets over online data streams|Conventional data mining methods for finding frequent itemsets require considerable computing time to produce their results from a large data set. Due to this reason, it is almost impossible to apply them to an analysis task in an online data stream where a new transaction is continuously generated at a rapid rate. An algorithm for finding frequent itemsets over an online data stream should support flexible trade-off between processing time and mining accuracy. Furthermore, the most up-to-date resulting set of frequent itemsets should be available quickly at any moment. To satisfy these requirements, this paper proposes a data mining method for finding frequent itemsets over an online data stream. The proposed method examines each transaction one-by-one without any candidate generation process. The count of an itemset that appears in each transaction is monitored by a lexicographic tree resided in main memory. The current set of monitored itemsets in an online data stream is minimized by two major operations: delayed-insertion and pruning. The former is delaying the insertion of a new itemset in recent transactions until the itemset becomes significant enough to be monitored. The latter is pruning a monitored itemset when the itemset turns out to be insignificant. The number of monitored itemsets can be flexibly controlled by the thresholds of these two operations. As the number of monitored itemsets is decreased, frequent itemsets in the online data stream are more rapidly traced while they are less accurate. The performance of the proposed method is analyzed through a series of experiments in order to identify its various characteristics.
48|7||Encapsulating windows-based software applications into reusable components with design patterns|Reusing software by integrating Commercial Off-The-Shelf (COTS) applications into a software system is maturing in practice. Our previous work [1] presented a component wrapping approach to convert Windows-based COTS applications into CORBA objects. A formal and generalized representation of the conversion process for a Windows-based COTS application into a reusable software component would be useful and desirable for applying such software reuse to COTS-based system development. This study addresses a pattern-based representation of our experience. The patterns in this study offer clear documentation and sufficient information for a software developer to develop a COTS-based software system rapidly. An example system, Graphic Mechanical Part Management System (GMPMS) assembling two COTS applications under MS-DOS and MS-Windows 2000/XP, respectively, is also developed in this study to reveal how the patterns are utilized.
48|8|http://www.sciencedirect.com/science/journal/09505849/48/8|Towards a consistent terminology for software measurement|Although software measurement plays an increasingly important role in Software Engineering, there is no consensus yet on many of the concepts and terminology used in this field. Even worse, vocabulary conflicts and inconsistencies can be frequently found amongst the many sources and references commonly used by software measurement researchers and practitioners. This article presents an analysis of the current situation, and provides a comparison framework that can be used to identify and address the discrepancies, gaps, and terminology conflicts that current software measurement proposals present. A basic software measurement ontology is introduced, that aims at contributing to the harmonization of the different software measurement proposals and standards, by providing a coherent set of common concepts used in software measurement. The ontology is also aligned with the metrology vocabulary used in other more mature measurement engineering disciplines.
48|8||A method for assigning a value to a communication protocol test case|One of the main problems in industrial testing is the enormous number of test cases derived from any complex communication protocol. Due to budget constraints and tight schedule, the number of test cases has to be within a certain limit. However, by having a limit on the number of test cases, it raises some issues. For instances, what criteria should be used for selecting the test cases? How can we ensure that important test cases have not been excluded? We are proposing that assigning a value to each of the test cases of a test suite can provide a solution. By doing so, the relative importance of each of the test cases can be ranked and an optimal test suite can then be designed. The value of a test case is to be measured in economic terms, which could be based on the probability that a particular case will occur, and the probability that an error is likely to be uncovered. This paper presents a method for assigning a value to a test case of a communication protocol; it is based on sensitivity analysis, which involves execution, infection and propagation probabilities. To illustrate the method, the results of applying it to the INRES protocol are presented.
48|8||Design dysphasia and the pattern maintenance cycle|
48|8||The collateral coverage of data flow criteria when branch testing|When exercising program code with test data in an attempt to satisfy a given testing criterion, there will be a concurrent accrual of coverage in respect of other testing criteria. Knowledge of the extent of such ‘collateral coverage’ can be used to advantage both in providing better estimates of the overheads entailed by the overall testing exercise, and in helping to determine an optimal sequence for the application of a set of testing methods.
48|8||The HWS hybrid web search|The widespread availability of machine understandable information on the Semantic Web offers some opportunities to improve traditional search. In this paper, we propose a hybrid web search architecture-HWS, which combines text search with semantic search to improve precision and recall. The components in HWS are described in detail, including several novel algorithms proposed to support the hybrid web search.
48|8||Improving test quality using robust unique input/output circuit sequences (UIOCs)|In finite state machine (FSM) based testing, the problem of fault masking in the unique input/output (UIO) sequence may degrade the test performance of the UIO based methods. This paper investigates this problem and proposes the use of a new type of unique input/output circuit (UIOC) sequence for state verification, which may help to overcome the drawbacks that exist in the UIO based techniques. When constructing a UIOC, overlap and internal state observation schema are used to increase the robustness of a test sequence. Test quality is compared by using the forward UIO method (F-method), the backward UIO method (B-method) and the UIOC method (C-method) separately. Robustness of the UIOCs constructed by the algorithm given in this paper is also compared with those constructed by the algorithm given previously. Experimental results suggest that the C-method outperforms the F- and the B-methods and the UIOCs constructed by the algorithm given in this paper, are more robust than those constructed by other proposed algorithms.
48|8||A high concurrency XPath-based locking protocol for XML databases|Providing efficient access to XML documents becomes crucial in XML database systems. More and more concurrency control protocols for XML database systems were proposed in the past few years. Being an important language for addressing data in XML documents, XPath expressions are the basis of several query languages, such as XQurey and XSLT. In this paper, we propose a lock-based concurrency control protocol, called XLP, for transactions accessing XML data by the XPath model. XLP is based on the XPath model and has the features of rich lock modes, low lock conflict and lock conversion. XLP is also proved to ensure conflict serializability. In sum, there are three major contributions in this paper. The proposed XLP supports most XPath axes, rather than simple path expressions only. Conflict conditions and rules in the XPath model are analyzed and derived. Moreover, a lightweighted lock mode, P-lock, is invented and integrated into XLP for better concurrency.
48|8||Information flow control in multithread applications based on access control lists|Information flow control models prevent information leakage during the execution of an application. We developed a model OORBAC to control information flows in object-oriented systems. Soon after the development of OORBAC, we identified that the model cannot solve the problems induced by multithreaded applications. We thus adapted the concepts of OORBAC to develop a new information flow control model. It offers the features of OORBAC and solves the problems induced by multithread object-oriented applications. The new model is named MtACL (information flow control model for multithreaded object-oriented applications based on access control lists). The multithreaded problems solved by MtACL include the shared memory problem, the non-interference problem, and the combination leakage problem. This paper presents MtACL and proves that the model solves the multithreaded problems.
48|8||A framework for anonymous but accountable self-organizing communities|In this paper we propose a novel architecture and approach to provide accountability for Web communities that require a high-level of privacy. A two-layered privacy protection architecture is proposed, that supports (i) registration of participants and enforcement of community rules, called internal accountability, and (ii) rule-based interaction with real world organizations, called external accountability. Our security protocols build upon community-based trust and limit the exposure of private data on trusted third parties.
48|8||A systematic review of statistical power in software engineering experiments|Statistical power is an inherent part of empirical studies that employ significance testing and is essential for the planning of studies, for the interpretation of study results, and for the validity of study conclusions. This paper reports a quantitative assessment of the statistical power of empirical software engineering research based on the 103 papers on controlled experiments (of a total of 5,453 papers) published in nine major software engineering journals and three conference proceedings in the decade 1993–2002. The results show that the statistical power of software engineering experiments falls substantially below accepted norms as well as the levels found in the related discipline of information systems research. Given this study's findings, additional attention must be directed to the adequacy of sample sizes and research designs to ensure acceptable levels of statistical power. Furthermore, the current reporting of significance tests should be enhanced by also reporting effect sizes and confidence intervals.
48|8||Incremental mining of generator representation using border sets|
48|9|http://www.sciencedirect.com/science/journal/09505849/48/9|Guest editorial for the special section on distributed software development|
48|9||An integration centric approach for the coordination of distributed software development projects|This paper presents an approach for Distributed Software Development (DSD) that is based on two foundations. The first one is an integration centric engineering process, which aims at managing crucial dependencies in DSD projects. The second foundation is a strategy for operationalizing the coordination of the engineering process. The purpose of this strategy is to simultaneously provide global information system support for coordination and achieve common understanding about what should be coordinated and how. The approach has been successfully used at Ericsson, a major supplier of telecommunication systems worldwide, for coordinating extraordinary complex projects developing nodes in the third generation of mobile systems. Although many obstacles have to be addressed, the results indicate that the approach is a viable way to manage DSD during very demanding circumstances.
48|9||Essential communication practices for Extreme Programming in a global software development team|We conducted an industrial case study of a distributed team in the USA and the Czech Republic that used Extreme Programming. Our goal was to understand how this globally-distributed team created a successful project in a new problem domain using a methodology that is dependent on informal, face-to-face communication. We collected quantitative and qualitative data and used grounded theory to identify four key factors for communication in globally-distributed XP teams working within a new problem domain. Our study suggests that, if these critical enabling factors are addressed, methodologies dependent on informal communication can be used on global software development projects.
48|9||Evaluation of composite object replication schemes for dependable server applications|
48|9||A comparison of two approaches to utilizing XML in parametric databases for temporal data|
48|9||Scenario-based multitasking for real-time object-oriented models|Contemporary embedded systems quite often employ extremely complicated software consisting of a number of interrelated components, and this has made object-oriented design methodologies widely used in practice. To implement an object-oriented model in given target hardware, it is imperative to derive a set of tasks from the designed objects. This process of determining tasks and the events they handle greatly influences the real-time performance of the resultant system including response times and real-time guarantees. However, the innate discrepancies between objects and tasks make this exceedingly difficult, and many developers are forced to find their task sets through trial and error. In this paper, we propose Scenario-based Implementation Synthesis Architecture (SISA), an architecture consisting of a method for deriving a task set from a given object-oriented model and the development tools and run-time system architecture to support the method. A system developed with SISA guarantees the optimal response time for each event while deriving the smallest possible number of tasks. We have fully implemented SISA by extending the RoseRT development tool and applied it to an existing industrial PBX (private branch exchange) system. The experimental results show that SISA outperforms the best known conventional techniques by reducing maximum response times an average of 30.3%.
48|9||Using use case patterns to estimate reusability in software systems|The applicability of using use case patterns as a basis for software cost estimation in the early stages of software development is described. This required the construction of a use case patterns catalogue using a novel process. The catalogue has been analysed to estimate the potential reusability in different software applications. This has shown that 43% of system functions are generally application domain independent, whereas 57% are application domain dependent. Statistical tests showed that the level of specialisation in software systems could be as low as 20%, which supports the direction taken in this research to build a use case patterns catalogue as a basis for the development of use case based software cost estimation models.
48|9||B-SCP: A requirements analysis framework for validating strategic alignment of organizational IT based on strategy, context, and process|Ensuring that organizational IT is in alignment with and provides support for an organization's business strategy is critical to business success. Despite this, business strategy and strategic alignment issues are all but ignored in the requirements engineering research literature. We present B-SCP, a requirements engineering framework for organizational IT that directly addresses an organization's business strategy and the alignment of IT requirements with that strategy. B-SCP integrates the three themes of strategy, context, and process using a requirements engineering notation for each theme. We demonstrate a means of cross-referencing and integrating the notations with each other, enabling explicit traceability between business processes and business strategy. In addition, we show a means of defining requirements problem scope as a Jackson problem diagram by applying a business modeling framework. Our approach is illustrated via application to an exemplar. The case example demonstrates the feasibility of B-SCP, and we present a comparison with other approaches.
48|9||Online aggregation with tight error bounds in dynamic environments|OLAP is a category of database technology that allows analysts to gain insight into the aggregation of data by enabling them to gain access to a variety of different views of the information contained in a database. It is very important to provide analysts with guaranteed error bounds for approximate results to aggregation queries in enterprise applications such as decision support systems. We propose a general method of providing tight error bounds for approximate results to OLAP range-sum queries. We perform an extensive experiment on diverse data sets and examine the effectiveness of the proposed method for various data cube dimensions and query sizes.
48|9||Verification framework and algorithms for integrating information distribution systems|When two competitive companies merge into one bigger company, reusing existing technical resources in each company to form a common technology becomes a priority integration task. One of the specific problems occurring during integration is the resulting integrated scopes' requirements specifications become faulty while integrating two sets of software systems from two participating companies. The integrated scopes refer to the domains of information software systems, business policies, business processes, business rules, interface functions, and data that are being integrated in each participating company.
48|9||Experiences on establishing software processes in small companies|In order to guide the tailoring of existing approaches for the establishment of software processes in small companies, we report our experiences on defining and implementing software processes in two small software companies. The paper describes the principal steps performed and presents information on costs and duration. We analyse, if and how process guides are used, their impacts and how they are improved. Our findings indicate that also in this specific kind of organisation, software processes can be established successfully at low cost considering typical business models, goals and characteristics of small organisations.
48|9||Extending the UML concepts to transform natural language queries with fuzzy semantics into SQL|Database applications tend toward getting more versatile and broader to comply with the expansion of various organizations. However, naïve users usually suffer from accessing data arbitrarily by using formal query languages. Therefore, we believe that accessing databases using natural language constructs will become a popular interface in the future. The concept of object-oriented modeling makes the real world to be well represented or expressed in some kinds of logical form. Since the class diagram in UML is used to model the static relationships of databases, in this paper, we intend to study how to extend the UML class diagram representations to capture natural language queries with fuzzy semantics. By referring to the conceptual schema throughout the class diagram representation, we propose a methodology to map natural language constructs into the corresponding class diagram and employ Structured Object Model (SOM) methodology to transform the natural language queries into SQL statements for query executions. Moreover, our approach can handle queries containing vague terms specified in fuzzy modifiers, like ‘good’ or ‘bad’. By our approach, users obtain not only the query answers but also the corresponding degree of vagueness, which can be regarded as the same way we are thinking.
48|9||The increase of productivity over timeâan industrial case study|Introducing new and specialized technology is often seen as a way of meeting increasing non-functional requirements. An example of such a technology is a software platform that provides high performance and availability. The novelty of such a platform and lack of related experience and competence among the staff may affect initial development productivity. The competence problems should disappear with time. In this paper, we present a study, which we conducted at Ericsson. The purpose of the study was to assess the impact of experience and maturity on productivity in software development on the specialized platform. We quantify the impact by comparing productivity of two projects. One represents an initial development stage while the other represents a subsequent and thus more matured development stage. Both projects resulted in large commercial products. We reveal a factor of four difference in productivity. The difference was caused by a higher code delivery rate and a lower number of code lines per functionality in the latter project. We assess the impact of both these issues on productivity and explain their nature. Based on our findings, we suggest a number of improvement suggestions and guidelines for the process of introducing a new technology.
48|9||A multiple-depth structural index for branching query|XML structural index, which acts as a schema, plays an important role in XML query optimization and formulation. To provide a reasonable structural index for branching path query under space constraint, we propose an adaptive index of multiple local branching depths and multiple local bisimilarities, which is constructed by maximizing marginal gain for given query load. It cannot only give good support to branching path queries but also have much smaller size compared with that of same sort of index. Detailed experiments have shown that the index is effective and efficient for XML branching path query.
48|9||Computing simple and complex matchings between XML schemas for transforming XML documents|This paper presents a schema matching method for the transformation of XML documents. The proposed method consists of two steps: computing preliminary matching relationships between leaf nodes in the two XML schemas based on proposed ontology and leaf node similarity, and extracting final matchings based on a proposed path similarity. Particularly, for a sophisticated schema matching, the proposed ontology is incrementally updated by users' feedback. Furthermore, since the ontology can describe various relationships between concepts, the proposed method can compute complex matchings as well as simple matchings. Experimental results with schemas used in various domains show that the proposed method performs better than previous methodologies, resulting in a precision of 97% and a recall of 83% on the average.
48|9||Contents continued|
49|1|http://www.sciencedirect.com/science/journal/09505849/49/1|Most cited journal articles in software engineering|
49|1||An analysis of the most cited articles in software engineering journals - 2000|Citations and related work are crucial in any research to position the work and to build on the work of others. A high citation count is an indication of the influence of specific articles. The importance of citations means that it is interesting to analyze which articles are cited the most. Such an analysis has been conducted using the ISI Web of Science to identify the most cited software engineering journal articles published in 2000. The objective of the analysis is to identify and list the articles that have influenced others the most as measured by citation count. An understanding of which research is viewed by the research community as most valuable to build upon may provide valuable insights into what research to focus on now and in the future. Based on the analysis, a list of the 20 most cited articles is presented here. The intention of the analysis is twofold. First, to identify the most cited articles, and second, to invite the authors of the most cited articles in 2000 to contribute to a special issue of Information and Software Technology. Five authors have accepted the invitation and their articles appear in this special issue. Moreover, an analysis of the most cited software engineering journal articles in the last 20 years is presented. The presentation includes both the most cited articles in absolute numbers and the most cited articles when looking at the average number of citations per year. The article describing the SPIN model checker by G.J. Holzmann published in 1997 is first on both these lists.
49|1||Moving architectural description from under the technology lamppost|In 2000, we published an extensive study of existing software architecture description languages (ADLs), which has served as a useful reference to software architecture researchers and practitioners. Since then, circumstances have changed. The Unified Modeling Language (UML) has gained popularity and wide adoption, and many of the ADLs we studied have been pushed into obscurity. We argue that this progression can be attributed to early ADLs’ nearly exclusive focus on technological aspects of architecture, ignoring application domain and business contexts within which software systems and development organizations exist. These three concerns – technology, domain, and business – constitute three “lampposts” needed to appropriately “illuminate” software architecture and architectural description.
49|1||Predicting software defects in varying development lifecycles using Bayesian nets|An important decision in software projects is when to stop testing. Decision support tools for this have been built using causal models represented by Bayesian Networks (BNs), incorporating empirical data and expert judgement. Previously, this required a custom BN for each development lifecycle. We describe a more general approach that allows causal models to be applied to any lifecycle. The approach evolved through collaborative projects and captures significant commercial input. For projects within the range of the models, defect predictions are very accurate. This approach enables decision-makers to reason in a way that is not possible with regression-based models.
49|1||Software, regulation, and domain specificity|
49|1||ATerms for manipulation and exchange of structured data: Itâs all about sharing|Some data types are so simple that they tend to be reimplemented over and over again. This is certainly true for terms, tree-like data structures that can represent prefix formulae, syntax trees, intermediate code, and more. We first describe the motivation to introduce Annotated Terms (ATerms): unifying several term formats, optimizing storage requirements by introducing maximal subterm sharing, and providing a language-neutral exchange format. Next, we present a brief overview of the ATerm technology itself and of its wide range of applications. A discussion of competing technologies and the future of ATerms concludes the paper.
49|1||Autonomic resource provisioning for software business processes|Software development nowadays involves several levels of abstraction: starting from the programming of single objects, to their combination into components, to their publication as services and the overall architecture linking elements at each level. As a result, software engineering is dealing with a wider range of artifacts and concepts (i.e., in the context of this paper: services and business processes) than ever before. In this paper we explore the importance of having an adequate engine for executing business processes written as compositions of Web services. The paper shows that, independently of the composition language used, the overall scalability of the system is determined by how the run-time engine treats the process execution. This is particularly relevant at the service level because publishing a process through a Web service interface makes it accessible to an unpredictable and potentially very large number of clients. As a consequence, the process developer is confronted with the difficult question of resource provisioning. Determining the optimal configuration of the distributed engine that runs the process becomes sensitive both to the actual number of clients and to the kinds of processes to be executed. The main contribution of the paper is to show how resource provisioning for software business processes can be solved using autonomic computing techniques. The engine separates execution in two stages (navigation and dispatching) and uses a controller to allocate the node of a cluster of computers to each one of those stages as the workload changes. The controller can be configured with different policies that define how to reconfigure the system. To prove the feasibility of the concept, we have implemented the autonomic controller and evaluated its performance with an extensive set of experiments.
49|11-12|http://www.sciencedirect.com/science/journal/09505849/49/11-12|A systematic review of effect size in software engineering experiments|An effect size quantifies the effects of an experimental treatment. Conclusions drawn from hypothesis testing results might be erroneous if effect sizes are not judged in addition to statistical significance. This paper reports a systematic review of 92 controlled experiments published in 12 major software engineering journals and conference proceedings in the decade 1993–2002. The review investigates the practice of effect size reporting, summarizes standardized effect sizes detected in the experiments, discusses the results and gives advice for improvements. Standardized and/or unstandardized effect sizes were reported in 29% of the experiments. Interpretations of the effect sizes in terms of practical importance were not discussed beyond references to standard conventions. The standardized effect sizes computed from the reviewed experiments were equal to observations in psychology studies and slightly larger than standard conventions in behavioral science.
49|11-12||A state-based approach to integration testing based on UML models|Correct functioning of object-oriented software depends upon the successful integration of classes. While individual classes may function correctly, several new faults can arise when these classes are integrated together. In this paper, we present a technique to enhance testing of interactions among modal classes. The technique combines UML collaboration diagrams and statecharts to automatically generate an intermediate test model, called SCOTEM (State COllaboration TEst Model). The SCOTEM is then used to generate valid test paths. We also define various coverage criteria to generate test paths from the SCOTEM model. In order to assess our technique, we have developed a tool and applied it to a case study to investigate its fault detection capability. The results show that the proposed technique effectively detects all the seeded integration faults when complying with the most demanding adequacy criterion and still achieves reasonably good results for less expensive adequacy criteria.
49|11-12||Capturing quality requirements of product family architecture|Software quality is one of the major issues with software intensive systems. Moreover, quality is a critical success factor in software product families exploiting shared architecture and common components in a set of products. Our contribution is the QRF (Quality Requirements of a software Family) method, which explicitly focuses on how quality requirements have to be defined, represented and transformed to architectural models. The method has been applied to two experiments; one in a laboratory environment and the other in industry. The use of the QRF method is exemplified by the Distribution Service Platform (DiSeP), the laboratory experiment. The lessons learned are also based on our experiences of applying the method in industrial settings.
49|11-12||Negotiation support systems and team negotiations: The coalition formation perspective|The use of software to support negotiations has captured the attention of academics and practitioners for some three decades and the research stream of negotiation support systems (NSS) has emerged. Over the years, many NSS have been developed and used in training and research but they have been rarely deployed in organizations. Our speculation is that much existing research is confined to dyadic (i.e., one-to-one) settings which may not adequately reflect the real-world situations in which teams, rather than individuals, often engage in negotiations. To address the gap, our current research aspires to conceptualize the NSS in supporting team negotiations and to theoretically examine their impact. Coalition formation has been a prevalent organizational phenomenon that constitutes important dynamics in any negotiating team; it will be conceptualized as the mechanism through which NSS impacts upon team negotiation outcomes in our paper. Globalization has rendered cross-cultural negotiations a commonplace; at the same time, culture serves as a most salient attribute in activating coalitions. In light of the above, cultural diversity is studied as the antecedent to coalition formation and moderates the influence of NSS on the extent of coalition formation. A set of research propositions, derived from a theoretical framework, are raised and their implications discussed.
49|11-12||Using classification techniques for informal requirements in the requirements analysis-supporting system|In order to efficiently develop large-scale and complicated software, it is important for system engineers to correctly understand users’ requirements. Most requirements in large-scale projects are collected from various stakeholders located in various regions, and they are generally written in natural language. Therefore, the initial collected requirements must be classified into various topics prior to analysis phases in order to be usable as input in several requirements analysis methods. If this classification process is manually done by analysts, it becomes a time-consuming task. To solve this problem, we propose a new bootstrapping method which can automatically classify requirements sentences into each topic category using only topic words as the representative of the analysts’ views. The proposed method is verified through experiments using two requirements data sets: one written in English and the other in Korean. The significant performances were achieved in the experiments: the 84.28 and 87.91 F1 scores for the English and Korean data sets, respectively. As a result, the proposed method can provide an effective function for an Internet-based requirements analysis-supporting system so as to efficiently gather and analyze requirements from various and distributed stakeholders by using the Internet.
49|11-12||An ECA-based framework for decentralized coordination of ubiquitous web services|Emerging ubiquitous computing network is expected to consist of a variety of heterogeneous and distributed devices. While web services technology is increasingly being considered as a promising solution to support the inter-operability between such heterogeneous devices via well-defined protocol, currently there is no effective framework reported in the literature that can address the problem of coordinating the web services-enabled devices. This paper considers a ubiquitous computing environment that is comprised of active, autonomous devices interacting with each other through web services, and presents an ECA (Event-Condition-Action)-based framework for effective coordination of those devices. Specifically, we first present an XML-based language for describing ECA rules that are embedded in web service-enabled devices. An ECA rule, when triggered by an internal or external event to the device, can result in the invocation of appropriate web services in the system. Subsequently, we consider the situation in which the rules are introduced and managed by multiple, independent users, and propose effective mechanisms that can detect and resolve potential inconsistencies among the rules. The presented ECA-based coordination approach is expected to facilitate seamless inter-operation among the web service-enabled devices in the emerging ubiquitous computing environments.
49|11-12||Modelling non-functional requirements of business processes|This paper presents an approach to the identification and inclusion of ‘non-functional’ aspects of a business process in modelling for business improvement. The notion of non-functional requirements (NFRs) is borrowed from software engineering, and a method developed in that field for linking NFRs to conceptual models is adapted and applied to business process modelling. Translated into this domain, NFRs are equated with the general or overall quality attributes of a business process, which, though essential aspects of any effective process, are not well captured in a functionally oriented process model. Using an example of a healthcare process (cancer registration in Jordan). We show how an analysis and evaluation of NFRs can be applied to a process model developed with role activity diagramming (RAD) to operationalise desirable quality features more explicitly in the model. This gives a useful extension to RAD and similar modelling methods, as well as providing a basis for business improvement.
49|11-12||Call for papers|
49|2|http://www.sciencedirect.com/science/journal/09505849/49/2|Reverse-engineering 1-n associations from Java bytecode using alias analysis|
49|2||Semantic similarity-based grading of student programs|An automatic grading approach is presented based on program semantic similarity. Automatic grading of a student program is achieved by calculating semantic similarities between the student program and each correct model program after they are standardized. This approach was implemented in an on-line examination system for the programming language C. Different form other existing approaches, it can evaluate how close a student’s source code is to a correct solution and give a matching accuracy.
49|2||Early detection of COTS component functional suitability|The adoption of COTS-based development brings with it many challenges about the identification and finding of candidate components for reuse. Particularly, the first stage in the identification of COTS candidates is commonly carried out by dealing with unstructured information on the Web, which makes the evaluation process highly costly when applying complex evaluation criteria. To facilitate this process, our proposal introduces an early measurement procedure for suitability of COTS candidates. Considering that filtering is about a first-stage selection, functionality evaluation might drive the analysis, proceeding with the evaluation of other properties only on the pre-selected candidates. In this way, a few candidates are fully evaluated making in principle the whole process more cost-effective. In this paper, we illustrate how functional measures at an initial state are calculated for an E-payment case study.
49|2||A quality framework for developing and evaluating original software components|Component-based software development is being identified as the emerging method of developing complex applications consisting of heterogeneous systems. Although more research attention has been given to Commercial Off The Shelf (COTS) components, original software components are also widely used in the software industry. Original components are smaller in size, they have a narrower functional scope and they usually find more uses when it comes to specific and dedicated functions. Therefore, their need for interoperability is equal or greater, than that of COTS components. A quality framework for developing and evaluating original components is proposed in this paper, along with an application methodology that facilitates their evaluation. The framework is based on the ISO9126 quality model which is modified and refined so as to reflect better the notion of original components. The quality model introduced can be tailored according to the organization-reuser and the domain needs of the targeted component. The proposed framework is demonstrated and validated through real case examples, while its applicability is assessed and discussed.
49|2||Validating the enforcement of access control policies and separation of duty principle in requirement engineering|Validating the compliance of software requirements with the access control policies during the early development life cycle improves the security of the software. It prevents authorizing unauthorized subject during the specification of requirements and analysis before proceeding to other phases where the cost of fixing defects is augmented. This paper provides a logical-based framework that analyzes the authorization requirements specified in the Unified Modeling Language (UML). It ensures that the access requirements are consistent, complete and conflict-free. The framework proposed in this paper is an extension to AuthUML framework. We refine AuthUML and extend it by expanding its analysis to validate the enforcement of the Separation of Duty (SoD) during the requirement engineering. We enhance and extend AuthUML with the necessary phase, predicates and rules. The paper shows the various types of SoD and how each type can be validated. The extension shows the flexibility and scalability of AuthUML to validate new policies. Also, the extension makes AuthUML spans to different phases of the software development process that widen the application of AuthUML.
49|2||Automatic test case generation from UML communication diagrams|We present a method to generate cluster level test cases based on UML communication diagrams. In our approach, we first construct a tree representation of communication diagrams. We then carry out a post-order traversal of the constructed tree for selecting conditional predicates from the communication diagram. We transform the conditional predicates on the communication diagram and apply function minimization technique to generate the test data. The generated test cases achieve message paths coverage as well as boundary coverage. We have implemented our technique and tested it on several example problems.
49|2||Generalization of strategies for fuzzy query translation in classical relational databases|Users of information systems would like to express flexible queries over the data possibly retrieving imperfect items when the perfect ones, which exactly match the selection conditions, are not available. Most commercial DBMSs are still based on the SQL for querying. Therefore, providing some flexibility to SQL can help users to improve their interaction with the systems without requiring them to learn a completely novel language. Based on the fuzzy set theory and the Î±-cut operation of fuzzy number, this paper presents the generic fuzzy queries against classical relational databases and develops the translation of the fuzzy queries. The generic fuzzy queries mean that the query condition consists of complex fuzzy terms as the operands and complex fuzzy relations as the operators in a fuzzy query. With different thresholds that the user chooses for the fuzzy query, the user’s fuzzy queries can be translated into precise queries for classical relational databases.
49|2||State of the practice: An exploratory analysis of schedule estimation and software project success prediction|During discussions with a group of U.S. software developers we explored the effect of schedule estimation practices and their implications for software project success. Our objective is not only to explore the direct effects of cost and schedule estimation on the perceived success or failure of a software development project, but also to quantitatively examine a host of factors surrounding the estimation issue that may impinge on project outcomes. We later asked our initial group of practitioners to respond to a questionnaire that covered some important cost and schedule estimation topics. Then, in order to determine if the results are generalizable, two other groups from the US and Australia, completed the questionnaire. Based on these convenience samples, we conducted exploratory statistical analyses to identify determinants of project success and used logistic regression to predict project success for the entire sample, as well as for each of the groups separately. From the developer point of view, our overall results suggest that success is more likely if the project manager is involved in schedule negotiations, adequate requirements information is available when the estimates are made, initial effort estimates are good, take staff leave into account, and staff are not added late to meet an aggressive schedule. For these organizations we found that developer input to the estimates did not improve the chances of project success or improve the estimates. We then used the logistic regression results from each single group to predict project success for the other two remaining groups combined. The results show that there is a reasonable degree of generalizability among the different groups.
49|2||Managing the business of software product line: An empirical investigation of key business factors|Business has been highlighted as a one of the critical dimensions of software product line engineering. This paper’s main contribution is to increase the understanding of the influence of key business factors by showing empirically that they play an imperative role in managing a successful software product line. A quantitative survey of software organizations currently involved in the business of developing software product lines over a wide range of operations, including consumer electronics, telecommunications, avionics, and information technology, was designed to test the conceptual model and hypotheses of the study. This is the first study to demonstrate the relationships between the key business factors and software product lines. The results provide evidence that organizations in the business of software product line development have to cope with multiple key business factors to improve the overall performance of the business, in addition to their efforts in software development. The conclusions of this investigation reinforce current perceptions of the significance of key business factors in successful software product line business.
49|3|http://www.sciencedirect.com/science/journal/09505849/49/3|Working Conference on Reverse Engineering 2005|
49|3||Recovering UML class models from C++: A detailed explanation|An approach to recovering design-level UML class models from C++ source code to support program comprehension is presented. A set of mappings are given that focus on accurately identifying such elements as relationship types, multiplicities, and aggregation semantics. These mappings are based on domain knowledge of the C++ language and common programming conventions and idioms. Additionally, formal concept analysis is used to detect design-level attributes of UML classes. An application implementing these mappings is used to reverse engineer a moderately sized, open-source application and the resultant class model is compared against those produced by other UML reverse engineering tools. This comparison shows that the presented mapping rules effectively produce meaningful and semantically accurate UML models.
49|3||Semantic clustering: Identifying topics in source code|Many of the existing approaches in Software Comprehension focus on program structure or external documentation. However, by analyzing formal information the informal semantics contained in the vocabulary of source code are overlooked. To understand software as a whole, we need to enrich software analysis with the developer knowledge hidden in the code naming. This paper proposes the use of information retrieval to exploit linguistic information found in source code, such as identifier names and comments. We introduce Semantic Clustering, a technique based on Latent Semantic Indexing and clustering to group source artifacts that use similar vocabulary. We call these groups semantic clusters and we interpret them as linguistic topics that reveal the intention of the code. We compare the topics to each other, identify links between them, provide automatically retrieved labels, and use a visualization to illustrate how they are distributed over the system. Our approach is language independent as it works at the level of identifier names. To validate our approach we applied it on several case studies, two of which we present in this paper.
49|3||Clustering large software systems at multiple layers|Software clustering algorithms presented in the literature rarely incorporate in the clustering process dynamic information, such as the number of function invocations during runtime. Moreover, the structure of a software system is often multi-layered, while existing clustering algorithms often create flat system decompositions.
49|3||Automated clustering to support the reflexion method|A significant aspect in applying the Reflexion Method is the mapping of components found in the source code onto the conceptual components defined in the hypothesized architecture. To date, this mapping is established manually, which requires a lot of work for large software systems. In this paper, we present a new approach, in which clustering techniques are applied to support the user in the mapping activity. The result is a semi-automated mapping technique that accommodates the automatic clustering of the source model with the user’s hypothesized knowledge about the system’s architecture.
49|3||Case study: Re-engineering C++ component models via automatic program transformation|Automated program transformation holds promise for a variety of software life cycle endeavors, particularly where the size of legacy systems makes manual code analysis, re-engineering, and evolution difficult and expensive. But constructing highly scalable transformation tools supporting modern languages in full generality is itself a painstaking and expensive process. This cost can be managed by developing a common transformation system infrastructure re-useable by derived tools that each address specific tasks, thus leveraging the infrastructure costs. This paper describes the Design Maintenance System (DMS1), a practical, commercial program analysis and transformation system, and discusses how it was employed to construct a custom modernization tool being applied to a large C++ avionics system. The tool transforms components developed in a 1990s-era component style to a more modern CORBA-like component framework, preserving functionality.
49|3||An infrastructure to support interoperability in reverse engineering|
49|4|http://www.sciencedirect.com/science/journal/09505849/49/4|A holistic architecture assessment method for software product lines|The success of architecture-centric development of software product lines is critically dependent upon the availability of suitable architecture assessment methods. While a number of architecture assessment methods are available and some of them have been widely used in the process of evaluating single product architectures, none of them is equipped to deal with the main challenges of product line development. In this paper we present an adaptation of the Architecture Tradeoff Analysis Method (ATAM) for the task of assessing product line architectures. The new method, labeled Holistic Product Line Architecture Assessment (HoPLAA), uses a holistic approach that focuses on risks and quality attribute tradeoffs – not only for the common product line architecture, but for the individual product architectures as well. In addition, it prescribes a qualitative analytical treatment of variation points using scenarios. The use of the new method is illustrated through a case study.
49|4||Indexing range sum queries in spatio-temporal databases|Although spatio-temporal databases have received considerable attention recently, there has been little work on processing range sum queries on the historical records of moving objects despite their importance. Since the direct access to a huge amount of data to answer range sum queries incurs prohibitive computation cost, materialization techniques based on existing index structures are suggested. A simple but effective solution is to apply the materialization technique to the MVR-tree known as the most efficient structure for window queries with spatio-temporal conditions. Aggregate structures based on other index structures such as the HR-tree and the 3DR-tree do not provide satisfactory query performance. In this paper, we propose a new index structure called the Adaptively Partitioned Aggregate R-Tree (APART) and query processing algorithms to efficiently process range sum queries in many situations. Our experimental results show that the performance of the APART is typically 1.3 times better than that of its competitor for a wide range of scenarios.
49|4||Efficient index-based KNN join processing for high-dimensional data|In many advanced database applications (e.g., multimedia databases), data objects are transformed into high-dimensional points and manipulated in high-dimensional space. One of the most important but costly operations is the similarity join that combines similar points from multiple datasets. In this paper, we examine the problem of processing K-nearest neighbor similarity join (KNN join). KNN join between two datasets, R and S, returns for each point in R its K most similar points in S. We propose a new index-based KNN join approach using the iDistance as the underlying index structure. We first present its basic algorithm and then propose two different enhancements. In the first enhancement, we optimize the original KNN join algorithm by using approximation bounding cubes. In the second enhancement, we exploit the reduced dimensions of data space. We conducted an extensive experimental study using both synthetic and real datasets, and the results verify the performance advantage of our schemes over existing KNN join algorithms.
49|4||Practice-driven approach for creating project-specific software development methods|Both practitioners and researchers agree that if software development methods were more adjustable to project-specific situations, this would increase their use in practice. Empirical investigations show that otherwise methods exist just on paper while in practice developers avoid them or do not follow them rigorously. In this paper we present an approach that deals with this problem. Process Configuration, as we named the approach, tells how to create a project-specific method from an existing one, taking into account the project circumstances. Compared to other approaches that deal with the creation of project-specific methods, our approach tends to be more flexible and easier to implement in practice as it introduces few simplifications. The proposed approach is practice-driven, i.e. it has been developed in cooperation with software development companies.
49|4||Experimental evaluation of an object-oriented function point measurement procedure|This paper presents an empirical study that evaluates OO-Method Function Points (OOmFP), a functional size measurement procedure for object-oriented systems that are specified using the OO-Method approach. A laboratory experiment with students was conducted to compare OOmFP with the IFPUG – Function Point Analysis (FPA) procedure on a range of variables, including efficiency, reproducibility, accuracy, perceived ease of use, perceived usefulness and intention to use. The results show that OOmFP is more time-consuming than FPA but the measurement results are more reproducible and accurate. The results also indicate that OOmFP is perceived to be more useful and more likely to be adopted in practice than FPA in the context of OO-Method systems development. We also report lessons learned and suggest improvements to the experimental procedure employed and replications of this study using samples of industry practitioners.
49|4||On the design of more secure software-intensive systems by use of attack patterns|Retrofitting security implementations to a released software-intensive system or to a system under development may require significant architectural or coding changes. These late changes can be difficult and more costly than if performed early in the software process. We have created regular expression-based attack patterns that show the sequential events that occur during an attack. By performing a Security Analysis for Existing Threats (SAFE-T), software engineers can match the symbols of a regular expression to their system design. An architectural analysis that identifies security vulnerabilities early in the software process can prepare software engineers for which security implementations are necessary when coding starts. A case study involving students in an upper-level undergraduate security course suggests that SAFE-T can be performed by relatively inexperienced engineers who are not experts in security. Data from the case study also suggest that the attack patterns do not restrict themselves to vulnerabilities in specific environments.
49|4||Mutating database queries|A set of mutation operators for SQL queries that retrieve information from a database is developed and tested against a set of queries drawn from the NIST SQL Conformance Test Suite. The mutation operators cover a wide spectrum of SQL features, including the handling of null values. Additional experiments are performed to explore whether the cost of executing mutants can be reduced using selective mutation or the test suite size can be reduced by using an appropriate ordering of the mutants. The SQL mutation approach can be helpful in assessing the adequacy of database test cases and their development, and as a tool for systematically injecting faults in order to compare different database testing techniques.
49|5|http://www.sciencedirect.com/science/journal/09505849/49/5|Software systems in-house integration: Architecture, process practices, and strategy selection|As organizations merge or collaborate closely, an important question is how their existing software assets should be handled. If these previously separate organizations are in the same business domain – they might even have been competitors – it is likely that they have developed similar software systems. To rationalize, these existing software assets should be integrated, in the sense that similar features should be implemented only once. The integration can be achieved in different ways. Success of it involves properly managing challenges such as making as well founded decisions as early as possible, maintaining commitment within the organization, managing the complexities of distributed teams, and synchronizing the integration efforts with concurrent evolution of the existing systems.
49|5||Evaluation of object-oriented design patterns in game development|The use of object-oriented design patterns in game development is being evaluated in this paper. Games’ quick evolution, demands great flexibility, code reusability and low maintenance costs. Games usually differentiate between versions, in respect of changes of the same type (additional animation, terrains etc). Consequently, the application of design patterns in them can be beneficial regarding maintainability. In order to investigate the benefits of using design patterns, a qualitative and a quantitative evaluation of open source projects is being performed. For the quantitative evaluation, the projects are being analyzed by reverse engineering techniques and software metrics are calculated.
49|5||Cognitive evaluation of information modeling methods|
49|5||Language subsetting in an industrial context: A comparison of MISRA C 1998 and MISRA C 2004|The MISRA (Motor Industry Software Research Association) C standard first appeared in 1998 with the object of restricting the use of features in the ISO C programming language of known undefined or otherwise dangerous behaviour in embedded control systems in the motor car industry. The first edition gained significant attention around the world and in October 2004, a further edition was issued to a wider intended target audience, with the intention of correcting ambiguous wording undermining the effectiveness of the first edition and also improving its ability to restrict features of dangerous behaviour. This paper measures how well the two versions of this document compare on the same population of software and also determines how well the 2004 version achieved its stated goals. Given its increasing influence, the results raise important concerns, specifically that the false positive rate is still unacceptably high with the accompanying danger that compliance may make things worse not better.
49|5||Object-oriented software fault prediction using neural networks|This paper introduces two neural network based software fault prediction models using Object-Oriented metrics. They are empirically validated using a data set collected from the software modules developed by the graduate students of our academic institution. The results are compared with two statistical models using five quality attributes and found that neural networks do better. Among the two neural networks, Probabilistic Neural Networks outperform in predicting the fault proneness of the Object-Oriented modules developed.
49|5||Tool support for iterative software process modeling|To formalize a process, its important aspects must be extracted and described in a model. This model is often written in a formal language so that the process itself can be automated. Since models are often developed iteratively, this language should support this iterative development cycle. However, many existing languages do not. In this paper, we use an existing high-level process modeling language and present a tool that we have developed for supporting iterative development. We have used our tool to develop and refine a process model of distributed software development for NetBeans.
49|5||Software maintenance seen as a knowledge management issue|Creating and maintaining software systems is a knowledge intensive task. One needs to have a good understanding of the application domain, the problem to solve and all its requirements, the software process used, technical details of the programming language(s), the system’s architecture and how the different parts fit together, how the system interacts with its environment, etc. All this knowledge is difficult and costly to gather. It is also difficult to store and usually lives only in the mind of the software engineers who worked on a particular project.
49|6|http://www.sciencedirect.com/science/journal/09505849/49/6|For the Special issue on Qualitative Software Engineering Research|
49|6||Ethnographically-informed empirical studies of software practice|
49|6||The impact of the Abilene Paradox on double-loop learning in an agile team|This paper presents a qualitative investigation of learning failures associated with the introduction of a new software development methodology by a project team. This paper illustrates that learning is more than the cognitive process of acquiring a new skill; learning also involves changes in behaviour and even beliefs. Extreme Programming (XP), like other software development methodologies, provides a set of values and guidelines as to how software should be developed. As these new values and guidelines involve behavioural changes, the study investigates the introduction of XP as a new learning experience. Researchers use the concepts of single and double-loop learning to illustrate how social actors learn to perform tasks effectively and to determine the best task to perform. The concept of triple-loop learning explains how this learning process can be ineffective, accordingly it is employed to examine why the introduction of XP was ineffective in the team studied. While XP should ideally foster double-loop learning, triple-loop learning can explain why this does not necessarily occur. Research illustrates how power factors influence learning among groups of individuals; this study focuses on one specific power factor – the power inherent in the desire to conform. The Abilene Paradox describes how groups can make ineffective decisions that are contrary to that which group members personally desire or believe. Ineffective decision-making occurs due to the desire to conform among group members; this was shown as the cause of ineffective learning in the software team studied. This desire to conform originated in how the project team cohered as a group, which was, in turn, influenced by the social values embraced by XP.
49|6||Self-organization of teams for free/libre open source software development|This paper provides empirical evidence about how free/libre open source software development teams self-organize their work, specifically, how tasks are assigned to project team members. Following a case study methodology, we examined developer interaction data from three active and successful FLOSS projects using qualitative research methods, specifically inductive content analysis, to identify the task-assignment mechanisms used by the participants. We found that ‘self-assignment’ was the most common mechanism across three FLOSS projects. This mechanism is consistent with expectations for distributed and largely volunteer teams. We conclude by discussing whether these emergent practices can be usefully transferred to mainstream practice and indicating directions for future research.
49|6||Revealing actual documentation usage in software maintenance through war stories|War stories are a form of qualitative data that capture informants’ specific accounts of surmounting great challenges. The rich contextual detail afforded by this approach warrants its inclusion in the methodological arsenal of empirical software engineering research. We ground this assertion in an exemplar field study that examined the use of documentation in software maintenance environments. Specific examples are unpacked to reveal a depth of insight that would not have been possible using standard interviews. This afforded a better understanding of the complex relationship between project personnel and documentation, including individuals’ roles as pointers, gatekeepers, or barriers to documentation.
49|6||Requirements engineering challenges in market-driven software development â An interview study with practitioners|Requirements engineering for market-driven software development entails special challenges. This paper presents results from an empirical study that investigates these challenges, taking a qualitative approach using interviews with fourteen employees at eight software companies and a focus group meeting with practitioners. The objective of the study is to increase the understanding of the area of market-driven requirements engineering and provide suggestions for future research by describing encountered challenges. A number of challenging issues were found, including bridging communication gaps between marketing and development, selecting the right level of process support, basing the release plan on uncertain estimates, and managing the constant flow of requirements.
49|6||Management competences, not tools and techniques: A grounded examination of software project management at WM-data|Traditional software project management theory often focuses on desk-based development of software and algorithms, much in line with the traditions of the classical project management and software engineering. This can be described as a tools and techniques perspective, which assumes that software project management success is dependent on having the right instruments available, rather than on the individual qualities of the project manager or the cumulative qualities and skills of the software organisation. Surprisingly, little is known about how (or whether) these tools techniques are used in practice. This study, in contrast, uses a qualitative grounded theory approach to develop the basis for an alternative theoretical perspective: that of competence. A competence approach to understanding software project management places the responsibility for success firmly on the shoulders of the people involved, project members, project leaders, managers. The competence approach is developed through an investigation of the experiences of project managers in a medium sized software development company (WM-data) in Denmark. Starting with a simple model relating project conditions, project management competences and desired project outcomes, we collected data through interviews, focus groups and one large plenary meeting with most of the company’s project managers. Data analysis employed content analysis for concept (variable) development and causal mapping to trace relationships between variables. In this way we were able to build up a picture of the competences project managers use in their daily work at WM-data, which we argue is also partly generalisable to theory. The discrepancy between the two perspectives is discussed, particularly in regard to the current orientation of the software engineering field. The study provides many methodological and theoretical starting points for researchers wishing to develop a more detailed competence perspective of software project managers’ work.
49|6||Practical knowledge and its importance for software product quality|
49|6||A model of design decision making based on empirical results of interviews with software designers|Despite the impact of design decisions on software design, we have little understanding about how design decisions are made. This hinders our ability to provide design metrics, processes and training that support inherent design work. By interviewing 25 software designers and using content analysis and explanation building as our analysis technique, we provide qualitative and quantitative results that highlight aspects of rational and naturalistic decision making in software design. Our qualitative multi-case study results in a model of design decision making to answer the question: how do software designers make design decisions? We find the structure of the design problem determines the aspects of rational and naturalistic decision making used. The more structured the design decision, the less a designer considers options.
49|6||Using grounded theory to understand software process improvement: A study of Irish software product companies|Software process improvement (SPI) aims to understand the software process as it is used within an organisation and thus drive the implementation of changes to that process to achieve specific goals such as increasing development speed, achieving higher product quality or reducing costs. Accordingly, SPI researchers must be equipped with the methodologies and tools to enable them to look within organisations and understand the state of practice with respect to software process and process improvement initiatives, in addition to investigating the relevant literature. Having examined a number of potentially suitable research methodologies, we have chosen Grounded Theory as a suitable approach to determine what was happening in actual practice in relation to software process and SPI, using the indigenous Irish software product industry as a test-bed. The outcome of this study is a theory, grounded in the field data, that explains when and why SPI is undertaken by the software industry. The objective of this paper is to describe both the selection and usage of grounded theory in this study and evaluate its effectiveness as a research methodology for software process researchers. Accordingly, this paper will focus on the selection and usage of grounded theory, rather than results of the SPI study itself.
49|6||Software process improvement as emergent change: A structurational analysis|This paper presents a framework that draws on Structuration theory and dialectical hermeneutics to explicate the dynamics of software process improvement (SPI) in a packaged software organisation. Adding to the growing body of qualitative research, this approach overcomes some of the criticisms of interpretive studies, especially the need for the research to be reflexive in nature.
49|6||Interpretation, interaction and reality construction in software engineering: An explanatory model|The incorporation of social issues in software engineering is limited. Still, during the last 20 years the social element inherent in software development has been addressed in a number of publications that identified a lack of common concepts, models, and theories for discussing software development from this point of view. It has been suggested that we need to take interpretative and constructive views more seriously if we are to incorporate the social element in software engineering. Up till now we have lacked papers presenting ‘simple’ models explaining why. This article presents a model that helps us better to understand interpretation, interaction and reality construction from a natural language perspective. The concepts and categories following with the model provide a new frame of reference useful in software engineering research, teaching, and methods development.
49|7|http://www.sciencedirect.com/science/journal/09505849/49/7|Emotional agents: A modeling and an application|This paper proposes modeling of artificial emotions through agents based on symbolic approach. The symbolic approach utilizes symbolic emotional rule-based systems (rule base that generated emotions) with continuous interactions with environment and an internal “thinking” machinery that comes as a result of series of inferences, evaluation, evolution processes, adaptation, learning, and emotions. We build two models for agent based systems; one is supported with artificial emotions and the other one without emotions. We use both in solving a bench mark problem; “The Orphanage Care Problem”. The two systems are simulated and results are compared. Our study shows that systems with proper model of emotions can perform in many cases better than systems without emotions. We try to shed the light here on how artificial emotions can be modeled in a simple rule-based agent systems and if emotions as they exist in “real intelligence” can be helpful for “artificial intelligence”. Agent architectures are presented as a generic blueprint on which the design of agents can be based. Our focus is on the functional design, including flow of information and control. With this information provided, the generic blueprints of architectures should not be difficult to implement agents, thus putting these theoretical models into practice. We build the agents using this architecture, and many experiments and analysis are shown.
49|7||Classifying variability modeling techniques|Variability modeling is important for managing variability in software product families, especially during product derivation. In the past few years, several variability modeling techniques have been developed, each using its own concepts to model the variability provided by a product family. The publications regarding these techniques were written from different viewpoints, use different examples, and rely on a different technical background. This paper sheds light on the similarities and differences between six variability modeling techniques, by exemplifying the techniques with one running example, and classifying them using a framework of key characteristics for variability modeling. It furthermore discusses the relation between differences among those techniques, and the scope, size, and application domain of product families.
49|7||A framework for evaluating reusability of core asset in product line engineering|Product line engineering (PLE) is a new effective approach to software reuse, where applications are generated by instantiating a core asset which is a large-grained reuse unit. Hence, a core asset is a key element of PLE, and therefore the reusability of the core asset largely determines the success of PLE projects. However, current quality models to evaluate reusability do not adequately address the unique characteristics of core assets in PLE. This paper proposes a comprehensive framework for evaluating the reusability of core assets. We first identify the key characteristics of core assets, and derive a set of quality attributes that characterizes the reusability of core assets. Then, we define metrics for each quality attribute and finally present practical guidelines for applying the evaluation framework in PLE projects. Using the proposed framework, the reusability of core assets can be more effectively and precisely evaluated.
49|7||A method engineering approach to developing aspect-oriented modelling processes based on the OPEN process framework|Aspect-oriented software development (AOSD) is an approach to software development in which aspect-oriented techniques are integrated with traditional (mainly OO) development techniques. Identifying the appropriate method components for supporting aspect-oriented development is facilitated by the use of a method engineering approach. We demonstrate this approach by using the OPEN Process Framework (OPF) to identify previous deficiencies in the method fragments stored in the OPF repository so that the enhanced OPF repository is able to fully support AOSD.
49|7||Identification of factors that influence defect injection and detection in development of software intensive products|The objective of this study is the identification of factors that influence defect injection and defect detection. The study is part of a broader research project with the goal to lower the number of residual defects in software intensive products, by using the influencing factors to decrease injection of defects and to increase detection of defects. As a first step, we performed an extensive literature search to find influencing factors and processed the factors to achieve consistently formulated sets of factors without duplications. As a second step, we used a cluster analysis to reduce the number influencing factors to manageable-sized sets for practical application. As a last step, final groupings of factors were obtained by expert interpretation of the cluster analysis results. These steps were separately performed for defect injection and detection influencing factors, resulting in sets of, respectively, 16 and 17 factors. Finally, the resulting factor groupings were evaluated.
49|7||A controlled empirical evaluation of a requirements abstraction model|Requirement engineers in industry are faced with the complexity of handling large amounts of requirements as development moves from traditional bespoke projects towards market-driven development. There is a need for usable and useful models that recognize this reality and support the engineers in a continuous effort of choosing which requirements to accept and which to dismiss off hand using the goals and product strategies put forward by management. This paper presents an evaluation of such a model that is built based on needs identified in industry. The evaluation’s primary goal is to test the model’s usability and usefulness in a lab environment prior to large scale industry piloting, and is a part of a large technology transfer effort. The evaluation uses 179 subjects from three different Swedish Universities, which is a large portion of the university students educated in requirements engineering in Sweden during 2004 and 2005. The results provide a strong indication that the model is indeed both useful and usable and ready for industry trials.
49|7||Estimating nested selectivity in object-oriented and object-relational databases|A search condition in object-oriented/object-relational queries consists of nested predicates, which are predicates on path expressions. In this paper, we propose a new technique for estimating selectivity for nested predicates. Selectivity of a nested predicate, nested selectivity, is defined as the ratio of the number of the qualified objects of the starting class in the path expression to the total number of objects of the class. The new technique takes into account the effects of direct representation of the many-to-many relationship and the partial participation of objects in the relationship. These two features occur frequently in object-oriented/object-relational databases, but have not been properly handled in the conventional selectivity estimation techniques. For the many-to-many relationship, we generalize the block-hit function proposed by S.B. Yao to allow the cases that an object belongs to more than one block. For the partial participation, we propose the concept of active objects and extend our technique for total participation to handle active objects. We also propose efficient methods for obtaining statistical information needed for our estimation technique. We finally analyze the accuracy of our technique through a series of experiments and compare with the conventional ones. The experiment results showed that there was a significant inaccuracy in the estimation by the conventional ones, confirming the advantage of our technique.
49|8|http://www.sciencedirect.com/science/journal/09505849/49/8|A new indexing method with high storage utilization and retrieval efficiency for large spatial databases|Storing and querying high-dimensional data are important problems in designing an information retrieval system. Two crucial issues, time and space efficiencies, must be considered when evaluating the performance of such a system. The KDB-tree and its variants have been reported to have good performance by using them as the index structure for retrieving multidimensional data. However, they all suffer from low storage utilization problem caused by imperfect “splitting policies.” Unnecessary splits increase the size of the index structure and deteriorate the performance of the system. In this paper, a new data insertion algorithm with a better splitting policy was proposed, which arranges data entries in the leaf nodes as many as possible. Our new index scheme can increase the storage utilization up to nearly 100% and reduce the index size to a smaller scale. As a result, both time and space efficiencies are significantly improved. Analytical and experimental results show that our indexing method outperforms the traditional KDB-tree and its variants.
49|8||Factors affecting duration and effort estimation errors in software development projects|The purpose of this research was to fill a gap in the literature pertaining to the influence of project uncertainty and managerial factors on duration and effort estimation errors. Four dimensions were considered: project uncertainty, use of estimation development processes, use of estimation management processes, and the estimator’s experience. Correlation analysis and linear regression models were used to test the model and the hypotheses on the relations between the four dimensions and estimation errors, using a sample of 43 internal software development projects executed during the year 2002 in the IT division of a large government organization in Israel. Our findings indicate that, in general, a high level of uncertainty is associated with higher effort estimation errors while increased use of estimation development processes and estimation management processes, as well as greater estimator experience, are correlated with lower duration estimation errors. From a practical perspective, the specific findings of this study can be used as guidelines for better duration and effort estimation. Accounting for project uncertainty while managing expectations regarding estimate accuracy; investing more in detailed planning and selecting estimators based on the number of projects they have managed rather than their cumulative experience in project management, may reduce estimation errors.
49|8||Semantic model-driven architecting of service-based software systems|Model-driven development is a software development framework that emphasises model-based abstraction and automated code generation. Service-based software architectures benefit in particular from semantic, ontology-based modelling. We present ontology-based transformation and reasoning techniques for layered semantic service architecture modelling. Integrated ontological layers support abstract domain modelling, architectural design, and interoperability aspects. Ontologies are beneficial due to their potential to formally define models, to allow reasoning about semantic models, and to automate transformations at all layers. Ontologies are suitable in particular for the Web Services platform due to their ubiquity within the Semantic Web and their application to support semantic Web services.
49|8||Metrics for data warehouse conceptual models understandability|Due to the principal role of Data warehouses (DW) in making strategy decisions, data warehouse quality is crucial for organizations. Therefore, we should use methods, models, techniques and tools to help us in designing and maintaining high quality DWs. In the last years, there have been several approaches to design DWs from the conceptual, logical and physical perspectives. However, from our point of view, none of them provides a set of empirically validated metrics (objective indicators) to help the designer in accomplishing an outstanding model that guarantees the quality of the DW. In this paper, we firstly summarise the set of metrics we have defined to measure the understandability (a quality subcharacteristic) of conceptual models for DWs, and present their theoretical validation to assure their correct definition. Then, we focus on deeply describing the empirical validation process we have carried out through a family of experiments performed by students, professionals and experts in DWs. This family of experiments is a very important aspect in the process of validating metrics as it is widely accepted that only after performing a family of experiments, it is possible to build up the cumulative knowledge to extract useful measurement conclusions to be applied in practice. Our whole empirical process showed us that several of the proposed metrics seems to be practical indicators of the understandability of conceptual models for DWs.
49|8||Object-based and class-based composition of transitive mixins|In object-oriented composition, classes and class inheritance are applied to realize type relationships and reusable building blocks. Unfortunately, these two goals might be contradictory in many situations, leading to classes and inheritance hierarchies that are hard to reuse. Some approaches exist to remedy this problem, such as mixins, aspects, roles, and meta-objects. However, in all these approaches, situations where the mixins, aspects, roles, or meta-objects have complex interdependencies among each other are not well solved yet. In this paper, we propose transitive mixins as an extension of the mixin concept. This approach provides a simple and reusable solution to define “mixins of mixins”. Moreover, because mixins can be easily realized on top of aspects, roles, and meta-objects, the same solution can also be applied to those other approaches.
49|8||Testing UML designs|Early detection and correction of faults in the software design phase can reduce total cost and time to market of a software product. In this paper we describe an approach for testing UML design models to uncover inconsistencies. Our approach uses behavioral views such as Sequence Diagrams to simulate state change in an aggregate model. The aggregate model is the artifact of merging information from behavioral and structural UML views. OCL pre-conditions, post-conditions and invariants are used as a test oracle.
49|8||Critical success factors for a customer relationship management strategy|Most organizations have perceived the customer relationship management (CRM) concept as a technological solution for problems in individual areas, accompanied by a great deal of uncoordinated initiatives. Nevertheless, CRM must be conceived as a strategy, due to its human, technological, and processes implications, at the time an organization decides to implement it. On this basis, the main goal stated in this research is to propose, justify, and validate a model based on critical success factors (CSFs) that will constitute a guide for companies in the implementation and diagnosis of a CRM strategy. The model is conformed by a set of 13 CSFs with their 55 corresponding metrics, which will serve as a guide for organizations wishing to apply this type of strategy. These factors cover the three key aspects of every CRM strategy (human factor, processes, and technology); giving a global focus and propitiating success in the implementation of a CRM strategy. These CSFs – and their metrics – were evaluated by a group of internationally experts allowing determining guidelines for a CRM implementation as well as the probable causes of the deficiencies in past projects.
49|9-10|http://www.sciencedirect.com/science/journal/09505849/49/9-10|Consistent data for inconsistent XML document|XML document may contain inconsistencies that violate predefined integrity constraints, which causes the data inconsistency problem. In this paper, we consider how to get the consistent data from an inconsistent XML document. There are two basic concepts for this problem: Repair is the data consistent with the integrity constraints, and also minimally differs from the original one. Consistent data is the data common for every possible repair. First we give a general constraint model for XML, which can express the commonly discussed integrity constraints, including functional dependencies, keys and multivalued dependencies. Next we provide a repair framework for inconsistent XML document with three basic update operations: node insertion, node deletion and node value modification. Following this approach, we introduce the concept of repair for inconsistent XML document, discuss the chase method to generate repairs, and prove some important properties of the chase. Finally we give a method to obtain the greatest lower bound of all possible repairs, which is sufficient for consistent data. We also implement prototypes of our method, and evaluate our framework and algorithms in the experiment.
49|9-10||Distributed priority ranking of strategic preliminary requirements for management information systems in economic organizations|The development and construction of a management information system (MIS) is a complex task. Selection of the correct requirements to be implemented in the MIS is a serious problem. The problem is made even more difficult by inadequate methods of requirements priority ranking. This paper describes instruments for distributed priority ranking of strategic preliminary requirements for MISs in organizations, profit-making or non-profit making, that are involved in the economy. The instrument consists of a metamethod that combines several methods, each accomplishing a different subtask of the priority ranking. A Web-based tool is provided to assist the requirements engineers in applying the instrument with a distributed group of stakeholders. The instrument and the tool are validated as effective by their use in an effort to create requirements for the management information system for city government. The paper reports lessons learned in this validation exercise.
49|9-10||Method and implementation for investigating code clones in a software system|Maintaining software systems is becoming more difficult as the size and complexity of software increase. One factor that complicates software maintenance is the presence of code clones. A code clone is a code fragment that has identical or similar code fragments to it in the source code. Code clones are introduced for various reasons such as reusing code by ‘copy and paste’. If modifying a code clone with many similar code fragments, we must consider whether to modify each of them. Especially for large-scale software, such a process is very complicated and expensive. In this paper, we propose methods of visualizing and featuring code clones to support their understanding in large-scale software. The methods have been implemented as a tool called Gemini, which has applied to an open source software system. Application results show the usefulness and capability of our system.
49|9-10||An object-oriented approach to formally analyze the UML 2.0 activity partitions|Nowadays, UML is the de-facto standard for object-oriented analysis and design. Unfortunately, the deficiency of its dynamic semantics limits the possibility of early specification analysis. UML 2.0 comes to precise and complete this semantics but it remains informal and still lacks tools for automatic validation. The main purpose of this study is to automate the formal validation, according a value-oriented approach, of the behavior of systems expressed in UML. The marriage of Petri nets with temporal logics seems a suitable formalism for translating and then validating UML state-based models. The contributions of the paper are threefold. We first, consider how UML 2.0 activity partitions can be transformed into Object Petri Nets to formalize the object dynamics, in an object-oriented context. Second, we develop an approach based on the object and sequence diagram information to initialize the derived Petri nets in terms of objects and events. Finally, to thoroughly verify if the UML model meets the system required properties, we suggest to use the OCL invariants exploiting their association end constructs. The verification is performed on a predicate/transition net explored by model checking. A case study is given to illustrate this methodology throughout the paper.
49|9-10||Designing nesting structures of user-defined types in object-relational databases|This paper presents a methodology for designing proper nesting structures of user-defined types in object-relational databases. Briefly, we envision that users model a real-world application by using the EER model, which results in an EER schema. Our algorithm then uses the theory we developed for nested relations to generate scheme trees from the EER schema. We shall prove that the resulting scheme trees have exactly the same information content as the EER schema, and the scheme-tree instances over the resulting scheme trees do not store information redundantly. Finally, the scheme trees are transformed to Oracle Database 10g nested object types for implementation. The algorithm in this paper forms the core of a computerized object-relational database design tool we shall develop in the future.
49|9-10||Goal-oriented test data generation for pointer programs|
49|9-10||An objective comparison of the cost effectiveness of three testing methods|
49|9-10||The case for mesodata: An empirical investigation of an evolving database system|Database evolution can be considered a combination of schema evolution, in which the structure evolves with the addition and deletion of attributes and relations, together with domain evolution in which an attribute’s specification, semantics and/or range of allowable values changes. We present the results of an empirical investigation of the evolution of a commercial database system that measures and delineates between changes to the database that are (a) structural and (b) attribute domain related. We also estimate the impact that modelling using the mesodata approach would have on the evolving system.
50|1-2|http://www.sciencedirect.com/science/journal/09505849/50/1-2|Editorial changes and a special issue|
50|1-2||Introduction to section most cited journal articles in software engineering|
50|1-2||An analysis of the most cited articles in software engineering journals â 2001|Citations and related work are crucial in any research to position the work and to build on the work of others. A high citation count is an indication of the influence of specific articles. The importance of citations means that it is interesting to analyze which articles are cited the most. Such an analysis has been conducted using the ISI Web of Science to identify the most cited software engineering journal articles published in 2001. The objective of the analysis is to identify and list the articles that have influenced others the most as measured by citation count. An understanding of which research is viewed by the research community as most valuable to build upon may provide valuable insights into what research to focus on now and in the future. Based on the analysis, a list of the 20 most cited articles is presented here. The intention of the analysis is twofold. First, to identify the most cited articles, and second, to invite the authors of the most cited articles in 2001 to contribute to a special section of Information and Software Technology. Three authors have accepted the invitation and their articles appear in this special section. Moreover, an analysis has been conducted regarding which authors are most productive in terms of software engineering journal publications. The latter analysis focuses on the publications in the last 20 years, which is intended as a complement to last year’s analysis focusing on the most cited articles in the last 20 years [C. Wohlin, An Analysis of the Most Cited Articles in Software Engineering Journals – 2007, Information and Software Technology 49 (1) 2–11]. The most productive author in the last 20 years is Professor Victor Basili.
50|1-2||JADE: A software framework for developing multi-agent applications. Lessons learned|Since a number of years agent technology is considered one of the most innovative technologies for the development of distributed software systems. While not yet a mainstream approach in software engineering at large, a lot of work on agent technology has been done, many research results and applications have been presented, and some software products exists which have moved from the research community to the industrial community. One of these is JADE, a software framework that facilitates development of interoperable intelligent multi-agent systems and that is distributed under an Open Source License. JADE is a very mature product, used by a heterogeneous community of users both in research activities and in industrial applications. This paper presents JADE and its technological components together with a discussion of the possible reasons for its success and lessons learned from the somewhat detached perspective possible nine years after its inception.
50|1-2||On adopting Content-Based Routing in service-oriented architectures|Two requirements of SOAs are the need for a global discovery agency, which assists requesters in finding their required services, and the need for new interaction paradigms, which overcome the limitations of the usual request/reply style. Content-Based Routing (CBR) holds the promise of addressing both these aspects with a single technology and a single routing infrastructure. To provide arguments for our hypothesis, we review the on going efforts for service retrieval and asynchronous communication in SOAs, identify their limitations and the advantages, and discuss how incorporating CBR into SOAs allows to solve most limitations, but also poses some interesting challenges.
50|1-2||Engineering contextual knowledge for autonomic pervasive services|In this paper, we identify the key software engineering challenges introduced by the need of accessing and exploiting huge amount of heterogeneous contextual information. Following, we survey the relevant proposals in the area of context-aware pervasive computing, data mining and granular computing discussing their potentials and limitations with regard to their adoption in the development of context-aware pervasive services. On these bases, we propose the W4 model for contextual data and show how it can represent a simple yet effective model to enable flexible general-purpose management of contextual knowledge by pervasive services. A summarizing discussion and the identification of current limitations and open research directions conclude the paper.
50|1-2||Editorsâ Introduction|
50|1-2||Architecting-problems rooted in requirements|Requirements permeate many parts of the software development process outside the requirements engineering (RE) process. It is thus important to determine whether software developers in these other areas of software development face any requirements-oriented (RO) problems in carrying out their tasks. Feedback so obtained can be invaluable for improving both requirements and RE technologies. In this paper, we describe an exploratory case study of requirements-oriented problems experienced by 16 architecting teams designing the same banking application. The study found that there were several different types of RO problems, of varying severity, which the architects faced in using the given requirements; those architects with RE background also faced RO problems; and about a third of all problems were RO problems. There was much concurrence of our findings with software-expert opinion from a large insurance company. There were also areas where there were relatively few RO problems. The paper also describes some implications of the findings for the RE field, particularly in the areas of: expression of quality requirements for different stakeholders; empirical studies on quality scenarios; tighter integration of RE and software architecting processes; and requirements to architecture mapping. There are opportunities for further research based on two emergent hypotheses which are also described in this paper.
50|1-2||Requirements engineering: In search of the dependent variables|When software development teams modify their requirements engineering process as an independent variable, they often examine the implications of these process changes by assessing the quality of the products of the requirements engineering process, e.g., a software requirements specification (SRS). Using the quality of the SRS as the dependent variable is flawed. As an alternative, this paper presents a framework of dependent variables that serves as a full range for requirements engineering quality assessment. In this framework, the quality of the SRS itself is just the first level. Other higher, and more significant levels, include whether the project was successful and whether the resulting product was successful. And still higher levels include whether or not the company was successful and whether there was a positive or negative impact on society as a whole.
50|1-2||PRiM: An iâ-based process reengineering method for information systems specification|Information system development can often be addressed as a business process reengineering practice, either because it automates some human-based processes or because it replaces an existing legacy system. Therefore, observing and analysing current processes can enable analysts to converge on the specification of the new system, generating and evaluating new system alternatives throughout. In this paper, we propose a method to support this reengineering process that analyses the strengths and weaknesses of the current process; considers the strategic needs of the organization; provides guidelines for the prescriptive construction of i∗ models; and drives the systematic generation and evaluation of alternative technological and organizational solutions for the new system.
50|1-2||Software product release planning through optimization and what-if analysis|We present a mathematical formalization of release planning with a corresponding optimization tool that supports product and project managers during release planning. The tool is based on integer linear programming and assumes that an optimal set of requirements is the set with maximal projected revenue against available resources. The input for the optimization is twofold. The first type of input data concerns the list of candidate requirements, estimated revenues, and resources needed. Second, managerial steering mechanisms enable what-if analysis in the optimization environment. Experiments based on real-life data made a sound case for the applicability of our approach.
50|1-2||Rigorous engineering of product-line requirements: A case study in failure management|We consider the failure detection and management function for engine control systems as an application domain where product line engineering is indicated. The need to develop a generic requirement set – for subsequent system instantiation – is complicated by the addition of the high levels of verification demanded by this safety-critical domain, subject to avionics industry standards. We present our case study experience in this area as a candidate method for the engineering, validation and verification of generic requirements using domain engineering and Formal Methods techniques and tools. For a defined class of systems, the case study produces a generic requirement set in UML and an example system instance. Domain analysis and engineering produce a validated model which is integrated with the formal specification/verification method B by the use of our UML-B profile. The formal verification both of the generic requirement set, and of a simple system instance, is demonstrated using our U2B, ProB and prototype Requirements Manager tools.
50|1-2||Reviewers List|
50|11|http://www.sciencedirect.com/science/journal/09505849/50/11|Knowledge management in software engineering: A systematic review of studied concepts, findings and research methods used|Software engineering is knowledge-intensive work, and how to manage software engineering knowledge has received much attention. This systematic review identifies empirical studies of knowledge management initiatives in software engineering, and discusses the concepts studied, the major findings, and the research methods used. Seven hundred and sixty-two articles were identified, of which 68 were studies in an industry context. Of these, 29 were empirical studies and 39 reports of lessons learned. More than half of the empirical studies were case studies.
50|11||Enhancing conflict detecting mechanism for Web Services composition: A business process flow model transformation approach|
50|11||Towards software process patterns: An empirical analysis of the behavior of student teams|
50|11||The software product line architecture: An empirical investigation of key process activities|Software architecture has been a key area of concern in software industry due to its profound impact on the productivity and quality of software products. This is even more crucial in case of software product line, because it deals with the development of a line of products sharing common architecture and having controlled variability. The main contributions of this paper is to increase the understanding of the influence of key software product line architecture process activities on the overall performance of software product line by conducting a comprehensive empirical investigation covering a broad range of organizations currently involved in the business of software product lines. This is the first study to empirically investigate and demonstrate the relationships between some of the software product line architecture process activities and the overall software product line performance of an organization at the best of our knowledge. The results of this investigation provide empirical evidence that software product line architecture process activities play a significant role in successfully developing and managing a software product line.
50|11||Exploring the underlying aspects of pair programming: The impact of personality|With the recent advent of agile software process methods, a number of seldom used and unorthodox practices have come to the forefront in the field of computer programming. One such practice is that of pair programming, which is characterized by two programmers sharing the same computer for collaborative programming purposes. The very nature of pair programming implies a psychological and social interaction between the participating programmers and thus brings into play a unique element that we do not see with the conventional individual programming model. This paper focuses on the effects that one of these psychosocial factors, a programmer’s personality type, may have on the pair programming environment. In this study, a group of university students, 68 undergraduate students and 60 master’s degree graduate students, each of whom had been personality type profiled using the Myers–Briggs Type Indicator (MBTI) model, was split into three sub-groups. One group consisted of subjects who were alike in MBTI type. Another group consisted of subjects who were opposite to each other in MBTI type, and the last group was comprised of subjects who were diverse – partially alike and partially opposite – in MBTI type. Through two pair programming sessions, the pairs in each group were assessed for their output, in code productivity. The result showed that the sub-group of subjects who were diverse in MBTI type exhibited higher productivity than both alike and opposite groups. In a comparison between alike and opposite groups, the productivity of the opposite group was greater than that of the alike group.
50|11||A methodology for business process improvement and IS development|This paper aims at discussing business process modelling and improvement as an essential work to create a successful and competitive enterprise. To achieve this goal, we use a methodology called TAD, which consists of six phases. The first three deal with business process identification, modelling and improvement. The methodology presents a new unique way for business process identification, modelling, and improvement. The last three phases continue with the implementation of the improved business process(es) by developing its information system. The business process “Surgery” is used as an example to show the implementation of the methodology.
50|11||Time-line based model for software project scheduling with genetic algorithms|Effective management of complex software projects depends on the ability to solve complex, subtle optimization problems. Most studies on software project management do not pay enough attention to difficult problems such as employee-to-task assignments, which require optimal schedules and careful use of resources. Commercial tools, such as Microsoft Project, assume that managers as users are capable of assigning tasks to employees to achieve the efficiency of resource utilization, while the project continually evolves. Our earlier work applied genetic algorithms (GAs) to these problems. This paper extends that work, introducing a new, richer model that is capable of more realistically simulating real-world situations. The new model is described along with a new GA that produces optimal or near-optimal schedules. Simulation results show that this new model enhances the ability of GA-based approaches, while providing decision support under more realistic conditions.
50|11||Ontological modelling of content management and provision|
50|11||The impacts of function extraction technology on program comprehension: A controlled experiment|Program comprehension is a critical, time-consuming, and highly error-prone task for software developers. Function extraction (FX) is a theory and technology that automates and supports program comprehension by calculating the semantic behaviors of programs at many levels of abstraction and displaying those behaviors in a standard, readable format in terms of the “as-built” specification of the program. In this experimental study, developers using an FX prototype tool to assist them in determining the behavior of software modules have significantly more effective program comprehension, in both increased accuracy of understanding and reduced time on task. Moreover, developers have a positive reaction toward the use of the FX technology, and use of FX does not reduce their overall comprehension of the program.
50|12|http://www.sciencedirect.com/science/journal/09505849/50/12|An empirical study of the CobbâDouglas production function properties of software development effort|In this paper we study whether software development effort exhibits Cobb–Douglas functional form with respect to team size and software size. We empirically test this relationship using real-world software engineering data set containing over 500 software projects. The results of our experiments indicate that the hypothesized Cobb–Douglas function form for software development effort with respect to team size and software size is true. We also find increasing returns to scale relationship between software size and team size with software development effort.
50|12||Locating dependence structures using search-based slicing|This paper introduces an approach to locating dependence structures in a program by searching the space of the powerset of the set of all possible program slices. The paper formulates this problem as a search-based software engineering problem. To evaluate the approach, the paper introduces an instance of a search-based slicing problem concerned with locating sets of slices that decompose a program into a set of covering slices that minimize inter-slice overlap. The paper reports the result of an empirical study of algorithm performance and result-similarity for Hill Climbing, Genetic, Random Search and Greedy Algorithms applied to a set of 12 C programs.
50|12||From page-centric to portlet-centric Web development: Easing the transition using MDD|Portlet syndication is the next wave following the successful use of content syndication in current portals. Portlets can be regarded as Web components, and the portal as the component container where portlets are aggregated to provide higher-order applications. This perspective requires a departure from how current Web portals are envisaged. The portal is no longer perceived as a set of pages but as an integrated set of Web components that are now delivered through the portal. From this perspective, the portal page now acts as a mere conduit for portlets. Page and page navigation dilute in favor of portlet and portlet orchestration. However, the mapping from portlet orchestration (design time) to page navigation (implementation time) is too tedious and error prone. For instance, the fact that the same portlet can be placed in distinct pages produces code clones that are repeated along the pages that contain this portlet. This redundancy substantiates in the first place the effort to move to model-driven development. This work uses the eXo platform as the target PSM, and the PIM is based on Hypermedia Model Based on Statecharts. The paper shows how this approach accounts for portal validation/verification to be conducted earlier at the PIM level, and streamlines both design and implementation of eXo portals. A running example is used throughout the paper.
50|12||Does software reliability growth behavior follow a non-homogeneous Poisson process|It is widely believed in software reliability community that software reliability growth behavior follows a non-homogeneous Poisson process (NHPP) based on analyzing the behavior of the mean of the cumulative number of observed software failures. In this paper we present two controlled software experiments to examine this belief. The behavior of the mean of the cumulative number of observed software failures and that of the corresponding variance are examined simultaneously. Both empirical observations and statistical hypothesis testing suggest that software reliability behavior does not follow a non-homogeneous Poisson process in general, and does not fit the Goel–Okumoto NHPP model in particular. Although this new finding should be further tested on other software experiments, it is reasonable to cast doubt on the validity of the NHPP framework for software reliability modeling. The importance of the work presented in this paper is not only for the new finding which is distinctly different from existing popular belief of software reliability modeling, but also for the adopted research approach which is to examine the behavior of the mean and that of the corresponding variance simultaneously on basis of controlled software experiments.
50|12||Generating CAM aspect-oriented architectures using Model-Driven Development|Aspect-Oriented Software Development promotes the separation of those concerns that cut across several components and/or are tangled with the base functionality of a component, through all phases of the software lifecycle. The benefit of identifying these crosscutting concerns (aspects) at the architectural level in particular is to improve the architecture design and its subsequent evolution, before moving onto detailed design and implementation. However, software architects are not usually experts on using specific AO architecture notations. Therefore, the aim of this paper is to provide support to define and specify aspect-oriented (AO) architectures using non-AO ones as the source. We will use the Model-Driven Development approach to transform a component-based architecture model into an AO architecture model. The CAM (component and aspect model) model and the DAOP–ADL language are the proposals used for modelling and specifying AO architectures. We will show how we automated part of the process and the tool support.
50|12||SOPHIE: Use case and evaluation|Services communicate with each other by exchanging self-contained messages. Depending on the specific requirements of the business model they serve and the application domain for which services were deployed, a number of mismatches (i.e. sequence and cardinality of messages exchanges, structure and format of messages and content semantics), can occur which prevent interoperation among a priori compatible services. This paper presents the evaluation of SOPHIE, a conceptual framework for supporting the conceptualization of ontology-based services choreographies. In doing so a three fold approach is taken that considers formal, epistemological and technical aspects. The formal evaluation tries to prove the consistency, completeness and conciseness of ontological model used. The epistemological evaluation enumerates the improvements and differentiating aspect of SOPHIE with respect to existing related work and reviews a number of application areas where the work was successfully applied. Finally, the technical feasibility evaluation tries to demonstrate the viability of the approach from the point of view of the engineering process required to allow the interaction of heterogeneous Semantic Services.
50|12||Semantics and analysis of business process models in BPMN|The Business Process Modelling Notation (BPMN) is a standard for capturing business processes in the early phases of systems development. The mix of constructs found in BPMN makes it possible to create models with semantic errors. Such errors are especially serious, because errors in the early phases of systems development are among the most costly and hardest to correct. The ability to statically check the semantic correctness of models is thus a desirable feature for modelling tools based on BPMN. Accordingly, this paper proposes a mapping from BPMN to a formal language, namely Petri nets, for which efficient analysis techniques are available. The proposed mapping has been implemented as a tool that, in conjunction with existing Petri net-based tools, enables the static analysis of BPMN models. The formalisation also led to the identification of deficiencies in the BPMN standard specification.
50|12||Transformation techniques can make students excited about formal methods|Formal methods have always been controversial. In spite of the fact that the disbelief about their usefulness has been corrected by a growing number of applications and even more publications, it remains a challenge to demonstrate the strengths and weaknesses of formal methods within the time constraints of a typical semester course. This article1 reports on a new course at the University of Antwerp in which the introduction of a new formalism yields a better understanding of previously taught ones. While the exercises are designed to reveal the limitations of the formalisms used, students remain convinced that their formal models have more value than conventional source code.
50|3|http://www.sciencedirect.com/science/journal/09505849/50/3|Translating unstructured workflow processes to readable BPEL: Theory and implementation|
50|3||Applying static analysis for automated extraction of database interactions in web applications|Database interactions are among the most essential functional features in web applications. Therefore, for the testing and maintenance of a web application, it is important that the web engineer could identify all the database interactions in the web application. However, the highly dynamic nature of web applications makes it challenging to extract all the possible database interactions from source code.
50|3||An evaluation and selection framework for interoperability standards|There is a wide range of standards available for the integration and interoperability of applications and information systems, both on domain-specific and domain-neutral levels. The evaluation and selection of interoperability standards are necessary in the application development and integration projects, when there is a need to assess the usefulness of existing models or to find open solutions. In addition, standards have to be evaluated when recommendations are made for a given domain or when their quality is examined. The evaluation of the scope and other aspects of interoperability standards is usually performed against project-specific requirements, but generic frameworks can be used for supporting the evaluation. In this article, we present a conceptual framework which has been developed for the systematic evaluation of interoperability standards. We also present an overview of a process for the evaluation of interoperability standards. We illustrate the use of these models with practical experience and examples.
50|3||Scenario support for effective requirements|Scenarios are widely used as requirements, and the quality of requirements is an important factor in the efficiency and success of a development project. The informal nature of scenarios requires that analysts do much manual work with them, and much tedious and detailed effort is needed to make a collection of scenarios well-defined, relatively complete, minimal, and coherent. We discuss six aspects of scenarios having inherent structure on which automated support may be based, and the results of using such support. This automated support frees analysts to concentrate on tasks requiring human intelligence, resulting in higher-quality scenarios for better system requirements. Two studies validating the work are presented.
50|3||Improving analogy-based software cost estimation by a resampling method|Estimation by analogy (EbA) is a well-known technique for software cost estimation. The popularity of the method is due to its straightforwardness and its intuitively appealing interpretation. However, in spite of the simplicity in application, the theoretical study of EbA is quite complicated. In this paper, we exploit the relation of EbA method to the nearest neighbor non-parametric regression in order to suggest a resampling procedure, known as iterated bagging, for reducing the prediction error. The improving effect of iterated bagging on EbA is validated using both artificial and real datasets from the literature, obtaining very promising results.
50|3||Pair programming in software development teams â An empirical study of its benefits|We present the results of an extensive and substantial case study on pair programming, which was carried out in courses for software development at the University of Dortmund, Germany. Thirteen software development teams with about 100 students took part in the experiments. The groups were divided into two sets with different working conditions. In one set, the group members worked on their projects in pairs. Even though the paired teams could only use half of the workstations the teams of individual workers could use, the paired teams produced nearly as much code as the teams of individual workers at the same time. In addition, the code produced by the paired teams was easier to read and to understand. This facilitates finding errors and maintenance.
50|4|http://www.sciencedirect.com/science/journal/09505849/50/4|Towards management of software as assets: A literature review with additional sources|How should and how can software be managed? What is the management concept or paradigm? Software professionals, if they think about management of software at all, think in terms of Configuration Management. This is not a method for over-all software management; it merely controls software items’ versions. This is much too fine a level of granularity.
50|4||Micro and macro workflow variability design techniques of component|
50|4||An evaluation of the degree of agility in six agile methods and its applicability for method engineering|While agile methods are in use in industry, little research has been undertaken into what is meant by agility and how a supposed agile method can be evaluated with regard to its veracity to belong to this category of software development methodological approaches. Here, an analytical framework, called 4-DAT, is developed and applied to six well-known agile methods and, for comparison, two traditional methods. The results indicate the degree of agility to be found in each method, from which a judgement can be made as to whether the appellation of “agile” to that method is appropriate. This information is shown to be useful, for example, when constructing a methodology from method fragments (method engineering) and when comparing agile and traditional methods.
50|4||On the interplay between inconsistency and incompleteness in multi-perspective requirements specifications|A major challenge for dealing with multi-perspective specifications, and more concretely, with merging of several descriptions or views is toleration of incompleteness and inconsistency: views may be inconclusive, and may have conflicts over the concepts being modeled. The desire of being able to tolerate both phenomena introduces the need to evaluate and quantify the significance of a detected inconsistency as well as to measure the degree of conflict and uncertainty of the merged view as the specification process evolves.
50|4||Achieving Mobile Agent Systems interoperability through software layering|Interoperability is a key issue for a wider adoption of mobile agent systems (MASs) in heterogeneous and open distributed environments where agents, in order to fulfill their tasks, must interact with non-homogeneous agents and traverse different agent platforms to access remote resources. To date, while several approaches have been proposed to deal with different aspects of MAS interoperability, they all lack the necessary flexibility to provide an adequate degree of interoperability among the currently available MASs. In this paper, we propose an application-level approach grounded in the software layering concept, which enables execution, migration and communication interoperability between Java-based mobile agent systems, thus overcoming major setbacks affecting the other approaches currently proposed for supporting MAS interoperability. In particular, we define a Java-based framework, named JIMAF, which relies on an event-driven, proxy-based mobile agent model and supports interoperable mobile agents which can be easily coded and adapted to existing MASs without any modifications of the MAS infrastructures. Results from the performance evaluation of MAS interoperability was carried by using JIMAF atop Aglets, Ajanta, Grasshopper, and Voyager, demonstrating that the high-level JIMAF approach offers high efficacy while maintaining overhead at acceptable levels for target computing environments.
50|4||Patterns and technologies for enabling supply chain traceability through collaborative e-business|Industrial traceability systems are designed to operate over complex supply chains, with a large and dynamic group of participants. These systems need to agree on processing and marketing of goods, information management, responsibility, and identification. In addition, they should guarantee context independence, scalability, and interoperability. In this paper, we first discuss the main issues emerging at different abstraction levels in developing traceability systems. Second, we introduce a data model for traceability and a set of suitable patterns to encode generic traceability semantics. Then, we discuss suitable technological standards to define, register, and enable business collaborations. Finally, we show a practical implementation of a traceability system through a real world experience on food supply chains.
50|5|http://www.sciencedirect.com/science/journal/09505849/50/5|Self-organization process in open-source software: An empirical study|Software systems must continually evolve to adapt to new functional requirements or quality requirements to remain competitive in the marketplace. However, different software systems follow different strategies to evolve, affecting both the release plan and the quality of these systems. In this paper, software evolution is considered as a self-organization process and the difference between closed-source software and open-source software is discussed in terms of self-organization. In particular, an empirical study of the evolution of Linux from version 2.4.0 to version 2.6.13 is reported. The study shows how open-source software systems self-organize to adapt to functional requirements and quality requirements.
50|5||Efficient mining of frequent XML query patterns with repeating-siblings|
50|5||A preliminary study on various implementation approaches of domain-specific language|Various implementation approaches for developing a domain-specific language are available in literature. There are certain common beliefs about the advantages/disadvantages of these approaches. However, it is hard to be objective and speak in favor of a particular one, since these implementation approaches are normally compared over diverse application domains.
50|5||Constraint-driven development|To obtain the full benefits of model-driven development (MDD) approaches such as MDA, a suitable level of abstraction needs to be chosen which enables the core functionality and properties of a system to be expressed, independent of programming language or implementation platform, so that this specification can be reused for a wide variety of different environments.
50|5||Accommodating mesodata into conceptual modelling methodologies|Mesodata modelling is a recently developed approach for enhancing a data model’s capabilities by providing for more advanced semantics to be associated with the domain of an attribute. Mesodata supplies both an inter-value structure to the domain and a set of operations applicable to that structure that may be used to facilitate additional functionality in a database. We argue that conceptual modelling methodologies would be semantically richer if they were able to express the semantics of complex data types for attribute domains. This paper investigates the accommodation of mesodata into the entity-relationship and object role modelling, presenting the Mesodata Entity-Relationship (MDER) model and Mesodata Object Role Modelling (MDORM), which show how the mesodata concept can be incorporated into conceptual modelling methodologies to include the semantics of complex-domain structures.
50|5||Specifying and validating structural constraints of analysis class models using OCL|
50|5||An approach for the maintenance of input validation|Input validation is the enforcement of constraints that an input must satisfy before it is accepted in a program. It is an essential and important feature in a large class of systems and usually forms a major part of a data-intensive system. Currently, the design and implementation of input validation are carried out by application developers. The recovery and maintenance of input validation implemented in a system is a challenging issue. In this paper, we introduce a variant of control flow graph, called validation flow graph as a model to analyze input validation implemented in a program. We have also discovered some empirical properties that characterizing the implementation of input validation. Based on the model and the properties discovered, we then propose a method that recovers the input validation model from source and use program slicing techniques to aid the understanding and maintenance of input validation. We have also evaluated the proposed method through case studies. The results show that the method can be very useful and effective for both experienced and inexperienced developers.
50|5||XTRON: An XML data management system using relational databases|Recently, there has been plenty of interest in XML. Since the amount of data in XML format has rapidly increased, the need for effective storage and retrieval of XML data has arisen. Many database researchers and vendors have proposed various techniques and tools for XML data storage and retrieval in recent years. In this paper, we present an XML data management system using a relational database as a repository. Our XML management system stores XML data in a schema independent manner, and translates a comprehensive subset of XQuery expressions into a single SQL statement. Also, our system does not modify the relational engine. In this paper, we also present the experimental results in order to demonstrate the efficiency and scalability of our system compared with well-known XML processing systems.
50|6|http://www.sciencedirect.com/science/journal/09505849/50/6|A framework to analyze information systems as knowledge flow facilitators|This paper presents a framework which can be used to analyze information systems as knowledge flow facilitators in organizational processes. This framework may be useful, particularly to small organizations, for two main reasons: it can help them to start seeing the implications of KM in their current technical infrastructure, and as a result, they should be in a better position to know how to include their current working tools in part of a KM strategy, thus facilitating the alignment of such a strategy to the daily work of the organization. Second, identifying the role that their current tools play in the flow of knowledge should help such organizations to identify means by which to improve such tools as KM enablers, before becoming engaged in costly KM efforts that could require the acquisition of new tools and often also big changes in their current work processes. The applicability of the framework is illustrated with a case study conducted in a software development environment in which it was successfully applied.
50|6||An empirical investigation of the drivers of software outsourcing decisions in Japanese organizations|Although Japan represents the single largest Asian market and 10% of the global software outsourcing market, little is understood about how Japanese companies make software project outsourcing decisions. Tried-and-tested outsourcing models consistently fail to predict the outsourcing decisions of Japanese companies, leaving global software development companies with little usable guidance in the Japanese outsourcing market. Analyses of 396 software project outsourcing decisions made by 33 IT managers in Toshiba, Hitachi, Fujitsu, IBM-Japan, and Mitsubishi provides novel insights into the drivers of Japanese software outsourcing decisions. The objective of this paper is to develop an analytic tool to predict the likelihood of a software project being outsourced by Japanese IT managers.
50|6||Investigating Knowledge Management practices in software development organisations â An Australian experience|This study, using both quantitative and qualitative methods, investigates current practice of Knowledge Management (KM) in Software Engineering (SE) processes in two Australian companies on the basis that they both claimed to apply KM practices in their software development work. It also describes the KM activities and KM process used in SE practice, and examines the enablers of KM process for SE in terms of leadership, technology, culture, process and measurement.
50|6||An experimental study of four typical test suite reduction techniques|In software development, developers often rely on testing to reveal bugs. Typically, a test suite should be prepared before initial testing, and new test cases may be added to the test suite during the whole testing process. This may usually cause the test suite to contain more or less redundancy. In other words, a subset of the test suite (called the representative set) may still satisfy all the test objectives. As the redundancy can increase the cost of executing the test suite, quite a few test suite reduction techniques have been brought out in spite of the NP-completeness of the general problem of finding the optimal representative set of a test suite. In the literature, there have been some experimental studies of test suite reduction techniques, but the limitations of the these experimental studies are quite obvious. Recently proposed techniques are not experimentally compared against each other, and reported experiments are mainly based on small programs or even simulation data. This paper presents a new experimental study of the four typical test suite reduction techniques, including Harrold et al.’s heuristic, and three other recently proposed techniques such as Chen and Lau’s GRE heuristic, Mansour and El-Fakin’s genetic algorithm-based approach, and Black et al.’s ILP-based approach. Based on the results of this experimental study, we also provide a guideline for choosing the appropriate test suite reduction technique and some insights into why the techniques vary in effectiveness and efficiency.
50|6||Enhancing usability testing through datamining techniques: A novel approach to detecting usability problem patterns for a context of use|Usability is a software attribute usually associated with the “ease of use and to learn” of a given interactive system. Nowadays usability evaluation is becoming an important part of software development, providing results based on quantitative and qualitative estimations. In this context, qualitative results are usually obtained through a Qualitative Usability Testing process which includes a number of different methods focused on analyzing the interface of a particular interactive system. These methods become complex when a large number of interactive systems belonging to the same context of use have to be jointly considered to provide a general diagnosis, as a considerable amount of information must be visualized and treated simultaneously. However, diagnosing the most general usability problems of a context of use as a whole from a qualitative viewpoint is a challenge for UE nowadays. Identifying such problems can help to evaluate a new interface belonging to this context, and to prevent usability errors when a novel interactive system is being developed. From a quantitative viewpoint, condensing results in singles scores, metrics or statistical functions is an acceptable solution for processing huge amounts of usability related information. Nevertheless, QUT processes need to keep their richness by prioritizing the “what” over the “how much/how many” questions related to the detection of usability problems.
50|6||On-line generation association rules over data streams|In order to efficiently trace the changes of association rules over an online data stream, this paper proposes a method of generating association rules directly over the changing set of currently frequent itemsets. While all of the currently frequent itemsets in an online data stream are monitored by the estDec method, all the association rules of every frequent itemset in the prefix tree of the estDec method are generated by the proposed method in this paper. For this purpose, a traversal stack is introduced to efficiently enumerate all association rules in the prefix tree. This online implementation can avoid the drawbacks of the conventional two-step approach. In addition, the prefix tree itself can be utilized as an index structure for finding the current support of the antecedent of an association rule. Finally, the performance of the proposed method is analyzed by a series of experiments to identify its various characteristics.
50|6||Effectively utilizing project, product and process knowledge|Improving project management, product development and engineering processes is for many companies crucial to survive in a fast changing environment. However, these activities are rarely integrated well due to the diversity of stakeholders with individual knowledge about projects, products and processes. This case study shows how Alcatel-Lucent over time achieved effective interaction of engineering processes, tools and people on the basis of a knowledge-centric product life-cycle management (PLM). Starting from identifying project, product and process knowledge, we show how they can be effectively integrated for best possible usage across the enterprise. The case study provides insight into how to best embark on PLM and how to effectively integrate product development with supportive tools. It describes how the concepts can be transferred to software engineering teams and IT departments in other companies. Concrete results from several product lines, such as efficiency improvement and better global development underline the business value.
50|6||Estimating the coverage of the framework application reusable cluster-based test cases|Object-oriented frameworks support both software code and design reusability. In addition, it is found that providing class-based tests with the framework reduces considerably the class-based testing time and effort of the applications developed using the frameworks. Similarly, reusable cluster-based test cases can be generated using the framework hooks, and they, too, can be provided with the framework to reduce the cluster testing time and effort of the framework applications.
50|7-8|http://www.sciencedirect.com/science/journal/09505849/50/7-8|Systematic review of organizational motivations for adopting CMM-based SPI|Background: Software Process Improvement (SPI) is intended to improve software engineering, but can only be effective if used. To improve SPI’s uptake, we should understand why organizations adopt SPI. CMM-based SPI approaches are widely known and studied. Objective: We investigated why organizations adopt CMM-based SPI approaches, and how these motivations relate to organizations’ size. Method: We performed a systematic review, examining reasons reported in more than forty primary studies. Results: Reasons usually related to product quality and project performance, and less commonly, to process. Organizations reported customer reasons infrequently and employee reasons very rarely. We could not show that reasons related to size. Conclusion: Despite its origins in helping to address customer-related issues for the USAF, CMM-based SPI has mostly been adopted to help organizations improve project performance and product quality issues. This reinforces a view that the goal of SPI is not to improve process per se, but instead to provide business benefits.
50|7-8||A test driven approach for aspectualizing legacy software using mock systems|Aspect-based refactoring, called aspectualization, involves moving program code that implements cross-cutting concerns into aspects. Such refactoring can improve the maintainability of legacy systems. Long compilation and weave times, and the lack of an appropriate testing methodology are two challenges to the aspectualization of large legacy systems. We propose an iterative test driven approach for creating and introducing aspects. The approach uses mock systems that enable aspect developers to quickly experiment with different pointcuts and advice, and reduce the compile and weave times. The approach also uses weave analysis, regression testing, and code coverage analysis to test the aspects. We developed several tools for unit and integration testing. We demonstrate the test driven approach in the context of large industrial C++ systems, and we provide guidelines for mock system creation.
50|7-8||Heuristics-based infeasible path detection for dynamic test data generation|Automated test data generation plays an important part in reducing the cost and increasing the reliability of software testing. However, a challenging problem in path-oriented test data generation is the existence of infeasible program paths, where considerable effort may be wasted in trying to generate input data to traverse the paths. In this paper, we propose a heuristics-based approach to infeasible path detection for dynamic test data generation. Our approach is based on the observation that many infeasible program paths exhibit some common properties. Through realizing these properties in execution traces collected during the test data generation process, infeasible paths can be detected early with high accuracy. Our experiments show that the proposed approach efficiently detects most of the infeasible paths with an average precision of 96.02% and a recall of 100% of all the cases.
50|7-8||Combining probabilistic models for explanatory productivity estimation|In this paper Association Rules (AR) and Classification and Regression Trees (CART) are combined in order to deliver an effective conceptual estimation framework. AR descriptive nature is exploited by identifying logical associations between project attributes and the required effort for the development of the project. CART method on the other hand has the benefit of acquiring general knowledge from specific examples of projects and is able to provide estimates for all possible projects. The particular methods have the ability of learning and modelling associations in data and hence they can be used to describe complex relationships in software cost data sets that are not immediately apparent. Potential benefits of combining these probabilistic methods involve the ability of the final model to reveal the way in which particular attributes can increase or decrease productivity and the fact that such assumptions vary among different ranges of productivity values. Experimental results on two data sets indicate efficient overall performance of the suggested integrated method.
50|7-8||A new calibration for Function Point complexity weights|Function Point (FP) is a useful software metric that was first proposed 25 years ago, since then, it has steadily evolved into a functional size metric consolidated in the well-accepted Standardized International Function Point Users Group (IFPUG) Counting Practices Manual – version 4.2. While software development industry has grown rapidly, the weight values assigned to count standard FP still remain same, which raise critical questions about the validity of the weight values. In this paper, we discuss the concepts of calibrating Function Point, whose aims are to estimate a more accurate software size that fits for specific software application, to reflect software industry trend, and to improve the cost estimation of software projects. A FP calibration model called Neuro-Fuzzy Function Point Calibration Model (NFFPCM) that integrates the learning ability from neural network and the ability to capture human knowledge from fuzzy logic is proposed. The empirical validation using International Software Benchmarking Standards Group (ISBSG) data repository release 8 shows a 22% accuracy improvement of mean magnitude relative error (MMRE) in software effort estimation after calibration.
50|7-8||A framework for ensuring consistency of Web Services Transactions|For efficiently managing Web Services (WS) transactions which are executed across multiple loosely-coupled autonomous organizations, isolation is commonly relaxed. A Web service operation of a transaction releases locks on its resources once its jobs are completed without waiting for the completions of other operations. However, those early unlocked resources can be seen by other transactions, which can spoil data integrity and cause incorrect outcomes. Existing WS transaction standards do not consider this problem. In this paper, we propose a mechanism to ensure the consistent executions of isolation-relaxing WS transactions. The mechanism effectively detects inconsistent states of transactions with a notion of an end-state dependency and recovers them to consistent states. We also propose a new Web services Transaction Dependency management Protocol (WTDP). WTDP helps organizations manage the WS transactions easily without data inconsistency. WTDP is designed to be compliant with a representative WS transaction standard, the Web Services Transactions specifications, for easy integration into existing WS transaction systems. We prototyped a WTDP-based WS transaction management system to validate our protocol.
50|7-8||MOBMAS: A methodology for ontology-based multi-agent systems development|Ontologies offer significant benefits to multi-agent systems: interoperability, reusability, support for multi-agent system (MAS) development activities (such as system analysis and agent knowledge modeling) and support for MAS operation (such as agent communication and reasoning). This paper presents an ontology-based methodology, MOBMAS, for the analysis and design of multi-agent systems. MOBMAS is the first methodology that explicitly identifies and implements the various ways in which ontologies can be used in the MAS development process and integrated into the MAS model definitions. In this paper, we present comprehensive documentation and validation of MOBMAS.
50|7-8||Deriving an approximation algorithm for automatic computation of ripple effect measures|The ripple effect measures impact, or how likely it is that a change to a particular module may cause problems in the rest of a program. It can also be used as an indicator of the complexity of a particular module or program. Central to this paper is a reformulation in terms of matrix arithmetic of the original ripple effect algorithm produced by Yau and Collofello in 1978. The main aim of the reformulation is to clarify the component parts of the algorithm making the calculation more explicit. The reformulated algorithm has been used to implement REST (Ripple Effect and Stability Tool) which produces ripple effect measures for C programs. This paper describes the reformulation of Yau and Collofello’s ripple effect algorithm focusing on the computation of matrix Zm which holds intramodule change propagation information. The reformulation of the ripple effect algorithm is validated using fifteen programs which have been grouped by type. Due to the approximation spurious 1s are contained within matrix Zm. It is discussed whether this has an impact on the accuracy of the reformulated algorithm. The conclusion of this research is that the approximated algorithm is valid and as such can replace Yau and Collofello’s original algorithm.
50|7-8||Consistency in multi-viewpoint design of enterprise information systems|Different stakeholders in the design of an enterprise information system have their own view on that design. To help produce a coherent design this paper presents a framework that aids in specifying relations and consistency rules between such views. The contribution of our framework is that it provides a collection of basic concepts. These basic concepts aid in relating viewpoints by providing: (i) a common terminology that helps stakeholders to understand each others concepts; and (ii) re-usable consistency rules. We show that our framework can be applied, by performing a case study in which we specify the relations and consistency rules between three RM-ODP viewpoints.
50|7-8||Aligning the economic modeling of software reuse with reuse practices|In contrast to current practices where software reuse is applied recursively and reusable assets are tailored trough parameterization or specialization, existing reuse economic models assume that (i) the cost of reusing a software asset depends on its size and (ii) reusable assets are developed from scratch. The contribution of this paper is that it provides modeling elements and an economic model that is better aligned with current practices. The functioning of the model is illustrated in an example. The example also shows how the model can support practitioners in deciding whether it is economically feasible to apply software reuse recursively.
50|7-8||Initiating software process improvement in very small enterprises: Experience with a light assessment tool|The paper concerns software process improvement in Very Small Enterprises (VSEs). It presents briefly a gradual methodology to initiate software process improvement in VSE through three steps approach and develops the first and most original step. This first step is based on a light evaluation achieved by means of a dedicated Micro-Evaluation approach. It has been experimented during 7 years in 86 organizations from three countries. The experience with that utilization tends to show that such a light approach is practicable and promising, at least for the targeted enterprises.
50|7-8||The size and effort estimates in iterative development|The quick delivery of a functionally truncated product is one of the most common results in iterative development, and has become the predominant development approach. One of its drawbacks is the appearance of incomplete artifacts between iterations. Consequently, well-known size-estimation methods can not be used in iterative development. This paper addresses the problem of size estimation in iterative development. We present a novel approach that enables early size estimation using Unified Modeling Language (UML) artifacts. The approach incorporates self-improvement steps that increase the estimation accuracy in subsequent iterations. The demonstration of its applicability and research results are also presented. The results anticipate the possibility of a significant improvement in size and effort estimates by applying the approach presented here.
50|7-8||Predicting weekly defect inflow in large software projects based on project planning and test status|Defects discovered during the testing phase in software projects need to be removed before the software is shipped to the customers. The removal of defects can constitute a significant amount of effort in a project and project managers are faced with a decision whether to continue development or shift some resources to cope with defect removal. The goal of this research is to improve the practice of project management by providing a method for predicting the number of defects reported into the defect database in the project. In this paper we present a method for predicting the number of defects reported into the defect database in a large software project on a weekly basis. The method is based on using project progress data, in particular the information about the test progress, to predict defect inflow in the next three coming weeks. The results show that the prediction accuracy of our models is up to 72% (mean magnitude of relative error for predictions of 1 week in advance is 28%) when used in ongoing large software projects. The method is intended to support project managers in more accurate adjusting resources in the project, since they are notified in advance about the potentially large effort needed to correct defects.
50|7-8||Using formal metamodels to check consistency of functional views in information systems specification|UML notations require adaptation for applications such as Information Systems (IS). Thus we have defined IS-UML. The purpose of this article is twofold. First, we propose an extension to this language to deal with functional aspects of IS. We use two views to specify IS transactions: the first one is defined as a combination of behavioural UML diagrams (collaboration and state diagrams), and the second one is based on the definition of specific classes of an extended class diagram. The final objective of the article is to consider consistency issues between the various diagrams of an IS-UML specification. In common with other UML languages, we use a metamodel to define IS-UML. We use class diagrams to summarize the metamodel structure and a formal language, B, for the full metamodel. This allows us to formally express consistency checks and mapping rules between specific metamodel concepts.
50|7-8||Integrating B-SCP and MAP to manage the evolution of strategic IT requirements|This paper presents the first steps in a research project that integrates two requirements engineering methodologies, B-SCP and MAP, in order to manage the evolution of strategic IT. Our integration approach presents a mechanism to validate and verify MAP requirements against B-SCP requirements and vice versa. MAP has an inbuilt Gap Analysis process which saves the overhead of inventing a new approach to deal with requirements evolution. In addition, MAP extends B-SCP’s capability by the addition of non-deterministic process modelling. Our solution is evaluated on an exemplar case study which looks at a system built for Seven Eleven Japan.
50|9-10|http://www.sciencedirect.com/science/journal/09505849/50/9-10|Empirical studies of agile software development: A systematic review|Agile software development represents a major departure from traditional, plan-based approaches to software engineering. A systematic review of empirical studies of agile software development up to and including 2005 was conducted. The search strategy identified 1996 studies, of which 36 were identified as empirical studies. The studies were grouped into four themes: introduction and adoption, human and social factors, perceptions on agile methods, and comparative studies. The review investigates what is currently known about the benefits and limitations of, and the strength of evidence for, agile methods. Implications for research and practice are presented. The main implication for research is a need for more and better empirical studies of agile software development within a common research agenda. For the industrial readership, the review provides a map of findings, according to topic, that can be compared for relevance to their own settings and situations.
50|9-10||Motivation in Software Engineering: A systematic literature review|In this paper, we present a systematic literature review of motivation in Software Engineering. The objective of this review is to plot the landscape of current reported knowledge in terms of what motivates developers, what de-motivates them and how existing models address motivation.
50|9-10||A comparative evaluation on the accuracies of software effort estimates from clustered data|Precision in estimating the required software development effort plays a critical factor in the success of software project management. Most existing software effort estimation models only compare the accuracies of software effort estimates from the historical data without clustering. A potential factor that can affect the accuracies of the established effort estimation models is the homogeneity of the data. However, such investigation on the effects of the accuracies of the derived effort estimates is seldom explored in software effort estimation literature. Therefore, this paper aims to explore the effects of accuracies of the software effort estimation models established from the clustered data by using the International Software Benchmarking Standards Group (ISBSG) repository. The ordinary least square (OLS) regression method is adopted to establish a respective effort estimation model in each cluster of datasets. The empirical experiment results show that the estimation accuracies do not reveal significant differences within the respective dataset clustered by each software effort driver. It also demonstrates that software effort estimation models from the clustered data present almost similar accuracy results compared to models from the entire data without clustering.
50|9-10||The impact of software process standardization on software flexibility and project management performance: Control theory perspective|It has been assumed for years that process standardization in the development of software will improve the efficiency of the development project by the virtues of applying a learned procedure and tight controls. Past research, however, is inconclusive in the elements that must be in place to achieve the benefits. In this study, we employ the software quality principle of flexibility as a mediator variable to determine if certain design aspects play a key role in achieving the benefits to the project of process standardization. A survey of computer professionals indicates that software flexibility is a positive influence. System designers should apply standard processes but with an eye toward quality design principles.
50|9-10||Applying UML and software simulation for process definition, verification, and validation|Process definition, verification, and validation are recognized as critical elements in software process improvement, whereas CMMI is a process improvement approach that provides organizations with the essential elements of effective processes. Organizations must define their own processes to meet the requirements of CMMI. A friendly, unambiguous process modeling language and tool are thus very important for organizations to define, verify, and validate the processes. Nevertheless, hardly has any research yet been done on how to embed CMMI process area goals into process definition stage to satisfy organization process improvement. In this research, we propose a UML-based approach to define, verify, and validate an organization’s process. Our approach can also be applied to a process learning environment for students and project members.
50|9-10||An approach to fuzzy granule-based hierarchical polynomial networks for empirical data modeling in software engineering|
50|9-10||Securing services in nomadic computing environments|This work addresses the existing research gap regarding the security of service oriented architectures and their integration in the context of nomadic computing. The state of the art of Service Oriented Architectures (SOAs) is thoroughly investigated to understand what secure service provision means for different SOAs and whether an established notion of secure SOA existed. Based on the analysis of existing SOAs, we define a set of requirements for securing services among different nomadic computing domains. Such requirements concern the security of service registration and that of the discovery and delivery phases. The surveyed SOAs are then evaluated in the light of the defined requirements, revealing interesting observations about how current SOAs address security issues. The second part of this work addresses the research issue of achieving secure service provision in a nomadic computing environment characterized by a number of heterogeneous service oriented architectures. A solution is presented in the form of an architectural model, named Secure Nomadic Computing Architecture. The model relies on a novel three-phase discovery-delivery protocol which allows the enforcement of a number of security requirements, identified as a result of the first part of the work. Finally, we present an exemplary implementation of the proposed architectural model developed within the context of a distributed management information system for the discovery of digital educational content.
50|9-10||MARS: A metamodel recovery system using grammar inference|Domain-specific modeling (DSM) assists subject matter experts in describing the essential characteristics of a problem in their domain. When a metamodel is lost, repositories of domain models can become orphaned from their defining metamodel. Within the purview of model-driven engineering, the ability to recover the design knowledge in a repository of legacy models is needed.
50|9-10||Integrating aspects in software architectures: PRISMA applied to robotic tele-operated systems|Aspect-Oriented Software Development (AOSD) has emerged as a new approach to develop software systems by improving their structure, reuse, maintenance and evolution properties. It is being applied to all stages of the software life cycle. In this paper, we present the PRISMA approach, which introduces AOSD in software architectures. PRISMA is characterized by integrating aspects as first-order citizens of software architectures. This paper shows how the PRISMA methodology is applied to develop a case study of the tele-operation system domain. We illustrate how the PRISMA approach can improve the development and maintenance processes of these kinds of industrial systems.
50|9-10||Challenges and strategies in the use of Open Source Software by Independent Software Vendors|Open Source Software (OSS) has already been adopted by a large number of organizations. An important – but sometimes neglected – group of OSS users are Independent Software Vendors (ISVs). ISVs often develop their applications on top of OSS platform software. Frequently, this requires making several extensions and modifications to these OSS components. We identify a number of challenges that ISVs face in handling these extensions and modifications. Next, we describe several strategies ISVs can follow in maintaining these modifications. Finally, we suggest an opportunity for a closer collaboration between OSS projects and ISVs which could be mutually beneficial.
50|9-10||A catalog of architectural primitives for modeling architectural patterns|Architectural patterns are a fundamental aspect of the architecting process and subsequently the architectural documentation. Unfortunately, there is only poor support for modeling architectural patterns for two reasons. First, patterns describe recurring design solutions and hence do not directly match the elements in modeling languages. Second, they support an inherent variability in the solution space that is hard to model using a single modeling solution. This paper proposes to address this problem by finding and representing architectural primitives: fundamental, formalized modeling elements in representing patterns. In particular, we examined architectural patterns from the components and connectors architectural view, and we discovered recurring primitive abstractions among the patterns, that also demonstrate a degree of variability for each pattern. We used UML 2 as the language for representing these primitive abstractions as extensions of the standard UML elements. The contribution of this approach is that we provide a generic and extensible concept for modeling architectural patterns by means of architectural primitives. Also, we can demonstrate a first set of primitives that participate in several well-known architectural patterns.
50|9-10||Do secure information system design methods provide adequate modeling support?|Information system development (ISD) methods lack security features. To address this problem, various secure information system (SIS) design methods have been proposed. An important feature of these methods is modeling support, which manifests itself through modeling notations. This paper explores the extent to which the alternative SIS design methods offer modeling support. The results suggest that extant SIS design methods provide only limited modeling support. No single SIS design method offers comprehensive modeling support. This result has implications for practice and research. Practitioners may need to combine different SIS design methods for the development of secure information systems (IS). In turn, scholars and SIS design method developers should ensure that future SIS design methods offer comprehensive modeling support. Finally, empirical studies should be conducted to explore the usability of the current conceptual models of secure systems design methods in practice.
||||
volume|issue|url|title|abstract
51|1|http://www.sciencedirect.com/science/journal/09505849/51/1|Introduction to section most cited journal articles in Software Engineering|
51|1||An analysis of the most cited articles in software engineering journals â 2002|
51|1||Systematic literature reviews in software engineering â A systematic literature review|In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference.
51|1||Automating regression test selection based on UML designs|This paper presents a methodology and tool to support test selection from regression test suites based on change analysis in object-oriented designs. We assume that designs are represented using the Unified Modeling Language (UML) 2.0 and we propose a formal mapping between design changes and a classification of regression test cases into three categories: Reusable, Retestable, and Obsolete. We provide evidence of the feasibility of the methodology and its usefulness by using our prototype tool on an industrial case study and two student projects.
51|1||Modes in component behavior specification via EBP and their application in product lines|
51|1||Performance analysis of allocation policies for interGrid resource provisioning|Several Grids have been established and used for varying science applications during the last years. Most of these Grids, however, work in isolation and with different utilisation levels. Previous work has introduced an architecture and a mechanism to enable resource sharing amongst Grids. It has demonstrated that there can be benefits for a Grid to offload requests or provide spare resources to another Grid. In this work, we address the problem of resource provisioning to Grid applications in multiple-Grid environments. The provisioning is carried out based on availability information obtained from queueing-based resource management systems deployed at the provider sites which are the participants of the Grids. We evaluate the performance of different allocation policies. In contrast to existing work on load sharing across Grids, the policies described here take into account the local load of resource providers, imprecise availability information and the compensation of providers for the resources offered to the Grid. In addition, we evaluate these policies along with a mechanism that allows resource sharing amongst Grids. Experimental results obtained through simulation show that the mechanism and policies are effective in redirecting requests thus improving the applications’ average weighted response time.
51|1||More on graph theoretic software watermarks: Implementation, analysis, and attacks|This paper presents an implementation of the watermarking method proposed by Venkatesan et al. in their paper [R. Venkatesan, V. Vazirani, S. Sinha, A graph theoretic approach to software watermarking, in: Fourth International Information Hiding Workshop, Pittsburgh, PA, 2001]. An executable program is marked by the addition of code for which the topology of the control-flow graph encodes a watermark. We discuss issues that were identified during construction of an actual implementation that operates on Java bytecode. We present two algorithms for splitting a watermark number into a redundant set of pieces and an algorithm for turning a watermark number into a control-flow graph. We measure the size and time overhead of watermarking, and evaluate the algorithm against a variety of attacks.
51|1||RESRES: The story behind the paper âResearch in software engineering: An analysis of the literatureâ|This article is a background report describing a comprehensive study of research in the three computing disciplines Computer Science, Software Engineering, and Information Systems. Findings relate to research topics, approaches, methods, reference disciplines, and levels of analysis. The article informally describes the process used and the research products produced.
51|1||A systematic review of quasi-experiments in software engineering|Experiments in which study units are assigned to experimental groups nonrandomly are called quasi-experiments. They allow investigations of cause–effect relations in settings in which randomization is inappropriate, impractical, or too costly.
51|1||Evaluating the validity of data instances against ontology evolution over the Semantic Web|It is natural for ontologies to evolve over time. These changes could be at structural and semantic levels. Due to changes to an ontology, its data instances may become invalid, and as a result, may become non-interpretable. In this paper, we address precisely this problem, validity of data instances due to ontological evolution. Towards this end, we make the following three novel contributions to the area of Semantic Web. First, we propose formal notions of structural validity and semantic validity of data instances, and then present approaches to ensure them. Second, we propose semantic view as part of an ontology, and demonstrate that it is sufficient to validate a data instance against the semantic view rather than the entire ontology. We discuss how the semantic view can be generated through an implication analysis, i.e., how semantic changes to one component imply semantic changes to other components in the ontology. Third, we propose a validity identification approach that employs locally maintaining a hash value of the semantic view at the data instance.
51|1||Multiversion join index for multiversion data warehouse|The data warehouse (DW) technology is developed in order to support the integration of external data sources (EDSs) for the purpose of advanced data analysis by On-Line Analytical Processing (OLAP) applications. Since contents and structures of integrated EDSs may evolve in time, the content and schema of a DW must evolve too in order to correctly reflect the evolution of EDSs. In order to manage a DW evolution, we developed the multiversion data warehouse (MVDW) approach. In this approach, different states of a DW are represented by the sequence of persistent DW versions that correspond either to the real world state or to a simulation scenario. Typically, OLAP applications execute star queries that join multiple fact and dimension tables. An important optimization technique for this kind of queries is based on join indexes. Since in the MVDW fact and dimension data are physically distributed among multiple DW versions, standard join indexes need extensions. In this paper we present the concept of a multiversion join index (MVJI) applicable to indexing dimension and fact tables in the MVDW. The MVJI has a two-level structure, where an upper level is used for indexing attributes and a lower level is used for indexing DW versions. The paper also presents the theoretical upper bound (pessimistic) analysis of the MVJI performance characteristic with respect to I/O operations. The analysis is followed by experimental evaluation. It shows that the MVJI increases a system performance for queries addressing multiple DW versions with exact match and range predicates.
51|1||Adaptive Agent Model: Software Adaptivity using an Agent-oriented Model-Driven Architecture|Model-Driven Architecture (MDA) promotes the development of software systems through successive building and generation of models, improving the reusability of models. Applying the same principles to the area of Agent-Oriented Software Engineering (AOSE) advances the ideas behind MDA even more significantly, due to the inherent adaptivity of software agents We describe an appropriate set of models originating from requirements specification and transformable to models understandable and executable by agents thus demonstrating an Agent-oriented Model-Driven Architecture (AMDA) approach. In AMDA, agents use hierarchical business knowledge models with business process rules at the top, business rules to control policy and logic in the middle and a base layer defining business concepts. Being externalised, knowledge is easily configurable by human beings and applied by software agents. A real case study is used to illustrate the process. The main advances over the object-oriented MDA are (i) the addition of component dynamics (ii) the use of agent-executable rule-based business models and (iii) a proposed higher level of abstraction with the direct representation of business requirements.
51|1||Object-oriented transformations for extracting aspects|In the migration of object-oriented systems towards the aspect technology, after locating fragments of code presenting a crosscutting behavior and before extracting such code to aspects, transformations may be needed in the base program. Such transformations aim to associate crosscutting code to points of the base program that can be captured using the pointcut descriptor model of aspect-oriented languages. In this paper, we present a catalog of object-oriented transformations and demonstrate the importance of such transformations by reporting on a case study involving four systems that have been aspectized using AspectJ.
51|1||Improving the effectiveness of root cause analysis in post mortem analysis: A controlled experiment|Retrospective analysis is a way to share knowledge following the completion of a project or major milestone. However, in the busy workday of a software project, there is rarely time for such reviews and there is a need for effective methods that will yield good results quickly without the need for external consultants or experts. Building on an existing method for retrospective analysis and theories of group involvement, we propose improvements to the root cause analysis phase of a lightweight retrospective analysis method known as post mortem analysis (PMA). In particular, to facilitate brainstorming during the root cause analysis phase of the PMA, we propose certain processual changes to facilitate more active individual participation and the use of less rigidly structured diagrams. We conducted a controlled experiment to compare this new variation of the method with the existing one, and conclude that in our setting of small software teams with no access to an experienced facilitator, the new variation is more effective when it comes to identifying possible root causes of problems and successes. The modified method also produced more specific starting points for improving the software development process.
51|1||Dynamic project performance estimation by combining static estimation models with system dynamics|Changes in user requirements or project personnel occur frequently during project execution particularly in long-term and large-size projects. We need a tool which can estimate the effects of changing conditions to effectively manage the project.
51|1||Accuracy and efficiency comparisons of single- and multi-cycled software classification models|Software classification models have been regarded as an essential support tool in performing measurement and analysis processes. Most of the established models are single-cycled in the model usage stage, and thus require the measurement data of all the model’s variables to be simultaneously collected and utilized for classifying an unseen case within only a single decision cycle. Conversely, the multi-cycled model allows the measurement data of all the model’s variables to be gradually collected and utilized for such a classification within more than one decision cycle, and thus intuitively seems to have better classification efficiency but poorer classification accuracy. Software project managers often have difficulties in choosing an appropriate classification model that is better suited to their specific environments and needs. However, this important topic is not adequately explored in software measurement and analysis literature. By using an industrial software measurement dataset of NASA KC2, this paper explores the quantitative performance comparisons of the classification accuracy and efficiency of the discriminant analysis (DA)- and logistic regression (LR)-based single-cycled models and the decision tree (DT)-based (C4.5 and ECHAID algorithms) multi-cycled models. The experimental results suggest that the re-appraisal cost of the Type I MR, the software failure cost of Type II MR and the data collection cost of software measurements should be considered simultaneously when choosing an appropriate classification model.
51|1||Empirical investigation towards the effectiveness of Test First programming|The Test First (TF) programming, which is based on an iterative process of “setting up test cases, implementing the functionality, and having all test cases passed”, has been put forward for decades, however knowledge of the evidence of the Test First programming’s success is limited. This paper describes a controlled experiment that investigated the distinctions between the effectiveness of Test First and that of Test Last (TL) (the traditional approach). The experimental results showed that Test First teams spent a larger percentage of time on testing. The achievable minimum external quality of delivered software applications increased with the percentage of time spent on testing regardless of the testing strategy (TF or TL) applied, although there does not exist a linear correlation between them. With four years’ data, it is also found that a strong linear correlation between the amount of effort spent on testing and coding in Test First teams, while this phenomenon was not observed in Test Last teams.
51|1||Variability assessment in software product families|Software variability management is a key factor in the success of software systems and software product families. An important aspect of software variability management is the evolution of variability in response to changing markets, business needs, and advances in technology. To be able to determine whether, when, and how variability should evolve, we have developed the COVAMOF software variability assessment method (COSVAM). The contribution of COSVAM is that it is a novel, and industry-strength assessment process that addresses the issues that are associated to the current variability assessment practice. In this paper, we present the successful validation of COSVAM in an industrial software product family.
51|1||Models of motivation in software engineering|Motivation in software engineering is recognized as a key success factor for software projects, but although there are many papers written about motivation in software engineering, the field lacks a comprehensive overview of the area. In particular, several models of motivation have been proposed, but they either rely heavily on one particular model (the job characteristics model), or are quite disparate and difficult to combine. Using the results from our previous systematic literature review (SLR), we constructed a new model of motivation in software engineering. We then compared this new model with existing models and refined it based on this comparison. This paper summarises the SLR results, presents the important existing models found in the literature and explains the development of our new model of motivation in software engineering.
51|10|http://www.sciencedirect.com/science/journal/09505849/51/10|Guest Editorial for the Special Issue on Source Code Analysis andManipulation, SCAM 2008|
51|10||Higher Order Mutation Testing|This paper introduces a new paradigm for Mutation Testing, which we call Higher Order Mutation Testing (HOM Testing). Traditional Mutation Testing considers only first order mutants, created by the injection of a single fault. Often these first order mutants denote trivial faults that are easily killed. Higher order mutants are created by the insertion of two or more faults. The paper introduces the concept of a subsuming HOM; one that is harder to kill than the first order mutants from which it is constructed. By definition, subsuming HOMs denote subtle fault combinations. The paper reports the results of an empirical study of HOM Testing using 10 programs, including several non-trivial real-world subjects for which test suites are available.
51|10||Change impact graphs: Determining the impact of prior codechanges|The source code of a software system is in constant change. The impact of these changes spreads out across the software system and may lead to the sudden manifestation of failures in unchanged parts. To help developers fix such failures, we propose a method that, in a pre-processing stage, analyzes prior code changes to determine what functions have been modified. Next, given a particular period of time in the past, the functions changed during that period are propagated throughout the rest of the system using the dependence graph of the system. This information is visualized using Change Impact Graphs (CIGs). Through a case study based on the Apache Web Server, we demonstrate the benefit of using CIGs to investigate several real defects.
51|10||Decompilation of Java bytecode to Prolog by partial evaluation|
51|10||Fast and precise points-to analysis|Many software engineering applications require points-to analysis. These client applications range from optimizing compilers to integrated program development environments (IDEs) and from testing environments to reverse-engineering tools. Moreover, software engineering applications used in an edit-compile cycle need points-to analysis to be fast and precise.
51|10||May/must analysis and the DFAGen data-flow analysis generator|Data-flow analysis is a common technique for gathering program information for use in program transformations such as register allocation, dead-code elimination, common subexpression elimination, and scheduling. Current tools for generating data-flow analysis implementations enable analysis details to be specified orthogonally to the iterative analysis algorithm but still require implementation details regarding the may and must use and definition sets that occur due to the effects of pointers, side effects, arrays, and user-defined structures. This paper presents the Data-Flow Analysis Generator tool (DFAGen), which enables analysis writers to generate analyses for separable and nonseparable data-flow analyses that are pointer, aggregate, and side-effect cognizant from a specification that assumes only scalars. By hiding the compiler-specific details behind predefined set definitions, the analysis specifications for the DFAGen tool are typically less than ten lines long and similar to those in standard compiler textbooks. The main contribution of this work is the automatic determination of when to use the may or must variant of a predefined set usage in the analysis specification.
51|10||Recovering structured data types from a legacy data model with overlays|
51|10||The life and death of statically detected vulnerabilities: An empirical study|Vulnerable statements constitute a major problem for developers and maintainers of networking systems. Their presence can ease the success of security attacks, aimed at gaining unauthorized access to data and functionality, or at causing system crashes and data loss. Examples of attacks caused by source code vulnerabilities are buffer overflows, command injections, and cross-site scripting.
51|11|http://www.sciencedirect.com/science/journal/09505849/51/11|Advancing test automation technology to meet the challenges of model-based software testing â Guest editorsâ introduction to the special section of the Third IEEE International Workshop on Automation of Software Test (AST 2008)|
51|11||Model-based testing approaches selection for software projects|Selecting software technologies for software projects represents a challenge to software engineers. It is known that software projects differ from each other by presenting different characteristics that can complicate the selection of such technologies. This is not different when considering model-based testing. There are many approaches with different characteristics described in the technical literature that can be used in software projects. However, there is no indication as to how they can fit a software project. Therefore, a strategy to select model-based testing approaches for software projects called Porantim is fully described in this paper. Porantim is based on a body of knowledge describing model-based testing approaches and their characterization attributes (identified by secondary and primary experimental studies), and a process to guide by adequacy and impact criteria regarding the use of this sort of software technology that can be used by software engineers to select model-based testing approaches for software projects.
51|11||Validation of SDL specifications using EFSM-based test generation|Existing methods for testing an SDL specification mainly allow for either black box simulation or conformance testing to verify that the behavior of an implementation matches its corresponding model. However, this relies on the potentially hazardous assumption that the model is completely correct. We propose a test generation method that can accomplish conformance verification as well as coverage criteria-driven white box testing of the specification itself. We first reformat a set of EFSMs equivalent to the processes in an SDL specification and identify “hot spots” – nodes or edges in the EFSM which should be prioritized during testing to effectively increase coverage. Then, we generate test sequences intended to cover selected hot spots; we address the possible infeasibility of such a test sequence by allowing for its rejection decided by a constraint solver and re-generation of an alternate test sequence to the hot spot. In this paper, we present our test generation method and tool, and provide case studies on five SDL processes demonstrating the effectiveness of our coverage-based test sequence selection.
51|11||A methodology for evaluating test coverage criteria of high levelPetri nets|High level Petri nets have been extensively used for modeling concurrent systems; however, their strong expressive power reduces their ability to be easily analyzed. Currently there are few effective formal analysis techniques to support the validation of high level Petri nets. The executable nature of high level Petri nets means that during validation they can be analyzed using test criteria defined on the net model. Recently, theoretical test adequacy coverage criteria for concurrent systems using high level Petri nets have been proposed. However, determining the applicability of these test adequacy criteria has not yet been undertaken. In this paper, we present an approach for evaluating the proposed test adequacy criteria for high level Petri nets through experimentation. In our experiments we use the simulation functionality of the model checker SPIN to analyze various test coverage criteria on high level Petri nets.
51|11||Test Case Evaluation and Input Domain Reduction strategies for the Evolutionary Testing of Object-Oriented software|In Evolutionary Testing, meta-heuristic search techniques are used for generating test data. The focus of our research is on employing evolutionary algorithms for the structural unit-testing of Object-Oriented programs. Relevant contributions include the introduction of novel methodologies for automation, search guidance and Input Domain Reduction; the strategies proposed were empirically evaluated with encouraging results.
51|11||Engineering quality software â Guest editorâs introduction to the special section of the Eighth International Conference on Quality Software (QSIC 2008)|
51|11||Using machine learning to refine Category-Partition test specifications and test suites|In the context of open source development or software evolution, developers often face test suites which have been developed with no apparent rationale and which may need to be augmented or refined to ensure sufficient dependability, or even reduced to meet tight deadlines. We refer to this process as the re-engineering of test suites. It is important to provide both methodological and tool support to help people understand the limitations of test suites and their possible redundancies, so as to be able to refine them in a cost effective manner. To address this problem in the case of black-box, Category-Partition testing, we propose a methodology and a tool based on machine learning that has shown promising results on a case study involving students as testers.
51|11||Integrating top-down and scenario-based methods for constructing software specifications|How to achieve a complete and consistent software specification by construction is an important issue for software quality assurance but still remains an open problem. The difficulty lies in the fact that the assurance of the completeness needs user’s judgments and the specification keeps changing as requirements analysis progresses. To allow the user to easily make such judgments and to reduce chances for creating inconsistencies due to frequent specification modifications, in this paper we describe an intuitive, formal, and expressive specification method that integrates top-down decompositional and scenario-based compositional methods. The decompositional method is used at an informal level with the goal of achieving a complete coverage of the user’s functional requirements, while the compositional method is used to precisely define the functionality of each scenario and to construct complex scenarios by composition of simple scenarios in a formal, intuitive language called SOFL. Combination of the decompositional and compositional processes can facilitate the analyst in completing a specification in a hierarchical structure. We present an example to illustrate how the integrated method is used in practice and describe a software support tool for the method.
51|11||Is non-parametric hypothesis testing model robust for statistical fault localization?|Fault localization is one of the most difficult activities in software debugging. Many existing statistical fault-localization techniques estimate the fault positions of programs by comparing the program feature spectra between passed runs and failed runs. Some existing approaches develop estimation formulas based on mean values of the underlying program feature spectra and their distributions alike. Our previous work advocates the use of a non-parametric approach in estimation formulas to pinpoint fault-relevant positions. It is worthy of further study to resolve the two schools of thought by examining the fundamental, underlying properties of distributions related to fault localization. In particular, we ask: Can the feature spectra of program elements be safely considered as normal distributions so that parametric techniques can be soundly and powerfully applied? In this paper, we empirically investigate this question from the program predicate perspective. We conduct an experimental study based on the Siemens suite of programs. We examine the degree of normality on the distributions of evaluation biases of the predicates, and obtain three major results from the study. First, almost all examined distributions of evaluation biases are either normal or far from normal, but not in between. Second, the most fault-relevant predicates are less likely to exhibit normal distributions in terms of evaluation biases than other predicates. Our results show that normality is not common as far as evaluation bias can represent. Furthermore, the effectiveness of our non-parametric predicate-based fault-localization technique weakly correlates with the distributions of evaluation biases, making the technique robust to this type of uncertainty in the underlying program spectra.
51|11||Architecture compliance checking at run-time|In this paper, we report on our experiences with architecture compliance checking – the process of checking whether the planned or specified software architecture is obeyed by the running system – of an OSGi-based, dynamically evolving application in the office domain. To that end, we first show how to dynamically instrument a running system in the context of OSGi in order to collect run-time traces. Second, we explain how to bridge the abstraction gap between run-time traces and software architectures, through the construction of hierarchical Colored Petri nets (CP-nets). In addition, we demonstrate how to design reusable hierarchical CP-nets. In an industry example, we were able to extract views that helped us to identify a number of architecturally relevant issues (e.g., architectural style violations, behavior violations) that would not have been detected otherwise, and could have caused serious problems like system malfunctioning or unauthorized access to sensitive data. Finally, we package valuable experiences and lessons learned from this endeavor.
51|11||Using coverage to automate and improve test purpose based testing|Test purposes have been presented as a solution to avoid the state space explosion when selecting test cases from formal models. Although such techniques work very well with regard to the speed of the test derivation, they leave the tester with one important task that influences the quality of the overall testing process: test purposes have to be formulated manually. In this paper, we present an approach that assists a test engineer with test purpose design in two ways: it allows automatic generation of coverage based test suites and can be used to automatically exercise those aspects of the system that are missed by hand-crafted test purposes. We consider coverage of Lotos specifications, and show how labeled transition systems derived from such specifications have to be extended in order to allow the application of logical coverage criteria to Lotos specifications. We then show how existing tools can be used to efficiently derive test cases and suggest how to use the coverage information to minimize test suites while generating them.
51|11||Linux bugs: Life cycle, resolution and architectural analysis|Efforts to improve application reliability can be irrelevant if the reliability of the underlying operating system on which the application resides is not seriously considered. An important first step in improving the reliability of an operating system is to gain insights into why and how the bugs originate, contributions of the different modules to the bugs, their distribution across severities, the different ways in which the bugs may be resolved and the impact of bug severities on their resolution times. To acquire this insight, we conducted an extensive analysis of the publicly available bug data on the Linux kernel over a period of seven years. We also justify and explain the statistical bug occurrence trends observed from the data, using the architecture of the Linux kernel as an anchor. The statistical analysis of the Linux bug data suggests that the Linux kernel may draw significant benefits from the continual reliability improvement efforts of its developers. These efforts, however, are disproportionately targeted towards popular configurations and hardware platforms, due to which the reliability of these configurations may be better than those that are not commonly used. Thus, a key finding of our study is that it may be prudent to restrict to using common configurations and platforms when using open source systems such as Linux in applications with stringent reliability expectations. Finally, our study of the architectural properties of the bugs suggests that the dependence among the modules rather than the unreliabilities of the individual modules is the primary cause of the bugs and their impact on system reliability.
51|12|http://www.sciencedirect.com/science/journal/09505849/51/12|Quality of UML models|
51|12||A systematic review of UML model consistency management|Information System (IS) development has been beset by consistency problems since its infancy. These problems are greater still in UML software development, and are principally caused by the existence of multiple views (models) for the same system, and may involve potentially contradictory system specifications. Since a considerable amount of work takes place within the scope of model consistency management, this paper presents a systematic literature review (SLR) which was carried out to discover the various current model consistency conceptions, proposals, problems and solutions provided. To do this, a total of 907 papers related to UML model consistency published in literature and extracted from the most relevant scientific sources (IEEE Computer Society, ACM Digital Library, Google Scholar, ScienceDirect, and the SCOPUS Database) were considered, of which 42 papers were eventually analyzed. This systematic literature review resulted in the identification of the current state-of-the-art with regard to UML model consistency management research along with open issues, trends and future research within this scope. A formal approach for the handling of inconsistency problems which fulfils the identified limitations is also briefly presented.
51|12||Definitions and approaches to model quality in model-based software development â A review of literature|More attention is paid to the quality of models along with the growing importance of modelling in software development. We performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. From forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. We further present six practices proposed for developing high-quality models together with examples of empirical evidence. The contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research.
51|12||Level of detail in UML models and its impact on model comprehension: A controlled experiment|Previous studies have shown that the style and rigor used in UML models vary widely across software projects 1, 2 and 3. However, notwithstanding the varying use of styles and rigor, little research has been conducted to investigate the drivers and effects of using different styles and rigor in modeling on software development. In this paper, we evaluate Level of Detail (LoD) in UML models as a form of style and rigor in UML modeling. Using a UML model of a library system, we experimentally investigate the impact of LoD on model comprehension. More specifically, we explore whether LoD in UML models affects the correctness and efficiency in comprehending UML models. Using two independent groups of graduate students majoring in computer science, we performed a controlled experiment. The results of the experiment confirm the significant effect of LoD in UML models on model comprehension. Nevertheless, replication of this study is necessary, especially in contexts that involve professional software engineers, to improve the generalizability of the results.
51|12||Guidelines on the aesthetic quality of UML class diagrams|In the past, formatting guidelines have proved to be a successful method to improve the readability of source code. With the increasing success of visual specification languages such as UML for model-driven software engineering visual guidelines are needed to standardize the presentation and the exchange of modeling diagrams with respect to human communication, understandability and readability. In this article, we introduce a new and encompassing taxonomy of visual guidelines capturing the aesthetic quality of UML class diagrams. We propose these guidelines as a framework to improve the aesthetic quality and thus the understandability of UML class diagrams. To validate this claim, we describe in detail a controlled experiment carried out as a pilot study to gather preliminary insights on the effects of some of the guideline rules on the understandability of UML class diagrams.
51|12||An investigation of use case quality in a large safety-critical software development project|
51|12||The practical application of a process for eliciting and designing security in web service systems|Best practices currently state that the security requirements and security architectures of distributed software-intensive systems should be based on security risk assessments, which have been designed from security patterns, are implemented in security standards and are tool-supported throughout their development life-cycle. Web service-based information systems uphold inter-enterprise relations through the Internet, and this technology has been revealed as the reference solution with which to implement Service-Oriented Architectures. In this paper, we present the application of the Process for Web Service Security (PWSSec), developed by the authors, to a real web service-based case study. The manner in which security in inter-organizational information systems can be analyzed, designed and implemented by applying PWSSec, which combines a risk analysis and management, along with a security architecture and a standard-based approach, is also shown. We additionally present a tool built to provide support to the PWSSec process.
51|12||Ontology-based modelling of architectural styles|The conceptual modelling of software architectures is of central importance for the quality of a software system. A rich modelling language is required to integrate the different aspects of architecture modelling, such as architectural styles, structural and behavioural modelling, into a coherent framework. Architectural styles are often neglected in software architectures. We propose an ontological approach for architectural style modelling based on description logic as an abstract, meta-level modelling instrument. We introduce a framework for style definition and style combination. The application of the ontological framework in the form of an integration into existing architectural description notations is illustrated.
51|2|http://www.sciencedirect.com/science/journal/09505849/51/2|A roadmap to electronic payment transaction guarantees and a Colored Petri Net model checking approach|Electronic payment systems play a vital role in modern business-to-consumer and business-to-business e-commerce. Atomicity, fault tolerance and security concerns form a problem domain of interdependent issues that are taken into account to assure the transaction guarantees of interest. We focus on the most notable payment transaction guarantees: money conservation, no double spending, goods atomicity, distributed payment atomicity, certified delivery or validated receipt and the high-level guarantees of fairness and protection of payment participants’ interests. Apart from a roadmap to the forenamed transaction guarantees, this work’s contribution is basically a full-fledged methodology for building and validating high-level protocol models and for proving payment transaction guarantees by model checking them from different participants perspectives (payer perspective, as well as payee perspective). Our approach lies on the use of Colored Petri Nets and the CPN Tools environment (i) for editing and analyzing protocol models, (ii) for proving the required transaction guarantees by CTL-based (Computation Tree Temporal Logic) model checking and (iii) for evaluating the need of candidate security requirements.
51|2||VxBPEL: Supporting variability for Web services in BPEL|Web services provide a way to facilitate the business integration over the Internet. Flexibility is an important and desirable property of Web service-based systems due to dynamic business environments. The flexibility can be provided or addressed by incorporating variability into a system. In this study, we investigate how variability can be incorporated into service-based systems. We propose a language, VxBPEL, which is an adaptation of an existing language, BPEL, and able to capture variability in these systems. We develop a prototype to interpret this language. Finally, we illustrate our method by using it to handle variability of an example.
51|2||Using acceptance tests as a support for clarifying requirements: A series of experiments|One of the main reasons for the failure of many software projects is the late discovery of a mismatch between the customers’ expectations and the pieces of functionality implemented in the delivered system. At the root of such a mismatch is often a set of poorly defined, incomplete, under-specified, and inconsistent requirements. Test driven development has recently been proposed as a way to clarify requirements during the initial elicitation phase, by means of acceptance tests that specify the desired behavior of the system.
51|2||Analysis of workflow dynamic changes based on Petri net|Dynamic adaptability has become one of the major research topics in the area of workflow management system. When adjusting a workflow process to some structural changes, there is a potential problem: the new workflow may contain errors, such as deadlock, inconsistency and even loss of instance. This paper primary addresses the issues related to workflow structural changes. It firstly defines a class of structural change called compatible change. This kind of change can be applied to the workflow process, without causing any structural errors or behavioral inconsistencies. Secondly, an algorithm is put forward to calculate the minimal region affected by the changes. Furthermore, it proves that the change regions can be used to check the compatibility of workflow changes. This approach is applicable and efficient in terms of time and space for large-scale and complex systems. Lastly, this paper discusses the problem to decide whether an active workflow instance can be smoothly evolved to the new workflow, and provides a sufficient condition for valid migration. In the end, an example is given to illustrate the effectiveness of the proposed concepts and method.
51|2||The effect of task order on the maintainability of object-oriented software|This paper presents results from a quasi-experiment that investigates how the sequence in which maintenance tasks are performed affects the time required to perform them and the functional correctness of the changes made. Specifically, the study compares how time required and correctness are affected by (1) starting with the easiest change task and progressively performing the more difficult tasks (Easy-First), versus (2) starting with the most difficult change task and progressively performing the easier tasks (Hard-First). In both cases, the experimental tasks were performed on two alternative types of design of a Java system to assess whether the choice of the design strategy moderates the effects of task order on effort and correctness.
51|2||An ADL dealing with aspects at software architecture stage|Managing complex software systems is one of the most important problems to be solved by software engineering. The software engineer needs to apply new techniques that allow for their adequate manipulation. Software architecture is becoming an important part of software design, helping the designer to handle the structure and the complexity of large systems, and AOSD is a paradigm proposed to manage this complexity by considering crosscutting concerns throughout the software’s life-cycle. The suitability of the existence of an Aspect-Oriented (AO) architectural design appears when AO concepts are extended to the whole life-cycle. In order to adequately specify the AO design, aspect-oriented architecture description languages are needed. The formal basis of these will allow architects to reason about the properties of the software architecture. In this paper, a new architecture description language – AspectLEDA – is formally described in order to adequately manipulate AO concepts at the software architecture stage. The AspectLEDA translation process is also described. A toolkit assists the architect during the process. Finally, a prototype of the system can be obtained, and the correctness of the architecture obtained can be checked.
51|2||A heuristics-based approach to reverse engineering of electronic services|
51|2||Towards the development of privacy-aware systems|Privacy and data protection are pivotal issues in nowadays society. They concern the right to prevent the dissemination of sensitive or confidential information of individuals. Many studies have been proposed on this topic from various perspectives, namely sociological, economic, legal, and technological. We have recognized the legal perspective as being the basis of all other perspectives. Actually, data protection regulations set the legal principles and requirements that must be met by organizations when processing personal data. The objective of this work is to provide a reference base for the development of methodologies tailored to design privacy-aware systems to be compliant with data protection regulations.
51|2||Evaluating the relationship between process improvement and schedule deviation in software maintenance|A basic proposition of process assessment models is that higher process maturity is associated with improved project performance and product quality. This study provides empirical evidence to support this proposition by testing the hypothesis that higher process maturity is negatively associated with schedule deviation in software maintenance. Next, the present study investigates whether two process context factors (organizational size and geographical region) modify the relationship between process maturity and schedule deviation by using a moderator testing method. Our results show that organizational size does not influence the relationship, while geographical region is deemed to be an independent variable.
51|2||Efficient tracesâ collection mechanisms for passive testing of Web Services|Web Services are a novel approach for business-to-business interactions. Their management, especially fault and performance management, is becoming necessary for their success and emergence. Nowadays, this management is platform-dependent and does not allow third parties to be involved. In this paper, we consider management of Web Services by passive testing where the tester itself is a Web Service. We propose different architectures for observation of simple and composite Web Services. We also study a set of online traces collection mechanisms and discuss their performances in terms of required CPU/RAM resources and introduced network overhead. These performances are then maximized by selecting best locations of observers. Observation considers both functional and non-functional (QoS) properties of Web Services. The paper presents also our experiments using different observation architectures and traces collection mechanisms while observing a simple and a composite Web Service.
51|2||Integrating in-process software defect prediction with association mining to discover defect pattern|Rather than detecting defects at an early stage to reduce their impact, defect prevention means that defects are prevented from occurring in advance. Causal analysis is a common approach to discover the causes of defects and take corrective actions. However, selecting defects to analyze among large amounts of reported defects is time consuming, and requires significant effort. To address this problem, this study proposes a defect prediction approach where the reported defects and performed actions are utilized to discover the patterns of actions which are likely to cause defects. The approach proposed in this study is adapted from the Action-Based Defect Prediction (ABDP), an approach uses the classification with decision tree technique to build a prediction model, and performs association rule mining on the records of actions and defects. An action is defined as a basic operation used to perform a software project, while a defect is defined as software flaws and can arise at any stage of the software process. The association rule mining finds the maximum rule set with specific minimum support and confidence and thus the discovered knowledge can be utilized to interpret the prediction models and software process behaviors. The discovered patterns then can be applied to predict the defects generated by the subsequent actions and take necessary corrective actions to avoid defects.
51|2||Algorithms and tool support for dynamic information flow analysis|A new approach to dynamic information flow analysis (DIFA) is presented, and its applications to intrusion detection, software testing and program debugging are discussed. The approach is based on a new forward-computing algorithm that enables online analysis when fast response is not critical. A new forward-computing algorithm for dynamic slicing is also presented, which is more precise than previous forward-computing algorithms and is not restricted to programs with structured control flow. The DIFA and slicing algorithms both rely on a new, precise direct dynamic control dependence algorithm, which requires only constant time per program action. The correctness of this algorithm depends on special, graph-theoretic properties of control dependence, which are established here. A tool called DynFlow is described that implements the proposed approach in order to support analysis of Java byte code programs, and two case studies are presented to illustrate how DynFlow can be used to detect and debug insecure flows. Finally, since dynamic analysis alone is inherently unable to detect implicit information flows, an extension to our approach is described that enables it to detect most implicit information flows at runtime.
51|2||Revising cohesion measures by considering the impact of write interactions between class members|
51|2||Automatic generation of test specifications for coverage of system state transitions|Adequate system testing of present day application programs requires satisfactory coverage of system states and transitions. This can be achieved by using a system state model. However, the system state models are rarely constructed by system developers, as these are large and complex. The only state models that are constructed by the developers are those of individual objects. However test case generation for state-based system testing based on traversal of statecharts of individual objects appears to be infeasible, since system test cases would have to be specified in the form of scenario sequences rather than transitions on individual object statecharts. In this paper, we propose a novel approach to coverage of elementary transition paths of an automatically synthesized system state model. Our technique for coverage of elementary transition paths would also ensure coverage of all states and transitions of the system model.
51|2||Evaluating legacy system migration technologies through empirical studies|We present two controlled experiments conducted with master students and practitioners and a case study conducted with practitioners to evaluate the use of MELIS (Migration Environment for Legacy Information Systems) for the migration of legacy COBOL programs to the web. MELIS has been developed as an Eclipse plug-in within a technology transfer project conducted with a small software company [16]. The partner company has developed and marketed in the last 30 years several COBOL systems that need to be migrated to the web, due to the increasing requests of the customers. The goal of the technology transfer project was to define a systematic migration strategy and the supporting tools to migrate these COBOL systems to the web and make the partner company an owner of the developed technology. The goal of the controlled experiments and case study was to evaluate the effectiveness of introducing MELIS in the partner company and compare it with traditional software development environments. The results of the overall experimentation show that the use of MELIS increases the productivity and reduces the gap between novice and expert software engineers.
51|2||CompAS: A new approach to commonality and variability analysis with applications in computer assisted orthopaedic surgery|In rapidly evolving domains such as Computer Assisted Orthopaedic Surgery (CAOS) emphasis is often put first on innovation and new functionality, rather than in developing the common infrastructure needed to support integration and reuse of these innovations. In fact, developing such an infrastructure is often considered to be a high-risk venture given the volatility of such a domain. We present CompAS, a method that exploits the very evolution of innovations in the domain to carry out the necessary quantitative and qualitative commonality and variability analysis, especially in the case of scarce system documentation. We show how our technique applies to the CAOS domain by using conference proceedings as a key source of information about the evolution of features in CAOS systems over a period of several years. We detect and classify evolution patterns to determine functional commonality and variability. We also identify non-functional requirements to help capture domain variability. We have validated our approach by evaluating the degree to which representative test systems can be covered by the common and variable features produced by our analysis.
51|2||Building test cases and oracles to automate the testing of web database applications|Many organizations rely on web applications that use back-end databases to store important data. Testing such applications requires significant effort. Manual testing alone is often impractical, so testers also rely on automated testing techniques. However, current automated testing techniques may produce false positives (or false negatives) even in a perfectly working system because the outcome of a test case depends on the state of the database which changes over time as data is inserted and deleted. The Automatic Database Tester (AutoDBT) generates functional test cases that account for database updates. AutoDBT takes as input a model of the application and a set of testing criteria. The model consists of a state transition diagram that shows how users navigate pages, a data specification that captures how data flows, and an update specification that shows how the database is updated. AutoDBT generates guard queries to determine whether the database is in a state conducive to performing and evaluating tests. AutoDBT also generates partial oracles to help validate whether a back-end database is updated correctly during testing. This paper describes the design of AutoDBT, a prototype implementation, several experiments with the prototype, and four case studies.
51|2||Consistency preserving co-evolution of formal specifications and agent-oriented conceptual models|
51|2||Uncertainty explicit assessment of off-the-shelf software: A Bayesian approach|Assessment of software COTS components is an essential part of component-based software development. Poorly chosen components may lead to solutions of low quality and that are difficult to maintain. The assessment may be based on incomplete knowledge about the COTS component itself and other aspects (e.g. vendor’s credentials, etc.), which may affect the decision of selecting COTS component(s). We argue in favor of assessment methods in which uncertainty is explicitly represented (‘uncertainty explicit’ methods) using probability distributions. We provide details of a Bayesian model, which can be used to capture the uncertainties in the simultaneous assessment of two attributes, thus, also capturing the dependencies that might exist between them. We also provide empirical data from the use of this method for the assessment of off-the-shelf database servers which illustrate the advantages of ‘uncertainty explicit’ methods over conventional methods of COTS component assessment which assume that at the end of the assessment the values of the attributes become known with certainty.
51|2||Automated traceability analysis for UML model refinements|During iterative, UML-based software development, various UML diagrams, modeling the same system at different levels of abstraction are developed. These models must remain consistent when changes are performed. In this context, we refine the notion of impact analysis and distinguish horizontal impact analysis–that focuses on changes and impacts at one level of abstraction–from vertical impact analysis–that focuses on changes at one level of abstraction and their impacts on another level. Vertical impact analysis requires that some traceability links be established between model elements at the two levels of abstraction. We propose a traceability analysis approach for UML 2.0 class diagrams which is based on a careful formalization of changes to those models, refinements which are composed of those changes, and traceability links corresponding to refinements. We show how actual refinements and corresponding traceability links are formalized using the OCL. Tool support and a case study are also described.
51|2||An approach for concurrent evaluation of technical and social aspects of software development methodologies|The paper presents an approach for evaluation of software development methodologies (SDM) that considers the aspects of a SDM’s social adoption and technical efficiency. It builds on existing evaluation models used in the field of SDM. Case study approach was used to validate the model in four software development organisations. In all four cases the management confirmed that the model provided valuable new insights into adoption and efficiency of the companies’ SDM.
51|2||Covering code behavior on input validation in functional testing|Input validation is the enforcement built in software systems to ensure that only valid input is accepted to raise external effects. It is essential and very important to a large class of systems and usually forms a major part of a data-intensive system. Most existing methods for input validation testing are specification-based. However, to test input validation more accurately, a code-based method is also required. In this paper, we propose an approach to extract path partition and input conditions from code for testing input validation. The path partition can be used to design white-box test cases for testing input validation. It can also be used to measure the coverage of input validation testing. The valid and invalid input conditions recovered can be used to check against the specifications and aid the test suite design in black-box testing. We have also evaluated the proposed method through experimental study.
51|3|http://www.sciencedirect.com/science/journal/09505849/51/3|Evaluating and selecting software packages: A review|Evaluating and selecting software packages that meet an organization’s requirements is a difficult software engineering process. Selection of a wrong software package can turn out to be costly and adversely affect business processes. The aim of this paper is to provide a basis to improve the process of evaluation and selection of the software packages. This paper reports a systematic review of papers published in journals and conference proceedings. The review investigates methodologies for selecting software packages, software evaluation techniques, software evaluation criteria, and systems that support decision makers in evaluating software packages. The key findings of the review are: (1) analytic hierarchy process has been widely used for evaluation of the software packages, (2) there is lack of a common list of generic software evaluation criteria and its meaning, and (3) there is need to develop a framework comprising of software selection methodology, evaluation technique, evaluation criteria, and system to assist decision makers in software selection.
51|3||Model-checking for adventure videogames|This paper describes a model-checking approach for adventure games focusing on ãe-Adventureã, a platform for the development of adaptive educational adventure videogames. In ãe-Adventureã, games are described using a domain-specific language oriented to game writers. By defining a translation from this language to suitable state-based models, it is possible to automatically extract a verification model for each ãe-Adventureã game. In addition, temporal properties to be verified are described using an extensible assertion language, which can be tailored to each specific application scenario. When the framework determines that some of these properties do not hold, it generates an animation of a counterexample. This approach facilitates the collaboration of multidisciplinary teams of experts during the verification of the integrity of the game scripts, exchanging hours of manual verification for semi-automatic verification processes that also facilitate the diagnosis of the conditions that may potentially break the games.
51|3||An economic model to compare the profitability of pay-per-use and fixed-fee licensing|This paper develops an economic model to compare the profitability of two strategies for the pricing of packaged software: fixed-fee and pay-per-use licensing. It is assumed that the market consists of a monopoly software vendor who is selling packaged software to customers who are homogeneous in marginal value of software use but heterogeneous in level of use. In addition to obtaining the software package from the market, customers can develop the required software in-house. When in-house development costs are constant across customers, the results show that the software vendor prefers pay-per-use licensing over fixed-fee licensing if in-house development is relatively expensive, whereas fixed-fee licensing is optimal if the cost of in-house development drops below a certain threshold value. When the assumption of a constant in-house development cost is relaxed by letting it vary among customers, it still holds that pay-per-use licensing is optimal if its average is relatively large. For low and medium values of the average cost of in-house development, however, fixed-fee licensing may no longer be optimal as the relative attractiveness of the two licensing strategies now depends on how dispersed the in-house development costs of individual customers are.
51|3||On automated prepared statement generation to remove SQL injection vulnerabilities|Since 2002, over 10% of total cyber vulnerabilities were SQL injection vulnerabilities (SQLIVs). This paper presents an algorithm of prepared statement replacement for removing SQLIVs by replacing SQL statements with prepared statements. Prepared statements have a static structure, which prevents SQL injection attacks from changing the logical structure of a prepared statement. We created a prepared statement replacement algorithm and a corresponding tool for automated fix generation. We conducted four case studies of open source projects to evaluate the capability of the algorithm and its automation. The empirical results show that prepared statement code correctly replaced 94% of the SQLIVs in these projects.
51|3||Customizing ISO 9126 quality model for evaluation of B2B applications|A software quality model acts as a framework for the evaluation of attributes of an application that contribute to the software quality. In this paper, a quality model is presented for evaluation of B2B applications. First, the most well-known quality models are studied, and reasons for using ISO 9126 quality model as the basis are discussed. This model, then, is customized in accordance with special characteristics of B2B applications. The customization is done by extracting the quality factors from web applications and B2B e-commerce applications, weighting these factors from the viewpoints of both developers and end users, and adding them to the model. Finally, as a case study, ISACO portal is evaluated by the proposed model.
51|3||Complexity metrics for Workflow nets|Process modeling languages such as EPCs, BPMN, flow charts, UML activity diagrams, Petri nets, etc., are used to model business processes and to configure process-aware information systems. It is known that users have problems understanding these diagrams. In fact, even process engineers and system analysts have difficulties in grasping the dynamics implied by a process model. Recent empirical studies show that people make numerous errors when modeling complex business processes, e.g., about 20% of the EPCs in the SAP reference model have design flaws resulting in potential deadlocks, livelocks, etc. It seems obvious that the complexity of the model contributes to design errors and a lack of understanding. It is not easy to measure complexity, however. This paper presents three complexity metrics that have been implemented in the process analysis tool ProM. The metrics are defined for a subclass of Petri nets named Workflow nets, but the results can easily be applied to other languages. To demonstrate the applicability of these metrics, we have applied our approach and tool to 262 relatively complex Protos models made in the context of various student projects. This allows us to validate and compare the different metrics. It turns out that our new metric focusing on the structuredness outperforms existing metrics.
51|3||How do personality, team processes and task characteristics relate to job satisfaction and software quality?|This article analyses the relationships between personality, team processes, task characteristics, product quality and satisfaction in software development teams. The data analysed here were gathered from a sample of 35 teams of students (105 participants). These teams applied an adaptation of an agile methodology, eXtreme Programming (XP), to develop a software product. We found that the teams with the highest job satisfaction are precisely the ones whose members score highest for the personality factors agreeableness and conscientiousness. The satisfaction levels are also higher when the members can decide how to develop and organize their work. On the other hand, the level of satisfaction and cohesion drops the more conflict there is between the team members. Finally, the teams exhibit a significant positive correlation between the personality factor extraversion and software product quality.
51|3||Handling imprecision and uncertainty in software development effort prediction: A type-2 fuzzy logic based framework|Traditional approaches for software projects effort prediction such as the use of mathematical formulae derived from historical data, or the use of experts judgments are plagued with issues pertaining to effectiveness and robustness in their results. These issues are more pronounced when these effort prediction approaches are used during the early phases of the software development lifecycle, for example requirements development, whose effort predictors along with their relationships to effort are characterized as being even more imprecise and uncertain than those of later development phases, for example design. Recent works have demonstrated promising results using approaches based on fuzzy logic. Effort prediction systems that use fuzzy logic can deal with imprecision; they, however, can not deal with uncertainty. This paper presents an effort prediction framework that is based on type-2 fuzzy logic to allow handling imprecision and uncertainty inherent in the information available for effort prediction. Evaluation experiments have shown the framework to be promising.
51|3||A framework for understanding creativity in requirements engineering|Creativity is important in the discovery and analysis of user and business requirements to achieve innovative uses of information and communication technologies. This paper builds a theoretical framework for understanding creativity in requirements engineering. The framework provides a systematic means of understanding creativity in requirements engineering and comprises five elements (product, process, domain, people and socio-organisational context). The framework provides researchers with a sound basis for exploring how the five elements of creativity can be incorporated within RE methods and techniques to support creative requirements engineering. It provides practitioners with a systematic means of creating environments that nurture and develop creative people, cognitive and collaborative processes and products.
51|3||Investigating the relationship between schedules and knowledge transfer in software testing|This empirical study investigates the relationship between schedules and knowledge transfer in software testing. In our exploratory survey, statistical analysis indicated that increased knowledge transfer between testing and earlier phases of software development was associated with testing schedule over-runs. A qualitative case study was conducted to interpret this result. We found that this relationship can be explained with the size and complexity of software, knowledge management issues, and customer involvement. We also found that the primary strategies for avoiding testing schedule over-runs were reducing the scope of testing, leaving out features from the software, and allocating more resources to testing.
51|4|http://www.sciencedirect.com/science/journal/09505849/51/4|Analysis of test suite reduction with enhanced tie-breaking techniques|Test suite minimization techniques try to remove redundant test cases of a test suite. However, reducing the size of a test suite might reduce its ability to reveal faults. In this paper, we present a novel approach for test suite reduction that uses an additional testing criterion to break the ties in the minimization process. We integrated the proposed approach with two existing algorithms and conducted experiments for evaluation. The experiment results show that our approach can improve the fault detection effectiveness of reduced suites with a negligible increase in the size of the suites. Besides, under specific conditions, the proposed approach can also accelerate the process of minimization.
51|4||A bottom-up pointer analysis using the update history|Pointer analysis is an important part for the source code analysis of C programs. In this paper, we propose a bottom-up and flow- and context-sensitive pointer analysis algorithm, where bottom-up refers to the ability to perform the analysis from callee modules to caller modules. Our approach is based on a new modular pointer analysis domain named the update history that can abstract memory states of a procedure independently of the information on aliases between memory locations and keep the information on the order of side effects performed. Such a memory representation not only enables the analysis to be formalized as a bottom-up analysis, but also helps the analysis to effectively identify killed side effects and relevant alias contexts. The experiments performed on a pilot implementation of the method shows that our approach is effective for improving the precision of a client analysis.
51|4||Automated test data generation using a scatter search approach|The techniques for the automatic generation of test cases try to efficiently find a small set of cases that allow a given adequacy criterion to be fulfilled, thus contributing to a reduction in the cost of software testing. In this paper we present and analyze two versions of an approach based on the scatter search metaheuristic technique for the automatic generation of software test cases using a branch coverage adequacy criterion. The first test case generator, called TCSS, uses a diversity property to extend the search of test cases to all branches of the program under test in order to generate test cases that cover these. The second, called TCSS-LS, is an extension of the previous test case generator which combines the diversity property with a local search method that allows the intensification of the search for test cases that cover the difficult branches. We present the results obtained by our generators and carry out a detailed comparison with many other generators, showing a good performance of our approach.
51|4||A framework for developing measurement systems and its industrial evaluation|As in every engineering discipline, metrics play an important role in software development, with the difference that almost all software projects need the customization of metrics used. In other engineering disciplines, the notion of a measurement system (i.e. a tool used to collect, calculate, and report quantitative data) is well known and defined, whereas it is not as widely used in software engineering. In this paper we present a framework for developing custom measurement systems and its industrial evaluation in a software development unit within Ericsson. The results include the framework for designing measurement systems and its evaluation in real life projects at the company. The results show that with the help of ISO/IEC standards, measurement systems can be effectively used in software industry and that the presented framework improves the way of working with metrics. This paper contributes with the presentation of how automation of metrics collection and processing can be successfully introduced into a large organization and shows the benefits of it: increased efficiency of metrics collection, increased adoption of metrics in the organization, independence from individuals and standardized nomenclature for metrics in the organization.
51|4||Comparison of estimation methods of cost and duration in IT projects|Producing accurate and reliable project cost estimations at an early stage of a project’s life cycle remains a substantial challenge in the information technology field. This research benchmarks the performance of various approaches to estimating IT project effort and duration. Empirical data were gathered from various “real-world” organizations including several prominent Israeli high-tech companies as well as from the International Software Benchmarking Standards Group (ISBSG) IT project database. The study contrasts two types of models that have been employed to estimate project duration and effort separately: linear regression estimation models and models deriving from a more novel approach based on artificial neural networks (ANNs).
51|4||Introducing requirements traceability support in model-driven development of web applications|In this work, we present an approach that introduces requirements traceability capabilities in the context of model-driven development of Web applications. This aspect allows us to define model-to-model transformations that not only provide a software artifact of lower abstraction (as model-to-model transformations usually do) but also to provide feedback about how they are applied. This feedback helps us to validate whether transformations are correctly applied. In particular, we present a model-to-model transformation that allows us to obtain navigational models of the Web engineering method OOWS from a requirements model. This transformation is defined as a set of mappings between these two models that have been implemented by means of graph transformations. The use of graph transformations allows us to develop a tool-supported strategy for applying mappings automatically. In addition, mechanisms for tracing requirements are also included in the definition of graph transformations. These mechanisms allow us to link each conceptual element to the requirements from which it is derived. In particular, we focus on tracing requirements throughout the navigational model, which describe the navigational structure of a Web application. To take advantage of these traceability mechanisms, we have developed a tool that obtains traceability reports after applying transformations. These reports help us to study aspects such as whether requirements are all supported, the impact of changing a requirement, or how requirements are modelled.
51|4||Supporting user-oriented analysis for multi-view domain-specific visual languages|The integration of usable and flexible analysis support in modelling environments is a key success factor in Model-Driven Development. In this paradigm, models are the core asset from which code is automatically generated, and thus ensuring model correctness is a fundamental quality control activity. For this purpose, a common approach is to transform the system models into formal semantic domains for verification. However, if the analysis results are not shown in a proper way to the end-user (e.g. in terms of the original language) they may become useless.
51|4||The repertory grid technique: Its place in empirical software engineering research|
51|4||Factor oriented requirement coverage based system test case prioritization of new and regression test cases|Test case prioritization involves scheduling test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects the most severe faults at the earliest in its testing life cycle. In this paper, we propose to put forth a model for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software that can also be cost effective and to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the six factors: customer priority, changes in requirement, implementation complexity, completeness, traceability and fault impact. The proposed prioritization technique is validated with two different validation techniques and is experimented in three phases with student projects and two sets of industrial projects and the results show convincingly that the proposed prioritization technique improves the rate of severe fault detection.
51|5|http://www.sciencedirect.com/science/journal/09505849/51/5|Model-Driven Development for secure information systems|
51|5||Automated analysis of security-design models|We have previously proposed SecureUML, an expressive UML-based language for constructing security-design models, which are models that combine design specifications for distributed systems with specifications of their security policies. Here, we show how to automate the analysis of such models in a semantically precise and meaningful way. In our approach, models are formalized together with scenarios that represent possible run-time instances. Queries about properties of the security policy modeled are expressed as formulas in UML’s Object Constraint Language. The policy may include both declarative aspects, i.e., static access-control information such as the assignment of users and permissions to roles, and programmatic aspects, which depend on dynamic information, namely the satisfaction of authorization constraints in a given scenario. We show how such properties can be evaluated, completely automatically, in the context of the metamodel of the security-design language. We demonstrate, through examples, that this approach can be used to formalize and check non-trivial security properties. The approach has been implemented in the SecureMOVA tool and all of the examples presented have been checked using this tool.
51|5||A security-aware metamodel for multi-agent systems (MAS)|This paper adopts a model-based security (MBS) approach to identify security requirements during the early stages of multi-agent system development. Our adopted MBS approach is underpinned by a metamodel independent of any specific methodology. It allows for security considerations to be embedded within any situated agent methodology which then prescribes security considerations within its work products. Using a standard model-driven engineering (MDE) approach, these work products are initially constructed as high abstraction models and then transformed into more precise models until code-specific models can be produced. A multi-agent system case study is used to illustrate the applicability of the proposed security-aware metamodel.
51|5||An aspect-oriented methodology for designing secure applications|We propose a methodology, based on aspect-oriented modeling (AOM), for incorporating security mechanisms in an application. The functionality of the application is described using the primary model and the attacks are specified using aspects. The attack aspect is composed with the primary model to obtain the misuse model. The misuse model describes how much the application can be compromised. If the results are unacceptable, then some security mechanism must be incorporated into the application. The security mechanism, modeled as security aspect, is composed with the primary model to obtain the security-treated model. The security-treated model is analyzed to give assurance that it is resilient to the attack.
51|5||A model-based aspect-oriented framework for building intrusion-aware software systems|Security is a critical issue for software systems, especially for those systems which are connected to networks and the Internet, since most of them suffer from various malicious attacks. Intrusion detection is an approach to protect software against such attacks. However, security vulnerabilities that are exploited by intruders cut across multiple modules in software systems and are difficult to address and monitor. These kinds of concerns, called cross-cutting concerns, can be handled by aspect-oriented software development (AOSD) for better modularization. A number of works have utilized AOSD to address security issues of software systems, but none of them has employed AOSD for intrusion detection. In this paper, we propose a model-based aspect-oriented framework for building intrusion-aware software systems. We model attack scenarios and intrusion detection aspects using an aspect-oriented Unified Modeling Language (UML) profile. Based on the UML model, the intrusion detection aspects are implemented and woven into the target system. The resulting target system has the ability to detect the intrusions automatically. We present an experimental evaluation by applying this framework for some of the most common attacks included in the Web Application Security Consortium (WASC) web security threat classification. The experimental results demonstrate that the framework is effective in specifying and implementing intrusion detection and can be applied for a wide range of attacks.
51|5||XRound: A reversible template language and its application in model-based security analysis|Successful analysis of the models used in Model-Driven Development requires the ability to synthesise the results of analysis and automatically integrate these results with the models themselves. This paper presents a reversible template language called XRound which supports round-trip transformations between models and the logic used to encode system properties. A template processor that supports the language is described, and the use of the template language is illustrated by its application in an analysis workbench, designed to support analysis of security properties of UML and MOF-based models. As a result of using reversible templates, it is possible to seamlessly and automatically integrate the results of a security analysis with a model.
51|5||Model-Based Development of firewall rule sets: Diagnosing model inconsistencies|The design and management of firewall rule sets is a very difficult and error-prone task because of the difficulty of translating access control requirements into complex low-level firewall languages. Although high-level languages have been proposed to model firewall access control lists, none has been widely adopted by the industry. We think that the main reason is that their complexity is close to that of many existing low-level languages. In addition, none of the high-level languages that automatically generate firewall rule sets verifies the model prior to the code-generation phase. Error correction in the early stages of the development process is cheaper compared to the cost associated with correcting errors in the production phase. In addition, errors generated in the production phase usually have a huge impact on the reliability and robustness of the generated code and final system.
51|5||Experimental comparison of attack trees and misuse cases for security threat identification|A number of methods have been proposed or adapted to include security in the requirements analysis stage, but the industrial take-up has been limited and there are few empirical and comparative evaluations. This paper reports on a pair of controlled experiments that compared two methods for early elicitation of security threats, namely attack trees and misuse cases. The 28 and 35 participants in the two experiments solved two threat identification tasks individually by means of the two techniques, using a Latin-Squares design to control for technique and task order. The dependent variables were effectiveness of the techniques measured as the number of threats found, coverage of the techniques measured in terms of the types of threats found and perceptions of the techniques measured through a post-task questionnaire based on the Technology Acceptance Model. The only difference was that, in the second experiment, the participants were given a pre-drawn use-case diagram to use as a starting point for solving the tasks. In the first experiment, no pre-drawn use-case diagram was provided. The main finding was that attack trees were more effective for finding threats, in particular when there was no pre-drawn use-case diagram. However, the participants had similar opinions of the two techniques, and perception of a technique was not correlated with performance with that technique. The study underlines the need for further comparisons in a broader range of settings involving additional techniques, and it suggests several concrete experiments and other paths for further work.
51|5||An adaptive security model using agent-oriented MDA|Model-driven architecture (MDA) supports model-centred software development via successive model transformation. In MDA, the reusability of models is improved as well as the traceability of requirements. Agent-oriented model-driven architecture (AMDA) associates adaptive agents with a business-oriented interaction model and lets agents dynamically interpret their behaviour from the continuously maintained model via which the current business needs are deployed at runtime. The continuous re-interpretation rather than discrete re-transformation of models means immediate requirements deployment after re-configuration, no system down time being required to affect changes and results in a development process that is oriented to business experts rather than developers. Adopting the adaptive agent model, an AMDA paradigm, we put forward a security–aware model-driven mechanism by using an extension of the role-based access control (RBAC) model. For this purpose, the concept of agent role proposed in agent-oriented software engineering (AOSE) is integrated with the one proposed in RBAC. Agent duties are specified in an interaction model and describe the roles that agents can play to fulfil their functional responsibilities. Agent rights are specified in a security policy rule model attached to the interaction model and describe constraints upon agent capabilities caused by their associated social roles. The role-based interaction and policy-driven model incorporates both agent rights and duties. Hence, functional requirements and non-functional security constraint requirements are put together, related by the concept of role. Consequently, agents can continuously use the re-configurable model to play their roles in order to fulfil their responsibilities, and at the same time respect the security constraints. The major contribution from the approach is a method for building adaptive and secure MAS, following model-driven architecture. The approach is illustrated with an actual British railway management system.
51|6|http://www.sciencedirect.com/science/journal/09505849/51/6|A systematic review of search-based testing for non-functional system properties|Search-based software testing is the application of metaheuristic search techniques to generate software tests. The test adequacy criterion is transformed into a fitness function and a set of solutions in the search space are evaluated with respect to the fitness function using a metaheuristic search technique. The application of metaheuristic search techniques for testing is promising due to the fact that exhaustive testing is infeasible considering the size and complexity of software under test. Search-based software testing has been applied across the spectrum of test case design methods; this includes white-box (structural), black-box (functional) and grey-box (combination of structural and functional) testing. In addition, metaheuristic search techniques have also been applied to test non-functional properties. The overall objective of undertaking this systematic review is to examine existing work into non-functional search-based software testing (NFSBST). We are interested in types of non-functional testing targeted using metaheuristic search techniques, different fitness functions used in different types of search-based non-functional testing and challenges in the application of these techniques. The systematic review is based on a comprehensive set of 35 articles obtained after a multi-stage selection process and have been published in the time span 1996–2007. The results of the review show that metaheuristic search techniques have been applied for non-functional testing of execution time, quality of service, security, usability and safety. A variety of metaheuristic search techniques are found to be applicable for non-functional testing including simulated annealing, tabu search, genetic algorithms, ant colony methods, grammatical evolution, genetic programming (and its variants including linear genetic programming) and swarm intelligence methods. The review reports on different fitness functions used to guide the search for each of the categories of execution time, safety, usability, quality of service and security; along with a discussion of possible challenges in the application of metaheuristic search techniques.
51|6||A UML profile for the conceptual modelling of data-mining with time-series in data warehouses|Time-series analysis is a powerful technique to discover patterns and trends in temporal data. However, the lack of a conceptual model for this data-mining technique forces analysts to deal with unstructured data. These data are represented at a low-level of abstraction and their management is expensive. Most analysts face up to two main problems: (i) the cleansing of the huge amount of potentially-analysable data and (ii) the correct definition of the data-mining algorithms to be employed. Owing to the fact that analysts’ interests are also hidden in this scenario, it is not only difficult to prepare data, but also to discover which data is the most promising. Since their appearance, data warehouses have, therefore, proved to be a powerful repository of historical data for data-mining purposes. Moreover, their foundational modelling paradigm, such as, multidimensional modelling, is very similar to the problem domain. In this article, we propose a unified modelling language (UML) extension through UML profiles for data-mining. Specifically, the UML profile presented allows us to specify time-series analysis on top of the multidimensional models of data warehouses. Our extension provides analysts with an intuitive notation for time-series analysis which is independent of any specific data-mining tool or algorithm. In order to show its feasibility and ease of use, we apply it to the analysis of fish-captures in Alicante. We believe that a coherent conceptual modelling framework for data-mining assures a better and easier knowledge-discovery process on top of data warehouses.
51|6||Requirements-based Access Control Analysis and Policy Specification (ReCAPS)|Access control (AC) is a mechanism for achieving confidentiality and integrity in software systems. Access control policies (ACPs) express rules concerning who can access what information, and under what conditions. ACP specification is not an explicit part of the software development process and is often isolated from requirements analysis activities, leaving systems vulnerable to security breaches because policies are specified without ensuring compliance with system requirements. In this paper, we present the Requirements-based Access Control Analysis and Policy Specification (ReCAPS) method for deriving and specifying ACPs, and discuss three validation efforts. The method integrates policy specification into the software development process, ensures consistency across software artifacts, and provides prescriptive guidance for how to specify ACPs. It also improves the quality of requirements specifications and system designs by clarifying ambiguities and resolving conflicts across these artifacts during the analysis, making a significant step towards ensuring that policies are enforced in a manner consistent with a system’s requirements specifications. To date, the method has been applied within the context of four operational systems. Additionally, we have conducted an empirical study to evaluate its usefulness and effectiveness. A software tool, the Security and Privacy Requirements Analysis Tool (SPRAT), was developed to support ReCAPS analysis activities.
51|6||Reduction rules for YAWL workflows with cancellation regions and OR-joins|As the need for concepts such as cancellation and OR-joins occurs naturally in business scenarios, comprehensive support in a workflow language is desirable. However, there is a clear trade-off between the expressive power of a language (i.e., introducing complex constructs such as cancellation and OR-joins) and ease of verification. When a workflow contains a large number of tasks and involves complex control flow dependencies, verification can take too much time or it may even be impossible. There are a number of different approaches to deal with this complexity. Reducing the size of the workflow, while preserving its essential properties with respect to a particular analysis problem, is one such approach. In this paper, we present a set of reduction rules for workflows with cancellation regions and OR-joins and demonstrate how they can be used to improve the efficiency of verification. Our results are presented in the context of the YAWL workflow language.
51|6||Tool-supported requirements prioritization: Comparing the AHP and CBRank methods|Requirements prioritization aims at identifying the most important requirements for a software system, a crucial step when planning for system releases and deciding which requirements to implement in each release. Several prioritization methods and supporting tools have been proposed so far. How to evaluate their properties, with the aim of supporting the selection of the most appropriate method for a specific project, is considered a relevant question.
51|6||An engineering process for developing Secure Data Warehouses|We present a new approach for the elicitation and development security requirements in the entire Data Warehouse (DWs) life cycle, which we have called a Secure Engineering process for DAta WArehouses (SEDAWA). Whilst many methods for the requirements analysis phase of the DWs have been proposed, the elicitation of security requirements as non-functional requirements has not received sufficient attention. Hence, in this paper we propose a methodology for the DW design based on Model Driven Architecture (MDA) and the standard Software Process Engineering Metamodel Specification (SPEM) from the Object Management Group (OMG). We define four phases comprising of several activities and steps, an d five disciplines which cover the whole DW design. Our methodology adapts the i∗ framework to be used under MDA and the SPEM approaches in order to elicit and develop security requirements for DWs. The benefits of our proposal are shown through an example related to the management of the pharmacies consortium business.
51|6||Malaca: A component and aspect-oriented agent architecture|The production of maintainable and reusable agents depends largely on how well the agent architecture is modularized. Most commercial agent toolkits provide an Object-Oriented (OO) framework, whose agent architecture does not facilitate separate (re)use of the domain-specific functionality of an agent from other concerns. This paper presents Malaca, an agent architecture that combines the use of Component-based Software Engineering and Aspect-Oriented Software Development, both of which promote better modularization of the agent architecture while increase at the architectural level. Malaca supports the separate (re)use of the domain-specific functionality of an agent from other communication concerns, providing explicit support for the design and configuration of agent architectures and allows the development of agent-based software so that it is easy to understand, maintain and reuse.
51|6||Software product integration: A case study-based synthesis of reference models|In software intensive systems the integration becomes complex since both software and hardware components are integrated and run in the execution environment for the first time. Support for this stage is thus essential. Practices for Product Integration are described in different reference models. We have investigated these and compared them with activities performed in seven product development projects.
51|6||Empirical evaluation in Computer Science research published by ACM|This paper repeats part of the analysis performed in the 1995 paper “Experimental evaluation in Computer Science: a quantitative study” by Tichy and collaborators, for 147 papers randomly selected from the ACM, published in the year 2005. The papers published in 2005 are classified in the following way: 4% theory, 17% empirical, 4.7% hypothesis testing, 3.4% other, and 70% design and modeling (using the 1995 paper categories). Within the design and modeling class, 33% of the papers have no evaluation. The numbers of the 2005 sample are very similar to the original figures for the 1995 sample, which shows that Computer Science research has not increased significantly its empirical or experimental component.
51|7|http://www.sciencedirect.com/science/journal/09505849/51/7|A systematic literature review to identify and classify software requirement errors|Most software quality research has focused on identifying faults (i.e., information is incorrectly recorded in an artifact). Because software still exhibits incorrect behavior, a different approach is needed. This paper presents a systematic literature review to develop taxonomy of errors (i.e., the sources of faults) that may occur during the requirements phase of software lifecycle. This taxonomy is designed to aid developers during the requirement inspection process and to improve overall software quality. The review identified 149 papers from the software engineering, psychology and human cognition literature that provide information about the sources of requirements faults. A major result of this paper is a categorization of the sources of faults into a formal taxonomy that provides a starting point for future research into error-based approaches to improving software quality.
51|7||The effectiveness of pair programming: A meta-analysis|Several experiments on the effects of pair versus solo programming have been reported in the literature. We present a meta-analysis of these studies. The analysis shows a small significant positive overall effect of pair programming on quality, a medium significant positive overall effect on duration, and a medium significant negative overall effect on effort. However, between-study variance is significant, and there are signs of publication bias among published studies on pair programming. A more detailed examination of the evidence suggests that pair programming is faster than solo programming when programming task complexity is low and yields code solutions of higher quality when task complexity is high. The higher quality for complex tasks comes at a price of considerably greater effort, while the reduced completion time for the simpler tasks comes at a price of noticeably lower quality. We conclude that greater attention should be given to moderating factors on the effects of pair programming.
51|7||Using status messages in the distributed test architecture|If the system under test has multiple interfaces/ports and these are physically distributed then in testing we place a tester at each port. If these testers cannot directly communicate with one another and there is no global clock then we are testing in the distributed test architecture. If the distributed test architecture is used then there may be input sequences that cannot be applied in testing without introducing controllability problems. Additionally, observability problems can allow fault masking. In this paper we consider the situation in which the testers can apply a status message: an input that causes the system under test to identify its current state. We show how such a status message can be used in order to overcome controllability and observability problems.
51|7||A data flow-based structural testing technique for FBD programs|With increased use of programmable logic controllers (PLCs) in implementing critical systems, quality assurance became an important issue. Regulation requires structural testing be performed for safety-critical systems by identifying coverage criteria to be satisfied and accomplishment measured. Classical coverage criteria, based on control flow graphs, are inadequate when applied to a data flow language function block diagram (FBD) which is a PLC programming language widely used in industry. We propose three structural coverage criteria for FBD programs, analyze relationship among them, and demonstrate their effectiveness using a real-world reactor protection system. Using test cases that had been manually prepared by FBD testing professionals, our technique found many aspects of the FBD logic that were not tested sufficiently. Domain experts, having found the approach highly intuitive, found the technique effective.
51|7||A stock recommendation system exploiting rule discovery in stock databases|This paper addresses an approach that recommends investment types to stock investors by discovering useful rules from past changing patterns of stock prices in databases. First, we define a new rule model for recommending stock investment types. For a frequent pattern of stock prices, if its subsequent stock prices are matched to a condition of an investor, the model recommends a corresponding investment type for this stock. The frequent pattern is regarded as a rule head, and the subsequent part a rule body. We observed that the conditions on rule bodies are quite different depending on dispositions of investors while rule heads are independent of characteristics of investors in most cases. With this observation, we propose a new method that discovers and stores only the rule heads rather than the whole rules in a rule discovery process. This allows investors to impose various conditions on rule bodies flexibly, and also improves the performance of a rule discovery process by reducing the number of rules to be discovered. For efficient discovery and matching of rules, we propose methods for discovering frequent patterns, constructing a frequent pattern base, and its indexing. We also suggest a method that finds the rules matched to a query from a frequent pattern base, and a method that recommends an investment type by using the rules. Finally, we verify the effectiveness and the efficiency of our approach through extensive experiments with real-life stock data.
51|7||Special section on Software Engineering for Secure Systems (SESS â07)|
51|7||On the secure software development process: CLASP, SDL and Touchpoints compared|Development processes for software construction are common knowledge and mainstream practice in most development organizations. Unfortunately, these processes offer little support in order to meet security requirements. Over the years, research efforts have been invested in specific methodologies and techniques for secure software engineering, yet dedicated processes have been proposed only recently.
51|7||Security enforcement aware software development|In the domain of security policy enforcement, the concerns of application developers are almost completely ignored. As a consequence, it is hard to develop useful and reliable applications that will function properly under a variety of policies. This paper addresses this issue for application security policies specified as security automata, and enforced through run-time monitoring. Our solution consists of three elements: the definition of an abstract interface to the policy that is being enforced, a sound construct to query that policy, and a static verification algorithm that guarantees absence of security policy violations in critical blocks of code.
51|8|http://www.sciencedirect.com/science/journal/09505849/51/8|Patterns-based evaluation of open source BPM systems: The cases of jBPM, OpenWFE, and Enhydra Shark|In keeping with the proliferation of free software development initiatives and the increased interest in the business process management domain, many open source workflow and business process management systems have appeared during the last few years and are now under active development. This upsurge gives rise to two important questions: What are the capabilities of these systems? and How do they compare to each other and to their closed source counterparts? In other words: What is the state-of-the-art in the area?. To gain an insight into these questions, we have conducted an in-depth analysis of three of the major open source workflow management systems – jBPM, OpenWFE, and Enhydra Shark, the results of which are reported here. This analysis is based on the workflow patterns framework and provides a continuation of the series of evaluations performed using the same framework on closed source systems, business process modelling languages, and web-service composition standards. The results from evaluations of the three open source systems are compared with each other and also with the results from evaluations of three representative closed source systems: Staffware, WebSphere MQ, and Oracle BPEL PM. The overall conclusion is that open source systems are targeted more toward developers rather than business analysts. They generally provide less support for the patterns than closed source systems, particularly with respect to the resource perspective, i.e. the various ways in which work is distributed amongst business users and managed through to completion.
51|8||Guideline for the definition of EMF metamodels using an Entity-Relationship approach|Metamodels are a formalism for defining the abstract syntax of modeling languages. However, designing a suitable metamodel from the features intended for the language is not a trivial task. This paper presents a guideline for defining such metamodels using an Entity-Relationship approach in the Eclipse Modeling Framework. This guideline proposes to begin by determining the structural features of the language, such as types of relationships and elements with attributes. Subsequently, it offers alternative representations for these features aimed at satisfying different requirements, such as changeability or optimized model processing. Two case studies illustrate the use of the guideline and its trade-offs.
51|8||Identifying high perceived value practices of CMMI level 2: An empirical study|In this paper, we present findings from an empirical study that was aimed at identifying the relative “perceived value” of CMMI level 2 specific practices based on the perceptions and experiences of practitioners of small and medium size companies. The objective of this study is to identify the extent to which a particular CMMI practice is used in order to develop a finer-grained framework, which encompasses the notion of perceived value within specific practices.
51|8||Model-driven development of composite context-aware web applications|Context-awareness constitutes an essential aspect of services, especially when interaction with end-users is involved. In this paper a solution for the context-aware development of web applications consisting of web services is presented. The methodology proposes a model based approach and advocates in favour of a complete separation of the web application functionality from the context adaptation at all development phases (analysis, design, implementation). In essence, context adaptation takes place on top of and is transparent to the web application business functionality. Starting from UML diagrams of independent web services and respective UML context models, our approach can produce a functional composite context-aware application. At execution level this independence is maintained through an adaptation framework based on message interception.
51|8||WS-BPEL Extensions for Versioning|This article proposes specific extensions for WS-BPEL (Business Process Execution Language) to support versioning of processes and partner links. It introduces new activities and extends existing activities, including partner links, invoke, receive, import, and onmessage activities. It proposes version-related extensions to variables and introduces version handlers. The proposed extensions represent a complete solution for process-level and scope-level versioning at development, deployment, and run-time. It also provides means to version applications that consist of several BPEL processes, and to put temporal constraints on versions. The proposed approach has been tested in real-world environment. It solves major challenges in BPEL versioning.
51|8||Utilizing domain models for application design and validation|Domain analysis enables identifying families of applications and capturing their terminology in order to assist and guide system developers to design valid applications in the domain. One major way of carrying out the domain analysis is modeling. Several studies suggest using metamodeling techniques, feature-oriented approaches, or architectural-based methods for modeling domains and specifying applications in those domains. However, these methods mainly focus on representing the domain knowledge, providing insufficient guidelines (if any) for creating application models that satisfy the domain rules and constraints. In particular, validation of the application models which include application-specific knowledge is insufficiently dealt. In order to fill these lacks, we propose a general approach, called Application-based DOmain Modeling (ADOM), which enables specifying domains and applications similarly, (re)using domain knowledge in application models, and validating the application models against the relevant domain models. In this paper we present the ADOM approach, demonstrating its application to UML 2.0 class and sequence diagrams.
51|9|http://www.sciencedirect.com/science/journal/09505849/51/9|On the generation of requirements specifications from software engineering models: A systematic literature review|System and software requirements documents play a crucial role in software engineering in that they must both communicate requirements to clients in an understandable manner and define requirements in precise detail for system developers. The benefits of both lists of textual requirements (usually written in natural language) and software engineering models (usually specified in graphical form) can be brought together by combining the two approaches in the specification of system and software requirements documents. If, moreover, textual requirements are generated from models in an automatic or closely monitored form, the effort of specifying those requirements is reduced and the completeness of the specification and the management of the requirements traceability are improved. This paper presents a systematic review of the literature related to the generation of textual requirements specifications from software engineering models.
51|9||A strategy-based process for effectively determining system requirements in eCRM development|Customer relationship management (CRM) is an important concept to maintain competitiveness at e-commerce. Thus, many organizations hastily implement eCRM and fail to achieve its goal. CRM concept consists of a number of compound components on product designs, marketing attributes, and consumer behaviors. This requires different approaches from traditional ones in developing eCRM. Requirements engineering is one of the important steps in software development. Without a well-defined requirements specification, developers do not know how to proceed with requirements analysis. This research proposes a strategy-based process for requirements elicitation. This framework contains three steps: define customer strategies, identify consumer and marketing characteristics, and determine system requirements. Prior literature lacks discussing the important role of customer strategies in eCRM development. Empirical findings reveal that this strategy-based view positively improves the performance of requirements elicitation.
51|9||Empirical investigation of refactoring effect on software quality|Developers and designers always strive for quality software. Quality software tends to be robust, reliable and easy to maintain, and thus reduces the cost of software development and maintenance. Several methods have been applied to improve software quality. Refactoring is one of those methods. The goal of this paper is to validate/invalidate the claims that refactoring improves software quality. We focused this study on different external quality attributes, which are adaptability, maintainability, understandability, reusability, and testability. We found that refactoring does not necessarily improve these quality attributes.
51|9||Sizing user stories using paired comparisons|Agile estimation approaches usually start by sizing the user stories to be developed by comparing them to one another. Various techniques, with varying degrees of formality, are used to perform the comparisons – plain contrasts, triangulation, planning poker, and voting. This article proposes the use of a modified paired comparison method in which a reduced number of comparisons is selected according to an incomplete cyclic design. Using two sets of data, the authors show that the proposed method produces good estimates, even when the number of comparisons is reduced by half those required by the original formulation of the method.
51|9||A method for detecting the theft of Java programs through analysis of the control flow information|A software birthmark refers to the inherent characteristics of a program that can be used to identify the program. In this paper, a method for detecting the theft of Java programs through a static software birthmark is proposed that is based on the control flow information. The control flow information shows the structural characteristics and the possible behaviors during the execution of program. Flow paths (FP) and behaviors in Java programs are formally described here, and a set of behaviors of FPs is used as a software birthmark. The similarity is calculated by matching the pairs of similar behaviors from two birthmarks. Experiments centered on the proposed birthmark with respect to precision and recall. The performance was evaluated by analyzing the F-measure curves. The experimental results show that the proposed birthmark is a more effective measure compared to earlier approaches for detecting copied programs, even in cases where such programs are aggressively modified.
51|9||Empirical evaluation of selected best practices in implementation of software process improvement|To be successfully applied in practice, software process improvement (SPI) needs not only identifying what needs to be improved, and which factors will influence its success, but also guidelines on how to implement those improvements and meet the factors. This paper proposes an SPI implementation strategy that has been developed in lines with standard SPI models and frameworks, intended to be suitable for global software development (GSD), and exploiting ideas of incremental deliveries, short iterations with frequent reviews and close collaboration with customers. Quantitative analyses of data from case studies within large GSD show that improvement teams implementing the strategy are more likely to have better progress and achieve better effectiveness in terms of improvement deployment within development teams. The results could be useful to SPI practitioners in making their SPI initiatives more successful.
51|9||A hybrid heuristic approach to optimize rule-based software quality estimation models|Software quality is defined as the degree to which a software component or system meets specified requirements and specifications. Assessing software quality in the early stages of design and development is crucial as it helps reduce effort, time and money. However, the task is difficult since most software quality characteristics (such as maintainability, reliability and reusability) cannot be directly and objectively measured before the software product is deployed and used for a certain period of time. Nonetheless, these software quality characteristics can be predicted from other measurable software quality attributes such as complexity and inheritance. Many metrics have been proposed for this purpose. In this context, we speak of estimating software quality characteristics from measurable attributes. For this purpose, software quality estimation models have been widely used. These take different forms: statistical models, rule-based models and decision trees. However, data used to build such models is scarce in the domain of software quality. As a result, the accuracy of the built estimation models deteriorates when they are used to predict the quality of new software components. In this paper, we propose a search-based software engineering approach to improve the prediction accuracy of software quality estimation models by adapting them to new unseen software products. The method has been implemented and favorable result comparisons are reported in this work.
52|1|http://www.sciencedirect.com/science/journal/09505849/52/1|A systematic review of domain analysis tools|The domain analysis process is used to identify and document common and variable characteristics of systems in a specific domain. In order to achieve an effective result, it is necessary to collect, organize and analyze several sources of information about different applications in this domain. Consequently, this process involves distinct phases and activities and also needs to identify which artifacts, arising from these activities, have to be traceable and consistent. In this context, performing a domain analysis process without tool support increases the risks of failure, but the used tool should support the complete process and not just a part of it. This article presents a systematic review of domain analysis tools that aims at finding out how the available tools offer support to the process. As a result, the review identified that these tools are usually focused on supporting only one process and there are still gaps in the complete process support. Furthermore, the results can provide insights for new research in the domain engineering area for investigating and defining new tools, and the study also aids in the identification of companies’ needs for a domain analysis tool.
52|1||A systematic review on regression test selection techniques|Regression testing is verifying that previously functioning software remains after a change. With the goal of finding a basis for further research in a joint industry-academia research project, we conducted a systematic review of empirical evaluations of regression test selection techniques. We identified 27 papers reporting 36 empirical studies, 21 experiments and 15 case studies. In total 28 techniques for regression test selection are evaluated. We present a qualitative analysis of the findings, an overview of techniques for regression test selection and related empirical evidence. No technique was found clearly superior since the results depend on many varying factors. We identified a need for empirical studies where concepts are evaluated rather than small variations in technical implementations.
52|1||Characterizing software architecture changes: A systematic review|With today’s ever increasing demands on software, software developers must produce software that can be changed without the risk of degrading the software architecture. One way to address software changes is to characterize their causes and effects. A software change characterization mechanism allows developers to characterize the effects of a change using different criteria, e.g. the cause of the change, the type of change that needs to be made, and the part of the system where the change must take place. This information then can be used to illustrate the potential impact of the change. This paper presents a systematic literature review of software architecture change characteristics. The results of this systematic review were used to create the Software Architecture Change Characterization Scheme (SACCS). This report addresses key areas involved in making changes to software architecture. SACCS’s purpose is to identify the characteristics of a software change that will have an impact on the high-level software architecture.
52|1||Scalability issues with using FSMWeb to test web applications|Web applications are fast becoming more widespread, larger, more interactive, and more essential to the international use of computers. It is well understood that web applications must be highly dependable, and as a field we are just now beginning to understand how to model and test Web applications. One straightforward technique is to model Web applications as finite state machines. However, large numbers of input fields, input choices and the ability to enter values in any order combine to create a state space explosion problem. This paper evaluates a solution that uses constraints on the inputs to reduce the number of transitions, thus compressing the FSM. The paper presents an analysis of the potential savings of the compression technique and reports actual savings from two case studies.
52|1||Framework for application management with dynamic aspects J-EARS case study|The cost of computer system maintenance rises together with the increasing complexity of such systems. The use of an autonomic system architecture saves money by delegating some forms of maintenance to the systems themselves. The aim of this paper is to describe the results of creating a tool which introduces elements of adaptivity to Java applications using dynamic aspects. The impact of introducing aspects on the performance of various Application Servers is also discussed. Finally, benefits and problems arising from the use of the tool are presented, basing on sample use cases.
52|1||Business-oriented software process improvement based on CMMI using QFD|Software Process Improvement (SPI) has become the key to the survival of many software development organizations. As a follow-up of a previous paper on SPI for the CMM using Quality Function Deployment (QFD), a new SPI framework integrating QFD with the CMMI is developed. Similar to the earlier SPI framework for the CMM, the proposed SPI framework based on the CMMI using QFD aims to achieve three objectives: (1) to map process requirements, including business requirements, to the CMMI with the help of QFD; (2) to develop a method based on QFD for the integration and prioritization of requirements from multiple perspectives (groups); and (3) to be able to prioritize software process improvement actions based on process requirements.
52|1||Software development effort prediction: A study on the factors impacting the accuracy of fuzzy logic systems|Reliable effort prediction remains an ongoing challenge to software engineers. Traditional approaches to effort prediction such as the use of models derived from historical data, or the use of expert opinion are plagued with issues pertaining to their effectiveness and robustness. These issues are more pronounced when the effort prediction is used during the early phases of the software development lifecycle. Recent works have demonstrated promising results obtained with the use of fuzzy logic. Fuzzy logic based effort prediction systems can deal better with imprecision, which characterizes the early phases of most software development projects, for example requirements development, whose effort predictors along with their relationships to effort are characterized as being even more imprecise and uncertain than those of later development phases, for example design. Fuzzy logic based prediction systems could produce further better estimates provided that various parameters and factors pertaining to fuzzy logic are carefully set. In this paper, we present an empirical study, which shows that the prediction accuracy of a fuzzy logic based effort prediction system is highly dependent on the system architecture, the corresponding parameters, and the training algorithms.
52|1||Change profiles of a reused class framework vs. two of its applications|
52|10|http://www.sciencedirect.com/science/journal/09505849/52/10|Investigating ERP systems procurement practice: Hong Kong and Australian experiences|Integration of information systems is now commonly recognized to be a powerful strategic weapon that sharpens the competitive edge of a firm in today’s highly volatile business environment. Such integration can be achieved by replacing the disconnected and incompatible legacy applications by enterprise resource planning (ERP) systems. Along with the remarkable growth of the ERP market, we have seen a number of failure cases of ERP adoption. Such failure cases indicate that not all firms know how to adopt an ERP solution effectively.
52|10||WSDL and BPEL extensions for Event Driven Architecture|Service Oriented Architecture (SOA) and Event Driven Architecture (EDA) are two acknowledged architectures for the development of business applications and information systems, which have evolved separately over the years.
52|10||Assessment methodology for software process improvement in small organizations|Diagnosing processes in a small company requires process assessment practices which give qualitative and quantitative results; these should offer an overall view of the process capability. The purpose is to obtain relevant information about the running of processes, for use in their control and improvement. However, small organizations have some problems in running process assessment, due to their specific characteristics and limitations.
52|10||Coproduction in successful software development projects|Coproduction of new products has been deemed successful in organizational partnerships by adding to the quality and scope of the product. Techniques that involve users during the development of software tend to mimic this environment, but differ in the type of product and internal client roles. The question is thus, whether coproduction improves the outcomes of a software development project as it has in other disciplines.
52|10||A method for forecasting defect backlog in large streamline software development projects and its industrial evaluation|Predicting a number of defects to be resolved in large software projects (defect backlog) usually requires complex statistical methods and thus is hard to use on a daily basis by practitioners in industry. Making predictions in simpler and more robust way is often required by practitioners in software engineering industry.
52|10||Improving device-aware Web services and their mobile clients through an aspect-oriented, model-driven approach|Mobile devices have become an essential element in our daily lives, even for connecting to the Internet. Consequently, Web services have become extremely important when offering services through the Internet. However, current Web services are very inflexible as regards their invocation from different types of device, especially if we consider the need for them to be adaptable when being invoked from mobile devices.
52|10||Security requirements engineering framework for software product lines|The correct analysis and understanding of security requirements are important because they assist in the discovery of any security or requirement defects or mistakes during the early stages of development. Security requirements engineering is therefore both a central task and a critical success factor in product line development owing to the complexity and extensive nature of software product lines (SPL). However, most of the current SPL practices in requirements engineering do not adequately address security requirements engineering.
52|10||Introducing knowledge redundancy practice in software development: Experiences with job rotation in support work|Job rotation is a widely known approach to increase knowledge redundancy but empirical evidence regarding introduction and adoption in software development is scant. A lack of knowledge redundancy is a limiting factor for collaboration, flexibility, and coordination within teams and within the organization.
52|11|http://www.sciencedirect.com/science/journal/09505849/52/11|Adoption of open source software in software-intensive organizations â A systematic literature review|Open source software (OSS) is changing the way organizations develop, acquire, use, and commercialize software.
52|11||GA-based method for feature selection and parameters optimization for machine learning regression applied to software effort estimation|In software industry, project managers usually rely on their previous experience to estimate the number men/hours required for each software project. The accuracy of such estimates is a key factor for the efficient application of human resources. Machine learning techniques such as radial basis function (RBF) neural networks, multi-layer perceptron (MLP) neural networks, support vector regression (SVR), bagging predictors and regression-based trees have recently been applied for estimating software development effort. Some works have demonstrated that the level of accuracy in software effort estimates strongly depends on the values of the parameters of these methods. In addition, it has been shown that the selection of the input features may also have an important influence on estimation accuracy.
52|11||Generating a catalog of unanticipated schemas in class hierarchies using Formal Concept Analysis|
52|11||A family of experiments to validate measures for UML activity diagrams of ETL processes in data warehouses|In data warehousing, Extract, Transform, and Load (ETL) processes are in charge of extracting the data from the data sources that will be contained in the data warehouse. Their design and maintenance is thus a cornerstone in any data warehouse development project. Due to their relevance, the quality of these processes should be formally assessed early in the development in order to avoid populating the data warehouse with incorrect data. To this end, this paper presents a set of measures with which to evaluate the structural complexity of ETL process models at the conceptual level. This study is, moreover, accompanied by the application of formal frameworks and a family of experiments whose aim is to theoretical and empirically validate the proposed measures, respectively. Our experiments show that the use of these measures can aid designers to predict the effort associated with the maintenance tasks of ETL processes and to make ETL process models more usable. Our work is based on Unified Modeling Language (UML) activity diagrams for modeling ETL processes, and on the Framework for the Modeling and Evaluation of Software Processes (FMESP) framework for the definition and validation of the measures.
52|11||Software Process Improvement barriers: A cross-cultural comparison|Software Process Improvement initiatives have been around for many years with the growing globalisation of software development is making them increasingly important.
52|11||Representing the behaviour of software projects using multi-dimensional timelines|
52|11||Special Section on Best Papers PROMISE 2009|
52|11||A Bayesian network approach to assess and predict software quality using activity-based quality models|Software quality is a complex concept. Therefore, assessing and predicting it is still challenging in practice as well as in research. Activity-based quality models break down this complex concept into concrete definitions, more precisely facts about the system, process, and environment as well as their impact on activities performed on and with the system. However, these models lack an operationalisation that would allow them to be used in assessment and prediction of quality. Bayesian networks have been shown to be a viable means for this task incorporating variables with uncertainty.
52|11||Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry|Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
52|12|http://www.sciencedirect.com/science/journal/09505849/52/12|Intelligent distributed control systems|The paper2 deals with distributed reconfigurable embedded control systems following the component-based International Industrial Standard IEC61499 in which a Function Block (abbreviated by FB) is an event-triggered software component owning data and a control application is a distributed network of Function Blocks. Nowadays, limited related works have been proposed to address particular cases of reconfigurations without considering distributed architectures. Our first problem is to be able to handle all possible forms of reconfigurations that can be applied at run-time to distributed Function Blocks. In this case, a coordination between devices of the execution environment should be applied to guarantee safe and coherent distributed reconfigurations. A second problem is to find the sufficient solutions for the correct implementation of this reconfigurable distributed architecture.
52|12||WISDOM: A website design method based on reusing design and software solutions|Websites are increasingly important for advertising and announcing information and they have become virtual business operations support tools. Thus, website designers have to take an increasing number of criteria (cost, delay, quality, security, maintenance) into account during design process to satisfy the needs.
52|12||FSM-based conformance testing methods: A survey annotated with experimental evaluation|The development of test cases is an important issue for testing software, communication protocols and other reactive systems. A number of methods are known for the development of a test suite based on a formal specification given in the form of a finite state machine. In this paper, we overview and experiment with these methods to assess their complexity, applicability, completeness, fault detection capability, length and derivation time of their test suites. The experiments are conducted on randomly generated specifications and on two realistic protocols called the Simple Connection Protocol and the ITU-T V.76 Recommendation.
52|12||A novel composite model approach to improve software quality prediction|How can quality of software systems be predicted before deployment? In attempting to answer this question, prediction models are advocated in several studies. The performance of such models drops dramatically, with very low accuracy, when they are used in new software development environments or in new circumstances.
52|12||Package Fingerprints: A visual summary of package interface usage|Object-oriented languages such as Java, Smalltalk, and C++ structure their programs using packages. Maintainers of large systems need to understand how packages relate to each other, but this task is complex because packages often have multiple clients and play different roles (class container, code ownership, etc.). Several approaches have been proposed, among which the use of cohesion and coupling metrics. Such metrics help identify candidate packages for restructuring; however, they do not help maintainers actually understand the structure and interrelationships between packages.
52|12||Quantification of interacting runtime qualities in software architectures: Insights from transaction processing in clientâserver architectures|Architecture is fundamental for fulfilling requirements related to the non-functional behavior of a software system such as the quality requirement that response time does not degrade to a point where it is noticeable. Approaches like the Architecture Tradeoff Analysis Method (ATAM) combine qualitative analysis heuristics (e.g. scenarios) for one or more quality metrics with quantitative analyses. A quantitative analysis evaluates a single metric such as response time. However, since quality metrics interact with each other, a change in the architecture can affect unpredictably multiple quality metrics.
52|12||An object-oriented high-level design-based class cohesion metric|Class cohesion is an important object-oriented software quality attribute. Assessing class cohesion during the object-oriented design phase is one important way to obtain more comprehensible and maintainable software. In practice, assessing and controlling cohesion in large systems implies measuring it automatically. One issue with the few existing cohesion metrics targeted at the high-level design phase is that they are not based on realistic assumptions and do not fulfill expected mathematical properties.
52|2|http://www.sciencedirect.com/science/journal/09505849/52/2|Seven process modeling guidelines (7PMG)|Business process modeling is heavily applied in practice, but important quality issues have not been addressed thoroughly by research. A notorious problem is the low level of modeling competence that many casual modelers in process documentation projects have. Existing approaches towards model quality might be of benefit, but they suffer from at least one of the following problems. On the one hand, frameworks like SEQUAL and the Guidelines of Modeling are too abstract to be applicable for novices and non-experts in practice. On the other hand, there are collections of pragmatic hints that lack a sound research foundation. In this paper, we analyze existing research on relationships between model structure on the one hand and error probability and understanding on the other hand. As a synthesis we propose a set of seven process modeling guidelines (7PMG). Each of these guidelines builds on strong empirical insights, yet they are formulated to be intuitive to practitioners. Furthermore, we analyze how the guidelines are prioritized by industry experts. In this regard, the seven guidelines have the potential to serve as an important tool of knowledge transfer from academia into modeling practice.
52|2||A pattern-based outlier detection method identifying abnormal attributes in software project data|Despite the importance of the quality of software project data, problematic data inevitably occurs during data collection. These data are the outliers with abnormal values on certain attributes, which we call the abnormal attributes of outliers. Manually detecting outliers and their abnormal attributes is laborious and time consuming. Although few existing approaches identify outliers and their abnormal attributes, these approaches are not effective in (1) identifying the abnormal attributes when the outlier has abnormal values on more than the specific number of its attributes or (2) discovering accurate rules to detect outliers and their abnormal attributes. In this paper, we propose a pattern-based outlier detection method that identifies abnormal attributes in software project data: after discovering the reliable frequent patterns that reflect the typical characteristics of the software project data, outliers and their abnormal attributes are detected by matching the software project data with those patterns. Empirical studies were performed on three industrial data sets and 48 artificial data sets with injected outliers. The results demonstrate that our approach outperforms five other approaches by an average of 35.27% and 107.5% in detecting the outliers and abnormal attributes, respectively, on the industrial data sets, and an average of 35.44% and 46.57%, respectively on the artificial data sets.
52|2||Identification of design motifs with pattern matching algorithms|Design patterns are important in software maintenance because they help in understanding and re-engineering systems. They propose design motifs, solutions to recurring design problems. The identification of occurrences of design motifs in large systems consists of identifying classes whose structure and organization match exactly or approximately the structure and organization of classes as suggested by the motif. We adapt two classical approximate string matching algorithms based on automata simulation and bit-vector processing to efficiently identify exact and approximate occurrences of motifs. We then carry out two case studies to show the performance, precision, and recall of our algorithms. In the first case study, we assess the performance of our algorithms on seven medium-to-large systems. In the second case study, we compare our approach with three existing approaches (an explanation-based constraint approach, a metric-enhanced explanation-based constraint approach, and a similarity scoring approach) by applying the algorithms on three small-to-medium size systems, JHotDraw, Juzzle, and QuickUML. Our studies show that approximate string matching based on bit-vector processing provides efficient algorithms to identify design motifs.
52|2||The impact of Test-First programming on branch coverage and mutation score indicator of unit tests: An experiment|Test-First programming is regarded as one of the software development practices that can make unit tests to be more rigorous, thorough and effective in fault detection. Code coverage measures can be useful as indicators of the thoroughness of unit test suites, while mutation testing turned out to be effective at finding faults.
52|2||An embeddable mobile agent platform supporting runtime code mobility, interaction and coordination of mobile agents and host systems|Agent technology is emerging as an important concept for the development of distributed complex systems. A number of mobile agent systems have been developed in the last decade. However, most of them were developed to support only Java mobile agents. In order to provide distributed applications with code mobility, this article presents a library, the Mobile-C library, that allows a mobile agent platform, Mobile-C, to be embeddable in an application to support mobile C/C++ codes carried by mobile agents. Mobile-C uses a C/C++ interpreter as its Agent Execution Engine (AEE). Through the Mobile-C library, Mobile-C can be embedded into an application to support mobile C/C++ codes carried by mobile agents. Using mobile C/C++ codes, it is easy to interface a variety of low-level hardware devices and legacy systems. Through the Mobile-C library, Mobile-C can run on heterogeneous platforms with various operating systems. The Mobile-C library has a small footprint to meet the stringent memory capacity for applications in mechatronic and embedded systems. The Mobile-C library contains different categories of Application Programming Interfaces (APIs) in both binary and agent spaces to facilitate the design of mobile agent based applications. In addition, a rich set of existing APIs for the C/C++ interpreter employed as the AEE allows an application to have complete information and control over the mobile C/C++ codes residing in Mobile-C. With the synchronization mechanism provided by the Mobile-C library for both binary and agent spaces, simultaneous processes across both spaces can be coordinated to get correct runtime order and avoid unexpected race condition. The study of performance comparisons indicates that Mobile-C is about two times faster than JADE in agent migration. The application of the Mobile-C library is illustrated by dynamic runtime control of a mobile robot’s behavior using mobile agents.
52|2||An effort prediction framework for software defect correction|This article tackles the problem of predicting effort (in person–hours) required to fix a software defect posted on an Issue Tracking System. The proposed method is inspired by the Nearest Neighbour Approach presented by the pioneering work of Weiss et al. (2007)  [1]. We propose four enhancements to Weiss et al. (2007)  [1]: Data Enrichment, Majority Voting, Adaptive Threshold and Binary Clustering. Data Enrichment infuses additional issue information into the similarity-scoring procedure, aiming to increase the accuracy of similarity scores. Majority Voting exploits the fact that many of the similar historical issues have repeating effort values, which are close to the actual. Adaptive Threshold automatically adjusts the similarity threshold to ensure that we obtain only the most similar matches. We use Binary Clustering if the similarity scores are very low, which might result in misleading predictions. This uses common properties of issues to form clusters (independent of the similarity scores) which are then used to produce the predictions. Numerical results are presented showing a noticeable improvement over the method proposed in Weiss et al. (2007)  [1].
52|2||Filtering false alarms of buffer overflow analysis using SMT solvers|Buffer overflow detection using static analysis can provide a powerful tool for software programmers to find difficult bugs in C programs. Sound static analysis based on abstract interpretation, however, often suffers from false alarm problem. Although more precise abstraction can reduce the number of the false alarms in general, the cost to perform such analysis is often too high to be practical for large software. On the other hand, less precise abstraction is likely to be scalable in exchange for the increased false alarms. In order to attain both precision and scalability, we present a method that first applies less precise abstraction to find buffer overflow alarms fast, and selectively applies a more precise analysis only to the limited areas of code around the potential false alarms. In an attempt to develop the precise analysis of alarm filtering for large C programs, we perform a symbolic execution over the potential alarms found in the previous analysis, which is based on the abstract interpretation. Taking advantage of a state-of-art SMT solver, our precise analysis efficiently filters out a substantial number of false alarms. Our experiment with the test cases from three open source programs shows that our filtering method can reduce about 68% of false alarms on average.
52|2||Intelligent security and access control framework for service-oriented architecture|One of the most significant difficulties with developing Service-Oriented Architecture (SOA) involves meeting its security challenges, since the responsibilities of SOA security are based on both the service providers and the consumers. In recent years, many solutions to these challenges have been implemented, such as the Web Services Security Standards, including WS-Security and WS-Policy. However, those standards are insufficient for the new generation of Web technologies, including Web 2.0 applications. In this research, we propose an intelligent SOA security framework by introducing its two most promising services: the Authentication and Security Service (NSS), and the Authorization Service (AS). The suggested autonomic and reusable services are constructed as an extension of WS-∗ security standards, with the addition of intelligent mining techniques, in order to improve performance and effectiveness. In this research, we apply three different mining techniques: the Association Rules, which helps to predict attacks, the Online Analytical Processing (OLAP) Cube, for authorization, and clustering mining algorithms, which facilitate access control rights representation and automation. Furthermore, a case study is explored to depict the behavior of the proposed services inside an SOA business environment. We believe that this work is a significant step towards achieving dynamic SOA security that automatically controls the access to new versions of Web applications, including analyzing and dropping suspicious SOAP messages and automatically managing authorization roles.
52|3|http://www.sciencedirect.com/science/journal/09505849/52/3|A systematic review on strategic release planning models|Strategic release planning (sometimes referred to as road-mapping) is an important phase of the requirements engineering process performed at product level. It is concerned with selection and assignment of requirements in sequences of releases such that important technical and resource constraints are fulfilled.
52|3||Model-driven development for early aspects|Currently, non-functional requirements (NFRs) consume a considerable part of the software development effort. The good news is that most of them appear time and again during system development and, luckily, their solutions can be often described as a pattern independently from any specific application or domain. A proof of this are the current application servers and middleware platforms that can provide configurable prebuilt services for managing some of these crosscutting concerns, or aspects. Nevertheless, these reusable pattern solutions presents two shortcomings, among others: (1) they need to be applied manually; and (2) most of these pattern solutions do not use aspect-orientation, and, since NFRs are often crosscutting concerns, this leads to scattered and tangled representations of these concerns. Our approach aims to overcome these limitations by: (1) using model-driven techniques to reduce the development effort associated to systematically apply reusable solutions for satisfying NFRs; and (2) using aspect-orientation to improve the modularization of these crosscutting concerns. Regarding the first contribution, since the portion of a system related to NFRs is usually significant, the reduction on the development effort associated to these NFRs is also significant. Regarding the second contribution, the use aspect-orientation improves maintenance and evolution of the non-functional requirements that are managed as aspects. An additional contribution of our work is to define a mapping and transition from aspectual requirements to aspect-oriented software architectures, which, in turn, contributes to improve the general issue of systematically relating requirements to architecture. Our approach is illustrated by applying it to a Toll Gate case study.
52|3||Automated verification of security pattern compositions|Software security becomes a critically important issue for software development when more and more malicious attacks explore the security holes in software systems. To avoid security problems, a large software system design may reuse good security solutions by applying security patterns. Security patterns document expert solutions to common security problems and capture best practices on secure software design and development. Although each security pattern describes a good design guideline, the compositions of these security patterns may be inconsistent and encounter problems and flaws. Therefore, the compositions of security patterns may be even insecure. In this paper, we present an approach to automated verification of the compositions of security patterns by model checking. We formally define the behavioral aspect of security patterns in CCS through their sequence diagrams. We also prove the faithfulness of the transformation from a sequence diagram to its CCS representation. In this way, the properties of the security patterns can be checked by a model checker when they are composed. Composition errors and problems can be discovered early in the design stage. We also use two case studies to illustrate our approach and show its capability to detect composition errors.
52|3||Analysis of virtual communities supporting OSS projects using social network analysis|This paper analyses the behaviour of virtual communities for Open Source Software (OSS) projects. The development of OSS projects relies on virtual communities, which are built on relationships among members, being their final objective sharing knowledge and improving the underlying project. This study addresses the interactive collaboration in these kinds of communities applying social network analysis (SNA). In particular, SNA techniques will be used to identify those members playing a middle-man role among other community members. Results will illustrate the importance of this role to achieve successful virtual communities.
52|3||A pattern-based approach to protocol mediation for web services composition|With the increasing popularity of Service Oriented Architecture (SOA), service composition is gaining momentum as the potential silver bullet for application integration. However, services are not always perfectly compatible and therefore cannot be directly composed. Service mediation, roughly classified into signature and protocol ones, thus becomes one key working area in SOA.
52|3||Requirements for product derivation support: Results from a systematic literature review and an expert survey|An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support.
52|3||An experimental study on the conversion between IFPUG and COSMIC functional size measurement units|The adoption of functional size measurement (FSM) methods in software organizations is growing. In particular, special attention is being paid to the COSMIC method, because of its novelties against 1st generation FSM methods such as IFPUG FPA. One of the main problems facing organizations wanting to use COSMIC is how to properly convert the software functional size of the projects in their portfolio measured by the previously adopted FSM method to the size measured by the new method.
52|4|http://www.sciencedirect.com/science/journal/09505849/52/4|Experimental program analysis|Program analysis techniques are used by software engineers to deduce and infer characteristics of software systems. Recent research has suggested that certain program analysis techniques can be formulated as formal experiments. This article reports the results of research exploring this suggestion. Building on principles and methodologies underlying the use of experimentation in other fields, we provide descriptive and operational definitions of experimental program analysis, illustrate them by example, and describe several differences between experimental program analysis and experimentation in other fields. We also explore the applicability of experimental program analysis to three software engineering problems: program transformation, program debugging, and program understanding. Our findings indicate that experimental program analysis techniques can provide new and potentially improved solutions to these problems, and suggest that experimental program analysis offers a promising new direction for program analysis research.
52|4||AAOP-based dynamically reconfigurable monitoring system|A key aspect of resource management is efficient and effective deployment of available resources whenever needed. The issue typically covers two areas: monitoring of resources used by software systems and managing the consumption of resources. A key aspect of each monitoring system is its reconfigurability – the ability of a system to limit the number of resources monitored at a given time to those that are really necessary at any particular moment. The authors of this article propose a fully dynamic and reconfigurable monitoring system based on the concept of Adaptable Aspect-Oriented Programming (AAOP) in which a set of AOP aspects is used to run an application in a manner specified by the adaptability strategy. The model can be used to implement systems that are able to monitor an application and its execution environment and perform actions such as changing the current set of resource management constraints applied to an application if the application/environment conditions change. Any aspect that implements a predefined interface may be used by the AAOP-based monitoring system as a source of information. The system utilizes the concept of dynamic AOP, meaning that the aspects (which are sources of information) may be dynamically enabled/disabled.
52|4||A tool for IT process construction|The field of IT processes lacks a scientifically-based tool that constructs organisation-specific IT processes according to the organisation’s socio-technical characteristics.
52|4||Current state and potential of variability management practices in software-intensive SMEs: Results from a regional industrial survey|More and more, small and medium-sized enterprises (SMEs) are using software to augment the functionality of their products and offerings. Variability management of software is becoming an interesting topic for SMEs with expanding portfolios and increasingly complex product structures. While the use of software product lines to resolve high variability is well known in larger organizations, there is less known about the practices in SMEs.
52|4||A framework for the definition of metamodels for Computer-Aided Software Engineering tools|Computer-Aided Software Engineering (CASE) tools support modeling-related activities in development projects. Given the variety of tools and functionalities, it is quite common to work with several tools in the same project. However, data cannot usually be exchanged between these tools without loss of information. Recent approaches address this model interchange problem using metamodels to characterize the involved information and transformations to export/import it. Nevertheless, most of these solutions focus on the abstract syntax of models. They fail to consider aspects such as the presentation of models or tool-specific information, which are either disregarded or represented in ad-hoc ways that make difficult their processing. In order to overcome these limitations, this paper introduces a framework to define metamodels of CASE tools and a process to carry out the model interchange using them. The proposed metamodels have a modular organization with several internal metamodels. Each of them is aimed at describing some specific information about content, structure and presentation for both models and tools. The paper illustrates this approach with a case study used for comparison with existing works for this problem.
52|4||Identification of non-functional requirements in textual specifications: A semi-supervised learning approach|Early detection of non-functional requirements (NFRs) is crucial in the evaluation of architectural alternatives starting from initial design decisions. The application of supervised text categorization strategies for requirements expressed in natural language has been proposed in several works as a method to help analysts in the detection and classification of NFRs concerning different aspects of software. However, a significant number of pre-categorized requirements are needed to train supervised text classifiers, which implies that analysts have to manually assign categories to numerous requirements before being able of accurately classifying the remaining ones.
52|4||Studying the impact of uncertainty in operational release planning â An integrated method and its initial evaluation|Uncertainty is an unavoidable issue in software engineering and an important area of investigation. This paper studies the impact of uncertainty on total duration (i.e., make-span) for implementing all features in operational release planning.
52|5|http://www.sciencedirect.com/science/journal/09505849/52/5|Does the technology acceptance model predict actual use? A systematic literature review|The technology acceptance model (TAM) was proposed in 1989 as a means of predicting technology usage. However, it is usually validated by using a measure of behavioural intention to use (BI) rather than actual usage.
52|5||A teamwork model for understanding an agile team: A case study of a Scrum project|Software development depends significantly on team performance, as does any process that involves human interaction.
52|5||HCI and business practices in a collaborative method for augmented reality systems|
52|5||Identification of more risks can lead to increased over-optimism of and over-confidence in software development effort estimates|
52|5||Analysis of Secure Mobile Grid Systems: A systematic approach|Developing software through systematic processes is becoming more and more important due to the growing complexity of software development. It is important that the development process used integrates security aspects from the first stages at the same level as other functional and non-functional requirements. Systems which are based on Grid Computing are a kind of systems that have clear differentiating features in which security is a highly important aspect. The Mobile Grid, which is relevant to both Grid and Mobile Computing, is a full inheritor of the Grid with the additional feature that it supports mobile users and resources. A development methodology for Secure Mobile Grid Systems is proposed in which the security aspects are considered from the first stages of the life-cycle and in which the mobile Grid technological environment is always present in each activity. This paper presents the analysis activity, in which the requirements (focusing on the grid, mobile and security requirements) of the system are specified and which is driven by reusable use cases through which the requirements and needs of these systems can be defined. These use cases have been defined through a UML-extension for security use cases and Grid use cases which capture the behaviour of this kind of systems. The analysis activity has been applied to a real case.
52|5||TAIC-PART 2008 â Testing: Academic & Industrial conference â Practice and research techniques, special section editorial|
52|5||Exploring the relationship of a fileâs history and its fault-proneness: An empirical method and its application to open source programs|The knowledge about particular characteristics of software that are indicators for defects is very valuable for testers because it helps them to focus the testing effort and to allocate their limited resources appropriately.
52|5||Iterative execution-feedback model-directed GUI testing|Current fully automatic model-based test-case generation techniques for GUIs employ a static model. Therefore they are unable to leverage certain state-based relationships between GUI events (e.g., one enables the other, one alters the other’s execution) that are revealed at run-time and non-trivial to infer statically. We present ALT – a new technique to generate GUI test cases in batches. Because of its “alternating” nature, ALT enhances the next batch by using GUI run-time information from the current batch. An empirical study on four fielded GUI-based applications demonstrated that ALT was able to detect new 4- and 5-way GUI interaction faults; in contrast, previous techniques, due to their requirement of too many test cases, were unable to even test 4- and 5-way GUI interactions.
52|6|http://www.sciencedirect.com/science/journal/09505849/52/6|Knowledge based quality-driven architecture design and evaluation|Modelling and evaluating quality properties of software is of high importance, especially when our every day life depends on the quality of services produced by systems and devices embedded into our surroundings. This paper contributes to the body of research in quality and model driven software engineering. It does so by introducing; (1) a quality aware software architecting approach and (2) a supporting tool chain. The novel approach with supporting tools enables the systematic development of high quality software by merging benefits of knowledge modelling and management, and model driven architecture design enhanced with domain-specific quality attributes. The whole design flow of software engineering is semi-automatic; specifying quality requirements, transforming quality requirements to architecture design, representing quality properties in architectural models, predicting quality fulfilment from architectural models, and finally, measuring quality aspects from implemented source code. The semi-automatic design flow is exemplified by the ongoing development of a secure middleware for peer-to-peer embedded systems.
52|6||Determinants of software quality: A survey of information systems project managers|Software quality is important for the success of any information systems (IS). In this research, we find the determinants of software quality. We used five attributes for software quality: system reliability, maintainability, ease of use, usefulness, and relevance. By surveying 112 IS project managers, we collected data about their perceptions on the software quality attributes and their determinants. We arrived at six factors through exploratory factor analysis. We determined the individual factors that impacted the software quality attributes; for example, reliability is associated with responsiveness of IS department; ease of use is influenced by the capabilities of users and attitude of management; and usefulness is impacted by capabilities of IS department and responsiveness of IS department. We show that organizational factors are more important than technical factors in impacting software quality in IS projects. We provide implications of our research to practice and to future research.
52|6||Links between the personalities, views and attitudes of software engineers|Successful software development and management depends not only on the technologies, methods and processes employed but also on the judgments and decisions of the humans involved. These, in turn, are affected by the basic views and attitudes of the individual engineers.
52|6||Experience and challenges with UML-driven performance engineering of a Distributed Real-Time System|Performance-related failures of Distributed and Real-Time Software Systems (DRTS’s) can be very costly, e.g., explosion of a nuclear reactor. We reported in a previous work a stress testing methodology to detect performance-related Real-Time (RT) faults in DRTS’s based on the design UML model of a System Under Test (SUT). The stress methodology aimed at increasing the chances of RT failures (violations in RT constraints).
52|6||Improving component selection and monitoring with controlled experimentation and automated measurements|Context: A number of approaches have been proposed for the general problem of software component evaluation and selection. Most approaches come from the field of Component-Based Software Development (CBSD), tackle the problem of Commercial-off-the-shelf component selection and use goal-oriented requirements modelling and multi-criteria decision making techniques. Evaluation of the suitability of components is carried out largely manually and partly relies on subjective judgement. However, in dynamic, distributed environments with high demands for transparent selection processes leading to trustworthy, auditable decisions, subjective judgements and vendor claims are not considered sufficient. Furthermore, continuous monitoring and re-evaluation of components after integration is sometimes needed.
52|6||Using the DEMO methodology for modeling open source software development processes|
52|6||User commitment and collaboration: Motivational antecedents and project performance|Prior research into the success of information system development projects views user commitment and collaboration as unrelated concepts in models that take either a perspective of mediators or one of processes. This perspective is limiting in that mediators and processes may interact during the course of an information system development project.
52|6||Empirical validation of the Classic Change Curve on a software technology change project|New processes, tools, and practices are being introduced into software companies at an increasing rate. With each new advance in technology, software managers need to consider not only whether it is time to change the technologies currently used, but also whether an evolutionary change is sufficient or a revolutionary change is required.
52|6||Test case generation for the task tree type of architecture|Emerging multicores and clusters of multicores that may operate in parallel have set a new challenge – development of massively parallel software composed of thousands of loosely coupled or even completely independent threads/processes, such as MapReduce and Java 3.0 workers, or Erlang processes, respectively. Testing and verification is a critical phase in the development of such software products.
52|7|http://www.sciencedirect.com/science/journal/09505849/52/7|A longitudinal study of development and maintenance|
52|7||Incremental method evolution in global software product management: A retrospective case study|Company growth in a global setting causes challenges in the adaptation and maintenance of an organization’s methods. In this paper, we will analyze incremental method evolution in software product management in a global environment. We validate a method increment approach, based on method engineering principles, by applying it to a retrospective case study conducted at a large ERP vendor. The results show that the method increment types cover all increments that were found in the case study. Also, we identified the following lessons learned for company growth in a global software product management context: method increment drivers, such as the change of business strategy, vary during evolution; a shared infrastructure is critical for rollout; small increments facilitate gradual process improvement; and global involvement is critical. We then claim that method increments enable software companies to accommodate evolutionary adaptations of development process in agreement with the overall company expansion.
52|7||A DSL toolkit for deferring architectural decisions in DSL-based software design|
52|7||Best practice fusion of CMMI-DEV v1.2 (PP, PMC, SAM) and PMBOK 2008|The establishment of effective and efficient project management practices still remains a challenge to software organizations. In striving to address these needs, “best practice” models, such as, CMMI or PMBOK, are being developed to assist organizations interested in improving project management. And, although, those models share overlapping content, there are still differences and, therefore, each of the models offers different advantages.
52|7||Simulating evolution in model-based product line engineering|Numerous approaches are available for modeling product lines and their variability. However, the long-term impacts of model-based development on maintenance effort and model complexity can hardly be investigated due to a lack of empirical data. Conducting empirical research in product line engineering is difficult as companies are typically reluctant to provide access to data from their product lines. Also, many benefits of product lines can be measured only in longitudinal studies, which are difficult to perform in most environments.
52|7||Case-based analysis in user requirements modelling for knowledge construction|Learning can be regarded as knowledge construction in which prior knowledge and experience serve as basis for the learners to expand their knowledge base. Such a process of knowledge construction has to take place continuously in order to enhance the learners’ competence in a competitive working environment. As the information consumers, the individual users demand personalised information provision which meets their own specific purposes, goals, and expectations.
52|8|http://www.sciencedirect.com/science/journal/09505849/52/8|Process models in the practice of distributed software development: A systematic review of the literature|Distributed Software Development (DSD) has recently become an active research area. Although considerable research effort has been made in this area, as yet, no agreement has been reached as to an appropriate process model for DSD.
52|8||Systematic literature reviews in software engineering â A tertiary study|In a previous study, we reported on a systematic literature review (SLR), based on a manual search of 13 journals and conferences undertaken in the period 1st January 2004 to 30th June 2007.
52|8||Requirements engineering for software product lines: A systematic literature review|Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed.
52|8||A language-independent and formal approach to pattern-based modelling with support for composition and analysis|
52|8||Traceability-centric model-driven object-oriented engineering|Object-oriented (OO) development method is a popular paradigm in developing target systems. However, the current practices of OO analysis and design (OOAD) and implementation largely rely on human developers’ experience and expertise, making it possible less efficient and more error-prone. Hence, there is room for improving the development efficiency while preserving high quality of programs.
52|9|http://www.sciencedirect.com/science/journal/09505849/52/9|A systematic review of comparative evidence of aspect-oriented programming|Aspect-oriented programming (AOP) promises to improve many facets of software quality by providing better modularization and separation of concerns, which may have system wide affect. There have been numerous claims in favor and against AOP compared with traditional programming languages such as Objective Oriented and Structured Programming Languages. However, there has been no attempt to systematically review and report the available evidence in the literature to support the claims made in favor or against AOP compared with non-AOP approaches.
52|9||Software engineering research for computer games: A systematic review|Currently, computer game development is one of the fastest growing industries in the worldwide economy. In addition to that, computer games are rapidly evolving in the sense that newer game versions arrive in a very short interval. Thus, software engineering techniques are needed for game development in order to achieve greater flexibility and maintainability, less cost and effort, better design, etc. In addition, games present several characteristics that differentiate their development from classical software development.
52|9||Survival analysis on the duration of open source projects|Open source (FLOSS) project survivability is an important piece of information for many open source stakeholders. Coordinators of open source projects would like to know the chances for the survival of the projects they coordinate. Companies are also interested in knowing how viable a project is in order to either participate or invest in it, and volunteers want to contribute to vivid projects.
52|9||Investigating the impact of a measurement program on software quality|Measurement programs have been around for several decades but have been often misused or misunderstood by managers and developers. This misunderstanding prevented their adoption despite their many advantages.
52|9||Evaluating logistic regression models to estimate software project outcomes|Software has been developed since the 1960s but the success rate of software development projects is still low. During the development of software, the probability of success is affected by various practices or aspects. To date, it is not clear which of these aspects are more important in influencing project outcome.
52|9||Semi-formal transformation of secure business processes into analysis class and use case models: An MDA approach|Model-Driven Development (MDD) is an alternative approach for information systems development. The basic underlying concept of this approach is the definition of abstract models that can be transformed to obtain models near implementation. One fairly widespread proposal in this sphere is that of Model Driven Architecture (MDA). Business process models are abstract models which additionally contain key information about the tasks that are being carried out to achieve the company’s goals, and two notations currently exist for modelling business processes: the Unified Modelling Language (UML), through activity diagrams, and the Business Process Modelling Notation (BPMN).
52|9||Bug localization using latent Dirichlet allocation|Some recent static techniques for automatic bug localization have been built around modern information retrieval (IR) models such as latent semantic indexing (LSI). Latent Dirichlet allocation (LDA) is a generative statistical model that has significant advantages, in modularity and extensibility, over both LSI and probabilistic LSI (pLSI). Moreover, LDA has been shown effective in topic model based information retrieval. In this paper, we present a static LDA-based technique for automatic bug localization and evaluate its effectiveness.
52|9||Specification of personalization in web application design|Personalization of websites has become an important issue in Web modeling methods due to their big and heterogeneous audience. However, due to the existence of too many notations to represent the same design concepts in different methodologies, personalization specifications cannot be used out of the scope of a single tool or method. Moreover, in some cases, personalization is not defined as a separate aspect, being difficult to maintain and update. This paper tackles the aforementioned problems presenting a generic modeling technique to facilitate the specification of the personalization. Personalization specifications can be reused across different websites and different development environments.
53|1|http://www.sciencedirect.com/science/journal/09505849/53/1|Taking novelty to a new level|
53|1||Software product line testing â A systematic mapping study|Software product lines (SPL) are used in industry to achieve more efficient software development. However, the testing side of SPL is underdeveloped.
53|1||A formal approach for the development of reactive systems|This paper deals with the development and verification of liveness properties on reactive systems using the Event-B method. By considering the limitation of the Event-B method to invariance properties, we propose to apply the language TLA+ to verify liveness properties on Event-B models.
53|1||Evolution of XML schemas and documents from stereotyped UML class models: A traceable approach|UML and XML are two of the most commonly used languages in software engineering processes. One of the most critical of these processes is that of model evolution and maintenance. More specifically, when an XML schema is modified, the changes should be propagated to the corresponding XML documents, which must conform with the new, modified schema.
53|1||Change the face of software engineering education: A field report from Taiwan|Context: In Taiwan, the supply of software engineers provided by universities has suffered from both a quantity problem and a quality problem. An effort to change the software engineering education is in need.
53|1||The agile requirements refinery: Applying SCRUM principles to software product management|Although agile software development methods such as SCRUM and DSDM are gaining popularity, the consequences of applying agile principles to software product management have received little attention until now.
53|1||Migration of information systems in the Italian industry: A state of the practice survey|
53|1||Applying CIM-to-PIM model transformations for the service-oriented development of information systems|Model-driven approaches deal with the provision of models, transformations between them and code generators to address software development. This approach has the advantage of defining a conceptual structure, where the models used by business managers and analysts can be mapped into more detailed models used by software developers. This alignment between high-level business specifications and the lower-level information technologies (ITs) models is crucial to the field of service-oriented development, where meaningful business services and process specifications are those relevant to real business scenarios.
53|1||A profiling method by PCB hooking and its application for memory fault detection in embedded system operational test|An operational test means a system test that examines whether or not all software or hardware components comply with the requirements given to a system which is deployed in an operational environment.
53|10|http://www.sciencedirect.com/science/journal/09505849/53/10|Business process archeology using MARBLE|Legacy information systems age over time. These systems cannot be thrown away because they store a significant amount of valuable business knowledge over time, and they cannot be entirely replaced at an acceptable cost. This circumstance is similar to that of the monuments of ancient civilizations, which have aged but still hold meaningful information about their civilizations. Evolutionary maintenance is the most suitable mechanism to deal with the software ageing problem since it preserves business knowledge. But first, recovering the underlying business knowledge in legacy systems is necessary in order to preserve this vital heritage.
53|10||Development and evaluation of a lightweight root cause analysis method (ARCA method) â Field studies at four software companies|The key for effective problem prevention is detecting the causes of a problem that has occurred. Root cause analysis (RCA) is a structured investigation of the problem to identify which underlying causes need to be fixed. The RCA method consists of three steps: target problem detection, root cause detection, and corrective action innovation. Its results can help with process improvement.
53|10||Empirical evaluation of the fault detection effectiveness and test effort efficiency of the automated AOP testing approaches|Testing process is a time-consuming, expensive, and labor-intensive activity in any software setting including aspect-oriented programming (AOP). To reduce the testing costs, human effort, and to achieve the improvements in both quality and productivity of AOP, it is desirable to automate testing of aspect-oriented programs as much as possible.
53|10||A method for assessing confidence in requirements analysis|
53|10||Guest Editorial for Special Section on Mutation Testing|
53|10||A mutation carol: Past, present and future|The field of mutation analysis has been growing, both in the number of published papers and the number of active researchers. This special issue provides a sampling of recent advances and ideas. But do all the new researchers know where we started?
53|10||Evolutionary mutation testing|Mutation testing is a testing technique that has been applied successfully to several programming languages. However, it is often regarded as computationally expensive, so several refinements have been proposed to reduce its cost. Moreover, WS-BPEL compositions are being widely adopted by developers, but present new challenges for testing, since they can take much longer to run than traditional programs of the same size. Therefore, it is interesting to reduce the number of mutants required.
53|10||Mutation testing on an object-oriented framework: An experience report|The increasing presence of Object-Oriented (OO) programs in industrial systems is progressively drawing the attention of mutation researchers toward this paradigm. However, while the number of research contributions in this topic is plentiful, the number of empirical results is still marginal and mostly provided by researchers rather than practitioners.
53|10||A logic mutation approach to selective mutation for programs and queries|Program mutation testing is a technique for measuring and generating high quality test data. However, traditional mutation operators are not necessarily efficient or effective. We address three specific issues. One, test data that kills all mutants generated by current mutation tools can still miss detection of some common logic faults because such tools lack appropriate logic mutation operators. Two, the number of mutants generated is often unnecessarily large. Three, many equivalent mutants can be generated and these can be difficult to eliminate.
53|10||Mutant generation for embedded systems using kernel-based software and hardware fault simulation|Mutation testing is a fault-injection-based technique to help testers generate test cases for detecting specific and predetermined types of faults.
53|11|http://www.sciencedirect.com/science/journal/09505849/53/11|Specifying aspect-oriented architectures in AO-ADL|Architecture description languages (ADLs) are a well-accepted approach to software architecture representation. The majority of well-known ADLs are defined by means of components and connectors. Architectural connectors are mainly used to model interactions among components, specifying component communication and coordination separately. However, there are other properties that cut across several components and also affect component interactions (e.g. security).
53|11||Data warehouse testing: A prototype-based methodology|Testing is an essential part of the development life-cycle of any software product. While most phases of data warehouse design have received considerable attention in the literature, not much has been written about data warehouse testing.
53|11||Determinants of software quality in offshore development â An empirical study of an Indian vendor|Cost advantage has been one of the primary drivers of successful offshoring engagements of Indian software and services companies. However, the emphasis has shifted to the ability of the vendors to provide high quality over cost advantage in delivering software products and services. Meeting high quality requirements of the clients is a challenge due to the very nature of development and delivery of software through offshoring.
53|11||Path dependent stochastic models to detect planned and actual technology use: A case study of OpenOffice|
53|11||Quantitative release planning in extreme programming|Extreme Programming (XP) is one of the most popular agile software development methodologies. XP is defined as a consistent set of values and practices designed to work well together, but lacks practices for project management and especially for supporting the customer role. The customer representative is constantly under pressure and may experience difficulties in foreseeing the adequacy of a release plan.
53|11||The value of software sizing|One of the difficulties faced by software development Project Managers is estimating the cost and schedule for new projects. Previous industry surveys have concluded that software size and cost estimation is a significant technical area of concern. In order to estimate cost and schedule it is important to have a good understanding of the size of the software product to be developed. There are a number of techniques used to derive software size, with function points being amongst the most documented.
53|11||Guest Editorial: Special Section from 6th Workshop on Advances in Model-Based Testing (A-MOST 2010)|
53|11||Generating asynchronous test cases from test purposes|Input/output transition system (IOTS) models are commonly used when next input can arrive even before outputs are produced. The interaction between the tester and an implementation under test (IUT) is usually assumed to be synchronous. However, as the IUT can produce outputs at any moment, the tester should be prepared to accept all outputs from the IUT, or else be able to block (refuse) outputs of the implementation. Testing distributed, remote applications under the assumptions that communication is synchronous and actions can be blocked is unrealistic, since synchronous communication for such applications can only be achieved if special protocols are used. In this context, asynchronous tests can be more appropriate, reflecting the underlying test architecture which includes queues.
53|11||Generating minimal fault detecting test suites for general Boolean specifications|Boolean expressions are a central aspect of specifications and programs, but they also offer dangerously many ways to introduce faults. To counter this effect, various criteria to generate and evaluate tests have been proposed. These are traditionally based on the structure of the expressions, but are not directly related to the possible faults. Often, they also require expressions to be in particular formats such as disjunctive normal form (DNF), where a strict hierarchy of faults is available to prove fault detection capability.
53|12|http://www.sciencedirect.com/science/journal/09505849/53/12|Static and dynamic adaptations for service-based systems|In service-oriented computing (SOC), service providers publish reusable services, and service consumers subscribe them. However, there exist potential problems in reusing services. Mismatch is a problem that occurs when a candidate service does not fully match to the feature expected. Fault is a problem that occurs when an invocation of services results in some abnormality at runtime. Without remedying mismatch problems, services would not be reusable. Without remedying fault problems, service invocations at runtime would result in failures. Static and dynamic adaptations are practical approaches to remedying the problems.
53|12||An integrated search-based approach for automatic testing from extended finite state machine (EFSM) models|The extended finite state machine (EFSM) is a modelling approach that has been used to represent a wide range of systems. When testing from an EFSM, it is normal to use a test criterion such as transition coverage. Such test criteria are often expressed in terms of transition paths (TPs) through an EFSM. Despite the popularity of EFSMs, testing from an EFSM is difficult for two main reasons: path feasibility and path input sequence generation. The path feasibility problem concerns generating paths that are feasible whereas the path input sequence generation problem is to find an input sequence that can traverse a feasible path.
53|12||A comparative study of challenges in integrating Open Source Software and Inner Source Software|Several large software-developing organizations have adopted Open Source Software development (OSSD) practices to develop in-house components that are subsequently integrated into products. This phenomenon is also known as “Inner Source”. While there have been several reports of successful cases of this phenomenon, little is known about the challenges that practitioners face when integrating software that is developed in such a setting.
53|12||Automating image segmentation verification and validation by learning test oracles|An image segmentation algorithm delineates (an) object(s) of interest in an image. Its output is referred to as a segmentation. Developing these algorithms is a manual, iterative process involving repetitive verification and validation tasks. This process is time-consuming and depends on the availability of experts, who may be a scarce resource (e.g., medical experts). We propose a framework referred to as Image Segmentation Automated Oracle (ISAO) that uses machine learning to construct an oracle, which can then be used to automatically verify the correctness of image segmentations, thus saving substantial resources and making the image segmentation verification and validation task significantly more efficient. The framework also gives informative feedback to the developer as the segmentation algorithm evolves and provides a systematic means of testing different parametric configurations of the algorithm. During the initial learning phase, segmentations from the first few (optimally two) versions of the segmentation algorithm are manually verified by experts. The similarity of successive segmentations of the same images is also measured in various ways. This information is then fed to a machine learning algorithm to construct a classifier that distinguishes between consistent and inconsistent segmentation pairs (as determined by an expert) based on the values of the similarity measures associated with each segmentation pair. Once the accuracy of the classifier is deemed satisfactory to support a consistency determination, the classifier is then used to determine whether the segmentations that are produced by subsequent versions of the algorithm under test, are (in)consistent with already verified segmentations from previous versions. This information is then used to automatically draw conclusions about the correctness of the segmentations. We have successfully applied this approach to 3D segmentations of the cardiac left ventricle obtained from CT scans and have obtained promising results (accuracies of 95%). Even though more experiments are needed to quantify the effectiveness of the approach in real-world applications, ISAO shows promise in increasing the quality and testing efficiency of image segmentation algorithms.
53|12||Taking context into account in conceptual models using a Model Driven Engineering approach|In public transport, travelers (considered as information systems users) do not have the same objectives and/or concerns at the same time. For this reason it is not always easy to provide them with the right information at the right time. If personalizing the information to the user allows to do this to some extend it is not enough since the information could also depend on the use of the context and the environment (e.g., place, time, etc.).
53|12||A relaxable service selection algorithm for QoS-based web service composition|Web Services are emerging technologies that enable application to application communication and reuse of autonomous services over Web. Composition of web services is a concept of integrating individual web services to conduct complex business transactions based on functionality and performance constraints
53|12||Contrasting ideal and realistic conditions as a means to improve judgment-based software development effort estimation|The effort estimates of software development work are on average too low. A possible reason for this tendency is that software developers, perhaps unconsciously, assume ideal conditions when they estimate the most likely use of effort. In this article, we propose and evaluate a two-step estimation process that may induce more awareness of the difference between idealistic and realistic conditions and as a consequence more realistic effort estimates. The proposed process differs from traditional judgment-based estimation processes in that it starts with an effort estimation that assumes ideal conditions before the most likely use of effort is estimated.
53|12||Assessing the influence of stereotypes on the comprehension of UML sequence diagrams: A family of experiments|The conventional wisdom states that stereotypes are used to clarify or extend the meaning of model elements and consequently should be helpful in comprehending the diagram semantics.
53|2|http://www.sciencedirect.com/science/journal/09505849/53/2|Components meet aspects: Assessing design stability of a software product line|It is important for Product Line Architectures (PLA) to remain stable accommodating evolutionary changes of stakeholder’s requirements. Otherwise, architectural modifications may have to be propagated to products of a product line, thereby increasing maintenance costs. A key challenge is that several features are likely to exert a crosscutting impact on the PLA decomposition, thereby making it more difficult to preserve its stability in the presence of changes. Some researchers claim that the use of aspects can ameliorate instabilities caused by changes in crosscutting features. Hence, it is important to understand which aspect-oriented (AO) and non-aspect-oriented techniques better cope with PLA stability through evolution.
53|2||Assessing PSP effect in training disciplined software development: A PlanâTrackâReview model|In training disciplined software development, the PSP is said to result in such effect as increased estimation accuracy, better software quality, earlier defect detection, and improved productivity. But a systematic mechanism that can be easily adopted to assess and interpret PSP effect is scarce within the existing literature.
53|2||Discriminative effect of user influence and user responsibility on information system development processes and project management|User participation in information system (IS) development has received much research attention. However, prior empirical research regarding the effect of user participation on IS success is inconclusive. This might be because previous studies overlook the effect of the particular components of user participation and other possible mediating factors.
53|2||A unit test approach for database schema evolution|The constant changes in today’s business requirements demand continuous database revisions. Hence, database structures, not unlike software applications, deteriorate during their lifespan and thus require refactoring in order to achieve a longer life span. Although unit tests support changes to application programs and refactoring, there is currently a lack of testing strategies for database schema evolution.
53|3|http://www.sciencedirect.com/science/journal/09505849/53/3|Testing in Service Oriented Architectures with dynamic binding: A mapping study|Service Oriented Architectures (SOA) have emerged as a new paradigm to develop interoperable and highly dynamic applications.
53|3||An effective sequential statistical test for probabilistic monitoring|A monitor checks if a system behaves according to a specified property at runtime. This is required for quality assurance purposes. Currently several approaches exist to monitor standard and real-time properties. However, a current challenge is to provide a comprehensive approach for monitoring probabilistic properties, as they are used to formulate quality of service requirements like performance, reliability, safety, and availability. The main problem of these probabilistic properties is that there is no binary acceptance condition.
53|3||Simplifying effort estimation based on Use Case Points|The Use Case Points (UCP) method can be used to estimate software development effort based on a use-case model and two sets of adjustment factors relating to the environmental and technical complexity of a project. The question arises whether all of these components are important from the effort estimation point of view.
53|3||Empirical extension of a classification framework for addressing consistency in model based development|
53|3||A two-stage framework for UML specification matching|Specification matching techniques are crucial for effective retrieval processes. Despite the prevalence for object-oriented methodologies, little attention has been given to Unified Modeling Language (UML) for matching.
53|3||Automated metamorphic testing on the analyses of feature models|A feature model (FM) represents the valid combinations of features in a domain. The automated extraction of information from FMs is a complex task that involves numerous analysis operations, techniques and tools. Current testing methods in this context are manual and rely on the ability of the tester to decide whether the output of an analysis is correct. However, this is acknowledged to be time-consuming, error-prone and in most cases infeasible due to the combinatorial complexity of the analyses, this is known as the oracle problem.
53|3||Formal model for assigning human resources to teams in software projects|Human resources play a critical role in software project success. However, people are still the least formalized factor in today’s process models. Generally, people are assigned to roles and project teams are formed on the basis of project leaders’ experience of people, constraints (e.g. availability) and skill requirements. Yet this process has to take multiple factors into account. Few works in the literature model this process. Most of these are informal proposals focusing on the individual assignment of people to project tasks and do not consider other aspects like team formation as a whole.
53|3||Agile methods rapidly replacing traditional methods at Nokia: A survey of opinions on agile transformation|Many organizations have started to deploy agile methods, but so far there exist only a few studies on organization-wide transformations. Are agile methods here to stay? Some claim that agile software development methods are in the mainstream adoption phase in the software industry, while others hope that those are a passing fad. The assumption here is that if agile would not provide real improvement, adopters would be eager at first but turn pessimistic after putting it into practice.
53|3||Corrigendum to âA novel composite model approach to improve software quality predictionâ [Information and Software Technology 52 (12) (2010) 1298â1311]|
53|4|http://www.sciencedirect.com/science/journal/09505849/53/4|Special section editorial â Software Engineering track of the 24th Annual Symposium on Applied Computing|
53|4||A test-driven approach to code search and its application to the reuse of auxiliary functionality|Software developers spend considerable effort implementing auxiliary functionality used by the main features of a system (e.g., compressing/decompressing files, encryption/decription of data, scaling/rotating images). With the increasing amount of open source code available on the Internet, time and effort can be saved by reusing these utilities through informal practices of code search and reuse. However, when this type of reuse is performed in an ad hoc manner, it can be tedious and error-prone: code results have to be manually inspected and integrated into the workspace.
53|4||Pattern-based framework for modularized software development and evolution robustness|Software development is now facing much more challenges than ever before due to the intrinsic high complexity and the increasing demands of the quick-service-ready paradigm.
53|4||Measuring and predicting software productivity: A systematic map and review|Software productivity measurement is essential in order to control and improve the performance of software development. For example, by identifying role models (e.g. projects, individuals, tasks) when comparing productivity data. The prediction is of relevance to determine whether corrective actions are needed, and to discover which alternative improvement action would yield the best results.
53|4||A systematic review of evaluation of variability management approaches in software product lines|Variability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated.
53|4||A systematic literature review of actionable alert identification techniques for automated static code analysis|Automated static analysis (ASA) identifies potential source code anomalies early in the software development lifecycle that could lead to field failures. Excessive alert generation and a large proportion of unimportant or incorrect alerts (unactionable alerts) may cause developers to reject the use of ASA. Techniques that identify anomalies important enough for developers to fix (actionable alerts) may increase the usefulness of ASA in practice.
53|4||An ant colony optimization algorithm to improve software quality prediction models: Case of class stability|Assessing software quality at the early stages of the design and development process is very difficult since most of the software quality characteristics are not directly measurable. Nonetheless, they can be derived from other measurable attributes. For this purpose, software quality prediction models have been extensively used. However, building accurate prediction models is hard due to the lack of data in the domain of software engineering. As a result, the prediction models built on one data set show a significant deterioration of their accuracy when they are used to classify new, unseen data.
53|4||Problems in the interplay of development and IT operations in system development projects: A Delphi study of Norwegian IT experts|The assumption of the presented work is that the ability of system developers and IT operations personnel to cooperate effectively in system development projects has great impact on the quality of the final system solution, as well as on the service level of its subsequent operation.
53|5|http://www.sciencedirect.com/science/journal/09505849/53/5|A systematic mapping study of software product lines testing|In software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development.
53|5||Process models for service-based applications: A systematic literature review|Service-Oriented Computing (SOC) is a promising computing paradigm which facilitates the development of adaptive and loosely coupled service-based applications (SBAs). Many of the technical challenges pertaining to the development of SBAs have been addressed, however, there are still outstanding questions relating to the processes required to develop them.
53|5||Research synthesis in software engineering: A tertiary study|Comparing and contrasting evidence from multiple studies is necessary to build knowledge and reach conclusions about the empirical support for a phenomenon. Therefore, research synthesis is at the center of the scientific enterprise in the software engineering discipline.
53|5||Modeling process-related RBAC models with extended UML activity models|Business processes are an important source for the engineering of customized software systems and are constantly gaining attention in the area of software engineering as well as in the area of information and system security. While the need to integrate processes and role-based access control (RBAC) models has been repeatedly identified in research and practice, standard process modeling languages do not provide corresponding language elements.
53|5||Modeling software requirement with timing diagram and Simulink Stateflow|A methodology is needed to model software specification with both timing diagram and Simulink/Stateflow (SL/SF) and to convert timing diagram model into SL/SF model.
53|5||Evaluating software engineering techniques for developing complex systems with multiagent approaches|Multiagent systems (MAS) allow complex systems to be developed in which autonomous and heterogeneous entities interact. Currently, there are a great number of methods and frameworks for developing MAS. The selection of one or another development environment is a crucial part of the development process. Therefore, the evaluation and comparison of MAS software engineering techniques is necessary in order to make the selection of the development environment easier.
53|5||Special Section on Best Papers from XP2010|
53|5||The relationship between organizational culture and the deployment of agile methods|Systems development normally takes place in a specific organizational context, including organizational culture. Previous research has identified organizational culture as a factor that potentially affects the deployment systems development methods.
53|5||The impact of inadequate customer collaboration on self-organizing Agile teams|Customer collaboration is a vital feature of Agile software development.
53|5||Beyond the customer: Opening the agile systems development process|A particular strength of agile systems development approaches is that they encourage a move away from ‘introverted’ development, involving the customer in all areas of development, leading to more innovative and hence more valuable information system. However, a move toward open innovation requires a focus that goes beyond a single customer representative, involving a broader range of stakeholders, both inside and outside the organisation in a continuous, systematic way.
53|5||Post-agility: What follows a decade of agility?|Agile information systems development (ISD) has received much attention from both the practitioner and researcher community over the last 10–15 years. However, it is still unclear what precisely constitutes agile ISD.
53|6|http://www.sciencedirect.com/science/journal/09505849/53/6|Impact of test-driven development on productivity, code and tests: A controlled experiment|Test-driven development is an approach to software development, where automated tests are written before production code in highly iterative cycles. Test-driven development attracts attention as well as followers in professional environment; however empirical evidence of its superiority regarding its effect on productivity, code and tests compared to test-last development is still fairly limited. Moreover, it is not clear if the supposed benefits come from writing tests before code or maybe from high iterativity/short development cycles.
53|6||Conceptual scheduling model and optimized release scheduling for agile environments|Release scheduling deals with the selection and assignment of deliverable features to a sequence of consecutive product deliveries while several constraints are fulfilled. Although agile software development represents a major approach to software engineering, there is no well-established conceptual definition and sound methodological support of agile release scheduling.
53|6||External social capital and information systems development team flexibility|ISD research based on the socio-technical perspective suggests that two sources of socio-technical change have a bearing on the performance of information systems development (ISD) projects: business requirements and development technology. To enhance project effectiveness, ISD teams need to enhance their flexibility in the face of the constant changes taking place from business and technical environments in which they operate. Flexibility is conceptualized as an outcome of capability development through constantly integrating and reconfiguring available resources within and outside of the organization where the team is embedded.
53|6||Applying agglomerative hierarchical clustering algorithms to component identification for legacy systems|Component identification, the process of evolving legacy system into finely organized component-based software systems, is a critical part of software reengineering. Currently, many component identification approaches have been developed based on agglomerative hierarchical clustering algorithms. However, there is a lack of thorough investigation on which algorithm is appropriate for component identification.
53|6||Guest Editorial: Special section on the selected papers from 14th International Conference on Evaluation and Assessment in Software Engineering (EASE 2010)|
53|6||A systematic review of research on open source software in commercial software product development|The popularity of the open source software development in the last decade, has brought about an increased interest from the industry on how to use open source components, participate in the open source community, build business models around this type of software development, and learn more about open source development methodologies. There is a need to understand the results of research in this area.
53|6||Identifying relevant studies in software engineering|Systematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries.
53|6||Using mapping studies as the basis for further research â A participant-observer case study|We are strong advocates of evidence-based software engineering (EBSE) in general and systematic literature reviews (SLRs) in particular. We believe it is essential that the SLR methodology is used constructively to support software engineering research.
53|6||Guest Editorsâ introduction to the special section of the 16th AsiaâPacific Software Engineering Conference (APSEC2009)|
53|6||Contribution-based call stack abstraction for call string based pointer analysis|Different method calls may have different contributions to the precision of the final application when abstracted into the call strings. The existing call string based pointer analysis algorithms do not consider such contribution difference and hence may not achieve best cost-effectiveness.
53|6||An approach to identifying causes of implied scenarios using unenforceable orders|The implied scenarios are unexpected behaviors in the scenario specifications. Detecting and handling them is essential for the correctness of the scenario specifications. To handle such implied scenarios, identifying the causes of implied scenarios is also essential. Most recent researches focus on detecting those implied scenarios, themselves or limited causes of implied scenarios.
53|6||A controlled experiment in assessing and estimating software maintenance tasks|Software maintenance is an important software engineering activity that has been reported to account for the majority of the software total cost. Thus, understanding the factors that influence the cost of software maintenance tasks helps maintainers to make informed decisions about their work.
53|7|http://www.sciencedirect.com/science/journal/09505849/53/7|Barriers in the selection of offshore software development outsourcing vendors: An exploratory study using a systematic literature review|Software development outsourcing is a contract-based relationship between client and vendor organisations in which a client contracts out all or part of its software development activities to a vendor, who provides agreed services for remuneration.
53|7||Analyzing evolution of variability in a software product line: From contexts and requirements to features|In the long run, features of a software product line (SPL) evolve with respect to changes in stakeholder requirements and system contexts. Neither domain engineering nor requirements engineering handles such co-evolution of requirements and contexts explicitly, making it especially hard to reason about the impact of co-changes in complex scenarios.
53|7||Partnering effects on userâdeveloper conflict and role ambiguity in information system projects|Information system development (ISD) has been plagued with high failure rates. This is partially due to the activities being a combination of both a technical and social processes involving stakeholders with conflicting interests.
53|7||The longitudinal, chronological case study research strategy: A definition, and an example from IBM Hursley Park|There is surprisingly little empirical software engineering research (ESER) that has analysed and reported the rich, fine-grained behaviour of phenomena over time using qualitative and quantitative data. The ESER community also increasingly recognises the need to develop theories of software engineering phenomena e.g. theories of the actual behaviour of software projects at the level of the project and over time.
53|7||An experimental assessment of module documentation-based testing|Testing a module that has memory using the black-box approach has been found to be expensive and relatively ineffective. Instead, testing without knowledge of the specifications (white-box approach) may not be effective in showing whether a program has been properly implemented as stated in its specifications. We propose instead a grey-box approach called Module Documentation-based Testing or MD-Test, the heart of which is an automatic generation of the test oracle from the external and internal views of the module.
53|7||Comparing the performance of metaheuristics for the analysis of multi-stakeholder tradeoffs in requirements optimisation|In requirements engineering, there will be many different stake holders. Often the requirements engineer has to find a set of requirements that reflect the needs of several different stake holders, while remaining within budget.
53|7||An automated framework for software test oracle|One of the important issues of software testing is to provide an automated test oracle. Test oracles are reliable sources of how the software under test must operate. In particular, they are used to evaluate the actual results that produced by the software. However, in order to generate an automated test oracle, oracle challenges need to be addressed. These challenges are output-domain generation, input domain to output domain mapping, and a comparator to decide on the accuracy of the actual outputs.
53|8|http://www.sciencedirect.com/science/journal/09505849/53/8|Usability evaluation methods for the web: A systematic mapping study|In recent years, many usability evaluation methods (UEMs) have been employed to evaluate Web applications. However, many of these applications still do not meet most customers’ usability expectations and many companies have folded as a result of not considering Web usability issues. No studies currently exist with regard to either the use of usability evaluation methods for the Web or the benefits they bring.
53|8||Cycle elimination for invocation graph-based context-sensitive pointer analysis|Pointer analysis is an important building block of optimizing compilers and program analyzers for C language. Various methods with precision and performance trade-offs have been proposed. Among them, cycle elimination has been successfully used to improve the scalability of context-insensitive pointer analyses without losing any precision.
53|8||Design guidelines for software processes knowledge repository development|Staff turnover in organizations is an important issue that should be taken into account mainly for two reasons:
53|8||Exploring a Bayesian and linear approach to requirements traceability|For large software projects it is important to have some traceability between artefacts from different phases (e.g.requirements, designs, code), and between artefacts and the involved developers. However, if the capturing of traceability information during the project is felt as laborious to developers, they will often be sloppy in registering the relevant traceability links so that the information is incomplete. This makes automated tool-based collection of traceability links a tempting alternative, but this has the opposite challenge of generating too many potential trace relationships, not all of which are equally relevant.
53|8||Guest editorial: Advances in functional size measurement and effort estimation â Extended best papers|
53|8||Convertibility of Function Points into COSMIC Function Points: A study using Piecewise Linear Regression|COSMIC Function Points and traditional Function Points (i.e., IFPUG Function Points and more recent variation of Function Points, such as NESMA and FISMA) are probably the best known and most widely used Functional Size Measurement methods. The relationship between the two kinds of Function Points still needs to be investigated. If traditional Function Points could be accurately converted into COSMIC Function Points and vice versa, then, by measuring one kind of Function Points, one would be able to obtain the other kind of Function Points, and one might measure one or the other kind interchangeably. Several studies have been performed to evaluate whether a correlation or a conversion function between the two measures exists. Specifically, it has been suggested that the relationship between traditional Function Points and COSMIC Function Points may not be linear, i.e., the value of COSMIC Function Points seems to increase more than proportionally to an increase of traditional Function Points.
53|8||Improving the reliability of transaction identification in use cases|The concept of transactions is used in Use Case Points (UCP), and in many other functional size measurement methods, to capture the smallest unit of functionality that should be considered while measuring the size of a system. Unfortunately, in the case of the UCP method at least four methods for use-case transaction identification have been proposed so far. The different approaches to transaction identification and difficulties related to the analysis of requirements expressed in natural language can lead to problems in the reliability of functional size measurement.
53|9|http://www.sciencedirect.com/science/journal/09505849/53/9|Six years of systematic literature reviews in software engineering: An updated tertiary study|Since the introduction of evidence-based software engineering in 2004, systematic literature review (SLR) has been increasingly used as a method for conducting secondary studies in software engineering. Two tertiary studies, published in 2009 and 2010, identified and analysed 54 SLRs published in journals and conferences in the period between 1st January 2004 and 30th June 2008.
53|9||Improving the applicability of object-oriented class cohesion metrics|Class cohesion is an important object-oriented quality attribute. It refers to the degree of relatedness between the methods and attributes of a class. Several metrics have been proposed to measure the extent to which the class members are related. Most of these metrics have undefined values for a relatively high percentage of classes, which limits their applicability. The classes that have undefined values lack methods, attributes, or parameter types, or they include only a single method.
53|9||Reliability analysis and optimal version-updating for open source software|Although reliability is a major concern of most open source projects, research on this problem has attracted attention only recently. In addition, the optimal version-dating for open source software considering its special properties is not yet discussed.
53|9||Identifying refactoring opportunities in process model repositories|In order to ensure high quality of a process model repository, refactoring operations can be applied to correct anti-patterns, such as overlap of process models, inconsistent labeling of activities and overly complex models. However, if a process model collection is created and maintained by different people over a longer period of time, manual detection of such refactoring opportunities becomes difficult, simply due to the number of processes in the repository. Consequently, there is a need for techniques to detect refactoring opportunities automatically.
53|9||Guest editorial: Studying work practices in Global Software Engineering|
53|9||Sociomaterial bricolage: The creation of location-spanning work practices by global software developers|
53|9||Methodological reflections on a field study of a globally distributed software project|We describe the methodology of a field study of a globally distributed software development project in a multinational corporation. The project spanned four sites in the US and one in India, and is a representative example of the complexities and intricacies of global corporate software development.
53|9||On qualitative methodologies and dispersed communities: Reflections on the process of investigating an open source community|Qualitative methodologies hold much potential for building an understanding of the principles and practices of free and open source software (FOSS) communities. Yet there is a scarcity in the literature of discussions focused on the practical and methodological challenges of this particular research context.
53|9||Understanding technology use in global virtual teams: Research methodologies and methods|The globalisation of activities associated with software development and use has introduced many challenges in practice, and also (therefore) many for research. While the predominant approach to research in software engineering has followed a positivist science model, this approach may be sub-optimal when addressing problems with a dominant social or cultural dimension, such as those frequently encountered when studying work practices in a globally distributed team setting.
53|9||Conducting a Business Ethnography in Global Software Development projects of small German enterprises|Studying work practices in the context of Global Software Development (GSD) projects entails multiple opportunities and challenges for the researchers. Understanding and tackling these challenges requires a careful and rigor application of research methods.
54|1|http://www.sciencedirect.com/science/journal/09505849/54/1|A systematic mapping study on the combination of static and dynamic quality assurance techniques|A lot of different quality assurance techniques exist to ensure high quality products. However, most often they are applied in isolation. A systematic combination of different static and dynamic quality assurance techniques promises to exploit synergy effects, such as higher defect detection rates or reduced quality assurance costs. However, a systematic overview of such combinations and reported evidence about achieving synergy effects with such kinds of combinations is missing.
54|1||A systematic review of software architecture evolution research|Software evolvability describes a software system’s ability to easily accommodate future changes. It is a fundamental characteristic for making strategic decisions, and increasing economic value of software. For long-lived systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. For this reason, many research studies have been proposed in this area both by researchers and industry practitioners. These studies comprise a spectrum of particular techniques and practices, covering various activities in software lifecycle. However, no systematic review has been conducted previously to provide an extensive overview of software architecture evolvability research.
54|1||Systematic literature review of machine learning based software development effort estimation models|Software development effort estimation (SDEE) is the process of predicting the effort required to develop a software system. In order to improve estimation accuracy, many researchers have proposed machine learning (ML) based SDEE models (ML models) since 1990s. However, there has been no attempt to analyze the empirical evidence on ML models in a systematic way.
54|1||Is software âgreenâ? Application development environments and energy efficiency in open source applications|The energy efficiency of IT systems, also referred to as Green IT, is attracting more and more attention. While several researchers have focused on the energy efficiency of hardware and embedded systems, the role of application software in IT energy consumption still needs investigation.
54|1||Towards an ontology-based retrieval of UML Class Diagrams|Software Reuse has always been an important area amongst software companies in order to increase their productivity and the quality of their products, but code reuse is not the only answer for this. Nowadays, reuse techniques proposals include software designs or even software specifications. Therefore, this research focuses on software design, specifically on UML Class Diagrams. A semantic technology has been applied to facilitate the retrieval process for an effective reuse.
54|1||A systematic approach to integrate common timed security rules within a TEFSM-based system specification|Formal methods are very useful in the software industry and are becoming of paramount importance in practical engineering techniques. They involve the design and modeling of various system aspects expressed usually through different paradigms. These different formalisms make the verification of global developed systems more difficult.
54|1||Human and program factors affecting the maintenance of programs with deployed design patterns|Practitioners may use design patterns to organize program code. Various empirical studies have investigated the effects of pattern deployment and work experience on the effectiveness and efficiency of program maintenance. However, results from these studies are not all consistent. Moreover, these studies have not considered some interesting factors, such as a maintainer’s prior exposure to the program under maintenance.
54|1||Controlled composition and abstraction for bottom-up integration and verification of abstract components|This work proposes a method for improving the scalability of model-checking compositions in the bottom-up construction of abstract components. The approach uses model checking in the model construction process for testing the composite behaviors of components, including process deadlock and inconsistency in inter-component call sequences. Assuming a single processor model, the scalability issue is addressed by introducing operational models for synchronous/asynchronous inter-component message passing, which are designed to reduce spurious behaviors caused by typical parallel compositions. Together with two abstraction techniques, synchronized abstraction and projection abstraction, that hide verified internal communication behavior, this operational model helps to reduce the complexity of composition and verification.
54|10|http://www.sciencedirect.com/science/journal/09505849/54/10|A systematic review of code generation proposals from state machine specifications|Model Driven Development (MDD) encourages the use of models for developing complex software systems. Following a MDD approach, modelling languages are used to diagrammatically model the structure and behaviour of object-oriented software, among which state-based languages (including UML state machines, finite state machines and Harel statecharts) constitute the most widely used to specify the dynamic behaviour of a system. However, generating code from state machine models as part of the final system constitutes one of the most challenging tasks due to its dynamic nature and because many state machine concepts are not supported by the object-oriented programming languages. Therefore, it is not surprising that such code generation has received great attention over the years.
54|10||Impact of physical ambiance on communication, collaboration and coordination in agile software development: An empirical evaluation|Communication, collaboration and coordination are key enablers of software development and even more so in agile methods. The physical environment of the workspace plays a significant role in effective communication, collaboration, and coordination among people while developing software.
54|10||A visual analysis approach to validate the selection review of primary studies in systematic reviews|Systematic Literature Reviews (SLRs) are an important component to identify and aggregate research evidence from different empirical studies. Despite its relevance, most of the process is conducted manually, implying additional effort when the Selection Review task is performed and leading to reading all studies under analysis more than once.
54|10||Reducing test effort: A systematic mapping study on existing approaches|Quality assurance effort, especially testing effort, is often a major cost factor during software development, which sometimes consumes more than 50% of the overall development effort. Consequently, one major goal is often to reduce testing effort.
54|10||Are you biting off more than you can chew? A case study on causes and effects of overscoping in large-scale software engineering|Scope management is a core part of software release management and often a key factor in releasing successful software products to the market. In a market-driven case, when only a few requirements are known a priori, the risk of overscoping may increase.
54|10||Constructing models for predicting extract subclass refactoring opportunities using object-oriented quality metrics|Refactoring is a maintenance task that refers to the process of restructuring software source code to enhance its quality without affecting its external behavior. Inspecting and analyzing the source code of the system under consideration to identify the classes in need of extract subclass refactoring (ESR) is a time consuming and costly process.
54|10||Requirements engineering tools: Capabilities, survey and assessment|There is a significant number of requirements engineering (RE) tools with different features and prices. However, existing RE tool lists do not provide detailed information about the features of the tools that they catalogue. It would therefore be interesting for both practitioners and tool developers to be aware of the state-of-the-art as regards RE tools.
54|11|http://www.sciencedirect.com/science/journal/09505849/54/11|Quality indicators for business process models from a gateway complexity perspective|Quality assurance of business process models has been recognized as an important factor for modeling success at an enterprise level. Since quality of models might be subject to different interpretations, it should be addressed in the most objective way, by the application of measures. That said, however, assessment of measurement results is not a straightforward task: it requires the identification of relevant threshold values, which are able to distinguish different levels of process model quality.
54|11||A Process Framework for Global Software Engineering Teams|Global Software Engineering (GSE) continues to experience substantial growth and is fundamentally different to collocated development. As a result, software managers have a pressing need for support in how to successfully manage teams in a global environment. Unfortunately, de facto process frameworks such as the Capability Maturity Model Integration (CMMI®) do not explicitly cater for the complex and changing needs of global software management.
54|11||Static analysis of Android programs|Android is a programming language based on Java and an operating system for embedded and mobile devices, whose upper layers are written in the Android language itself. As a language, it features an extended event-based library and dynamic inflation of graphical views from declarative XML layout files. A static analyzer for Android programs must consider such features, for correctness and precision.
54|11||Automated refactoring to the Strategy design pattern|The automated identification of code fragments characterized by common design flaws (or “code smells”) that can be handled through refactoring, fosters refactoring activities, especially in large code bases where multiple developers are engaged without a detailed view on the whole system. Automated refactoring to design patterns enables significant contributions to design quality even from developers with little experience on the use of the required patterns.
54|11||Analyzing the understandability of Requirements Engineering languages for CSCW systems: A family of experiments|A collaborative system is a special kind of software whose users can perform collaboration, communication and collaboration tasks. These systems usually have a high number of non-functional requirements, resulting from the users’ need of being aware of other users with whom to collaborate, that is, the workspace awareness.
54|11||Software quality assurance economics|Software companies invest in quality assurance in order to lower software development and maintenance cost, and to increase revenue and profit margins. To contribute to increase of net income, a quality assurance organization has to consider cost and value of the testware involved in assuring quality of software artifacts, such as requirements, specifications, designs, and code.
54|11||Domain model-driven software engineering: A method for discovery of dependency links|Dependency management often suffers from labor intensity and complexity in creating and maintaining the dependency relations in practice. This is even more critical in a distributed development, in which developers are geographically distributed and a wide variety of tools is used. In those settings, different interpretations of software requirements or usage of different terminologies make it challenging to predict the change impact.
54|11||An exploratory study on the accuracy of FPA to COSMIC measurement method conversion types|Functional size measurement methods are increasingly being adopted by software organizations due to the benefits they provide to software project managers. The Function Point Analysis (FPA) measurement method has been used extensively and globally in software organizations. The COSMIC measurement method is considered a second generation FSM method, because of the novel aspects it brings to the FSM field. After the COSMIC method was proposed, the issue of convertibility from FPA to COSMIC method arose, the main problem being the ability to convert FPA historical data to the corresponding COSMIC Function Point (CFP) data with a high level of accuracy, which would give organizations the ability to use the data in their future planning. Almost all the convertibility studies found in the literature involve converting FPA measures to COSMIC measures statistically, based on the final size generated by both methods.
54|11||Quality evaluation for Model-Driven Web Engineering methodologies|There are lots of approaches or methodologies in the Model-Driven Web Engineering (MDWE) context to develop Web Applications without reaching a consensus on the use of standards and scarcity of both, practical experience and tool support.
54|11||A framework for pathologies of message sequence charts|
54|12|http://www.sciencedirect.com/science/journal/09505849/54/12|Increasing clone maintenance support by unifying clone detection and refactoring activities|Clone detection tools provide an automated mechanism to discover clones in source code. On the other side, refactoring capabilities within integrated development environments provide the necessary functionality to assist programmers in refactoring. However, we have observed a gap between the processes of clone detection and refactoring.
54|12||Has open source software been institutionalized in organizations or not?|Almost a decade ago, researchers in information systems and analysts of the information technology (IT) industry were predicting a bright future for open source software (OSS). Recent examples appear to lend support to this, but there exist many detractors of OSS and resistance to the transformation it creates. Thus, it is relevant to take a closer look at the institutionalization of OSS.
54|12||The maturity of maturity model research: A systematic mapping study|Maturity models offer organizations a simple but effective possibility to measure the quality of their processes. Emerged out of software engineering, the application fields have widened and maturity model research is becoming more important. During the last two decades the publication amount steadily rose as well. Until today, no studies have been available summarizing the activities and results of the field of maturity model research.
54|12||Model-Driven Engineering as a new landscape for traceability management: A systematic literature review|Model-Driven Engineering provides a new landscape for dealing with traceability in software development.
54|12||A HCI technique for improving requirements elicitation|To develop usable software we need to understand the users that will interact with the system. Personas is a HCI technique that gathers information about users in order to comprehend their characteristics. This information is used to define fictitious persons on which development should focus. Personas provides an understanding of the user, often overlooked in SE developments.
54|12||Guest Editorial: Special section on software reliability and security|
54|12||Assessing practical usefulness and performance of the PREDIQT method: An industrial case study|When adapting a system to new usage patterns, processes or technologies, it is necessary to foresee the implications of the architectural design changes on system quality. Examination of quality outcomes through implementation of the different architectural design alternatives is often unfeasible. We have developed a method called PREDIQT with the aim to facilitate model-based prediction of impacts of architectural design changes on system quality. A recent case study indicated feasibility of the PREDIQT method when applied on a real-life industrial system. The promising results encouraged further and more structured evaluation of PREDIQT.
54|12||Comprehensive two-level analysis of role-based delegation and revocation policies with UML and OCL|Role-based access control (RBAC) has become the de facto standard for access management in various large-scale organizations. Often role-based policies must implement organizational rules to satisfy compliance or authorization requirements, e.g., the principle of separation of duty (SoD). To provide business continuity, organizations should also support the delegation of access rights and roles, respectively. This, however, makes access control more complex and error-prone, in particular, when delegation concepts interplay with SoD rules.
54|12||Validation of SDL-based architectural design models using communication-based coverage criteria|As the capability to automatically generate code from different models becomes more sophisticated, it is critical that these models be adequately tested for quality assurance prior to code generation.
54|12||Verification and analysis of domain-specific models of physical characteristics in embedded control software|A considerable portion of the software systems today are adopted in the embedded control domain. Embedded control software deals with controlling a physical system, and as such models of physical characteristics become part of the embedded control software.
54|2|http://www.sciencedirect.com/science/journal/09505849/54/2|Drivers of agile software development use: Dialectic interplay between benefits and hindrances|Agile software development with its emphasis on producing working code through frequent releases, extensive client interactions and iterative development has emerged as an alternative to traditional plan-based software development methods. While a number of case studies have provided insights into the use and consequences of agile, few empirical studies have examined the factors that drive the adoption and use of agile.
54|2||Conceptual framework for business processes compositional verification|To guarantee the success of Business Process Modelling (BPM) it is necessary to check whether the activities and tasks described by Business Processes (BPs) are sound and well coordinated.
54|2||Field study on requirements engineering: Investigation of artefacts, project parameters, and execution strategies|Requirements Engineering (RE) is a critical discipline mostly driven by uncertainty, since it is influenced by the customer domain or by the development process model used. Volatile project environments restrict the choice of methods and the decision about which artefacts to produce in RE.
54|2||Fault-based test suite prioritization for specification-based testing|Existing test suite prioritization techniques usually rely on code coverage information or historical execution data that serve as indicators for estimating the fault-detecting ability of test cases. Such indicators are primarily empirical in nature and not theoretically driven; hence, they do not necessarily provide sound estimates. Also, these techniques are not applicable when the source code is not available or when the software is tested for the first time.
54|2||Code churn estimation using organisational and code metrics: An experimental comparison|Source code revision control systems contain vast amounts of data that can be exploited for various purposes. For example, the data can be used as a base for estimating future code maintenance effort in order to plan software maintenance activities. Previous work has extensively studied the use of metrics extracted from object-oriented source code to estimate future coding effort. In comparison, the use of other types of metrics for this purpose has received significantly less attention.
54|2||On the relationship of concern metrics and requirements maintainability|Maintainability has become one of the most essential attributes of software quality, as software maintenance has shown to be one of the most costly and time-consuming tasks of software development. Many studies reveal that maintainability is not often a major consideration in requirements and design stages, and software maintenance costs may be reduced by a more controlled design early in the software life cycle. Several problem factors have been identified as harmful for software maintainability, such as lack of upfront consideration of proper modularity choices. In that sense, the presence of crosscutting concerns is one of such modularity anomalies that possibly exert negative effects on software maintainability. However, to the date there is little or no knowledge about how characteristics of crosscutting concerns, observable in early artefacts, are correlated with maintainability.
54|3|http://www.sciencedirect.com/science/journal/09505849/54/3|IT Service Management Process Improvement based on ISO/IEC 15504: A systematic review|In recent years, many software companies have considered Software Process Improvement (SPI) as essential for successful software development. These companies have also shown special interest in IT Service Management (ITSM). SPI standards have evolved to incorporate ITSM best practices.
54|3||Transfer learning for cross-company software defect prediction|Software defect prediction studies usually built models using within-company data, but very few focused on the prediction models trained with cross-company data. It is difficult to employ these models which are built on the within-company data in practice, because of the lack of these local data repositories. Recently, transfer learning has attracted more and more attention for building classifier in target domain using the data from related source domain. It is very useful in cases when distributions of training and test instances differ, but is it appropriate for cross-company software defect prediction?
54|3||API2MoL: Automating the building of bridges between APIs and Model-Driven Engineering|A software artefact typically makes its functionality available through a specialized Application Programming Interface (API) describing the set of services offered to client applications. In fact, building any software system usually involves managing a plethora of APIs, which complicates the development process. In Model-Driven Engineering (MDE), where models are the key elements of any software engineering activity, this API management should take place at the model level. Therefore, tools that facilitate the integration of APIs and MDE are clearly needed.
54|3||Defect proneness estimation and feedback approach for software design quality improvement|Modern software engineering demands professionals and researchers to proactively and collectively work towards exploring and experimenting viable and valuable mechanisms in order to extract all kinds of degenerative bugs, security holes, and possible deviations at the initial stage. Having understood the real need here, we have introduced a novel methodology for the estimation of defect proneness of class structures in object oriented (OO) software systems at design stage.
54|3||DELIVER! â An educational game for teaching Earned Value Management in computing courses|To meet the growing need for education in Software Project Management, educational games have been introduced as a beneficial instructional strategy. However, there are no low-cost board games openly available to teach Earned Value Management (EVM) in computing programs.
54|3||SOAdapt: A process reference model for developing adaptable service-based applications|The loose coupling of services and Service-Based Applications (SBAs) have made them the ideal platform for context-based run-time adaptation. There has been a lot of research into implementation techniques for adapting SBAs, without much effort focused on the software process required to guide the adaptation.
54|3||Damon: A distributed AOP middleware for large-scale scenarios|
54|4|http://www.sciencedirect.com/science/journal/09505849/54/4|A methodology to assess the impact of design patterns on software quality|Software quality is considered to be one of the most important concerns of software production teams. Additionally, design patterns are documented solutions to common design problems that are expected to enhance software quality. Until now, the results on the effect of design patterns on software quality are controversial.
54|4||Resolving unwanted couplings through interactive exploration of co-evolving software entities â An experience report|
54|4||Hybrid methodology for data warehouse conceptual design by UML schemas|Data warehouse conceptual design is based on the metaphor of the cube, which can be derived from either requirement-driven or data-driven methodologies. Each methodology has its own advantages. The first allows designers to obtain a conceptual schema very close to the user needs but it may be not supported by the effective data availability. On the contrary, the second ensures a perfect traceability and consistence with the data sources—in fact, it guarantees the presence of data to be used in analytical processing—but does not preserve from missing business user needs. To face this issue, the necessity emerged in the last years to define hybrid methodologies for conceptual design.
54|4||Business process model repositories â Framework and survey|Large organizations often run hundreds or even thousands of different business processes. Managing such large collections of business process models is a challenging task. Software can assist in performing that task, by supporting common management functions such as storage, search and version management of models. It can also provide advanced functions that are specific for managing collections of process models, such as managing the consistency of public and private processes. Software that supports the management of large collections of business process models is called: business process model repository software.
54|4||Fault prediction and the discriminative powers of connectivity-based object-oriented class cohesion metrics|Several metrics have been proposed to measure the extent to which class members are related. Connectivity-based class cohesion metrics measure the degree of connectivity among the class members.
54|4||A framework for analysis and design of software reference architectures|A software reference architecture is a generic architecture for a class of systems that is used as a foundation for the design of concrete architectures from this class. The generic nature of reference architectures leads to a less defined architecture design and application contexts, which makes the architecture goal definition and architecture design non-trivial steps, rooted in uncertainty.
54|5|http://www.sciencedirect.com/science/journal/09505849/54/5|The situational factors that affect the software development process: Towards a comprehensive reference framework|An optimal software development process is regarded as being dependent on the situational characteristics of individual software development settings. Such characteristics include the nature of the application(s) under development, team size, requirements volatility and personnel experience. However, no comprehensive reference framework of the situational factors affecting the software development process is presently available.
54|5||Assisting conformance checks between architectural scenarios and implementation|Conformance between architecture and implementation is a key aspect of architecture-centric development. Unfortunately, the architecture “as documented” and the architecture “as implemented” tend to diverge from each other over time. As this gap gets wider, the architects’ reliance on architecture-level analyses is compromised. Thus, conformance checks should be run periodically on the system in order to detect and correct differences. In practice, tool support is very beneficial for these checks.
54|5||Automated removal of cross site scripting vulnerabilities in web applications|Cross site scripting (XSS) vulnerability is among the top web application vulnerabilities according to recent surveys. This vulnerability occurs when a web application uses inputs received from users in web pages without properly checking them. This allows an attacker to inject malicious scripts in web pages via such inputs such that the scripts perform malicious actions when a client visits the exploited web pages. Such an attack may cause serious security violations such as account hijacking and cookie theft. Current approaches to mitigate this problem mainly focus on effective detection of XSS vulnerabilities in the programs or prevention of real time XSS attacks. As more sophisticated attack vectors are being discovered, vulnerabilities if not removed could be exploited anytime.
54|5||Software process improvement success factors for small and medium Web companies: A qualitative study|The context of this research is software process improvement (SPI) in small and medium Web companies.
54|5||MC Sandbox: Devising a tool for method-user-centered method configuration|Method engineering approaches are often based on the assumption that method users are able to explicitly express their situational method requirements. Similar to systems requirements, method requirements are often vague and hard to explicate. In this paper we address the issue of involving method users early in method configuration. This is done through borrowing ideas from user-centered design and prototyping, and implementing them on the method engineering layer.
54|6|http://www.sciencedirect.com/science/journal/09505849/54/6|Comparing alternatives for analyzing requirements trade-offs â In the absence of numerical data|Choosing a design solution most often involves dealing with trade-offs and conflicts among requirements and design objectives. Making such trade-offs during early stages of requirements and design is challenging because costs and benefits of alternatives are often hard to quantify.
54|6||Compliance in service-oriented architectures: A model-driven and view-based approach|Ensuring software systems conforming to multiple sources of relevant policies, laws, and regulations is significant because the consequences of infringement can be serious. Unfortunately, this goal is hardly achievable due to the divergence and frequent changes of compliance sources and the differences in perception and expertise of the involved stakeholders. In the long run, these issues lead to problems regarding complexity, understandability, maintainability, and reusability of compliance concerns.
54|6||Design and implementation of a harmony-search-based variable-strength t-way testing strategy with constraints support|Although useful, AI-based variable strength t-way strategies are lacking in terms of the support for high interaction strength. Additionally, most AI-based strategies generally do not address the support for constraints. Addressing the aforementioned issues, this paper elaborates the design, implementation, and evaluation of a novel variable-strength-based on harmony search algorithm, called Harmony Search Strategy (HSS).
54|6||A SysML-based approach to traceability management and design slicing in support of safety certification: Framework, tool support, and case studies|Traceability is one of the basic tenets of all safety standards and a key prerequisite for software safety certification. In the current state of practice, there is often a significant traceability gap between safety requirements and software design. Poor traceability, in addition to being a non-compliance issue on its own, makes it difficult to determine whether the design fulfills the safety requirements, mainly because the design aspects related to safety cannot be clearly identified.
54|6||Special section on engineering complex software systems through multi-agent systems and simulation|
54|6||On the combination of top-down and bottom-up methodologies for the design of coordination mechanisms in self-organising systems|In resource-flow systems, e.g. production lines, agents are processing resources by applying capabilities to them in a given order. Such systems benefit from self-organization as they become easier to manage and more robust against failures. In this paper, we demonstrate the conception of a decentralized coordination process for resource-flow systems and its integration into an agent-based software system. This process restores a system’s functionality after a failure by propagating information about the error through the system until a fitting agent is found that is able to perform the required function. The mechanism has been designed by combining a top-down design approach for self-organizing resource-flow system and a systemic development framework for the development of decentralized, distributed coordination processes. Using the latter framework, a process is designed and integrated in a system realization that follows the former conceptual model. Evaluations of convergence as well as performance of the mechanism and the required amount of redundancy of the system are performed by simulations.
54|6||ELDAMeth: An agent-oriented methodology for simulation-based prototyping of distributed agent systems|In application domains, such as distributed information retrieval, content management and distribution, e-Commerce, the agent-based computing paradigm has been demonstrated to be effective for the analysis, design and implementation of distributed software systems. In particular, several agent-oriented methodologies, incorporating suitable agent models, frameworks and tools, have been to date defined to support the development lifecycle of distributed agent systems (DAS). However, few of them provide effective validation methods to analyze design objects at different degrees of refinement before their actual implementation and deployment. In this paper, ELDAMeth, a simulation-based methodology for DAS, which enables rapid prototyping based on visual programming, validation, and automatic code generation for JADE-based DAS, is presented. ELDAMeth can be used both stand-alone for the modeling and evaluation of DAS and coupled with other agent-oriented methodologies for enhancing them with simulation-based validation. In particular, the proposed methodology, which is based on the ELDA (Event-driven Lightweight Distilled StateCharts-based Agents) agent model, provides key programming abstractions (event-driven computation, multi-coordination, and coarse-grained strong mobility) very suitable for highly dynamic distributed computing and is supported by a CASE tool-driven iterative process seamlessly covering the detailed design, simulation, and implementation phases of DAS. A simple yet effective case study in the distributed information retrieval domain is used to illustrate the proposed methodology.
54|6||On the engineering of agent-based simulations of social activities with social networks|Models of how people move around cities play a role in making decisions about urban and land-use planning. Previous models have been based on space and time, and have neglected the social aspect of travel. Recent work on agent-based modelling shows promise as a new approach, especially for models with both social and spatial elements.
54|6||Generating inspiration for agent design by reinforcement learning|
54|7|http://www.sciencedirect.com/science/journal/09505849/54/7|Software quality trade-offs: A systematic map|Software quality is complex with over investment, under investment and the interplay between aspects often being overlooked as many researchers aim to advance individual aspects of software quality.
54|7||Tools used in Global Software Engineering: A systematic mapping review|This systematic mapping review is set in a Global Software Engineering (GSE) context, characterized by a highly distributed environment in which project team members work separately in different countries. This geographic separation creates specific challenges associated with global communication, coordination and control.
54|7||Problems and their mitigation in system and software architecting|Today, software and embedded systems act as enablers for developing new functionality in traditional industries such as the automotive, process automation, and manufacturing automation domains. This differs from 25–30 years ago when these systems where based on electronics and electro-mechanical solutions. The architecture of the embedded system and of the software is important to ensure the qualities of these applications. However, the effort of designing and evolving the architecture is in practice often neglected during system development, whilst development efforts are centered on implementing new functionality.
54|7||Interactive specification and verification of behavioral adaptation contracts|
54|7||Improving the effectiveness of test suite reduction for user-session-based testing of web applications|Test suite reduction is the problem of creating and executing a set of test cases that are smaller in size but equivalent in effectiveness to an original test suite. However, reduced suites can still be large and executing all the tests in a reduced test suite can be time consuming.
54|7||How well does test case prioritization integrate with statistical fault localization?|Effective test case prioritization shortens the time to detect failures, and yet the use of fewer test cases may compromise the effectiveness of subsequent fault localization.
54|7||Towards a framework to characterize ubiquitous software projects|Ubiquitous Computing (or UbiComp) represents a paradigm in which information processing is thoroughly integrated into everyday objects and activities. From a Software Engineering point of view this development scenario brings new challenges in tailoring or building software processes, impacting current software technologies. However, it has not yet been explicitly shown how to characterize a software project with the perception of ubiquitous computing.
54|7||Utilizing architectural styles to enhance the adaptation support of middleware platforms|Modern middleware platforms provide the applications deployed on top of them with facilities for their adaptation. However, the level of adaptation support provided by the state-of-the-art middleware solutions is often limited to dynamically loading and off-loading of software components. Therefore, it is left to the application developers to handle the details of change such that the system’s consistency is not jeopardized.
54|7||Corrigendum to: âA systematic mapping study of software product lines testingâ [Inf. Softw. Technol. 53 (5) (2011) 407â423]|
54|8|http://www.sciencedirect.com/science/journal/09505849/54/8|Editorial: Voice of the editorial board|
54|8||Three empirical studies on the agreement of reviewers about the quality of software engineering experiments|During systematic literature reviews it is necessary to assess the quality of empirical papers. Current guidelines suggest that two researchers should independently apply a quality checklist and any disagreements must be resolved. However, there is little empirical evidence concerning the effectiveness of these guidelines.
54|8||Evaluating prediction systems in software project estimation|Software engineering has a problem in that when we empirically evaluate competing prediction systems we obtain conflicting results.
54|8||A systematic review and an expert survey on capabilities supporting multi product lines|Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing MPLs the research area is still quite fragmented.
54|8||Challenges of shared decision-making: A multiple case study of agile software development|Agile software development changes the nature of collaboration, coordination, and communication in software projects.
54|8||Exploratory case study research: Outsourced project failure|IT plays an increasingly strategic role in the business performance of organizations, however, the development of strategic IT systems involves a high degree of risk and outsourcing the development of such systems increases the risk.
54|8||Critical role of measures in decision processes: Managerial and technical measures in the context of large software development organizations|Today, many software development organizations struggle to establish measurement programs to monitor their projects, products and units. After overcoming the initial threshold of establishing the measurement program organizations stand before the questions of which measures should be collected in order to lead to actions or at least effectively trigger decision processes.
54|8||On generating mutants for AspectJ programs|Mutation analysis has been widely used in research studies to evaluate the effectiveness of test suites and testing techniques. Faulty versions (i.e., mutants) of a program are generated such that each mutant contains one seeded fault. The mutation score provides a measure of effectiveness.
54|9|http://www.sciencedirect.com/science/journal/09505849/54/9|Mutation based test case generation via a path selection strategy|Generally, mutation analysis has been identified as a powerful testing method. Researchers have shown that its use as a testing criterion exercises quite thoroughly the system under test while it achieves to reveal more faults than standard structural testing criteria. Despite its potential, mutation fails to be adopted in a widespread practical use and its popularity falls significantly short when compared with other structural methods. This can be attributed to the lack of thorough studies dealing with the practical problems introduced by mutation and the assessment of the effort needed when applying it. Such an incident, masks the real cost involved preventing the development of easy and effective to use strategies to circumvent this problem.
54|9||Issue-based variability management|Variability management is a key activity in software product line engineering. This paper focuses on managing rationale information during the decision-making activities that arise during variability management. By decision-making we refer to systematic problem solving by considering and evaluating various alternatives. Rationale management is a branch of science that enables decision-making based on the argumentation of stakeholders while capturing the reasons and justifications behind these decisions.
54|9||Composing web services enacted by autonomous agents through agent-centric contract net protocol|Agents are considered as one of the fundamental technologies underlying open and dynamic systems that are largely enabled by the semantic web and web services. Recently, there is a trend to introduce the notion of autonomy empowered by agents into web services. However, it has been argued that the characteristics of autonomy will make agents become available intermittently and behave variedly over time, which therefore increase the complexity on devising mechanisms for composing services enacted by autonomous agents.
54|9||Making the leap to a software platform strategy: Issues and challenges|While there are many success stories of achieving high reuse and improved quality using software platforms, there is a need to investigate the issues and challenges organizations face when transitioning to a software platform strategy.
54|9||The role of social interaction in software effort estimation: Unpacking the âmagic stepâ between reasoning and decision-making|Software effort estimation is a core task regarding planning, budgeting and controlling software development projects. However, providing accurate effort estimates is challenging. Estimation work is increasingly group based, and to support it, there is a need to reveal how work practices are carried out as collaborative efforts.
54|9||An advanced approach for modeling and detecting software vulnerabilities|Passive testing is a technique in which traces collected from the execution of a system under test are examined for evidence of flaws in the system.
54|9||The Pro-PD Process Model for Product Derivation within software product lines|The derivation of products from a software product line is a time consuming and expensive activity. Despite recognition that an effective process could alleviate many of the difficulties associated with product derivation, existing approaches have different scope, emphasise different aspects of the derivation process and are frequently too specialised to serve as a general solution.
54|9||Evaluation of the Pattern-based method for Secure Development (PbSD): A controlled experiment|Security in general, and database protection from unauthorized access in particular, are crucial for organizations. Although it has been long accepted that the important system requirements should be considered from the early stages of the development process, non-functional requirements such as security tend to get neglected or dealt with only at later stages of the development process.
55|1|http://www.sciencedirect.com/science/journal/09505849/55/1|A systematic review of software robustness|With the increased use of software for running key functions in modern society it is of utmost importance to understand software robustness and how to support it. Although there have been many contributions to the field there is a lack of a coherent and summary view.
55|1||An architectural model for software testing lesson learned systems|Software testing is a key aspect of software reliability and quality assurance in a context where software development constantly has to overcome mammoth challenges in a continuously changing environment. One of the characteristics of software testing is that it has a large intellectual capital component and can thus benefit from the use of the experience gained from past projects. Software testing can, then, potentially benefit from solutions provided by the knowledge management discipline. There are in fact a number of proposals concerning effective knowledge management related to several software engineering processes.
55|1||Reasoning with contextual requirements: Detecting inconsistency and conflicts|The environment in which the system operates, its context, is variable. The autonomous ability of a software to adapt to context has to be planned since the requirements analysis stage as a strong mutual influence between requirements and context does exist. On the one hand, context is a main factor to decide whether to activate a requirement, the applicable alternatives to meet an activated requirement as well as their qualities. On the other hand, the system actions to reach requirements could cause changes in the context.
55|1||A pattern-based approach for the verification of business process descriptions|
55|1||Model-driven performance analysis of rule-based domain specific visual models|
55|1||Guest Editorial: Special section of the best papers from the 2nd International Symposium on Search Based Software Engineering 2010|
55|1||AUSTIN: An open source tool for search based software testing of C programs|Despite the large number of publications on Search-Based Software Testing (SBST), there remain few publicly available tools. This paper introduces AUSTIN, a publicly available open source SBST tool for the C language.1 The paper is an extension of previous work [1]. It includes a new hill climb algorithm implemented in AUSTIN and an investigation into the effectiveness and efficiency of different pointer handling techniques implemented by AUSTIN’s test data generation algorithms.
55|1||Empirical evaluation of search based requirements interaction management|Requirements optimization has been widely studied in the Search Based Software Engineering (SBSE) literature. However, previous approaches have not handled requirement interactions, such as the dependencies that may exist between requirements, and, or, precedence, cost- and value-based constraints.
55|1||An identification of program factors that impact crossover performance in evolutionary test input generation for the branch coverage of C programs|Context: Genetic Algorithms are a popular search-based optimisation technique for automatically generating test inputs for structural coverage of a program, but there has been little work investigating the class of programs for which they will perform well.
55|1||Interactive requirements prioritization using a genetic algorithm|The order in which requirements are implemented affects the delivery of value to the end-user, but it also depends on technical constraints and resource availability. The outcome of requirements prioritization is a total ordering of requirements that best accommodates the various kinds of constraints and priorities. During requirements prioritization, some decisions on the relative importance of requirements or the feasibility of a given implementation order must necessarily resort to a human (e.g., the requirements analyst), possessing the involved knowledge.
55|10|http://www.sciencedirect.com/science/journal/09505849/55/10|Graphical user interface (GUI) testing: Systematic mapping and repository|GUI testing is system testing of a software that has a graphical-user interface (GUI) front-end. Because system testing entails that the entire software system, including the user interface, be tested as a whole, during GUI testing, test cases—modeled as sequences of user input events—are developed and executed on the software by exercising the GUI’s widgets (e.g., text boxes and clickable buttons). More than 230 articles have appeared in the area of GUI testing since 1991.
55|10||The state of the art in automated requirements elicitation|In large software development projects a huge number of unstructured text documents from various stakeholders becomes available and needs to be analyzed and transformed into structured requirements. This elicitation process is known to be time-consuming and error-prone when performed manually by a requirements engineer. Consequently, substantial research has been done to automate the process through a plethora of tools and technologies.
55|10||AREION: Software effort estimation based on multiple regressions with adaptive recursive data partitioning|Along with expert judgment, analogy-based estimation, and algorithmic methods (such as Function point analysis and COCOMO), Least Squares Regression (LSR) has been one of the most commonly studied software effort estimation methods. However, an effort estimation model using LSR, a single LSR model, is highly affected by the data distribution. Specifically, if the data set is scattered and the data do not sit closely on the single LSR model line (do not closely map to a linear structure) then the model usually shows poor performance. In order to overcome this drawback of the LSR model, a data partitioning-based approach can be considered as one of the solutions to alleviate the effect of data distribution. Even though clustering-based approaches have been introduced, they still have potential problems to provide accurate and stable effort estimates.
55|10||An object-oriented implementation of concurrent and hierarchical state machines|State machine diagrams are a powerful means to describe the behavior of reactive systems. Unfortunately, the implementation of state machines is difficult, because state machine concepts, like states, events and transitions, are not directly supported in commonly used programming languages. Most of the implementation approaches known so far have one or more serious drawbacks: they are difficult to understand and maintain, lack in performance, depend on the properties of a specific programming language or do not implement the more advanced state machine features like hierarchy, concurrency or history.
55|10||An investigation of âbuild vs. buyâ decision for software acquisition by small to medium enterprises|The prevalence of computing and communication technologies, combined with the availability of sophisticated and highly specialised software packages from software vendors has made package acquisition a viable option for many organisations. While some research has addressed the factors that influence the selection of the software acquisition method in large organisations, little is known about the factors affecting SMEs.
55|10||Context aware exception handling in business process execution language|Fault handling represents a very important aspect of business process functioning. However, fault handling has thus far been solved statically, requiring the definition of fault handlers and handling logic to be defined at design time, which requires a great deal of effort, is error-prone and relatively difficult to maintain and extend. It is sometimes even impossible to define all fault handlers at design time.
55|10||Predicting SQL injection and cross site scripting vulnerabilities through mining input sanitization patterns|SQL injection (SQLI) and cross site scripting (XSS) are the two most common and serious web application vulnerabilities for the past decade. To mitigate these two security threats, many vulnerability detection approaches based on static and dynamic taint analysis techniques have been proposed. Alternatively, there are also vulnerability prediction approaches based on machine learning techniques, which showed that static code attributes such as code complexity measures are cheap and useful predictors. However, current prediction approaches target general vulnerabilities. And most of these approaches locate vulnerable code only at software component or file levels. Some approaches also involve process attributes that are often difficult to measure.
55|10||Test case selection for black-box regression testing of database applications|This paper presents an approach for selecting regression test cases in the context of large-scale database applications. We focus on a black-box (specification-based) approach, relying on classification tree models to model the input domain of the system under test (SUT), in order to obtain a more practical and scalable solution. We perform an experiment in an industrial setting where the SUT is a large database application in Norway’s tax department.
55|10||Towards a simplified definition of Function Points|The measurement of Function Points is based on Base Functional Components. The process of identifying and weighting Base Functional Components is hardly automatable, due to the informality of both the Function Point method and the requirements documents being measured. So, Function Point measurement generally requires a lengthy and costly process.
55|10||A study of subgroup discovery approaches for defect prediction|Although many papers have been published on software defect prediction techniques, machine learning approaches have yet to be fully explored.
55|10||Comparing the comprehensibility of requirements models expressed in Use Case and Tropos: Results from a family of experiments|Over the years, several modeling languages for requirements have been proposed. These languages employ different conceptual approaches, including scenario-based and goal-oriented ones. Empirical studies providing evidence about requirements model comprehensibility are rare, especially when addressing languages that belong to different modeling approaches.
55|11|http://www.sciencedirect.com/science/journal/09505849/55/11|Combining service-orientation and software product line engineering: A systematic mapping study|Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems.
55|11||Software evolution visualization: A systematic mapping study|Software evolution is an important topic in software engineering. It generally deals with large amounts of data, as one must look at whole project histories as opposed to their current snapshot. Software visualization is the field of software engineering that aims to help people to understand software through the use of visual resources. It can be effectively used to analyze and understand the large amount of data produced during software evolution.
55|11||Enforcement of entailment constraints in distributed service-based business processes|A distributed business process is executed in a distributed computing environment. The service-oriented architecture (SOA) paradigm is a popular option for the integration of software services and execution of distributed business processes. Entailment constraints, such as mutual exclusion and binding constraints, are important means to control process execution. Mutually exclusive tasks result from the division of powerful rights and responsibilities to prevent fraud and abuse. In contrast, binding constraints define that a subject who performed one task must also perform the corresponding bound task(s).
55|11||AiOLoS: A model for assessing organizational learning in software development organizations|In an industry in which technological developments are rapid, in order to keep up with the continuously increasing competition and to obtain competitive advantage, the software development organizations (SDOs) need to obtain the correct knowledge, use it efficiently and pass it to future projects evolving it accordingly.
55|11||A formal framework for software product lines|
55|11||Towards the automatic and optimal selection of risk treatments for business processes using a constraint programming approach|The use of Business Process Management Systems (BPMS) has emerged in the IT arena for the automation of business processes. In the majority of cases, the issue of security is overlooked by default in these systems, and hence the potential cost and consequences of the materialization of threats could produce catastrophic loss for organizations. Therefore, the early selection of security controls that mitigate risks is a real and important necessity. Nevertheless, there exists an enormous range of IT security controls and their configuration is a human, manual, time-consuming and error-prone task. Furthermore, configurations are carried out separately from the organization perspective and involve many security stakeholders. This separation makes difficult to ensure the effectiveness of the configuration with regard to organizational requirements.
55|11||Modeling optimal release policy under fuzzy paradigm in imperfect debugging environment|In this study, a software optimal release time with cost-reliability criteria has been discussed in an imperfect debugging environment.
55|11||Is lines of code a good measure of effort in effort-aware models?|Effort-aware models, e.g., effort-aware bug prediction models aim to help practitioners identify and prioritize buggy software locations according to the effort involved with fixing the bugs. Since the effort of current bugs is not yet known and the effort of past bugs is typically not explicitly recorded, effort-aware bug prediction models are forced to use approximations, such as the number of lines of code (LOC) of the predicted files.
55|11||Efficient software clustering technique using an adaptive and preventive dendrogram cutting approach|Software clustering is a key technique that is used in reverse engineering to recover a high-level abstraction of the software in the case of limited resources. Very limited research has explicitly discussed the problem of finding the optimum set of clusters in the design and how to penalize for the formation of singleton clusters during clustering.
55|11||Defining a test coverage criterion for model-level testing of FBD programs|The Programmable Logic Controller (PLC) is being integrated into the automation and control of computer systems in safety–critical domains at an increasing rate. Thoroughly testing such software to ensure safety is crucial. Function Block Diagram (FBD) is a popular data-flow programming language for PLC. Current practice often involves translating an FBD program into an equivalent C program for testing. Little research has been conducted on coverage of direct testing a data-flow program, such as an FBD program, at the model level. There are no commonly accepted structural test coverage criteria for data-flow programs. The objective of this study is to develop effective structural test coverage criterion for testing model-level FBD programs. The proposed testing scheme can be used to detect mutation errors at the logical function level.
55|11||Object-oriented class maintainability prediction using internal quality attributes|Class maintainability is the likelihood that a class can be easily modified. Before releasing an object-oriented software system, it is impossible to know with certainty when, where, how, and how often a class will be modified. At that stage, this likelihood can be estimated using the internal quality attributes of a class, which include cohesion, coupling, and size. To reduce the future class maintenance efforts and cost, developers are encouraged to carefully test and well document low maintainability classes before releasing the object-oriented system.
55|12|http://www.sciencedirect.com/science/journal/09505849/55/12|A systematic review of systematic review process research in software engineering|Many researchers adopting systematic reviews (SRs) have also published papers discussing problems with the SR methodology and suggestions for improving it. Since guidelines for SRs in software engineering (SE) were last updated in 2007, we believe it is time to investigate whether the guidelines need to be amended in the light of recent research.
55|12||Mutation-oriented test data augmentation for GUI software fault localization|Fault localization lies at the heart of program debugging and often proceeds by contrasting the statistics of program constructs executed by passing and failing test cases. A vital issue here is how to obtain these “suitable” test cases. Techniques presented in the literature mostly assume the existence of a large test suite a priori. However, developers often encounter situations where a failure occurs, but where no or no appropriate test suite is available for use to localize the fault.
55|12||Ontological and linguistic metamodelling revisited: A language use approach|Although metamodelling is generally accepted as important for our understanding of software and systems development, arguments about the validity and utility of ontological versus linguistic metamodelling continue.
55|12||Estimating software testing complexity|Complexity measures provide us some information about software artifacts. A measure of the difficulty of testing a piece of code could be very useful to take control about the test phase.
55|12||BPELDebugger: An effective BPEL-specific fault localization framework|Business Process Execution Language (BPEL) is a widely recognized executable service composition language, which is significantly different from typical programming languages in both syntax and semantics, and especially shorter in program scale. How to effectively locate faults in BPEL programs is an open and challenging problem.
55|12||When agile meets the enterprise|While renowned agile methods like XP and Scrum were initially intended for projects with small teams, traditional enterprise environments, i.e. environments where plan-driven development is prevalent, have also become attracted by the promises of a faster time to market through agility. Agile software development methods emphasize lightweight software development. Projects within enterprise environments, however, are typically confronted with a large and complex IT landscape, where mission-critical information is at play whose criticality requires prudence regarding design and development. In many an organization, both approaches are used simultaneously.
55|12||A metric towards evaluating understandability of state machines: An empirical study|State machines are widely used to describe the dynamic behavior of objects, components, and systems. As a communication tool between various stakeholders, it is essential that state machines be easily and correctly comprehensible. Poorly understood state machines can lead to misunderstandings and communication overhead, thus adversely affecting the quality of the final product. Nevertheless, there is a lack of measurement research for state machines.
55|12||A formal approach for run-time verification of web applications using scope-extended LTL|
55|12||Comparison and integration of genetic algorithms and dynamic symbolic execution for security testing of cross-site scripting vulnerabilities|
55|12||To what extent can maintenance problems be predicted by code smell detection? â An empirical study|Code smells are indicators of poor coding and design choices that can cause problems during software maintenance and evolution.
55|2|http://www.sciencedirect.com/science/journal/09505849/55/2|MDD vs. traditional software development: A practitionerâs subjective perspective|Today’s project managers have a myriad of methods to choose from for the development of software applications. However, they lack empirical data about the character of these methods in terms of usefulness, ease of use or compatibility, all of these being relevant variables to assess the developer’s intention to use them.
55|2||Action-based discovery of satisfying subsets: A distributed method for model correction|Understanding and resolving counterexamples in model checking is a difficult task that often takes a significant amount of resources and many rounds of regression model checking after any fix. As such, it is desirable to have algorithmic methods that correct finite-state models when their model checking for a specific property fails without undermining the correctness of the rest of the properties, called the model correction problem.
55|2||A process for managing interaction between experimenters to get useful similar replications|A replication is the repetition of an experiment. Several efforts have been made to adopt replication as a common practice in software engineering. There are different types of replications, depending on their purpose. Similar replications keep the experimental conditions as alike as possible to the original ones. External similar replications, where the replicating experimenters are not the same people as the original experimenters, have been a stumbling block. Several attempts at combining the results of replications have resulted in failure. Software engineering does not appear to be well suited to such replications, because it works with complex experimentally immature contexts. Software engineering settings have a large number of variables, and the role that many of them play is unknown. A successful (or useful) similar replication helps to better understand the phenomenon under study by verifying results and/or identifying contextual variables that could influence (or not) the results, through the combination of experimental results.
55|2||How influential has academic and industrial research been in current software life cycles? A retrospective analysis of four mainstream activities|Knowledge transfer is an important responsibility of universities and research institutes as part of their contribution to society. In the field of software engineering, several studies have been performed to show the influence of research in popular technologies such as middleware systems. However, there is no scholarly analysis of the influence that research has had in mainstream activities of current software life cycles.
55|2||Probabilistic size proxy for software effort prediction: A framework|Software effort prediction is an important and challenging activity that takes place during the early stages of software development, where costing is needed. Software size estimate is one of the most popular inputs for software effort prediction models. Accordingly, providing a size estimate with good accuracy early in the lifecycle is very important; it is equally challenging too. Estimates that are computed early in the development lifecycle, when it is needed the most, are typically associated with uncertainty. However, none of the prominent software effort prediction techniques or software size metrics addresses this issue satisfactorily. In this paper, we propose a framework for developing probabilistic size proxies for software effort prediction using information from conceptual UML models created early in the software development lifecycle. The framework accounts for uncertainty in software size and effort prediction by providing the estimate as a probability density function instead of a certain value. We conducted a case study using open source datasets and the results were encouraging.
55|2||Usage and testability of AOP: An empirical study of AspectJ|Back in 2001, the MIT announced aspect-oriented programming as a key technology in the next 10 years. Nowadays, 10 years later, AOP is still not widely adopted.
55|2||Equality in cumulative voting: A systematic review with an improvement proposal|Prioritization is an essential part of requirements engineering, software release planning and many other software engineering disciplines. Cumulative Voting (CV) is known as a relatively simple method for prioritizing requirements on a ratio scale. Historically, CV has been applied in decision-making in government elections, corporate governance, and forestry. However, CV prioritization results are of a special type of data—compositional data.
55|2||Enhancing software reliability estimates using modified adaptive testing|Most software reliability models are based on a binary notion of correctness, i.e. “successful” or “failed.” However, in several instances, it is important to account of failure severity to obtain more descriptive and accurate estimates of the reliability of the software.
55|2||Automated generation of test oracles using a model-driven approach|Software development time has been reduced with new development tools and paradigms, testing must accompany these changes. In order to release software products in a timely manner as well as to minimise the impact of possible errors introduced during maintenance interventions, testing automation has become a central goal. Whilst research has produced significant results in test case generation and tools for test case (re)-execution, one of the most important open problems in testing is the automation of oracle generation. The oracle decides whether the program under test has or has not behaved correctly and then issues a pass/fail verdict. In most cases, writing the oracle is a time-consuming activity that, moreover, is manual in most cases.
55|2||Variability in quality attributes of service-based software systems: A systematic literature review|Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices.
55|2||Systematic scenario test case generation for nuclear safety systems|The current validation tests for nuclear software are routinely performed by random testing, which leads to uncertain test coverage. Moreover, validation tests should directly verify the system’s compliance with the original user’s needs. Unlike current model-based testing methods, which are generally based on requirements or design models, the proposed model is derived from the original user’s needs in text through domain-specific ontology, and then used to generate validation tests systematically.
55|2||System integration by developing adapters using a database abstraction|Large software systems are usually developed by integrating several smaller systems, which may have been developed independently. The integration of such systems often requires the development of a custom adapter (sometimes called mediator or glue logic) for bridging any technical incompatibilities between the systems.
55|2||A visual token-based formalization of BPMN 2.0 based on in-place transformations|The Business Process Model and Notation (BPMN) standard informally defines a precise execution semantics. It defines how process instances should be updated in a model during execution. Existing formalizations of the standard are incomplete and rely on mappings to other languages.
55|2||Aspect-oriented model-driven code generation: A systematic mapping study|Model-driven code generation is being increasingly applied to enhance software development from perspectives of maintainability, extensibility and reusability. However, aspect-oriented code generation from models is an area that is currently underdeveloped.
55|2||Interpretative case studies on agile team productivity and management|The management of software development productivity is a key issue in software organizations, where the major drivers are lower cost and shorter time-to-market. Agile methods, including Extreme Programming and Scrum, have evolved as “light” approaches that simplify the software development process, potentially leading to increased team productivity. However, little empirical research has examined which factors do have an impact on productivity and in what way, when using agile methods.
55|2||Constraints for the design of variability-intensive service-oriented reference architectures â An industrial case study|Service-oriented architecture has become a widely used concept in software industry. However, we currently lack support for designing variability-intensive service-oriented systems. Such systems could be used in different environments, without the need to design them from scratch. To support the design of variability-intensive service-oriented systems, reference architectures that facilitate variability in instantiated service-oriented architectures can help.
55|2||Guest Editorial for Special Section from Component-based Software Engineering (CBSE) 2011|
55|2||Testing component compatibility in evolving configurations|Software components are increasingly assembled from other components. Each component may further depend on others, and each may have multiple active versions. The total number of configurations—combinations of components and their versions—in use can be very large. Moreover, components are constantly being enhanced and new versions are being released. Component developers, therefore, spend considerable time and effort doing compatibility testing—determining whether their components can be built correctly for all deployed configurations–both for existing active component versions and new releases. In previous work we developed Rachet, a distributed, cache-aware mechanism to support large-scale compatibility testing of component-based software with a fixed set of component versions.
55|2||A modular package manager architecture|The success of modern software distributions in the Free and Open Source world can be explained, among other factors, by the availability of a large collection of software packages and the possibility to easily install and remove those components using state-of-the-art package managers. However, package managers are often built using a monolithic architecture and hard-wired and ad-hoc dependency solvers implementing some customized heuristics.
55|2||Efficient and deterministic application deployment in component-based enterprise distributed real-time and embedded systems|Component-based middleware, such as the Lightweight CORBA Component Model, is increasingly used to implement enterprise distributed real-time and embedded (DRE) systems. In addition to supporting the quality-of-service (QoS) requirements of individual DRE systems, component technologies must also support bounded latencies when effecting deployment changes to DRE systems in response to changing environmental conditions and operational requirements.
55|3|http://www.sciencedirect.com/science/journal/09505849/55/3|Guest Editorsâ Introduction: Special Issue on Software Reuse and Product Lines|
55|3||Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption|A software product line is a family of related software products, typically created from a set of common assets. Users select features to derive a product that fulfills their needs. Users often expect a product to have specific non-functional properties, such as a small footprint or a bounded response time. Because a product line may have an exponential number of products with respect to its features, it is usually not feasible to generate and measure non-functional properties for each possible product.
55|3||Model-based verification of quantitative non-functional properties for software product lines|Evaluating quality attributes of a design model in the early stages of development can significantly reduce the cost and risks of developing a low quality product. To make this possible, software designers should be able to predict quality attributes by reasoning on a model of the system under development. Although there exists a variety of quality-driven analysis techniques for software systems, only a few work address software product lines. This paper describes how probabilistic model checking techniques and tools can be used to verify non-functional properties of different configurations of a software product line. We propose a model-based approach that enables software engineers to assess their design solutions for software product lines in the early stages of development. Furthermore, we discuss how the analysis time can be surprisingly reduced by applying parametric model checking instead of classic model checking. The results show that the parametric approach is able to substantially alleviate the verification time and effort required to analyze non-functional properties of software product lines.
55|3||Trusted Product Lines|The paper addresses the use of a Software Product Line approach in the context of developing software for a high-integrity, regulated domain such as civil aerospace. The success of a Software Product Line approach must be judged on whether useful products can be developed more effectively (lower cost, reduced schedule) than with traditional single-system approaches. When developing products for regulated domains, the usefulness of the product is critically dependent on the ability of the development process to provide approval evidence for scrutiny by the regulating authority.
55|3||Improving software product line configuration: A quality attribute-driven approach|During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product’s intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task.
55|3||Architectural evolution of FamiWare using cardinality-based feature models|Ambient Intelligence systems domain is an outstanding example of modern systems that are in permanent evolution, as new devices, technologies or facilities are continuously appearing. This means it would be desirable to have a mechanism that helps with the propagation of evolution changes in deployed systems.
55|3||Test overlay in an emerging software product line â An industrial case study|In large software organizations with a product line development approach, system test planning and scope selection is a complex task. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of redundant testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests.
55|3||Experience from model and software reuse in aircraft simulator product line engineering|“Reuse” and “Model Based Development” are two prominent trends for improving industrial development efficiency. Product lines are used to reduce the time to create product variants by reusing components. The model based approach provides the opportunity to enhance knowledge capture for a system in the early stages in order to be reused throughout its lifecycle. This paper describes how these two trends are combined to support development and support of a simulator product line for the SAAB 39 Gripen fighter aircraft.
55|3||SimPL: A product-line modeling methodology for families of integrated control systems|Integrated control systems (ICSs) are heterogeneous systems where software and hardware components are integrated to control and monitor physical devices and processes. A family of ICSs share the same software code base, which is configured differently for each product to form a unique installation. Due to the complexity of ICSs and inadequate automation support, product configuration in this context is typically error-prone and costly.
55|3||MOD2-SCM: A model-driven product line for software configuration management systems|Software Configuration Management (SCM) is the discipline of controlling the evolution of large and complex software systems. Over the years many different SCM systems sharing similar concepts have been implemented from scratch. Since these concepts usually are hard-wired into the respective program code, reuse is hardly possible.
55|4|http://www.sciencedirect.com/science/journal/09505849/55/4|Concept location using program dependencies and information retrieval (DepIR)|The functionality of a software system is most often expressed in terms of concepts from its problem or solution domains. The process of finding where these concepts are implemented in the source code is known as concept location and it is a prerequisite of software change.
55|4||Successful extreme programming: Fidelity to the methodology or good teamworking?|Developing a theory of agile technology, in combination with empirical work, must include assessing its performance effects, and whether all or some of its key ingredients account for any performance advantage over traditional methods. Given the focus on teamwork, is the agile technology what really matters, or do general team factors, such as cohesion, primarily account for a team’s success? Perhaps the more specific software engineering team factors, for example the agile development method’s collective ownership and code management, are decisive.
55|4||Suitability assessment framework of agent-based software architectures|A common distributed intelligent system architecture is Multi Agent Systems (MASs). Creating systems with this architecture has been recently supported by Agent Oriented Software Engineering (AOSE) methodologies. But two questions remain: how do we determine the suitability of a MAS implementation for a particular problem? And can this be determined without AOSE expertise?
55|4||Vertical software industry evolution: The impact of software costs and limited customer base|Software systems are commonly used in a variety of industries as a means of automating organizational business processes. Initially, such software is often developed in-house by the vertical organizations possibly with the support of professional IT service providers; however, in many cases, internally developed software is eventually replaced with the software products provided by independent software vendors. These vendors often use license fees to recover their software development investments, as well as to gain some margin. However, if the vendor’s customer base for a specific type of software is limited, then either the license fees are too high and hence the customers may prefer to develop the software internally, or the margin has to be decreased. As a result, the market for software products of that type may not materialize.
55|4||Applying MDE to the (semi-)automatic development of model transformations|Model transformations play a key role in any software development project based on Model-Driven Engineering principles. However, despite the inherent complexity of developing model transformations, little attention has been paid to the application of MDE principles to the development of model transformations.
55|4||Guest Editorial: Special Section on International Conference on Program Comprehension, 2011|
55|4||Aspect-orientation is a rewarding investment into future code changes â As long as the aspects hardly change|Aspect-Oriented Programming (AOP) is often described as a technique which improves the resulting software’s modularity. However, previous experiments seem to indicate that AOP is a technique which potentially increases the development or maintenance time. A possible reason why previous experiments were not able to show such a benefit is that those experiments did not consider situations where AOP has its strength: situations where aspects change.
55|4||Applying a smoothing filter to improve IR-based traceability recovery processes: An empirical investigation|Traceability relations among software artifacts often tend to be missing, outdated, or lost. For this reason, various traceability recovery approaches—based on Information Retrieval (IR) techniques—have been proposed. The performances of such approaches are often influenced by “noise” contained in software artifacts (e.g., recurring words in document templates or other words that do not contribute to the retrieval itself).
55|4||Answering software evolution questions: An empirical evaluation|Developers often need to find answers to questions regarding the evolution of a system when working on its code base. While their information needs require data analysis pertaining to different repository types, the source code repository has a pivotal role for program comprehension tasks. However, the coarse-grained nature of the data stored by commit-based software configuration management systems often makes it challenging for a developer to search for an answer.
55|5|http://www.sciencedirect.com/science/journal/09505849/55/5|Application of knowledge-based approaches in software architecture: A systematic mapping study|Knowledge management technologies have been employed across software engineering activities for more than two decades. Knowledge-based approaches can be used to facilitate software architecting activities (e.g., architectural evaluation). However, there is no comprehensive understanding on how various knowledge-based approaches (e.g., knowledge reuse) are employed in software architecture.
55|5||A taxonomy of design methods process models|
55|5||A survey of experienced user perceptions about software design patterns|Although the concept of the software design pattern is well-established, there is relatively little empirical knowledge about the patterns that experienced users consider to be most valuable.
55|5||Supporting the verification of compliance to safety standards via model-driven engineering: Approach, tool-support and empirical validation|Many safety–critical systems are subject to safety certification as a way to provide assurance that these systems cannot unduly harm people, property or the environment. Creating the requisite evidence for certification can be a challenging task due to the sheer size of the textual standards based on which certification is performed and the amenability of these standards to subjective interpretation.
55|5||Guest Editorial: Special Section from the 11th International Conference on Quality Software (QSIC 2011)|
55|5||Metamorphic slice: An application in spectrum-based fault localization|Because of its simplicity and effectiveness, Spectrum-Based Fault Localization (SBFL) has been one of the popular approaches towards fault localization. It utilizes the execution result of failure or pass, and the corresponding coverage information (such as program slice) to estimate the risk of being faulty for each program entity (such as statement). However, all existing SBFL techniques assume the existence of a test oracle to determine the execution result of a test case. But, it is common that test oracles do not exist, and hence the applicability of SBFL has been severely restricted.
55|5||A general noise-reduction framework for fault localization of Java programs|Existing fault-localization techniques combine various program features and similarity coefficients with the aim of precisely assessing the similarities among the dynamic spectra of these program features to predict the locations of faults. Many such techniques estimate the probability of a particular program feature causing the observed failures. They often ignore the noise introduced by other features on the same set of executions that may lead to the observed failures. It is unclear to what extent such noise can be alleviated.
55|5||On the adoption of MC/DC and control-flow adequacy for a tight integration of program testing and statistical fault localization|Testing and debugging consume a significant portion of software development effort. Both processes are usually conducted independently despite their close relationship with each other. Test adequacy is vital for developers to assure that sufficient testing effort has been made, while finding all the faults in a program as soon as possible is equally important. A tight integration between testing and debugging activities is essential.
55|6|http://www.sciencedirect.com/science/journal/09505849/55/6|Systematic literature reviews in software engineering|
55|6||Obsolete software requirements|Coping with rapid requirements change is crucial for staying competitive in the software business. Frequently changing customer needs and fierce competition are typical drivers of rapid requirements evolution resulting in requirements obsolescence even before project completion.
55|6||Transforming and tracing reused requirements models to home automation models|Model-Driven Software Development (MDSD) has emerged as a very promising approach to cope with the inherent complexity of modern software-based systems. Furthermore, it is well known that the Requirements Engineering (RE) stage is critical for a project’s success. Despite the importance of RE, MDSD approaches commonly leave textual requirements specifications to one side.
55|6||Dynamic profiling-based approach to identifying cost-effective refactorings|Object-oriented software undergoes continuous changes—changes often made without consideration of the software’s overall structure and design rationale. Hence, over time, the design quality of the software degrades causing software aging or software decay. Refactoring offers a means of restructuring software design to improve maintainability. In practice, efforts to invest in refactoring are restricted; therefore, the problem calls for a method for identifying cost-effective refactorings that efficiently improve maintainability. Cost-effectiveness of applied refactorings can be explained as maintainability improvement over invested refactoring effort (cost). For the system, the more cost-effective refactorings are applied, the greater maintainability would be improved. There have been several studies of supporting the arguments that changes are more prone to occur in the pieces of codes more frequently utilized by users; hence, applying refactorings in these parts would fast improve maintainability of software. For this reason, dynamic information is needed for identifying the entities involved in given scenarios/functions of a system, and within these entities, refactoring candidates need to be extracted.
55|6||More testers â The effect of crowd size and time restriction in software testing|The questions of how many individuals and how much time to use for a single testing task are critical in software verification and validation. In software review and usability evaluation contexts, positive effects of using multiple individuals for a task have been found, but software testing has not been studied from this viewpoint.
55|6||Software verification and graph similarity for automated evaluation of studentsâ assignments|The number of students enrolled in universities at standard and on-line programming courses is rapidly increasing. This calls for automated evaluation of students assignments.
55|6||Translation of Z specifications to executable code: Application to the database domain|It is well-known that the use of formal methods in the software development process results in high-quality software products. Having specified the software requirements in a formal notation, the question is how they can be transformed into an implementation. There is typically a mismatch between the specification and the implementation, known as the specification-implementation gap.
55|6||Evaluating test suite characteristics, cost, and effectiveness of FSM-based testing methods|Testing from finite state machines has been investigated due to its well-founded and sound theory as well as its practical application. There has been a recurrent interest in developing methods capable of generating test suites that detect all faults in a given fault domain. However, the proposal of new methods motivates the comparison with traditional methods.
55|6||Guest Editorial for the Special Section on the Euromicro 2011 Conference on Software Engineering and Advanced Applications (SEAA)|
55|6||Retainment policies â A formal framework for change retainment for trace-based model transformations|Model-to-model (M2M) transformations play an important role within model-driven development. Modern M2M approaches support incremental updates to the target model according to changes in the source model(s). Bidirectional transformation approaches even allow to incrementally translate target model changes back to the source model.
55|6||Round-trip support for extra-functional property management in model-driven engineering of embedded systems|In order for model-driven engineering to succeed, automated code generation from models through model transformations has to guarantee that extra-functional properties specified at design level are preserved at code level.
55|6||Empirical evaluation of the effects of mixed project data on learning defect predictors|Defect prediction research mostly focus on optimizing the performance of models that are constructed for isolated projects (i.e. within project (WP)) through retrospective analyses. On the other hand, recent studies try to utilize data across projects (i.e. cross project (CP)) for building defect prediction models for new projects. There are no cases where the combination of within and cross (i.e. mixed) project data are used together.
55|7|http://www.sciencedirect.com/science/journal/09505849/55/7|Empirical studies concerning the maintenance of UML diagrams and their use in the maintenance of code: A systematic mapping study|The Unified Modelling Language (UML) has, after ten years, become established as the de facto standard for the modelling of object-oriented software systems. It is therefore relevant to investigate whether its use is important as regards the costs involved in its implantation in industry being worthwhile.
55|7||Empirical studies on the use of social software in global software development â A systematic mapping study|In Global Software Development (GSD), informal communication and knowledge sharing play an important role. Social Software (SoSo) has the potential to support and foster this key responsibility. Research on the use of SoSo in GSD is still at an early stage: although a number of empirical studies on the usage of SoSo are available in related fields, there exists no comprehensive overview of what has been investigated to date across them.
55|7||Software clone detection: A systematic review|Reusing software by means of copy and paste is a frequent activity in software development. The duplicated code is known as a software clone and the activity is known as code cloning. Software clones may lead to bug propagation and serious maintenance problems.
55|7||Applying Q-methodology to analyse the success factors in GSD|The context of this paper is Global Software Development (GSD) which is a current trend concerning the development of software in a distributed manner throughout different countries. This paradigm has several advantages, but unfortunately there are a number of challenges that hinder projects’ successful development.
55|7||Architecture-based testing of service-oriented applications in distributed systems|Testing distributed service-oriented applications (SOAs) is more challenging than testing monolithic applications since these applications have complex interactions between participant services. Test engineers can observe test results only through a front service that handles request messages sent by test engineers. Message exchanges between participant services are hidden behind the front service and cannot be easily observed or controlled through the front service. For this reason, testing SOAs suffer from limited observability and controllability problem.
55|7||An investigation of how quality requirements are specified in industrial practice|This paper analyses a sub-contractor specification in the mobile handset domain.
55|7||Analyzing an automotive testing process with evidence-based software engineering|Evidence-based software engineering (EBSE) provides a process for solving practical problems based on a rigorous research approach. The primary focus so far was on mapping and aggregating evidence through systematic reviews.
55|7||Quality evaluation of floss projects: Application to ERP systems|The selection and adoption of open source software can significantly influence the competitiveness of organisations. Open source software solutions offer great opportunities for cost reduction and quality improvement, especially for small and medium enterprises that typically have to address major difficulties due to the limited resources available for selecting and adopting a new software system.
55|7||Guest Editorial for Special Section from Empirical Software Engineering & Measurement (ESEM) 2011|
55|7||A comparison of the efficiency and effectiveness of vulnerability discovery techniques|Security vulnerabilities discovered later in the development cycle are more expensive to fix than those discovered early. Therefore, software developers should strive to discover vulnerabilities as early as possible. Unfortunately, the large size of code bases and lack of developer expertise can make discovering software vulnerabilities difficult. A number of vulnerability discovery techniques are available, each with their own strengths.
55|7||Discovering how end-user programmers and their communities use public repositories: A study on Yahoo! Pipes|End-user programmers are numerous, write software that matters to an increasingly large number of users, and face software engineering challenges that are similar to their professionals counterparts. Yet, we know little about how these end-user programmers create and share artifacts in repositories as part of a community.
55|7||Design of an empirical study for comparing the usability of concurrent programming languages|Context: Developing concurrent software has long been recognized as a difficult and error-prone task. To support developers, a multitude of language proposals exist that promise to make concurrent programming easier. Empirical studies are needed to support the claim that a language is more usable than another.
55|7||Team building criteria in software projects: A mix-method replicated study|The internal composition of a work team is an important antecedent of team performance and the criteria used to select team members play an important role in determining team composition. However, there are only a handful of empirical studies about the use of team building criteria in the software industry.
55|7||Systematic reviews in software engineering: An empirical investigation|Systematic Literature Reviews (SLRs) have gained significant popularity among Software Engineering (SE) researchers since 2004. Several researchers have also been working on improving the scientific and methodological infrastructure to support SLRs in SE. We argue that there is also an apparent and essential need for evidence-based body of knowledge about different aspects of the adoption of SLRs in SE.
55|8|http://www.sciencedirect.com/science/journal/09505849/55/8|The use of software product lines for business process management: A systematic literature review|Business Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), BPM and PL may help companies achieve strategic alignment between business and IT.
55|8||A systematic mapping study of web application testing|The Web has had a significant impact on all aspects of our society. As our society relies more and more on the Web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 147 papers in the area of web application testing, which have appeared between 2000 and 2011.
55|8||Software fault prediction metrics: A systematic literature review|Software metrics may be used in fault prediction models to improve software quality by predicting fault location.
55|8||Applying a selection method to choose Quality Attribute Techniques|Software products have requirements on software quality attributes such as safety and performance. Development teams use various specific techniques to achieve these quality requirements. We call these “Quality Attribute Techniques” (QATs). QATs are used to identify, analyse and control potential product quality problems. Although QATs are widely used in practice, there is no systematic approach to represent, select, and integrate them in existing approaches to software process modelling and tailoring.
55|8||FCAâCIA: An approach of using FCA to support cross-level change impact analysis for object oriented Java programs|Software Change Impact Analysis (CIA) is an essential technique in software engineering to identifying the potential influences of a change, or determining change entities to accomplish such a change. The results derived, in many cases, ambiguous for the software maintainers, introduces the problem of unclear starting point of these impacted entities.
55|8||Investigating measurement scales and aggregation methods in SPICE assessment method|This study identified three issues in SPICE (Software Process Assessment and Capability dEtermination) assessment method based on ISO/IEC 15504-2 (Performing an assessment). The issues include a lack of a measurement scale for characterizing the extent to which an outcome (practice) is achieved (implemented) and two shortcomings of the aggregation methods in order to generate a process attribute (PA) rating. Such issues may be weaknesses to the needs of retaining consistent assessment results comparable within and across assessed organizations.
55|8||Static analysis of source code security: Assessment of tools against SAMATE tests|Static analysis tools are used to discover security vulnerabilities in source code. They suffer from false negatives and false positives. A false positive is a reported vulnerability in a program that is not really a security problem. A false negative is a vulnerability in the code which is not detected by the tool.
55|8||Guest editorial for the Special Section on BEST PAPERS from the 2011 conference on Predictive Models in Software Engineering (PROMISE)|
55|8||Predicting failure-proneness in an evolving software product line|Previous work by researchers on 3 years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software.
55|8||Analyzing and handling local bias for calibrating parametric cost estimation models|Parametric cost estimation models need to be continuously calibrated and improved to assure more accurate software estimates and reflect changing software development contexts. Local calibration by tuning a subset of model parameters is a frequent practice when software organizations adopt parametric estimation models to increase model usability and accuracy. However, there is a lack of understanding about the cumulative effects of such local calibration practices on the evolution of general parametric models over time.
55|8||Ensembles and locality: Insight on improving software effort estimation|Ensembles of learning machines and locality are considered two important topics for the next research frontier on Software Effort Estimation (SEE).
55|9|http://www.sciencedirect.com/science/journal/09505849/55/9|Automated reasoning on UML conceptual schemas with derived information and queries|It is critical to ensure the quality of a software system in the initial stages of development, and several approaches have been proposed to ensure that a conceptual schema correctly describes the user’s requirements.
55|9||Using argumentation to evaluate software assurance standards|Many people and organisations rely upon software safety and security standards to provide confidence in software intensive systems. For example, people rely upon the Common Criteria for Information Technology Security Evaluation to establish justified and sufficient confidence that an evaluated information technology product’s contributions to security threats and threat management are acceptable. Is this standard suitable for this purpose?
55|9||ODEP-DPS: Ontology-driven engineering process for the collaborative development of semantic data providing services|Data services are services that handle operations involving the management of data. A problem with data services is that their interfaces are defined by their syntax alone. Consequently, Data Providing Services (DPSs) have been proposed to explicitly define semantics using ontologies for services that only retrieve data. However, the semantic annotations of DPSs are developed as afterthoughts to deployed data services.
55|9||Standardizing the narrative of use cases: A controlled vocabulary of web user tasks|For user interfaces design, the use of appropriate terminology in writing use case narratives may determine the effectiveness of the design process, facilitating communication within multidisciplinary web development teams and leading to agreed designs.
55|9||Determining the effectiveness of three software evaluation techniques through informal aggregation|An accepted fact in software engineering is that software must undergo verification and validation process during development to ascertain and improve its quality level. But there are too many techniques than a single developer could master, yet, it is impossible to be certain that software is free of defects. So, it is crucial for developers to be able to choose from available evaluation techniques, the one most suitable and likely to yield optimum quality results for different products. Though, some knowledge is available on the strengths and weaknesses of the available software quality assurance techniques but not much is known yet on the relationship between different techniques and contextual behavior of the techniques.
55|9||The impact of distributed programming abstractions on application energy consumption|
55|9||Acquiring and sharing tacit knowledge in software development teams: An empirical study|Sharing expert knowledge is a key process in developing software products. Since expert knowledge is mostly tacit, the acquisition and sharing of tacit knowledge along with the development of a transactive memory system (TMS) are significant factors in effective software teams.
55|9||Multidimensional characterization of evolutionary clusters: An experience report|
55|9||The influence of selection bias on effort overruns in software development projects|A potentially important, but neglected, reason for effort overruns in software projects is related to selection bias. Selection bias–induced effort overruns occur when proposals are more likely to be accepted and lead to actual projects when based on effort estimates that are too low rather than on realistic estimates or estimates that are too high. The effect of this bias may be particularly important in bidding rounds, but is potentially relevant in all situations where there is effort or cost-based selection between alternatives.
55|9||Development of Secure XML Data Warehouses with QVT|Data warehouses are systems which integrate heterogeneous sources to support the decision making process. Data from the Web is becoming increasingly more important as sources for these systems, which has motivated the extensive use of XML to facilitate data and metadata interchange among heterogeneous data sources from the Web and the data warehouse. However, the business information that data warehouses manage is highly sensitive and must, therefore, be carefully protected. Security is thus a key issue in the design of data warehouses, regardless of the implementation technology. It is important to note that the idiosyncrasy of the unstructured and semi-structured data requires particular security rules that have been specifically tailored to these systems in order to permit their particularities to be captured correctly. Unfortunately, although security issues have been considered in the development of traditional data warehouses, current research lacks approaches with which to consider security when the target platform is based on XML technology.
56|1|http://www.sciencedirect.com/science/journal/09505849/56/1|Global software engineering: Identifying challenges is important and providing solutions is even better|
56|1||Global software testing under deadline pressure: Vendor-side experiences|In the era of globally-distributed software engineering, the practice of global software testing (GST) has witnessed increasing adoption. Although there have been ethnographic studies of the development aspects of global software engineering, there have been fewer studies of GST, which, to succeed, can require dealing with unique challenges.
56|1||Software quality across borders: Three case studies on company internal alignment|Software quality issues are commonly reported when offshoring software development. Value-based software engineering addresses this by ensuring key stakeholders have a common understanding of quality.
56|1||Guest Editorial: Special section of the best papers from the 16th International Conference on Evaluation & Assessment in Software Engineering|
56|1||Investigating dependencies in software requirements for change propagation analysis|The dependencies between individual requirements have an important influence on software engineering activities e.g., project planning, architecture design, and change impact analysis. Although dozens of requirement dependency types were suggested in the literature from different points of interest, there still lacks an evaluation of the applicability of these dependency types in requirements engineering.
56|1||Risks and risk mitigation in global software development: A tertiary study|Context
56|1||Motivation in software engineering industrial practice: A cross-case analysis of two software organisations|The research about motivation in software engineering has provided important insights into characterizing factors and outcomes related to motivation. However, the complex relationships among these factors, including the moderating and mediating effects of organisational and individual characteristics, still require deeper explanatory investigation.
56|10|http://www.sciencedirect.com/science/journal/09505849/56/10|Quality models for web services: A systematic mapping|Quality of Service (QoS) is a major issue in various web service related activities. Quality models have been proposed as the engineering artefact to provide a common framework of understanding for QoS, by defining the quality factors that apply to web service usage.
56|10||On strategies for testing software product lines: A systematic literature review|Testing plays an important role in the quality assurance process for software product line engineering. There are many opportunities for economies of scope and scale in the testing activities, but techniques that can take advantage of these opportunities are still needed.
56|10||Software development in startup companies: A systematic mapping study|Software startups are newly created companies with no operating history and fast in producing cutting-edge technologies. These companies develop software under highly uncertain conditions, tackling fast-growing markets under severe lack of resources. Therefore, software startups present a unique combination of characteristics which pose several challenges to software development activities.
56|10||Testing scientific software: A systematic literature review|Scientific software plays an important role in critical decision making, for example making weather predictions based on climate models, and computation of evidence for research publications. Recently, scientists have had to retract publications due to errors caused by software faults. Systematic testing can identify such faults in code.
56|10||Knowledge transfer, translation and transformation in the work of information technology architects|Information Technology (IT) architects are the professionals responsible for designing the information systems for an organization. In order to do that, they take into account many aspects and stakeholders, including customers, software developers, the organization’s business, and its current IT infrastructure. Therefore, different aspects influence their work.
56|10||Model-based early and rapid estimation of COSMIC functional size â An experimental evaluation|Functional size measurement methods are widely used but have two major shortcomings: they require a complete and detailed knowledge of user requirements, and they involve relatively expensive and lengthy processes.
56|10||A language-independent approach to the extraction of dependencies between source code entities|Software networks are directed graphs of static dependencies between source code entities (functions, classes, modules, etc.). These structures can be used to investigate the complexity and evolution of large-scale software systems and to compute metrics associated with software design. The extraction of software networks is also the first step in reverse engineering activities.
56|10||Model-driven specification and enforcement of RBAC break-glass policies for process-aware information systems|In many organizational environments critical tasks exist which – in exceptional cases such as an emergency – must be performed by a subject although he/she is usually not authorized to perform these tasks. Break-glass policies have been introduced as a sophisticated exception handling mechanism to resolve such situations. They enable certain subjects to break or override the standard access control policies of an information system in a controlled manner.
56|10||The effect of governance on global software development: An empirical research in transactive memory systems|The way global software development (GSD) activities are managed impacts knowledge transactions between team members. The first is captured in governance decisions, and the latter in a transactive memory system (TMS), a shared cognitive system for encoding, storing and retrieving knowledge between members of a group.
56|10||Test suite reduction methods that decrease regression testing costs by identifying irreplaceable tests|In software development and maintenance, a software system may frequently be updated to meet rapidly changing user requirements. New test cases will be designed to ensure the correctness of new or modified functions, thus gradually increasing the test suite’s size. Test suite reduction techniques aim to decrease the cost of regression testing by removing the redundant test cases from the test suite and then obtaining a representative set of test cases that still yield a high level of code coverage.
56|10||Study of advanced separation of concerns approaches using the GoF design patterns: A quantitative and qualitative comparison|Since the emergence of the aspect oriented paradigm, several studies have been conducted to test the contribution of this new paradigm compared to the object paradigm. However, in addition to this type of studies, we need also comparative studies that assess the aspect approaches mutually. The motivations of the latter include the enhancement of each aspect approach, devising hybrid approaches or merely helping developers choosing the suitable approach according to their needs. Comparing advanced separation of concerns approaches is the context of our work.
56|10||Automatic test case generation for structural testing of function block diagrams|Function Block Diagram (FBD) is increasingly used in safety-critical applications. Test coverage issues for FBDs are frequently raised by regulators and users. However, there is little work at this aspect on testing FBD at model level. Our previous study has designed a new data-flow test coverage criterion, FB-Path Complete Condition Test Coverage (FPCC), that can directly test FBD structures and effectively detect function mutation errors. Nevertheless, because FPCC scheme involves several data-flow concepts and thus it is somewhat complicated to comprehend and to generate FPCC-complied test cases. An automatic test suite generator for FPCC is highly desirable.
56|10||Why software repositories are not used for defect-insertion circumstance analysis more often: A case study|Root-cause analysis is a data-driven technique for developing software process improvements in mature software organizations. The search for individual process correlates of high defect densities, which we call defect insertion circumstance analysis (DICA), is potentially both effective and cost-efficient as one approach to be used when attempting a general defect root cause analysis. In DICA, data from existing repositories (version archive, bug tracker) is evaluated largely automatically in order to determine conditions (such as the people, roles, components, or time-periods involved) that correlate with higher-than-normal defect insertion frequencies. Nevertheless, no reports of industrial use of DICA have been published.
56|10||Evaluating the productivity of a reference-based programming approach: A controlled experiment|Domain engineering aims at facilitating software development in an efficient and economical way. One way to measure that is through productivity indicators, which refer to the ability of creating a quality software product in a limited period and with limited resources. Many approaches have been devised to increase productivity; however, these approaches seem to suffer from a tension between expressiveness on the one hand, and applicability (or the lack of it) in providing guidance for developers.
56|10||Reasons for bottlenecks in very large-scale system of systems development|System of systems (SoS) is a set or arrangement of systems that results when independent and useful systems are to be incorporated into a larger system that delivers unique capabilities. Our investigation showed that the development life cycle (i.e. the activities transforming requirements into design, code, test cases, and releases) in SoS is more prone to bottlenecks in comparison to single systems.
56|11|http://www.sciencedirect.com/science/journal/09505849/56/11|Special issue editorial: Understanding software ecosystems|
56|11||Joining a smartphone ecosystem: Application developersâ motivations and decision criteria|The ecosystems surrounding current smartphones operating systems, especially the application markets, provide significant value for customers and therefore possibilities for provider differentiation. Why do independent application developers and innovators join these ecosystems, and which factors influence their choice between different platform options?
56|11||Software engineering beyond the project â Sustaining software ecosystems|The main part of software engineering methods, tools and technologies has developed around projects as the central organisational form of software development. A project organisation depends on clear bounds regarding scope, participants, development effort and lead-time. What happens when these conditions are not given? The article claims that this is the case for software product specific ecosystems. As software is increasingly developed, adopted and deployed in the form of customisable and configurable products, software engineering as a discipline needs to take on the challenge to support software ecosystems.
56|11||Characteristics of software ecosystems for Federated Embedded Systems: A case study|Traditionally, Embedded Systems (ES) are tightly linked to physical products, and closed both for communication to the surrounding world and to additions or modifications by third parties. New technical solutions are however emerging that allow addition of plug-in software, as well as external communication for both software installation and data exchange. These mechanisms in combination will allow for the construction of Federated Embedded Systems (FES). Expected benefits include the possibility of third-party actors developing add-on functionality; a shorter time to market for new functions; and the ability to upgrade existing products in the field. This will however require not only new technical solutions, but also a transformation of the software ecosystems for ES.
56|11||Analysis and design of software ecosystem architectures â Towards the 4S telemedicine ecosystem|Telemedicine, the provision of health care at a distance, is arguably an effective way of increasing access to, reducing cost of, and improving quality of care. However, the deployment of telemedicine is faced with standards that are hard to use, application-specific data models, and application stove-pipes that inhibit the adoption of telemedical solutions. To which extent can a software ecosystem approach to telemedicine alleviate this?
56|11||Bridges and barriers to hardware-dependent software ecosystem participation â A case study|Software ecosystems emerged as means for several actors to jointly provide more value to the market than any of them can do on its own. Recently, software ecosystems are more often used to support the development of hardware-dependent solutions.
56|11||Measuring the health of open source software ecosystems: Beyond the scope of project health|The livelihood of an open source ecosystem is important to different ecosystem participants: software developers, end-users, investors, and participants want to know whether their ecosystem is healthy and performing well. Currently, there exists no working operationalization available that can be used to determine the health of open source ecosystems. Health is typically looked at from a project scope, not from an ecosystem scope.
56|11||Variability mechanisms in software ecosystems|Software ecosystems are increasingly popular for their economic, strategic, and technical advantages. Application platforms such as Android or iOS allow users to highly customize a system by selecting desired functionality from a large variety of assets. This customization is achieved using variability mechanisms.
56|12|http://www.sciencedirect.com/science/journal/09505849/56/12|Human factors in software development: On its underlying theories and the value of learning from related disciplines. A guest editorial introduction to the special issue|
56|12||Who to follow recommendation in large-scale online development communities|Open source development allows a large number of people to reuse and contribute source code to the community. Social networking features open opportunities for information discovery, social collaborations, and improved recommendations of potential collaborators.
56|12||Communities of practice in a large distributed agile software development organization â Case Ericsson|Communities of practice—groups of experts who share a common interest or topic and collectively want to deepen their knowledge—can be an important part of a successful lean and agile adoption in particular in large organizations.
56|12||Understanding the attitudes, knowledge sharing behaviors and task performance of core developers: A longitudinal study|Prior research has established that a few individuals generally dominate project communication and source code changes during software development. Moreover, this pattern has been found to exist irrespective of task assignments at project initiation.
56|12||How are software defects found? The role of implicit defect detection, individual responsibility, documents, and knowledge|Prior research has focused heavily on explicit defect detection, such as formal testing and reviews. However, in reality, humans find software defects in various activities. Implicit defect detection activities, such as preparing a product demonstration or updating a user manual, are not designed for defect detection, yet through such activities defects are discovered. In addition, the type of documentation, and knowledge used, in defect detection is diverse.
56|12||Understanding reuse of software examples: A case study of prejudice in a community of practice|The context of this research is software developers’ perceptions about the use of code examples in professional software development.
56|2|http://www.sciencedirect.com/science/journal/09505849/56/2|Software process modeling languages: A systematic literature review|Organizations working in software development are aware that processes are very important assets as well as they are very conscious of the need to deploy well-defined processes with the goal of improving software product development and, particularly, quality. Software process modeling languages are an important support for describing and managing software processes in software-intensive organizations.
56|2||An empirical study on the implementation and evaluation of a goal-driven software development risk management model|Building a quality software product in the shortest possible time to satisfy the global market demand gives an enterprise a competitive advantage. However, uncertainties and risks exist at every stage of a software development project. These can have an extremely high influence on the success of the final software product. Early risk management practice is effective to manage such risks and contributes effectively towards the project success.
56|2||An integrated approach based on execution measures for the continuous improvement of business processes realized by services|Organizations are rapidly adopting Business Process Management (BPM) as they focus on their business processes (BPs), seeing them to be key elements in controlling and improving the way they perform their business. Business Process Intelligence (BPI) takes as its focus the collection and analysis of information from the execution of BPs for the support of decision making, based on the discovery of improvement opportunities. Realizing BPs by services introduces an intermediate service layer that enables us to separate the specification of BPs in terms of models from the technologies implementing them, thus improving their modifiability by decoupling the model from its implementation.
56|2||Enhancing software artefact traceability recovery processes with link count information|The intensive human effort needed to manually manage traceability information has increased the interest in using semi-automated traceability recovery techniques. In particular, Information Retrieval (IR) techniques have been largely employed in the last ten years to partially automate the traceability recovery process.
56|2||Data stream mining for predicting software build outcomes using source code metrics|Software development projects involve the use of a wide range of tools to produce a software artifact. Software repositories such as source control systems have become a focus for emergent research because they are a source of rich information regarding software development projects. The mining of such repositories is becoming increasingly common with a view to gaining a deeper understanding of the development process.
56|2||A hybrid class- and prototype-based object model to support language-neutral structural intercession|Dynamic languages have turned out to be suitable for developing specific applications where runtime adaptability is an important issue. Although .Net and Java platforms have gradually incorporated features to improve their support of dynamic languages, they do not provide intercession for every object or class. This limitation is mainly caused by the rigid class-based object model these platforms implement, in contrast to the flexible prototype-based model used by most dynamic languages.
56|2||Source code size estimation approaches for object-oriented systems from UML class diagrams: A comparative study|Source code size in terms of SLOC (source lines of code) is the input of many parametric software effort estimation models. However, it is unavailable at the early phase of software development.
56|2||Generating profile-based signatures for online intrusion and failure detection|Program execution profiles have been extensively and successfully used in several dynamic analysis fields such as software testing and fault localization.
56|2||Understanding the characteristics of quality for software engineering processes: A Grounded Theory investigation|Software engineering organizations routinely define and implement processes to support, guide and control project execution. An assumption underlying this process-centric approach to business improvement is that the quality of the process will influence the quality, cost and time-to-release of the software produced. A critical question thus arises of what constitutes quality for software engineering processes.
56|3|http://www.sciencedirect.com/science/journal/09505849/56/3|A systematic review on security in Process-Aware Information Systems â Constitution, challenges, and future directions|Security in Process-Aware Information Systems (PAIS) has gained increased attention in current research and practice. However, a common understanding and agreement on security is still missing. In addition, the proliferation of literature makes it cumbersome to overlook and determine state of the art and further to identify research challenges and gaps. In summary, a comprehensive and systematic overview of state of the art in research and practice in the area of security in PAIS is missing.
56|3||Comparing attack trees and misuse cases in an industrial setting|The last decade has seen an increasing focus on addressing security already during the earliest stages of system development, such as requirements determination. Attack trees and misuse cases are established techniques for representing security threats along with their potential mitigations. Previous work has compared attack trees and misuse cases in two experiments with students. The present paper instead presents an experiment where industrial practitioners perform the experimental tasks in their workplace. The industrial experiment confirms a central finding from the student experiments: that attack trees tend to help identifying more threats than misuse cases. It also presents a new result: that misuse cases tend to encourage identification of threats associated with earlier development stages than attack trees. The two techniques should therefore be considered complementary and should be used together in practical requirements work.
56|3||Facilitating contagion trust through tools in Global Systems Engineering teams|In Global Systems Engineering teams, researchers have found that trust can be transitive to some degree or imported (swift trust) under certain conditions. We argue that trust can be contagion and seeded by tools (spread from one individual to another through tools).
56|3||Model-based requirements verification method: Conclusions from two controlled experiments|Requirements engineering is one of the most important and critical phases in the software development life cycle, and should be carefully performed to build high quality and reliable software. However, requirements are typically gathered through various sources and are represented in natural language (NL), making requirements engineering a difficult, fault prone, and a challenging task.
56|3||A UML profile for the conceptual modelling of structurally complex data: Easing human effort in the KDD process|Domains where data have a complex structure requiring new approaches for knowledge discovery from data are on the increase. In such domains, the information related to each object under analysis may be composed of a very broad set of interrelated data instead of being represented by a simple attribute table. This further complicates their analysis.
56|3||Semantic-based automatic service composition with functional and non-functional requirements in design time: A genetic algorithm approach|In recent years, the composition of ready-made and loosely coupled services into desired systems is a common industrial approach and a widely followed research topic in academia. In the field, the current research trend is to automate this composition; however, each of the existing efforts automates only a component of the entire problem. Therefore, a real automation process that addresses all composition concerns is lacking.
56|4|http://www.sciencedirect.com/science/journal/09505849/56/4|Considering rigor and relevance when evaluating test driven development: A systematic review|Test driven development (TDD) has been extensively researched and compared to traditional approaches (test last development, TLD). Existing literature reviews show varying results for TDD.
56|4||Dynamic stopping criteria for search-based test data generation for path testing|Evolutionary algorithms have proved to be successful for generating test data for path coverage testing. However in this approach, the set of target paths to be covered may include some that are infeasible. It is impossible to find test data to cover those paths. Rather than searching indefinitely, or until a fixed limit of generations is reached, it would be desirable to stop searching as soon it seems likely that feasible paths have been covered and all remaining un-covered target paths are infeasible.
56|4||A tool supporting root cause analysis for synchronous retrospectives in distributed software teams|Root cause analysis (RCA) is a useful practice for software project retrospectives, and is typically carried out in synchronous collocated face-to-face meetings. Conducting RCA with distributed teams is challenging, as face-to-face meetings are infeasible. Lack of adequate real-time tool support exacerbates this problem. Furthermore, there are no empirical studies on using RCA in synchronous retrospectives of geographically distributed teams.
56|4||Simulating upgrades of complex systems: The case of Free and Open Source Software|The upgrade of complex systems is intrinsically difficult and requires techniques, algorithms, and methods which are both expressive and computationally feasible in order to be used in practice. In the case of FOSS (Free and Open Source Software) systems, many upgrade errors cannot be discovered by current upgrade managers and then a system upgrade can potentially lead the system to an inconsistent and incoherent state.
56|5|http://www.sciencedirect.com/science/journal/09505849/56/5|Performance in software development â Special issue editorial|
56|5||Analysing ISD performance using narrative networks, routines and mindfulness|While the ISD process and in particular Requirement Elicitation has been defined as a collaborative social interaction, visualisations fail to accurately capture the multifaceted nature of the social process. Instead, ISD visualisations focus on presenting a more mechanical/technical perspective, ultimately restricting an opportunity to better analyse the process.
56|5||Systematic analyses and comparison of development performance and product quality of Incremental Process and Agile Process|Although Agile software development models have been widely used as a base for the software project life-cycle since 1990s, the number of studies that follow a sound empirical method and quantitatively reveal the effect of using these models over Traditional models is scarce.
56|5||Performance appraisal of software testers|To determine the effectiveness of software testers a suitable performance appraisal approach is necessary, both for research and practice purposes. However, review of relevant literature reveals little information of how software testers are appraised in practice.
56|5||Performance on agile teams: Relating iteration objectives and critical decisions to project management success factors|While project management success factors have long been established via the golden triangle, little is known about how project iteration objectives and critical decisions relate to these success factors. It seems logical that teams’ iteration objectives would reflect project management success factors, but this may not always be the case. If not, how are teams’ objectives for iterations differing from the golden triangle of project management success factors?
56|5||Evaluating performance in the development of software-intensive products|Organizational performance measurements in software product development have received a lot of attention in the literature. Still, there is a general discontent regarding the way performance is evaluated in practice, with few studies really focusing on why this is the case. In this paper research focusing on the context of developing software-intensive products in large established multi-national organizations is reported on.
56|6|http://www.sciencedirect.com/science/journal/09505849/56/6|Potential and limitations of the ISBSG dataset in enhancing software engineering research: A mapping review|The International Software Benchmarking Standards Group (ISBSG) maintains a software development repository with over 6000 software projects. This dataset makes it possible to estimate a project’s size, effort, duration, and cost.
56|6||Knowledge-based approaches in software documentation: A systematic literature review|Software documents are core artifacts produced and consumed in documentation activity in the software lifecycle. Meanwhile, knowledge-based approaches have been extensively used in software development for decades, however, the software engineering community lacks a comprehensive understanding on how knowledge-based approaches are used in software documentation, especially documentation of software architecture design.
56|6||A systematic literature review of software requirements prioritization research|During requirements engineering, prioritization is performed to grade or rank requirements in their order of importance and subsequent implementation releases. It is a major step taken in making crucial decisions so as to increase the economic value of a system.
56|6||Querying large models efficiently|The paradigm of Model-Driven Engineering (MDE) has emerged as a new area of software engineering that uses models to improve the productivity and reusability of software in order to achieve industrial standards. As models grow in size and complexity, the need of model persistence and model querying solutions arises to efficiently store large models and obtain information from them in an efficient, usable and safe way. Morsa is a model repository that uses a No-SQL database backend; it has been recently presented [1] and achieves scalable access to models and transparent integration with tools.
56|6||Perceived causes of software project failures â An analysis of their relationships|Software project failures are common. Even though the reasons for failures have been widely studied, the analysis of their causal relationships is lacking. This creates an illusion that the causes of project failures are unrelated.
56|6||Systematizing requirements elicitation technique selection|This research deals with requirements elicitation technique selection for software product requirements and the overselection of open interviews.
56|6||Mockup-Driven Development: Providing agile support for Model-Driven Web Engineering|Agile software development approaches are currently becoming the industry standard for Web Application development. On the other hand, Model-Driven Web Engineering (MDWE) methodologies are known to improve productivity when building this kind of applications. However, current MDWE methodologies tend to ignore important aspects of Web Applications development supported by agile processes, such as constant customer feedback or early design of user interfaces.
56|7|http://www.sciencedirect.com/science/journal/09505849/56/7|An extended systematic literature review on provision of evidence for safety certification|Critical systems in domains such as aviation, railway, and automotive are often subject to a formal process of safety certification. The goal of this process is to ensure that these systems will operate safely without posing undue risks to the user, the public, or the environment. Safety is typically ensured via complying with safety standards. Demonstrating compliance to these standards involves providing evidence to show that the safety criteria of the standards are met.
56|7||Metamodeling generalization and other directed relationships in UML|Generalization is a fundamental relationship in object orientation and in the UML (Unified Modeling Language). The generalization relationship is represented in the UML metamodel as a “directed relationship”.
56|7||Agile product-line architecting in practice: A case study in smart grids|Software Product Line Engineering implies the upfront design of a Product-Line Architecture (PLA) from which individual product applications can be engineered. The big upfront design associated with PLAs is in conflict with the current need of “being open to change”. To make the development of product-lines more flexible and adaptable to changes, several companies are adopting Agile Product Line Engineering. However, to put Agile Product Line Engineering into practice it is still necessary to make mechanisms available to assist and guide the agile construction and evolution of PLAs.
56|7||Model-based testing of global properties on large-scale distributed systems|Large-scale distributed systems are becoming commonplace with the large popularity of peer-to-peer and cloud computing. The increasing importance of these systems contrasts with the lack of integrated solutions to build trustworthy software. A key concern of any large-scale distributed system is the validation of global properties, which cannot be evaluated on a single node. Thus, it is necessary to gather data from distributed nodes and to aggregate these data into a global view. This turns out to be very challenging because of the system’s dynamism that imposes very frequent changes in local values that affect global properties. This implies that the global view has to be frequently updated to ensure an accurate validation of global properties.
56|7||Building hybrid access control by configuring RBAC and MAC features|Role-Based Access Control (RBAC) and Mandatory Access Control (MAC) are widely used access control models. They are often used together in domains where both data integrity and information flow are concerned. However, there is little work on techniques for building hybrid access control of RBAC and MAC.
56|7||Analyzing the relationships between inspections and testing to provide a software testing focus|Quality assurance effort, especially testing effort, is frequently a major cost factor during software development. Consequently, one major goal is often to reduce testing effort. One promising way to improve the effectiveness and efficiency of software quality assurance is the use of data from early defect detection activities to provide a software testing focus. Studies indicate that using a combination of early defect data and other product data to focus testing activities outperforms the use of other product data only. One of the key challenges is that the use of data from early defect detection activities (such as inspections) to focus testing requires a thorough understanding of the relationships between these early defect detection activities and testing. An aggravating factor is that these relationships are highly context-specific and need to be evaluated for concrete environments.
56|7||Towards a theoretical framework of SPI success factors for small and medium web companies|The context of this research is software process improvement (SPI) success factors for small and medium Web companies.
56|8|http://www.sciencedirect.com/science/journal/09505849/56/8|Formal verification of static software models in MDE: A systematic review|Model-driven Engineering (MDE) promotes the utilization of models as primary artifacts in all software engineering activities. Therefore, mechanisms to ensure model correctness become crucial, specially when applying MDE to the development of software, where software is the result of a chain of (semi)automatic model transformations that refine initial abstract models to lower level ones from which the final code is eventually generated. Clearly, in this context, an error in the model/s is propagated to the code endangering the soundness of the resulting software. Formal verification of software models is a promising approach that advocates the employment of formal methods to achieve model correctness, and it has received a considerable amount of attention in the last few years.
56|8||A Systematic Mapping Study of Software Reliability Modeling|Software Reliability (SR) is a highly active and dynamic research area. Published papers have approached this topic from various and heterogeneous points of view, resulting in a rich body of literature on this topic. The counterpart to this is the considerable complexity of this body of knowledge.
56|8||Past and future of software architectural decisions â A systematic mapping study|The software architecture of a system is the result of a set of architectural decisions. The topic of architectural decisions in software engineering has received significant attention in recent years. However, no systematic overview exists on the state of research on architectural decisions.
56|8||Individual empowerment of agile and non-agile software developers in small teams|Empowerment of employees at work has been known to have a positive impact on job motivation and satisfaction. Software development is a field of knowledge work wherein one should also expect to see these effects, and the idea of empowerment has become particularly visible in agile methodologies, in which proponents emphasise team empowerment and individual control of the work activities as a central concern.
56|8||Empirical evaluations on the cost-effectiveness of state-based testing: An industrial case study|
56|8||Understanding agile software development practices using shared mental models theory|Agile software development is an alternative software development methodology that originated from practice to encourage collaboration between developers and users, to leverage rapid development cycles, and to respond to changes in a dynamic environment. Although agile practices are widely used in organizations, academics call for more theoretical research to understand the value of agile software development methodologies.
56|8||A CSCW Requirements Engineering CASE Tool: Development and usability evaluation|CSRML Tool 2012 is a Requirements Engineering CASE Tool for the Goal-Oriented Collaborative Systems Requirements Modeling Language (CSRML).
56|8||Change impact analysis for requirements: A metamodeling approach|Following the evolution of the business needs, the requirements of software systems change continuously and new requirements emerge frequently. Requirements documents are often textual artifacts with structure not explicitly given. When a change in a requirements document is introduced, the requirements engineer may have to manually analyze all the requirements for a single change. This may result in neglecting the actual impact of a change. Consequently, the cost of implementing a change may become several times higher than expected.
56|8||Personality, emotional intelligence and work preferences in software engineering: An empirical study|There is an increasing awareness among Software Engineering (SE) researchers and practitioners that more focus is needed on understanding the engineers developing software. Previous studies show significant associations between the personalities of software engineers and their work preferences.
56|8||A noun-based approach to feature location using time-aware term-weighting|
56|8||MoDisco: A model driven reverse engineering framework|Most companies, independently of their size and activity type, are facing the problem of managing, maintaining and/or replacing (part of) their existing software systems. These legacy systems are often large applications playing a critical role in the company’s information system and with a non-negligible impact on its daily operations. Improving their comprehension (e.g., architecture, features, enforced rules, handled data) is a key point when dealing with their evolution/modernization.
56|8||Understanding replication of experiments in software engineering: A classification|Replication plays an important role in experimental disciplines. There are still many uncertainties about how to proceed with replications of SE experiments. Should replicators reuse the baseline experiment materials? How much liaison should there be among the original and replicating experimenters, if any? What elements of the experimental configuration can be changed for the experiment to be considered a replication rather than a new experiment?
56|8||Needsâ elaboration between users, designers and project leaders: Analysis of a design process of a virtual reality-based software|The participation of users in the design process is recognized as a positive and a necessary element as artifacts suit their needs. Two complementary approaches of users’ involvement co-exist: the user-centered design and the participatory design. These approaches involve learning process from users to designers and vice versa. However, there has no research in design of virtual reality (VR)-based software dealing with how the elaboration of needs is actually distributed in time and among users, designers and project leaders, as well as how it is actually supported by tools and methods.
56|9|http://www.sciencedirect.com/science/journal/09505849/56/9|Investigating the use of duration-based moving windows to improve software effort prediction: A replicated study|Most research in software effort estimation has not considered chronology when selecting projects for training and testing sets. A chronological split represents the use of a projects starting and completion dates, such that any model that estimates effort for a new project p only uses as training data projects that were completed prior to p’s start. Four recent studies investigated the use of chronological splits, using moving windows wherein only the most recent projects completed prior to a projects starting date were used as training data. The first three studies (S1–S3) found some evidence in favor of using windows; they all defined window sizes as being fixed numbers of recent projects. In practice, we suggest that estimators think in terms of elapsed time rather than the size of the data set, when deciding which projects to include in a training set. In the fourth study (S4) we showed that the use of windows based on duration can also improve estimation accuracy.
56|9||A class loading sensitive approach to detection of runtime type errors in component-based Java programs|The employment of class loaders in component-based Java programs may introduce runtime type errors, which may happen at any statement related to class loading, and may be wrapped into various types of exceptions raised by JVM. Traditional static analysis approaches are inefficient to detect them.
56|9||Low-disruptive dynamic updating of Java applications|In-use software systems are destined to change in order to fix bugs or add new features. Shutting down a running system before updating it is a normal practice, but the service unavailability can be annoying and sometimes unacceptable. Dynamic software updating (DSU) migrates a running software system to a new version without stopping it. State-of-the-art Java DSU systems are unsatisfactory as they may cause a non-negligible system pause during updating.
56|9||Editorial for the special section on Software Product Line Engineering: Selected papers from Software Product Line conference in 2012|
56|9||A framework for variable content document generation with multiple actors|Advances in customization have highlighted the need for tools supporting variable content document management and generation in many domains. Current tools allow the generation of highly customized documents that are variable in both content and layout. However, most frameworks are technology-oriented, and their use requires advanced skills in implementation-related tools, which means their use by end users (i.e. document designers) is severely limited.
56|9||Efficient synthesis of feature models|Variability modeling, and in particular feature modeling, is a central element of model-driven software product line architectures. Such architectures often emerge from legacy code, but, creating feature models from large, legacy systems is a long and arduous task. We describe three synthesis scenarios that can benefit from the algorithms in this paper.
56|9||Toward automated feature model configuration with optimizing non-functional requirements|A software product line is a family of software systems that share some common features but also have significant variabilities. A feature model is a variability modeling artifact, which represents differences among software products with respect to the variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by binding the variation points in the feature model (called configuration process) and by instantiating the reference model.
57|-|http://www.sciencedirect.com/science/journal/09505849/57|Automatic generation algorithm of expected results for testing of component-based software system|A component-based software (CBS) system is a typical example of a composite component, which is composed of two or more atomic components. In the test of a CBS system, it is necessary to automatically generate expected results because they are compared with the actual results.
57|-||On the journey to continuous deployment: Technical and social challenges along the way|Continuous Deployment (CD) is an emerging software development process with organisations such as Facebook, Microsoft, and IBM successfully implementing and using the process. The CD process aims to immediately deploy software to customers as soon as new code is developed, and can result in a number of benefits for organisations, such as: new business opportunities, reduced risk for each release, and prevent development of wasted software. There is little academic literature on the challenges organisations face when adopting the CD process, however there are many anecdotal challenges that organisations have voiced on their online blogs.
57|-||A concern-oriented framework for dynamic measurements|Evolving software programs requires that software developers reason quantitatively about the modularity impact of several concerns, which are often scattered over the system. To this respect, concern-oriented software analysis is rising to a dominant position in software development. Hence, measurement techniques play a fundamental role in assessing the concern modularity of a software system. Unfortunately, existing measurements are still fundamentally module-oriented rather than concern-oriented. Moreover, the few available concern-oriented metrics are defined in a non-systematic and shared way and mainly focus on static properties of a concern, even if many properties can only be accurately quantified at run-time. Hence, novel concern-oriented measurements and, in particular, shared and systematic ways to define them are still welcome. This paper poses the basis for a unified framework for concern-driven measurement. The framework provides a basic terminology and criteria for defining novel concern metrics. To evaluate the framework feasibility and effectiveness, we have shown how it can be used to adapt some classic metrics to quantify concerns and in particular to instantiate new dynamic concern metrics from their static counterparts.
57|-||Towards a governance framework for chains of Scrum teams|Large companies operating in the information intensive industries increasingly adopt Agile/Scrum to swiftly change IT functionality because of rapid changing business demands. IT functionality in large enterprises however is typically delivered by a portfolio of interdependent software applications involving a chain of Scrum teams. Usually, each application from the portfolio is allocated to a single Scrum team, which necessitates collaboration between the Scrum teams to jointly deliver functionality.
57|-||Exception handling analysis and transformation using fault injection: Study of resilience against unanticipated exceptions|In software, there are the error cases that are anticipated at specification and design time, those encountered at development and testing time, and those that were never anticipated before happening in production. Is it possible to learn from the anticipated errors during design to analyze and improve the resilience against the unanticipated ones in production?
57|-||An empirically-based characterization and quantification of information seeking through mailing lists during Open Source developersâ software evolution|Several authors have proposed information seeking as an appropriate perspective for studying software evolution. Empirical evidence in this area suggests that substantial time delays can accrue, due to the unavailability of required information, particularly when this information must travel across geographically distributed sites.
57|-||BPMNt: A BPMN extension for specifying software process tailoring|Although SPEM 2.0 has great potential for software process modeling, it does not provide concepts or formalisms for precise modeling of process behavior. Indeed, SPEM fails to address process simulation, execution, monitoring and analysis, which are important activities in process management. On the other hand, BPMN 2.0 is a widely used notation to model business processes that has associated tools and techniques to facilitate the aforementioned process management activities. Using BPMN to model software development processes can leverage BPMN’s infrastructure to improve the quality of these processes. However, BPMN lacks an important feature to model software processes: a mechanism to represent process tailoring.
57|-||Operational release planning in large-scale Scrum with multiple stakeholders â A longitudinal case study at F-Secure Corporation|The analysis and selection of requirements are important parts of any release planning process. Previous studies on release planning have focused on plan-driven optimization models. Unfortunately, solving the release planning problem mechanistically is difficult in an agile development context.
57|-||Are team personality and climate related to satisfaction and software quality? Aggregating results from a twice replicated experiment|Research into software engineering teams focuses on human and social team factors. Social psychology deals with the study of team formation and has found that personality factors and group processes such as team climate are related to team effectiveness. However, there are only a handful of empirical studies dealing with personality and team climate and their relationship to software development team effectiveness.
57|-||Gamification in software engineering â A systematic mapping|Gamification seeks for improvement of the user’s engagement, motivation, and performance when carrying out a certain task, by means of incorporating game mechanics and elements, thus making that task more attractive. Much research work has studied the application of gamification in software engineering for increasing the engagement and results of developers.
57|-||Testing robot controllers using constraint programming and continuous integration|Testing complex industrial robots (CIRs) requires testing several interacting control systems. This is challenging, especially for robots performing process-intensive tasks such as painting or gluing, since their dedicated process control systems can be loosely coupled with the robot’s motion control.
57|-||An empirical analysis of package-modularization metrics: Implications for software fault-proneness|In a large object-oriented software system, packages play the role of modules which group related classes together to provide well-identified services to the rest of the system. In this context, it is widely believed that modularization has a large influence on the quality of packages. Recently, Sarkar, Kak, and Rama proposed a set of new metrics to characterize the modularization quality of packages from important perspectives such as inter-module call traffic, state access violations, fragile base-class design, programming to interface, and plugin pollution. These package-modularization metrics are quite different from traditional package-level metrics, which measure software quality mainly from size, extensibility, responsibility, independence, abstractness, and instability perspectives. As such, it is expected that these package-modularization metrics should be useful predictors for fault-proneness. However, little is currently known on their actual usefulness for fault-proneness prediction, especially compared with traditional package-level metrics.
57|-||Generating semantically valid test inputs using constrained input grammars|Generating test cases based on software input interface is a black-box testing technique that can be made more effective by using structured input models such as input grammars. Automatically generating grammar-based test inputs may lead to structurally valid but semantically invalid inputs that may be rejected in early semantic error checking phases of a system under test.
57|-||A comprehensive pattern-oriented approach to engineering security methodologies|Developing secure software systems is an issue of ever-growing importance. Researchers have generally come to acknowledge that to develop such systems successfully, their security features must be incorporated in the context of a systematic approach: a security methodology. There are a number of such methodologies in the literature, but no single security methodology is adequate for every situation, requiring the construction of “fit-to-purpose” methodologies or the tailoring of existing methodologies to the project specifics at hand. While a large body of research exists addressing the same requirement for development methodologies – constituting the field of Method Engineering – there is nothing comparable for security methodologies as such; in fact, the topic has never been studied before in such a context.
57|-||VIVACE: A framework for the systematic evaluation of variability support in process-aware information systems|The increasing adoption of process-aware information systems (PAISs) such as workflow management systems, enterprise resource planning systems, or case management systems, together with the high variability in business processes (e.g., sales processes may vary depending on the respective products and countries), has resulted in large industrial process model repositories. To cope with this business process variability, the proper management of process variants along the entire process lifecycle becomes crucial.
57|-||The impact of global dispersion on coordination, team performance and software quality â A systematic literature review|Global software development (GSD) contains different context setting dimensions, which are essential for effective teamwork and success of projects. Although considerable research effort has been made in this area, as yet, no agreement has been reached about the impact of these dispersion dimensions on team coordination and project outcomes.
57|-||The impact of inadequate and dysfunctional training on Agile transformation process: A Grounded Theory study|Training is an essential facilitator in moving from traditional to Agile software development.
57|-||A framework for comparing multiple cost estimation methods using an automated visualization toolkit|The importance of accurate predictions in Software Cost Estimation and the related challenging research problems, led to the introduction of a plethora of methodologies in literature. However, the wide variety of cost estimation methods, the techniques for improving them and the different measures of accuracy have caused new problems such as the inconsistent findings and the conclusion instability. Today, there is a confusion regarding the choice of the most appropriate method for a specific dataset and therefore a need for well-established statistical frameworks as well as for automated tools that will reinforce and lead a comprehensive experimentation and comparison process, based on the thorough study of the cost estimation errors.
57|-||CCIC: Clustering analysis classes to identify software components|Component identification during software design phase denotes a process of partitioning the functionalities of a system into distinct components. Several component identification methods have been proposed that cannot be customized to software architect’s preferences.
57|-||A comparative study of software tools for user story management|User stories have become widely accepted in agile software development. Consequently, a great number of software tools that provide, inter alia, support for practices based on user stories have emerged in recent years. These tools may have different features and focus in terms of support for agile requirements engineering (RE) concepts and practices.
57|-||Automated classification of software change messages by semi-supervised Latent Dirichlet Allocation|Topic models such as probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA) have demonstrated success in mining software repository tasks. Understanding software change messages described by the unstructured nature-language text is one of the fundamental challenges in mining these messages in repositories.
57|-||Knowledge management initiatives in software testing: A mapping study|Software testing is a knowledge intensive process, and, thus, Knowledge Management (KM) principles and techniques should be applied to manage software testing knowledge.
57|-||Shared service recommendations from requirement specifications: A hybrid syntactic and semantic toolkit|Software Requirement Specifications (SRSs) are central to software lifecycles. An SRS defines the functionalities and constraints of a desired software system, hence it often serves as reference for further development. Software lifecycles concerned with the conversion of traditional systems into more service-oriented infrastructures can benefit from understanding potential shared capabilities through the analysis of SRSs.
57|-||Validating a model-driven software architecture evaluation and improvement method: A family of experiments|Software architectures should be evaluated during the early stages of software development in order to verify whether the non-functional requirements (NFRs) of the product can be fulfilled. This activity is even more crucial in software product line (SPL) development, since it is also necessary to identify whether the NFRs of a particular product can be achieved by exercising the variation mechanisms provided by the product line architecture or whether additional transformations are required. These issues have motivated us to propose QuaDAI, a method for the derivation, evaluation and improvement of software architectures in model-driven SPL development.
57|-||State dependency probabilistic model for fault localization|Fault localization is an important and expensive activity in software debugging. Previous studies indicated that statistically-based fault-localization techniques are effective in prioritizing the possible faulty statements with relatively low computational complexity, but prior works on statistical analysis have not fully investigated the behavior state information of each program element.
57|-||Automated end user-centred adaptation of web components through automated description logic-based reasoning|This paper addresses one of the major end-user development (EUD) challenges, namely, how to pack today’s EUD support tools with composable elements. This would give end users better access to more components which they can use to build a solution tailored to their own needs. The success of later end-user software engineering (EUSE) activities largely depends on how many components each tool has and how adaptable components are to multiple problem domains.
57|-||Automated test generation technique for aspectual features in AspectJ|Aspect-oriented programming (AOP) has been promoted as a means for handling the modularization of software systems by raising the abstraction level and reducing the scattering and tangling of crosscutting concerns. Studies from literature have shown the usefulness and application of AOP across various fields of research and domains. Despite this, research shows that AOP is currently used in a cautious way due to its natural impact on testability and maintainability.
57|-||Besouro: A framework for exploring compliance rules in automatic TDD behavior assessment|The improvements promoted by Test-Driven Design (TDD) have not been confirmed by quantitative assessment studies. To a great extent, the problem lies in the lack of a rigorous definition for TDD. An emerging approach has been to measure the conformance of TDD practices with the support of automated systems that embed an operational definition, which represent the specific TDD process assumed and the validation tests used to determine its presence and quantity. The empirical construction of TDD understanding and consensus building requires the ability of comparing different definitions, evaluating them with regard to practitioners’ perception, and exploring code information for improvement of automatic assessment.
57|-||Using simulation to aid decision making in managing the usability evaluation process|This paper is developed in the context of Usability Engineering. More specifically, it focuses on the use of modelling and simulation to help decision-making in the scope of usability evaluation.
57|-||Using a multi-method approach to understand Agile software product lines|Software product lines (SPLs) and Agile are approaches that share similar objectives. The main difference is the way in which these objectives are met. Typically evidence on what activities of Agile and SPL can be combined and how they can be integrated stems from different research methods performed separately. The generalizability of this evidence is low, as the research topic is still relatively new and previous studies have been conducted using only one research method.
57|-||A semi-automated approach to adapt activity diagrams for new use cases|Web engineering methodologies generally assign a crucial role to design models. Therefore, providing a model reuse approach is very interesting since it reduces development costs and improves quality. Current works on model reuse mainly focus on retrieval of the promising reusable assets, and much less is done regarding adaptation of the retrieved assets. This research proposes a semi-automatic approach for adaptation of UML activity diagrams to new use cases.
57|-||Generating optimized configurable business process models in scenarios subject to uncertainty|The quality of business process models (i.e., software artifacts that capture the relations between the organizational units of a business) is essential for enhancing the management of business processes. However, such modeling is typically carried out manually. This is already challenging and time consuming when (1) input uncertainty exists, (2) activities are related, and (3) resource allocation has to be considered. When including optimization requirements regarding flexibility and robustness it becomes even more complicated potentially resulting into non-optimized models, errors, and lack of flexibility.
57|-||The contextual nature of innovation â An empirical investigation of three software intensive products|New products create significant opportunities for differentiation and competitive advantage. To increase the chances of new product success, a universal set of critical activities and determinants have been recommended. Some researchers believe, however, that these factors are not universal, but are contextual.
57|-||Special section from the International Conference on Evaluation and Assessment in Software Engineering, 2013|
57|-||Naming the pain in requirements engineering: A design for a global family of surveys and first results from Germany|For many years, we have observed industry struggling in defining a high quality requirements engineering (RE) and researchers trying to understand industrial expectations and problems. Although we are investigating the discipline with a plethora of empirical studies, they still do not allow for empirical generalisations.
57|-||Are Forward Designed or Reverse-Engineered UML diagrams more helpful for code maintenance?: A family of experiments|Although various success stories of model-based approaches are reported in literature, there is still a significant resistance to model-based development in many software organizations because the UML is perceived to be expensive and not necessarily cost-effective. It is also important to gather empirical evidence in which context and under which conditions the UML makes or does not make a practical difference.
57|-||Usage and usefulness of technical software documentation: An industrial case study|Software documentation is an integral part of any software development process. However, software practitioners are often concerned about the value, degree of usage and usefulness of documentation during development and maintenance.
57|-||Introduction to special section on Search Based Software Engineering|
57|-||Learning from optimization: A case study with Apache Ant|Software architecture degrades when changes violating the design-time architectural intents are imposed on the software throughout its life cycle. Such phenomenon is called architecture erosion. When changes are not controlled, erosion makes maintenance harder and negatively affects software evolution.
57|-||Search-based automated testing of continuous controllers: Framework, tool support, and case studies|Testing and verification of automotive embedded software is a major challenge. Software production in automotive domain comprises three stages: Developing automotive functions as Simulink models, generating code from the models, and deploying the resulting code on hardware devices. Automotive software artifacts are subject to three rounds of testing corresponding to the three production stages: Model-in-the-Loop (MiL), Software-in-the-Loop (SiL) and Hardware-in-the-Loop (HiL) testing.
58|-|http://www.sciencedirect.com/science/journal/09505849/58|Using intentional fragments to bridge the gap between organizational and intentional levels|Business process models provide a natural way to describe real-world processes to be supported by software-intensive systems. These models can be used to analyze processes in the system-as-is and describe potential improvements for the system-to-be. But, how well does a given business process model satisfy its business goals? How can different perspectives be integrated in order to describe an inter-organizational process?
58|-||Using CMMI together with agile software development: A systematic review|The search for adherence to maturity levels by using lightweight processes that require low levels of effort is regarded as a challenge for software development organizations.
58|-||Empirical evaluation of a cloud computing information security governance framework|Cloud computing is a thriving paradigm that supports an efficient way to provide IT services by introducing on-demand services and flexible computing resources. However, significant adoption of cloud services is being hindered by security issues that are inherent to this new paradigm. In previous work, we have proposed ISGcloud, a security governance framework to tackle cloud security matters in a comprehensive manner whilst being aligned with an enterprise’s strategy.
58|-||Using Bayesian regression and EM algorithm with missing handling for software effort prediction|Although independent imputation techniques are comprehensively studied in software effort prediction, there are few studies on embedded methods in dealing with missing data in software effort prediction.
58|-||Semantics for consistent activation in context-oriented systems|Context-oriented programming languages provide dedicated programming abstractions to define behavioral adaptations and means to combine those adaptations dynamically according to sensed context changes. Some of these languages feature programming abstractions to explicitly define interaction dependencies among contexts. However, the semantics of context activation and the meaning of dependency relations have been described only informally, which in some cases has led to incorrect specifications, faulty implementations and inconsistent system behavior.
58|-||Automatic transformation of iterative loops into recursive methods|In software engineering, taking a good election between recursion and iteration is essential because their efficiency and maintenance are different. In fact, developers often need to transform iteration into recursion (e.g., in debugging, to decompose the call graph into iterations); thus, it is quite surprising that there does not exist a public transformation from loops to recursion that can be used in industrial projects (i.e., it is automatic, it handles all kinds of loops, it considers exceptions, etc.).
58|-||Automated events identification in use cases|Use cases are a popular method of expressing functional requirements. One contains a main scenario and a set of extensions, each consisting of an event and an alternative sequence of activities. Events omitted in requirements specification can lead to rework. Unfortunately, as it follows from the previous research, manual identification of events is rather ineffective (less than 1/3 of events are identified) and it is slow.
58|-||Software test-code engineering: A systematic mapping|As a result of automated software testing, large amounts of software test code (script) are usually developed by software teams. Automated test scripts provide many benefits, such as repeatable, predictable, and efficient test executions. However, just like any software development activity, development of test scripts is tedious and error prone. We refer, in this study, to all activities that should be conducted during the entire lifecycle of test-code as Software Test-Code Engineering (STCE).
58|-||A systematic review on the relationship between user involvement and system success|For more than four decades it has been intuitively accepted that user involvement (UI) during system development lifecycle leads to system success. However when the researchers have evaluated the user involvement and system success (UI-SS) relationship empirically, the results were not always positive.
58|-||Current state of research on cross-site scripting (XSS) â A systematic literature review|Cross-site scripting (XSS) is a security vulnerability that affects web applications. It occurs due to improper or lack of sanitization of user inputs. The security vulnerability caused many problems for users and server applications.
58|-||A systematic literature review of studies on business process modeling quality|Business process modeling is an essential part of understanding and redesigning the activities that a typical enterprise uses to achieve its business goals. The quality of a business process model has a significant impact on the development of any enterprise and IT support for that process.
58|-||Analogy-based software development effort estimation: A systematic mapping and review|Analogy-based Software development Effort Estimation (ASEE) techniques have gained considerable attention from the software engineering community. However, existing systematic map and review studies on software development effort prediction have not investigated in depth several issues of ASEE techniques, to the exception of comparisons with other types of estimation techniques.
58|-||Identifying refactoring opportunities in object-oriented code: A systematic literature review|Identifying refactoring opportunities in object-oriented code is an important stage that precedes the actual refactoring process. Several techniques have been proposed in the literature to identify opportunities for various refactoring activities.
58|-||On the probability distribution of faults in complex software systems|There are several empirical principles related to the distribution of faults in a software system (e.g. the Pareto principle) widely applied in practice and thoroughly studied in the software engineering research providing evidence in their favor. However, the knowledge of the underlying probability distribution of faults, that would enable a systematic approach and refinement of these principles, is still quite limited.
58|-||Approach for estimating similarity between procedures in differently compiled binaries|Detection of an unauthorized use of a software library is a clone detection problem that in case of commercial products has additional complexity due to the fact that only binary code is available.
58|-||Investigating software testing and maintenance reports: Case study|Although many papers have been published on software development and defect prediction techniques, problem reports in real projects quite often differ from those described in the literature. Hence, there is still a need for deeper exploration of case studies from industry.
58|-||âOldâ theories, âNewâ technologies: Understanding knowledge sharing and learning in Brazilian software development companies|New technologies such as social networks, wikis, blogs and other social software enable collaborative work and are important facilitators of the learning process. They provide a simple mechanism for people to communicate and collaborate and thus support the creation of knowledge. In software-development companies they are used to creating an environment in which communication and collaboration between workers take place more effectively.
58|-||A Bayesian network model for likelihood estimations of acquirement of critical software vulnerabilities and exploits|Software vulnerabilities in general, and software vulnerabilities with publicly available exploits in particular, are important to manage for both developers and users. This is however a difficult matter to address as time is limited and vulnerabilities are frequent.
58|-||Product derivation in practice|The process of constructing a product from a product line of software assets is known product derivation. An effective product derivation process is important in order to ensure that the efforts required to develop these shared assets is lower than the benefits achieved through their use. Despite its importance, relatively little work has been dedicated to the product derivation process and the strategies applied in practice. Additionally, there is a lack of empirical reports describing product derivation in industrial settings, and, in general, where these reports are available, they have been conducted as informal studies.
58|-||A framework to identify primitives that represent usability within Model-Driven Development methods|Nowadays, there are sound methods and tools which implement the Model-Driven Development approach (MDD) satisfactorily. However, MDD approaches focus on representing and generating code that represents functionality, behaviour and persistence, putting the interaction, and more specifically the usability, in a second place. If we aim to include usability features in a system developed with a MDD tool, we need to extend manually the generated code.
58|-||Similarity testing for access control|Access control is among the most important security mechanisms, and XACML is the de facto standard for specifying, storing and deploying access control policies. Since it is critical that enforced policies are correct, policy testing must be performed in an effective way to identify potential security flaws and bugs. In practice, exhaustive testing is impossible due to budget constraints. Therefore the tests need to be prioritized so that resources are focused on their most relevant subset.
58|-||Categorization of risk factors for distributed agile projects|Organizations combine agile approach and Distributed Software Development (DSD) in order to develop better quality software solutions in lesser time and cost. It helps to reap the benefits of both agile and distributed development but pose significant challenges and risks. Relatively scanty evidence of research on the risks prevailing in distributed agile development (DAD) has motivated this study.
58|-||Software defect prediction using ensemble learning on selected features|Several issues hinder software defect data including redundancy, correlation, feature irrelevance and missing samples. It is also hard to ensure balanced distribution between data pertaining to defective and non-defective software. In most experimental cases, data related to the latter software class is dominantly present in the dataset.
58|-||Infeasible path generalization in dynamic symbolic execution|Automatic code-based test input generation aims at generating a test suite ensuring good code coverage. Dynamic Symbolic Execution (DSE) recently emerged as a strong code-based testing technique to increase coverage by solving path conditions with a combination of symbolic constraint solving and concrete executions.
58|-||Search based algorithms for test sequence generation in functional testing|The generation of dynamic test sequences from a formal specification, complementing traditional testing methods in order to find errors in the source code.
59|-|http://www.sciencedirect.com/science/journal/09505849/59|An architecture for automatically developing secure OLAP applications from models|Decision makers query enterprise information stored in Data Warehouses (DW) by using tools (such as On-Line Analytical Processing (OLAP) tools) which use specific views or cubes from the corporate DW or Data Marts, based on the multidimensional modeling. Since the information managed is critical, security constraints have to be correctly established in order to avoid unauthorized accesses.
59|-||New bounds for mixed covering arrays in t-way testing with uniform strength|Combinatorial testing (CT) can increase the effectiveness of software testing by ensuring that all t  -way input combinations are covered in a test suite. When software components have different input cardinalities, CT uses a mixed covering array (MCA) to represent the test suite. This study proposes a new methodology for constructing MCAs of t∈{2-6}t∈{2-6} by using Mixed-Tabu Search (MiTS) as the construction strategy.
59|-||Automated refactoring to the Null Object design pattern|Null-checking conditionals are a straightforward solution against null dereferences. However, their frequent repetition is considered a sign of poor program design, since they introduce source code duplication and complexity that impacts code comprehension and maintenance. The Null Object design pattern enables the replacement of null-checking conditionals with polymorphic method invocations that are bound, at runtime, to either a real object or a Null Object.
59|-||An efficient approach to identify multiple and independent Move Method refactoring candidates|Application of a refactoring operation creates a new set of dependency in the revised design as well as a new set of further refactoring candidates. In the studies of stepwise refactoring recommendation approaches, applying one refactoring at a time has been used, but is inefficient because the identification of the best candidate in each iteration of refactoring identification process is computation-intensive. Therefore, it is desirable to accurately identify multiple and independent candidates to enhance efficiency of refactoring process.
59|-||A systematic literature review on the barriers faced by newcomers to open source software projects|Numerous open source software projects are based on volunteers collaboration and require a continuous influx of newcomers for their continuity. Newcomers face barriers that can lead them to give up. These barriers hinder both developers willing to make a single contribution and those willing to become a project member.
59|-||Defining the resource perspective in the development of processes-aware information systems|
59|-||Synergy between Activity Theory and goal/scenario modeling for requirements elicitation, analysis, and evolution|It is challenging to develop comprehensive, consistent, analyzable requirements models for evolving requirements. This is particularly critical for certain highly interactive types of socio-technical systems that involve a wide range of stakeholders with disparate backgrounds; system success is often dependent on how well local social constraints are addressed in system design.
59|-||Analyzing impact of experience curve on ROI in the software product line adoption process|Experience curve is a well-known concept in management and education science, which explains the phenomenon of increased worker efficiency with repetitive production of a good or service.
59|-||A multivariate statistical framework for the analysis of software effort phase distribution|In software project management, the distribution of resources to various project activities is one of the most challenging problems since it affects team productivity, product quality and project constraints related to budget and scheduling.
59|-||An empirical study on software defect prediction with a simplified metric set|Software defect prediction plays a crucial role in estimating the most defect-prone components of software, and a large number of studies have pursued improving prediction accuracy within a project or across projects. However, the rules for making an appropriate decision between within- and cross-project defect prediction when available historical data are insufficient remain unclear.
59|-||Improving the management of product lines by performing domain knowledge extraction and cross product line analysis|Increase in market competition is one of the main reasons for developing and maintaining families of systems, termed Product Lines (PLs). Managing those PLs is challenging, let alone the management of several related PLs. Currently, those PLs are managed separately or their relations are analyzed assuming explicit specification of dependencies or use of an underlying terminology. Such assumptions may not hold when developing the PLs in different departments or companies applying various engineering processes.
59|-||A framework for software process deployment and evaluation|Software Process Engineering promotes the systematic production of software by following a set of well-defined technical and management processes. A comprehensive management of these processes involves the accomplishment of a number of activities such as model design, verification, validation, deployment and evaluation. However, the deployment and evaluation activities need more research efforts in order to achieve greater automation.
59|-||A measurement method for sizing the structure of UML sequence diagrams|The COSMIC functional size measurement method on UML diagrams has been investigated as a means to estimate the software effort early in the software development life cycle. Like other functional size measurement methods, the COSMIC method takes into account the data movements in the UML sequence diagrams for example, but does not consider the data manipulations in the control structure. This paper explores software sizing at a finer level of granularity by taking into account the structural aspect of a sequence diagram in order to quantify its structural size. These functional and structural sizes can then be used as distinct independent variables to improve effort estimation models.
59|-||An approach and tool for measurement of state variable based data-flow test coverage for aspect-oriented programs|Data-flow testing approaches have been used for procedural and object-oriented programs, and shown to be effective in detecting faults. However, few such approaches have been evaluated for aspect-oriented programs. In such programs, data-flow interactions can occur between base classes and aspects, which can affect the behavior of both. Faults resulting from such interactions are hard to detect unless the interactions are specifically targeted during testing.
60|-|http://www.sciencedirect.com/science/journal/09505849/60|Evidence management for compliance of critical systems with safety standards: A survey on the state of practice|Demonstrating compliance of critical systems with safety standards involves providing convincing evidence that the requirements of a standard are adequately met. For large systems, practitioners need to be able to effectively collect, structure, and assess substantial quantities of evidence.
60|-||An industrial case study on variability handling in large enterprise software systems|Enterprise software systems (e.g., enterprise resource planning software) are often deployed in different contexts (e.g., different organizations or different business units or branches of one organization). However, even though organizations, business units or branches have the same or similar business goals, they may differ in how they achieve these goals. Thus, many enterprise software systems are subject to variability and adapted depending on the context in which they are used.
60|-||The CARE platform for the analysis of behavior model inference techniques|Finite State Machine (FSM) inference from execution traces has received a lot of attention over the past few years. Various approaches have been explored, each holding different properties for the resulting models, but the lack of standard benchmarks limits the ability of comparing the proposed techniques. Evaluation is usually performed on a few case studies, which is useful for assessing the feasibility of the algorithm on particular cases, but fails to demonstrate effectiveness in a broad context. Consequently, understanding the strengths and weaknesses of inference techniques remains a challenging task.
60|-||Facilitating construction of safety cases from formal models in Event-B|Certification of safety–critical software systems requires submission of safety assurance documents, e.g., in the form of safety cases. A safety case is a justification argument used to show that a system is safe for a particular application in a particular environment. Different argumentation strategies (informal and formal) are applied to determine the evidence for a safety case. For critical software systems, application of formal methods is often highly recommended for their safety assurance.
60|-||Empirical evaluation of a decision support model for adopting software product line engineering|The software product line engineering (SPLE) community has provided several different approaches for assessing the feasibility of SPLE adoption and selecting transition strategies. These approaches usually include many rules and guidelines which are very often implicit or scattered over different publications. Hence, for the practitioners it is not always easy to select and use these rules to support the decision making process. Even in case the rules are known, the lack of automated support for storing and executing the rules seriously impedes the decision making process.
60|-||An empirical research agenda for understanding formal methods productivity|Formal methods, and particularly formal verification, is becoming more feasible to use in the engineering of large highly dependable software-based systems, but so far has had little rigorous empirical study. Its artefacts and activities are different to those of conventional software engineering, and the nature and drivers of productivity for formal methods are not yet understood.
||||
volume|issue|url|title|abstract
61|-|http://www.sciencedirect.com/science/journal/09505849/61|Manual test case derivation from UML activity diagrams and state machines: A controlled experiment|It is a difficult and challenging task to fully automatize model-based testing because this demands complete and unambiguous system models as input. Therefore, in practice, test cases, especially on the system level, are still derived manually from behavioral models like UML activity diagrams or state machines. But this kind of manual test case derivation is error-prone and knowing these errors makes it possible to provide guidelines to reduce them.
61|-||Supporting the semi-automatic semantic annotation of web services: A systematic literature review|Semantically annotating web services is gaining more attention as an important aspect to support the automatic matchmaking and composition of web services. Therefore, the support of well-known and agreed ontologies and tools for the semantical annotation of web services is becoming a key concern to help the diffusion of semantic web services.
61|-||A systematic mapping study of search-based software engineering for software product lines|Search-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces.
61|-||Requirements simulation for early validation using Behavior Trees and Datalog|The role of formal specification in requirements validation and analysis is generally considered to be limited because considerable expertise is required in developing and understanding the mathematical proofs. However, formal semantics of a language can provide a basis for step-by-step execution of requirements specification by building an easy to use simulator to assist in requirements elicitation, validation and analysis.
61|-||Comparing development approaches and reuse strategies: An empirical evaluation of developer views from the aerospace industry|There is a debate in the aerospace industry whether lessons from reuse successes and failures in nonembedded software can be applied to embedded software. Reuse supposedly reduces development time and errors. The aerospace industry was an early advocate of reuse, but in Aerospace, not all reuse experiences have been as successful as expected. Some major projects experienced large overruns in time, budget, as well as inferior performance, at least in part, due to the gap between reuse expectations and reuse outcomes. This seemed to be especially the case for embedded systems.
61|-||ELBlocker: Predicting blocking bugs with ensemble imbalance learning|Blocking bugs are bugs that prevent other bugs from being fixed. Previous studies show that blocking bugs take approximately two to three times longer to be fixed compared to non-blocking bugs.
61|-||Real-Time Reflexion Modelling in architecture reconciliation: A multi case study|Reflexion Modelling is considered one of the more successful approaches to architecture reconciliation. Empirical studies strongly suggest that professional developers involved in real-life industrial projects find the information provided by variants of this approach useful and insightful, but the degree to which it resolves architecture conformance issues is still unclear.
61|-||Estimating, planning and managing Agile Web development projects under a value-based perspective|The processes of estimating, planning and managing are crucial for software development projects, since the results must be related to several business strategies. The broad expansion of the Internet and the global and interconnected economy make Web development projects be often characterized by expressions like delivering as soon as possible, reducing time to market and adapting to undefined requirements. In this kind of environment, traditional methodologies based on predictive techniques sometimes do not offer very satisfactory results. The rise of Agile methodologies and practices has provided some useful tools that, combined with Web Engineering techniques, can help to establish a framework to estimate, manage and plan Web development projects.
61|-||Automated measurement of API usability: The API Concepts Framework|Usability is an important software quality attribute for APIs. Unfortunately, measuring it is not an easy task since many things like experienced evaluators, suitable test users, and a functional product are needed. This makes existing usability measurement methods difficult to use, especially for non-professionals.
61|-||Exploring principles of user-centered agile software development: A literature review|In the last decade, software development has been characterized by two major approaches: agile software development, which aims to achieve increased velocity and flexibility during the development process, and user-centered design, which places the goals and needs of the system’s end-users at the center of software development in order to deliver software with appropriate usability. Hybrid development models, referred to as user-centered agile software development (UCASD) in this article, propose to combine the merits of both approaches in order to design software that is both useful and usable.
62|-|http://www.sciencedirect.com/science/journal/09505849/62|A systematic literature review on Enterprise Architecture Implementation Methodologies|Enterprise Architecture (EA) is a strategy to align business and Information Technology (IT) within an enterprise. EA is managed, developed, and maintained throughout the EA Implementation Methodology (EAIM).
62|-||An adaptive middleware design to support the dynamic interpretation of domain-specific models|As the use of Domain-Specific Modeling Languages (DSMLs) continues to gain popularity, we have developed new ways to execute DSML models. The most popular approach is to execute code resulting from a model-to-code transformation. An alternative approach is to directly execute these models using a semantic-rich execution engine – Domain-Specific Virtual Machine (DSVM). The DSVM includes a middleware layer responsible for the delivery of services in a given domain.
62|-||Development of service-oriented architectures using model-driven development: A mapping study|Model-Driven Development (MDD) and Service-Oriented Architecture (SOA) are two challenging research areas in software engineering. MDD is about improving software development whilst SOA is a service-based conceptual development style, therefore investigating the available proposals in the literature to use MDD when developing SOA may be insightful. However, no studies have been found with this purpose.
62|-||Negative samples reduction in cross-company software defects prediction|Software defect prediction has been widely studied based on various machine-learning algorithms. Previous studies usually focus on within-company defects prediction (WCDP), but lack of training data in the early stages of software testing limits the efficiency of WCDP in practice. Thus, recent research has largely examined the cross-company defects prediction (CCDP) as an alternative solution.
62|-||Supporting distributed product configuration by integrating heterogeneous variability modeling approaches|In industrial settings products are developed by more than one organization. Software vendors and suppliers commonly typically maintain their own product lines, which contribute to a larger (multi) product line or software ecosystem. It is unrealistic to assume that the participating organizations will agree on using a specific variability modeling technique—they will rather use different approaches and tools to manage the variability of their systems.
62|-||Operationalised product quality models and assessment: The Quamoco approach|Software quality models provide either abstract quality characteristics or concrete quality measurements; there is no seamless integration of these two aspects. Quality assessment approaches are, hence, also very specific or remain abstract. Reasons for this include the complexity of quality and the various quality profiles in different domains which make it difficult to build operationalised quality models.
62|-||Assessing the use of slicing-based visualizing techniques on the understanding of large metamodels|Metamodels are cornerstones of various metamodeling activities. Such activities consist of, for instance, transforming models into code or comparing metamodels. These activities thus require a good understanding of a metamodel and/or its parts. Current metamodel editing tools are based on standard interactive visualization features, such as physical zooms.
62|-||Using metrics in Agile and Lean Software Development â A systematic literature review of industrial studies|Software industry has widely adopted Agile software development methods. Agile literature proposes a few key metrics but little is known of the actual metrics use in Agile teams.
62|-||In search of evidence for model-driven development claims: An experiment on quality, effort, productivity and satisfaction|Model-Driven Development (MDD) is a paradigm that prescribes building conceptual models that abstractly represent the system and generating code from these models through transformation rules. The literature is rife with claims about the benefits of MDD, but they are hardly supported by evidences.
62|-||Automating correctness verification of artifact-centric business process models|The artifact-centric methodology has emerged as a new paradigm to support business process management over the last few years. This way, business processes are described from the point of view of the artifacts that are manipulated during the process.
62|-||Combinatorial testing, random testing, and adaptive random testing for detecting interaction triggered failures|Software behavior depends on many factors, and some failures occur only when certain factors interact. This is known as an interaction triggered failure, and the corresponding selection of factor values can be modeled as a Minimal Failure-causing Schema (MFS). (An MFS involving m factors is an m-MFS.) Combinatorial Testing (CT) has been developed to exercise (“hit”) all MFS with few tests. Adaptive Random Resting (ART) endeavors to make tests as different as possible, ensuring that testing of MFS is not unnecessarily repeated. Random Testing (RT) chooses tests at random without regard to the MFS already treated. CT might be expected to improve on RT for finding interaction triggered faults, and yet some studies report no significant difference. CT can also be expected to be better than ART, and yet other studies report that ART can be much better than RT. In light of these, the relative merits of CT, ART, and RT for finding interaction triggered faults are unclear.
63|-|http://www.sciencedirect.com/science/journal/09505849/63|Distributed Pair Programming: A Systematic Literature Review|Geographically distributed teams have adopted agile practices as a work strategy. One of these practices is Distributed Pair Programming (DPP). DPP consists in two developers working remotely on the same design, algorithm or code.
63|-||A conceptual framework to study the role of communication through social software for coordination in globally-distributed software teams|In Global Software Development (GSD) the lack of face-to-face communication is a major challenge and effective computer-mediated practices are necessary to mitigate the effect of physical distance. Communication through Social Software (SoSo) supports team coordination, helping to deal with geographical distance; however, in Software Engineering literature, there is a lack of suitable theoretical concepts to analyze and describe everyday practices of globally-distributed software development teams and to study the role of communication through SoSo.
63|-||Measures of process harmonization|Many large organizations juggle an application portfolio that contains different applications that fulfill similar tasks in the organization. In an effort to reduce operating costs, they are attempting to consolidate such applications. Before consolidating applications, the work that is done with these applications must be harmonized. This is also known as process harmonization.
63|-||A fuzzy logic based approach for phase-wise software defects prediction using software metrics|The software defect prediction during software development has recently attracted the attention of many researchers. The software defect density indicator prediction in each phase of software development life cycle (SDLC) is desirable for developing a reliable software product. Software defect prediction at the end of testing phase may not be more beneficial because the changes need to be performed in the previous phases of SDLC may require huge amount of money and effort to be spent in order to achieve target software quality. Therefore, phase-wise software defect density indicator prediction model is of great importance.
63|-||Two controlled experiments on model-based architectural decision making|In recent years, architectural design decisions are becoming more and more common for documenting software architectures. Rather than describing the structure of software systems, architectural decisions capture the design rationale and – often reusable – architectural knowledge. Many approaches and tools have been proposed in the literature to support architectural decision making and documentation (for instance, based on models, ontologies, or templates). In this context, the capturing, organization, and effective reuse of architectural knowledge has gained a lot of attention.
64|-|http://www.sciencedirect.com/science/journal/09505849/64|Guidelines for conducting systematic mapping studies in software engineering: An update|Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines.
64|-||Passive testing of communicating systems with timeouts|The design of complex systems demands methodologies to analyze its correct behaviour. It is usual that a correct behaviour is determined by the compliance with temporal requirements. Currently, testing is the most used technology to validate the correctness of systems. Although several techniques that take into account time aspects have been proposed, most of them require the tester interacts with the system. However, if this is not possible, it is necessary to apply a passive testing approach where the tester monitors the behaviour of the system.
64|-||In defence of deep modelling|Since multi-level modelling emerged as a strategy for leveraging classification levels in conceptual models, there have been discussions about what it entails and how best to support it. Recently, some authors have claimed that the deep modelling approach to multi-level modelling entails paradoxes and significant weaknesses. By drawing upon concepts from speech act theory and foundational ontologies these authors argue that hitherto accepted principles for deep modelling should be abandoned and an alternative approach be adopted instead (Eriksson et al., 2013).
64|-||The financial aspect of managing technical debt: A systematic literature review|Technical debt is a software engineering metaphor, referring to the eventual financial consequences of trade-offs between shrinking product time to market and poorly specifying, or implementing a software product, throughout all development phases. Based on its inter-disciplinary nature, i.e. software engineering and economics, research on managing technical debt should be balanced between software engineering and economic theories.
64|-||Editorial for the special section on Empirical Studies in Software Engineering Selected, and extended papers from the Eighteenth International Conference on Evaluation and Assessment in Software Engineering, May 13thâ14th 2014, London, UK|
64|-||Investigations about replication of empirical studies in software engineering: A systematic mapping study|Two recent mapping studies which were intended to verify the current state of replication of empirical studies in Software Engineering (SE) identified two sets of studies: empirical studies actually reporting replications (published between 1994 and 2012) and a second group of studies that are concerned with definitions, classifications, processes, guidelines, and other research topics or themes about replication work in empirical software engineering research (published between 1996 and 2012).
64|-||On the usefulness of ownership metrics in open-source software projects|Code ownership metrics were recently defined in order to distinguish major and minor contributors of a software module, and to assess whether the ownership of such a module is strong or shared between developers.
64|-||Communication and personality profiles of global software developers|Prior research has established that a small proportion of individuals dominate team communication during global software development. It is not known, however, how these members’ contributions affect their teams’ knowledge diffusion process, or whether their personality profiles are responsible for their dominant presence.
64|-||Performance Alignment Work: How software developers experience the continuous adaptation of team performance in Lean and Agile environments|Companies increasingly strive to adapt to market and ecosystem changes in real time. Gauging and understanding team performance in such changing environments present a major challenge.
65|-|http://www.sciencedirect.com/science/journal/09505849/65|An Integer Linear Programming approach to the single and bi-objective Next Release Problem|The Next Release Problem involves determining the set of requirements to implement in the next release of a software project. When the problem was first formulated in 2001, Integer Linear Programming, an exact method, was found to be impractical because of large execution times. Since then, the problem has mainly been addressed by employing metaheuristic techniques.
65|-||How have we evaluated software pattern application? A systematic mapping study of research design practices|Software patterns encapsulate expert knowledge for constructing successful solutions to recurring problems. Although a large collection of software patterns is available in literature, empirical evidence on how well various patterns help in problem solving is limited and inconclusive. The context of these empirical findings is also not well understood, limiting applicability and generalizability of the findings.
65|-||A complete approach for CIM modelling and model formalising|Computation Independent Model (CIM) as a business model describes the requirements and environment of a business system and instructs the designing and development; it is a key to influencing software success. Although many studies currently focus on model driven development (MDD); those researches, to a large extent, study the PIM-level and PSM-level model, and few have dealt with CIM-level modelling for case in which the requirements are unclear or incomplete.
65|-||Investigating the penalty reward calculus of software users and its impact on requirements prioritization|The current requirements engineering techniques for prioritization of software requirements implicitly assume that each user requirement will have an independent and symmetric impact on user satisfaction. For example, it is assumed that implementing a high priority user requirement will positively impact his satisfaction and not implementing a high priority user requirement will negatively impact his satisfaction. Further, the impacts of implementing multiple user requirements on his satisfaction are expected to be additive. But is this always the case?
65|-||Test Governance Framework for contracted IS development: Ethnographically informed action research|Over the past two decades, interest has increased in software development and testing outsourcing. Although the decision to outsource development or test processes is founded on various background motives, e.g., costs, capacity, time-to-market, etc., additional factors that influence the outsourcing relationships are frequently overlooked by client organizations.
65|-||Editorial of special section from Software Evolution Week 2014|
65|-||SQA-Mashup: A mashup framework for continuous integration|Continuous Integration (CI) has become an established best practice of modern software development. Its philosophy of regularly integrating the changes of individual developers with the master code base saves the entire development team from descending into Integration Hell, a term coined in the field of extreme programming. In practice, CI is supported by automated tools to cope with this repeated integration of source code through automated builds and testing. One of the main problems, however, is that relevant information about the quality and health of a software system is both scattered across those tools and across multiple views.
65|-||Understanding the triaging and fixing processes of long lived bugs|Bug fixing is an integral part of software development and maintenance. A large number of bugs often indicate poor software quality, since buggy behavior not only causes failures that may be costly but also has a detrimental effect on the user’s overall experience with the software product. The impact of long lived bugs can be even more critical since experiencing the same bug version after version can be particularly frustrating for user. While there are many studies that investigate factors affecting bug fixing time for entire bug repositories, to the best of our knowledge, none of these studies investigates the extent and reasons of long lived bugs.
65|-||How Java APIs break â An empirical study|It has become common practice to build programs by using libraries. While the benefits of reuse are well known, an often overlooked risk are system runtime failures due to API changes in libraries that evolve independently. Traditionally, the consistency between a program and the libraries it uses is checked at build time when the entire system is compiled and tested. However, the trend towards partially upgrading systems by redeploying only evolved library versions results in situations where these crucial verification steps are skipped. For Java programs, partial upgrades create additional interesting problems as the compiler and the virtual machine use different rule sets to enforce contracts between the providers and the consumers of APIs.
65|-||Performance comparison of query-based techniques for anti-pattern detection|Program queries play an important role in several software evolution tasks like program comprehension, impact analysis, or the automated identification of anti-patterns for complex refactoring operations. A central artifact of these tasks is the reverse engineered program model built up from the source code (usually an Abstract Semantic Graph, ASG), which is traditionally post-processed by dedicated, hand-coded queries.
66|-|http://www.sciencedirect.com/science/journal/09505849/66|MSR4SM: Using topic models to effectively mining software repositories for software maintenance tasks|Mining software repositories has emerged as a research direction over the past decade, achieving substantial success in both research and practice to support various software maintenance tasks. Software repositories include bug repository, communication archives, source control repository, etc. When using these repositories to support software maintenance, inclusion of irrelevant information in each repository can lead to decreased effectiveness or even wrong results.
66|-||Achievement of minimized combinatorial test suite for configuration-aware software functional testing using the Cuckoo Search algorithm|Software has become an innovative solution nowadays for many applications and methods in science and engineering. Ensuring the quality and correctness of software is challenging because each program has different configurations and input domains. To ensure the quality of software, all possible configurations and input combinations need to be evaluated against their expected outputs. However, this exhaustive test is impractical because of time and resource constraints due to the large domain of input and configurations. Thus, different sampling techniques have been used to sample these input domains and configurations.
66|-||Conflict resolution effectiveness on the implementation efficiency and achievement of business objectives in IT programs: A study of IT vendors|The information technology (IT) field presents a unique context for the management of multiple projects because of the variety of stakeholders involved, the complexity of interdependencies among projects, and the frequent use of external vendors. In practice, IT vendors typically employ advanced project governance techniques such as program management to work effectively with the numbers and variety of clients while still pursuing the benefits of a single oversight. These structural features lend themselves to conflict across teams with individual requirements. However, little research exists on program management, much less in the IT context, that represents conflict across IT project teams.
66|-||Goal-oriented dynamic test generation|Memory safety errors such as buffer overflow vulnerabilities are one of the most serious classes of security threats. Detecting and removing such security errors are important tasks of software testing for improving the quality and reliability of software in practice.
66|-||Quantitative analysis of fault density in design patterns: An empirical study|There are many claimed advantages for the use of design patterns and their impact on software quality. However, there is no enough empirical evidence that supports these claimed benefits and some studies have found contrary results.
66|-||A scientific evaluation of the misuse case diagrams visual syntax|Misuse case modeling is a well-known technique in the domain of capturing and specifying functional security requirements. Misuse case modeling provides a mechanism for security analysts to consider and account for security requirements in the early stages of a development process instead of relying on generic defensive mechanisms that are augmented to software systems towards the latter stages of development.
||||
