volume|issue|url|title|abstract||||
46|1|http://www.sciencedirect.com/science/journal/13837621/46/1|Design techniques for low-power systems|Portable products are being used increasingly. Because these systems are battery powered, reducing power consumption is vital. In this report we give the properties of low-power design and techniques to exploit them on the architecture of the system. We focus on: minimizing capacitance, avoiding unnecessary and wasteful activity, and reducing voltage and frequency. We review energy reduction techniques in the architecture and design of a hand-held computer and the wireless communication system including error control, system decomposition, communication and MAC protocols, and low-power short range networks.||||
46|1||Characterizing and representing workloads for parallel computer architectures1|Experimental design of parallel computers calls for quantifiable methods to compare and evaluate the requirements of different workloads within an application domain. Such methods can help establish the basis for scientific design of parallel computers driven by application needs, to optimize performance to cost. In this paper, a framework is presented for representing and comparing workloads, based on the way they would exercise parallel machines. This workload characterization is derived from parallel instruction centroid and parallel workload similarity. The centroid is a workload approximation that captures the type and amount of parallel work generated by the workload on the average. The centroid is a simple measure that aggregates average parallelism, instruction mix, and critical path length. When captured with abstracted information about communication requirements, the result is a powerful tool in understanding the requirements of workloads and their potential performance on target machines. The workload similarity is based on measuring the normalized Euclidean distance (ned) between workload centroids. It will be shown that this workload representation method outperforms comparable ones in accuracy, as well as in time and space requirements. Analysis of the NAS Parallel Benchmark workloads and their performance will be presented to demonstrate some of the applications and insight provided by this framework. This will include the use of the proposed framework for predicting the performance of real-life workloads on target machines, with good accuracy.||||
46|1||Performance evaluation of a bus-based multistage multiprocessor architecture|This paper proposes and evaluates a class of interconnection networks, which provide performance comparable to a multiple bus network with considerably lower cost. These networks, referred to as hybrid networks, are formed by beginning with a multistage network and substituting buses in the second stage. Analytic models are developed to evaluate the performance of the system. The analysis includes both uniform and non-uniform distribution of requests. The results obtained are compared with simulation results.||||
46|1||Scheduling optional computations for adaptive real-time systems|At present, the critical computations of real-time systems are guaranteed before run-time by performing a worst-case analysis of the system's timing and resource requirements. The result is that real-time systems are engineered to have spare capacity, under normal operation. A challenge of current research is to make use of this spare capacity, in order to satisfy requirements for adaptivity in the system. Adaptivity can be implemented by optional computations with firm deadlines, which can be guaranteed at run-time by the use of flexible scheduling. This report assumes that the algorithms which attempt to guarantee optional computations at run-time, actually run on the same processor as the optional and critical computations themselves. The report starts with a brief survey of the complex requirements for adaptivity within real-time systems. Such requirements can include task hierarchies composed of interdependent subtasks each with its own utility. Evidence is cited which indicates that the run-time support for a computational model which supports all such requirements, would incur overheads so large, that little spare capacity would remain for the optional computations themselves. Following this, the report presents a constrained computational model, which, it is claimed, could be cost-effectively supported at run-time. The model is nevertheless general enough to satisfy many of the requirements for adaptivity. The constrained model uses Best Effort Admissions Policy to arbitrate between three categories of optional computation, each with its own utility level. The viability of the constrained model is demonstrated by simulation studies which compare the performance of the model to that of First-Come-First-Served Admissions Policy.||||
46|1||Extending conventional languages by distributed/concurrent exception resolution|The state of art in handling and resolving concurrent exceptions is discussed and a brief outline of all research in this area is given. Our intention is to demonstrate that exception resolution is a very useful concept which facilitates joint forward error recovery in concurrent and distributed systems. To do this, several new arguments are considered. We understand resolution as reaching an agreement among cooperating participants of an atomic action. It is provided by the underlying system to make it unified and less error prone, which is important for forward error recovery, complex by nature. We classify atomic action schemes into asynchronous and synchronous ones and discuss exception handling for schemes of both kinds. The paper also deals with introducing atomic action schemes based on exception resolution into existing concurrent and distributed languages, which usually have only local exceptions. We outline the basic approach and demonstrate its applicability by showing how exception resolution can be used in Ada 83, Ada 95 (for both concurrent and distributed systems) and Java. A discussion of ways to make this concept more object-oriented and, with the help of reflection, more flexible and useful, concludes the paper.||||
46|1||Traffic analysis in a double grain Dataflow array processor|One of the main problems of Dataflow architecture is the high communication overhead. This paper is dedicated to estimation of traffic densities in the communication links of a proposed Dataflow machine having a double grain array architecture. Results show a distinct advantage of it over existing Dataflow machines so far as communication bottlenecks are concerned. The machine however, is not infinitely scaleable.||||
46|10|http://www.sciencedirect.com/science/journal/13837621/46/10|Execution replay of parallel procedural programs|This article describes an execution model for the parallel procedural programming paradigm, which combines multithreading and communications. The model is used to prove sufficient conditions to guarantee the equivalence between two executions of the same program. An efficient mechanism for recording and replaying deterministically parallel procedural programs is derived from the model and implemented in a prototype. Performed on the prototype, systematic measurements of the time overhead of recording traces for replaying various program models indicate that this overhead remains very low.||||
46|10||Scheduling optimization through iterative refinement|Scheduling computations with communications is the theoretical basis for achieving efficient parallelism on distributed memory systems. We generalize Graham's task-level in a manner to incorporate the effects of computation and communication. A new scheduling is proposed by combining task priority with efficient management of processor idle time. We also propose an optimization called Iterative Refinement Scheduling (IRS) that iteratively schedules the forward and backward computation graph. The task-level used in some scheduling iteration is obtained from the schedule generated in the previous iteration. Each iteration produces a new schedule and new task-levels. This approach enables searching and optimizing solutions as the result of using more refined task-level in each scheduling iteration. Evaluation and analysis of the results are carried out for different instances of communication granularities and problem parallelism. It is shown that solutions obtained out of few iterations statistically outperforms those generated by other recently proposed scheduling. IRS allows exploring a space of solutions whose size grows with the amount of parallelism and communication granularity. IRS enables optimizing the solution specially for critical instances such as fine-grain computations and large parallelism.||||
46|10||Dynamic reconfiguration of node location in wormhole networks|Several techniques have been developed to increase the performance of parallel computers. Reconfigurable networks can be used as an alternative to increase the performance. Network reconfiguration can be carried out in different ways. Our research has focused on distributed memory systems with dynamic reconfiguration of node location. Briefly, this technique consists of positioning the processors in the network depending on the existing communication pattern among them, to suit the requirements of each computation.||||
46|10||Read-down conflict-preserving serializability as a correctness criterion for multilevel-secure optimistic concurrency control: CSR/RD|It is important to note that conflict-preserving serializability and related theory are artifacts of an era of database development where correctness alone was the overriding concern. With the advent of multilevel-secure databases, there is clearly a need to reexamine such theories. Any correctness criterion to govern transaction processing in the multilevel security context has to incorporate both secureness and correctness in a unified manner. This paper makes original contributions in two different but closely related areas to the optimistic concurrency control in multilevel-secure, single-version databases. First, read-down conflict-preserving serializability (CSR/RD) captures multilevel-secure database consistency requirements and secure transaction correctness properties via a single notion. Second, it presents a multilevel-secure optimistic concurrency control (MLS/OCC) scheme that has several desirable properties: If lower-level transactions were somehow allowed to continue with their executions in spite of the conflict with high-level transactions, covert timing-channel freeness would be satisfied. This sort of optimistic approach for conflict insensitiveness and the properties of non-blocking and deadlock freedom make the optimistic concurrency control scheme especially attractive to multilevel-secure transaction processing.||||
46|10||Broadcast directory: A scalable cache coherent architecture for mesh-connected multiprocessors|Large-scale shared memory multiprocessors favor a directory-based cache coherence scheme for its scalability. The directory space needed to record the information for sharers has a complexity of Î(N2) when a full-mapped vector is used for an N-node system. Although this overhead can be reduced by limiting the directory size assuming that the sharing degree is small, it will experience significant inefficiency when data is widely shared.||||
46|10||Efficient path-based multicast in wormhole-routed mesh networks|The capability of multidestination wormhole allows a message to be propagated along any valid path in a wormhole-routed network conforming to the underlying base routing scheme. The multicast on the path-based routing model is highly dependent on the spatial locality of destinations participating in multicasting. In this paper, we propose two proximity grouping schemes for efficient multicast in wormhole-routed mesh networks with multidestination capability by exploiting the spatial locality of the destination set. The first grouping scheme, graph-based proximity grouping, is proposed to group the destinations together with locality to construct several disjoint sub-meshes. This is achieved by modeling the proximity grouping problem to graph partitioning problem. The second one, pattern-based proximity grouping, is proposed by the pattern classification schemes to achieve the goal of the proximity grouping. By simulation results, we show the routing performance gains over the traditional Hamiltonian-path routing scheme.||||
46|10||Measurement based analysis of temporal behaviour as support for scheduling problems in parallel and distributed real-time systems|Static analysis, based on scheduling techniques, provides the most typical approach for validation of real-time systems. However, in the case of complex real-time systems such as parallel and distributed systems, many simplifications are made in order to make analysis tractable. This means that even if the system can be statically validated, the real behaviour of the system in execution may be different enough from its theoretical behaviour to make it invalid. In these cases, an analysis based on measurement of the system in execution constitutes an invaluable aid to the static analysis. This article describes a methodology for the analysis of the temporal behaviour of parallel and distributed real-time systems with end-to-end constraints. The analysis is based on the measurement of a prototype of the system in execution and is supported by a behavioural model. The main components of the model are the sequences of activities throughout the system tasks (transactions), which are carried out in response to input events, causing the corresponding output events. Thus, the temporal behaviour of the system is viewed as a set of real-time transactions competing for the available resources. This article also includes experimental results of applying the methodology to the analysis of a well-known case study.||||
46|10||Parameterization of efficient dynamic reconfigurable trees|We present an algorithm for dynamic reconfiguration from a set of processor nodes connected using a multistage interconnection network into a set of m-ary trees of height h. The algorithm allows parameterization based on the branching factor m, the height of the tree h and bias B and produces a set of isomorphic trees for each value of the bias B. The computation of the identities of the neighbors by the nodes is performed using simple binary operations in parallel.||||
46|11|http://www.sciencedirect.com/science/journal/13837621/46/11|Chained backplane communication architecture for scalable multiprocessor systems|A scalable backplane topology which allows a practically unlimited number of modules with identical interfaces is presented. Short, buffered, point-to-point connections overcome clock skew problems. Synchronized, pipelined data transfer operations ensure high throughput and reasonably low latency times for fine-grain parallel algorithms. A simple bus interface logic without any special hardware configuration guarantees a cheap implementation with standard FPGAs. The measured performance in our FPGA based prototype with 32 bit wide data bus shows a throughput of 160 Mbytes/s for each module with 75 ns latency time between modules.||||
46|11||Distributed vector architectures|Integrating processors and main memory is a promising approach to increase system performance. Such integration provides very high memory bandwidth that can be exploited efficiently by vector operations. However, traditional vector applications would easily overflow the limited memory of a single integrated node. To accommodate such workloads, we propose the Distributed Vector Architecture (DIVA), that uses multiple vector-capable processor/memory nodes in a distributed shared-memory configuration, while maintaining the simple vector programming model. The advantages of our approach are twofold: (i) we dynamically parallelize the execution of vector instructions across the nodes, (ii) we reduce external traffic, by mapping vector computation – rather than data – across the nodes. We propose run-time mechanisms to assign elements of the architectural vector registers on nodes, using the data layout across the nodes as a blueprint. We describe DIVA implementations with a traditional request-response memory model and a data-push model. Using traces of vector supercomputer programs, we demonstrate that DIVA generates considerably less external traffic compared to single or multiple-node alternatives that are based solely on caching or paging. With timing simulations we show that a DIVA system with 2–8 nodes is up to three times faster than a single node using its local memory as a large cache and can even outperform a hypothetical system where the application fits in local memory.||||
46|11||Fixed priority scheduling of tasks with arbitrary precedence constraints in distributed hard real-time systems|This paper considers the schedulability analysis of real-time distributed applications where tasks may present arbitrary precedence relations. It is assumed that tasks are periodic or sporadic and dynamically released. They have fixed priorities and hard end-to-end deadlines that are equal to or less than the respective period. We develop a method to transform arbitrary precedence relations into release jitter. By eliminating all precedence relations in the task set one can apply any available schedulability test that is valid for independent task sets.||||
46|11||An output queueing analysis of multipath ATM switches|The performance of output buffers in multipath ATM switches is closely related to the output traffic distribution, which characterizes the packet arrival rate at each output link connected to the output buffer of a given output port. Many multipath switches produce nonuniform output traffic distributions even if the input traffic patterns are uniform. Focusing on the nonuniform output traffic distribution, we analyze the output buffer performances of several multipath switches under both uniform and nonuniform input traffic patterns. It is shown that the output traffic distributions are different for the various multipath switches and the output buffer performance measured in terms of packet loss probability and mean waiting time improves as the nonuniformity of the output traffic distribution becomes higher.||||
46|11||Performance evaluation of system architectures with validated input data|In this paper we have extended our methodology, presented earlier in [1], for generating and validating representative traces. Our novel technique was applied to a relatively realistic and difficult multimedia benchmark suite called MediaMark. We have also introduced a new metric called the K-metric (Khalid – metric) that was used for validation. The aim of our present research was to demonstrate that the proposed methodology can be successful even for complex and challenging benchmarks like multimedia benchmarks. Earlier, our methodology was shown to be the most successful one as compared to the popular contemporary techniques for tracing relatively simple and primitive suite of applications contained within SPEC95 benchmark suite  [1]. Experimental results in this article demonstrate that our methodology works even in the worst case scenarios.||||
46|11||An efficient implementation of tree-based multicast routing for distributed shared-memory multiprocessors|This paper presents an efficient routing and flow control mechanism to implement multidestination message passing in wormhole networks. The mechanism is a variation of tree-based multicast with pruning to recover from deadlocks and it is well suited for distributed shared-memory multiprocessors (DSMs) with hardware cache coherence. It does not require any preprocessing of multicast messages reducing notably the software overhead required to send a multicast message. Also, it allows messages to use any deadlock-free routing function. The new scheme has been evaluated by simulation using synthetic loads. It achieves multicast latency reductions of 30% on average. Also it was compared with other multicast mechanisms proving its benefits. Finally, it can be easily implemented in hardware with minimal changes to existing unicast wormhole routers.||||
46|11||Performance of simultaneous multithreaded multimedia-enhanced processors for MPEG-2 video decompression|This paper explores microarchitecture models for a simultaneous multithreaded (SMT) processor with multimedia enhancements. We start with a wide-issue superscalar processor, enhance it by the SMT technique, by multimedia units, and by an additional on-chip RAM storage. Our workload is a multithreaded MPEG-2 video decompression algorithm that extensively uses multimedia units. The simulations show that a single-threaded, 8-issue maximum processor (assuming an abundance of resources) reaches an instructions per cycle (IPC) count of only 1.60, while an 8-threaded 8-issue processor is able to reach an IPC of 6.07. A more realistic processor model reaches an IPC of 1.27 in the single-threaded 8-issue vs 3.03 in the 4-threaded 4-issue and 3.21 in the 8-threaded 8-issue modes. Our conclusion on next generation’s microprocessors is that a 2- or 4-threaded 4-issue processor with a small on-chip RAM accessed by a local load/store unit will be superior to a wide-issue (single-threaded) superscalar processor at least for MPEG-2 style video decompression algorithms.||||
46|11||Efficient module selections for finding highly acceptable designs based on inclusion scheduling|In high level synthesis, module selection, scheduling, and resource binding are inter-dependent tasks. For a selected module set, the best schedule/binding should be generated in order to accurately assess the quality of a module selection. Exhaustively enumerating all module selections and constructing a schedule and binding for each one of them can be extremely expensive. In this paper, we present an iterative framework, called WiZard to solve module selection problem under resource, latency, and power constraints. The framework associates a utility measure with each module. This measurement reflects the usefulness of the module for a given a design goal. Using modules with high utility values should result in superior designs. We propose a heuristic which iteratively perturbs module utility values until they lead to good module selections. Our experiments show that by keeping modules with high utility values, WiZard can drastically reduce the module exploration space (approximately 99.2% reduction). Furthermore, the module selections formed by these modules belong to superior solutions in the enumerated set (top 15%).||||
46|12|http://www.sciencedirect.com/science/journal/13837621/46/12|YOMNA â An efficient deadlock-free multicast wormhole algorithm in 2-D mesh multicomputers|A mesh network is a popular architecture which has been implemented in many multicomputer systems. It is preferred because it offers useful edge connectivity and is partitioned into units that are still meshes. It is also scalable and has a number of features that make it particularly amenable to high-performance computing. The 2-D mesh topology has become increasingly important to multicomputer design because of its many desirable properties including scalability, low bandwidth and fixed degree of nodes.||||
46|12||Stride prefetching for the secondary data cache|A prefetch method that enables stride prefetching at the secondary cache without accessing the processor's internal resources is developed and evaluated. It uses a data-range-table that enables it to detect usable strides and memory access streams which fall into the same data range. By using program driven simulation of scientific applications in the context of shared-memory multiprocessors, it is shown that the proposed method can reduce load stall times by an amount comparable to a conventional stride driven prefetching method which requires access to the processor's instruction address register.||||
46|12||On the relative performance merits of hypercube and hypermesh networks|||||
46|12||Design of a large-scale Gbit/s MAN using a cyclic reservation-based MAC protocol|In this paper, a large-scale Gbit/s metropolitan area network (MAN) based on hierarchical ring topologies has been investigated. The network is constituted by backbone and local rings, which are connected by bridges. In the network, traffic congestion may occur in bridges due to the mismatch of transmission speed between backbone and local rings. To cope with the issue, a scheme based on a cyclic reservation-based access method is proposed for the medium access control (MAC) protocol of the network. With the scheme, the protocol can both achieve the fair access of network bandwidth and resolve the traffic congestion in bridges. Particularly, this approach reduces the implementation complexity and cost and enhances efficiency of the global network. Finally, several simulative experiments are performed, and some optimistic results are revealed.||||
46|12||Symbolic forward/backward traversals of large finite state machines|Symbolic traversals are state-of-the-art techniques for proving the input/output equivalence of finite state machines. Due to state space explosion, they are currently limited to medium-small circuits. Starting from the limits of standard techniques, this paper presents a mix of approximate forward and exact backward traversals that results in an exact exploration of the state space of large machines. This is possible, thanks to efficient pruning that restricts the search space during backward traversal using the information coming from the approximate forward traversal step. Experimental results confirm that we are able to explore for the first time some of the larger ISCAS'89 and MCNC circuits, that have been until now outside the scope of exact symbolic techniques. We are also able to generate the test patterns for or to tag as undetectable stuck-at faults with few exceptions.||||
46|13|http://www.sciencedirect.com/science/journal/13837621/46/13|Rapid prototyping of an ATM programmable associative operator|In this paper, we describe a semi-automatic method for designing a programmable architecture related to high speed communication protocols. A case study of associative based architecture of high speed communication system is presented with a validation environment. The environment provides an interesting estimation using XILINX prototyping board including memories (content addressable memory, CAM, RAM, DPRAM). In our approach, we try to perform a rapid prototyping of such architecture and allow the designer to interact easily in order to customize the architecture according to application requirements. This method of validation provides important benefits in hardware prototyping: better validation environment and reduced time to give a real estimation for a large variety of applications.||||
46|13||Role-based access control in DCOM|The explosive growth of the Web, the increasing popularity of PCs and the advances in high-speed network access have brought distributed computing into the mainstream. To simplify network programming and to realize component-based software architecture, distributed object models have emerged as standards. One of those models is distributed component object model (DCOM) which is a protocol that enables software components to communicate directly over a network in a reliable, and efficient manner. In this paper, we investigate an aspect of DCOM concerning software architecture and security mechanism. Also, we describe the concept of role-based access control (RBAC) which began with multi-user and multi-application on-line systems pioneered in the 1970s. And we investigate how we can enforce the role-based access control as a security provider within DCOM, specially in access security policy.||||
46|13||Efficient approaches for constructing a massively parallel processing system|How to construct a massively parallel processing system has drawn a lot of attention. An important feature affecting the performance and characteristics of the architecture with an interconnection of multiple processors is its configuration design and scalability of the system. Good performance of a parallel application is often the result of an appropriate match between the processes of the parallel application and the configuration of multiple processor system. In order to accommodate different parallel applications, it is highly desirable that the configuration of the architecture and the number of processors can be modified easily and extended flexibly. Two efficient technical approaches for constructing a massively parallel processing system with scalability are proposed. One is based on multilayered motherboards and the other on crossbar switches. The technique and the two approaches have been adapted in a real-life application which is a parallel implementation of backpropagation neural computation. Performance evaluation for the two approaches is included.||||
46|13||A scheduling policy for preserving cache locality in a multiprogrammed system|In a multiprogrammed system, when the operating system switches contexts, in addition to the cost for handling the processes being swapped out and in, the cache performance of processors also can be affected. If frequent context switching replaces the data loaded into cache memory before they are completely reused, the programs suffer from cache misses due to the damage in cache locality. In particular, for the programs with good cache locality, such as blocked programs, a scheduling mechanism of keeping cache locality against context switching is essential to achieve good processor utilization. To solve this requirement, we propose a preemption-safe policy to exploit the cache locality of blocked programs in a multiprogrammed system. The proposed policy delays context switching until a block is fully reused, but also compensates for the monopolized processor time on processor scheduling mechanisms. Our simulation results show that in a situation where blocked programs are run on multiprogrammed shared-memory multiprocessors, the proposed policy improves the performance of these programs due to a decrease in cache misses. In such situations, it also has a beneficial impact on the overall system performance due to the enhanced processor utilization.||||
46|13||Analytical modeling of multithreaded architectures|Multithreading is used for hiding long memory latency in uniprocessors and multiprocessor computer systems and aims at increasing system efficiency. In such an architecture, a number of threads are allocated to each processing element (PE) and whenever a running thread becomes suspended the PE switches to another ready thread.||||
46|13||Quantitative evaluation of pipelining and decoupling a dynamic instruction scheduling mechanism|||||
46|13||Optimal bandwidth allocation and stability of high-speed networks for CSMA/CD protocols|In this paper, we examine a non-persistent CSMA/CD protocol for high-speed communication systems, where the bandwidth is divided into two separate asymmetric channels. Free stations access the first channel while all retransmissions occur in the second channel. We define the stability regions and the rules for optimal bandwidth allocation among the two channels for optimization of the system performance in case of infinite population. Numerical results show that the optimal behaviour gives performance improvement as compared with the single-channel system with the same capacity.||||
46|13||On the derivation of a correct deadlock free communication kernel for loop connected message passing architecture from its user's specification|Method for the derivation of a correct deadlock free communication kernel for loop connected message passing architecture from its user's specification is described. Dijkstra's weakest pre-condition approach is used as the specification language.||||
46|14|http://www.sciencedirect.com/science/journal/13837621/46/14|Profiling in the ASP codesign environment|Automation of the hardware/software codesign (HSC) methodology brings with it the need to develop sophisticated high-level profiling tools. This paper presents a profiling tool which uses execution profiling on standard C code to obtain accurate and consistent times at the level of individual compound code sections. This tool is used in the ASP hardware/software codesign project. The results from this tool show that profiling must be performed on dedicated hardware which is as close as possible to the final implementation, as opposed to a workstation. Further, in this paper a formula is derived for the number of times a program has to be profiled in order to get an accurate estimate of the number of times a loop with an indeterminate loop count is executed.||||
46|14||Architecture for fractal image compression|||||
46|14||A section cache system designed for VLIW architectures|The static specification of operations executed in parallel using No Operations (NOPs) is another culprit to make code size to be increased in VLIW architecture. Some alternatives in the instruction encoding and memory subsystem are proposed to minimize the impact of NOP on the code size. One is the compressed cache using the packed encoding scheme and the other is the decompressed cache using the unpacked encoding scheme. The compressed cache shows high memory utilization but increases the pipeline branch penalty because it requires very complex fetch hardware. On the contrary, the fetch overhead can be decreased in the decompressed cache because the unpacked encoding scheme allows an instruction to be issued to the pipeline without any recovery process. However, it has a shortcoming that the memory utilization is deteriorated due to the memory allocation irrespective of the number of useful operations. In this research, a new instruction encoding scheme called a semi-packed encoding scheme and the section cache, which enables effective store and retrieval of semi-packed instructions, are proposed. This can decrease the hardware complexity to fetch an instruction and the wasted memory space due to NOPs via the partially fixed length of an instruction. The experimental results reveal that the memory utilization in the section cache is 3.4 times higher than in the decompressed cache. The memory subsystem using the section cache can provide about 15% performance improvement with the moderate size of chip area.||||
46|14||Synthesising an asynchronous DMA controller with Balsa|A DMA controller has been designed and implemented as part of the AMULET3i asynchronous microprocessor macrocell using a mixture of synchronous and asynchronous circuit techniques. The synthesis language Balsa has been used to implement the major part of the controller, the asynchronous control. The use of Balsa has allowed the controller to be rapidly re-engineered in response to a changing specification.||||
46|14||ACTion: Combining logic synthesis and technology mapping for MUX-based FPGAs|||||
46|14||A novel approach for implementing high-speed and long-distance networking protocols in a limited memory embedded kernel|STREAMS kernel mechanisms are being used to implement networking protocols in limited memory embedded systems. The current approaches for STREAMS-based networking protocols implementation suffer from some shortcomings when these approaches are used to implement high-speed and long distance WAN protocols that require large transport windows. Accumulation of duplicate copies of large transport layer windows in the subnet layer cause unnecessary hogging of kernel memory resources. This memory hogging leads to performance degradation since shortage of buffers forces the protocols to operate at smaller window sizes. The reason for this overhead is that the protocols cannot be implemented efficiently due to strict layering scheme of STREAMS. We have devised new kernel mechanisms to provide solution to this problem. We have defined new mapping mechanisms between protocol layers and have coupled these mechanisms with novel “event-based” flow control mechanisms. These mechanisms provided appropriate flow control handling in the kernel that led to significant reduction in memory buffers hogging. This makes embedded systems handle large windows efficiently.||||
46|14||Communication in a multi-layer MIMD system for computer vision|||||
46|15|http://www.sciencedirect.com/science/journal/13837621/46/15|An on-chip cache compression technique to reduce decompression overhead and design complexity|This research explores a compressed memory hierarchy model which can increase both the effective memory space and bandwidth of each level of memory hierarchy. It is well known that decompression time causes a critical effect to the memory access time and variable-sized compressed blocks tend to increase the design complexity of the compressed memory systems. This paper proposes a selective compressed memory system (SCMS) incorporating the compressed cache architecture and its management method. To reduce or hide decompression overhead, this SCMS employs several effective techniques, including selective compression, parallel decompression and the use of a decompression buffer. In addition, fixed memory space allocation method is used to achieve efficient management of the compressed blocks. Trace-driven simulation shows that the SCMS approach can not only reduce the on-chip cache miss ratio and data traffic by about 35% and 53%, respectively, but also achieve a 20% reduction in average memory access time (AMAT) over conventional memory systems (CMS). Moreover, this approach can provide both lower memory traffic at a lower cost than CMS with some architectural enhancement. Most importantly, the SCMS is a more attractive approach for future computer systems because it offers high performance in cases of long DRAM latency and limited bus bandwidth.||||
46|15||Efficient parity placement schemes for tolerating up to two disk failures in disk arrays|||||
46|15||KAIST image computing system (KICS): A parallel architecture for real-time multimedia data processing|An efficient parallel architecture is proposed for high-performance multimedia data processing using multiple multimedia video processors (MVP; TMS320C80), which are fully programmable general digital signal processors (DSP). This paper describes several requirements for a multimedia data processing system and the system architecture of an image computing system called the KAIST Image Computing System (KICS). The performance of the KICS is evaluated in terms of its I/O bandwidth and the execution time for some image processing functions. An application of the KICS to the real-time Moving Picture Expert Group 2 (MPEG-2) encoder is introduced. The programmability and the high-speed data-access capability of the KICS are its most important features as a high-performance system for real-time multimedia data processing.||||
46|15||Agents for information retrieval: Issues of mobility and coordination|This paper focuses on agent-based applications for information retrieval on the Web, by specifically analysing mobility and coordination issues. On the one hand, mobile agents well suit the requirements of information retrieval in the new dynamic scenario derived from the Internet. This is due to their capability of moving to the place where the information is stored – therefore saving bandwidth – and to their robustness in the presence of unreliable connections. On the other hand, the search for information by several mobile active agents calls for suitable models to rule the interactions among agents and between agents and execution environments. The paper surveys different coordination approaches and evaluates their impact in information retrieval applications based on mobile agents. The survey outlines the advantages of uncoupled coordination models and points out the suitability of a coordination model based on reactive and programmable tuple spaces: they may increase the safety and the security of the environment while simplifying the task of programming distributed mobile agent applications.||||
46|15||Embedded software verification in hardwareâsoftware codesign|Concurrent Embedded Real-Time Software (CERTS) is intrinsically different from traditional, sequential, independent, and temporally unconstrained software. The verification of software is more complex than hardware due to inherent flexibilities (dynamic behavior) that incur a multitude of possible system states. The verification of CERTS is all the more difficult due to its concurrency and embeddedness. The work presented here shows how the complexity of CERTS verification can be reduced significantly through answering common engineering questions such as when, where, and how one must verify embedded software. First, a new Schedule-Verify-Map strategy is proposed to answer the when question. Second, verification under system concurrency is proposed to answer the where question. Finally, a complete symbolic model checking procedure is proposed for CERTS verification. Several application examples illustrate the usefulness of our technique in increasing verification scalability.||||
46|15||A new cache architecture based on temporal and spatial locality|A data cache system is designed as low power/high performance cache structure for embedded processors. Direct-mapped cache is a favorite choice for short cycle time, but suffers from high miss rate. Hence the proposed dual data cache is an approach to improve the miss ratio of direct-mapped cache without affecting this access time. The proposed cache system can exploit temporal and spatial locality effectively by maximizing the effective cache memory space for any given cache size. The proposed cache system consists of two caches, i.e., a direct-mapped cache with small block size and a fully associative spatial buffer with large block size. Temporal locality is utilized by caching candidate small blocks selectively into the direct-mapped cache. Also spatial locality can be utilized aggressively by fetching multiple neighboring small blocks whenever a cache miss occurs. According to the results of comparison and analysis, similar performance can be achieved by using four times smaller cache size comparing with the conventional direct-mapped cache.And it is shown that power consumption of the proposed cache can be reduced by around 4% comparing with the victim cache configuration.||||
46|15||Early design stage exploration of fixed-length block structured architectures|An important challenge concerning the design of future microprocessors is that current design methodologies are becoming impractical due to long simulation runs and due to the fact that chip layout considerations are not incorporated in early design stages. In this paper, we show that statistical modeling can be used to speed up the architectural simulations and is thus viable for early design stage explorations of new microarchitectures. In addition, we argue that processor layouts should be considered in early design stages in order to tackle the growing importance of interconnects in future technologies. In order to show the applicability of our methodology which combines statistical modeling and processor layout considerations in an early design stage, we have applied our method on a novel architectural paradigm, namely a fixed-length block structured architecture. A fixed-length block structured architecture is an answer to the scalability problem of current architectures. Two important factors prevent contemporary out-of-order architectures from being scalable to higher levels of parallelism in future deep-submicron technologies: the increased complexity and the growing domination of interconnect delays. In this paper, we show by using statistical modeling and processor layout considerations, that a fixed-length block structured architecture is a viable architectural paradigm for future microprocessors in future technologies thanks to the introduction of decentralization and a reduced register file pressure.||||
46|15||Index|||||
46|15||Index|||||
46|2|http://www.sciencedirect.com/science/journal/13837621/46/2|GSPN models of bridged LAN configurations1|||||
46|2||Optimum reserved resource allocation scheme for handoff in CDMA cellular system|CDMA cellular systems support two types of handoff: soft handoff and hard handoff. Soft handoff has many advantages over hard handoff. Qualitatively, this feature provides more reliable handoff between base stations as a mobile moves from one cell to the adjacent one. Quantitatively, it considerably increases both the capacity of a heavily loaded multicellular system and the coverage of each individual cell in a lightly loaded system. In this paper, in order to increase the probability of soft handoff at the time of handoff, optimum reserved resource allocation scheme for handoff in CDMA cellular system is proposed, which allocates reserved resource to each frequency channel according to the number of neighbor cells using the same frequency channel. Performance analysis results show that the proposed scheme has higher probability of soft handoff at the time of handoff and higher total call processing performance which is a function of both new calls accepted probability and soft handoff probability than the conventional scheme allocating reserved resource irrelevantly to frequency channels being used by neighbor cells.||||
46|2||MAP: Design and implementation of a mobile agents' platform|The recent development of telecommunication networks has contributed to the success of applications such as information retrieval and electronic commerce, as well as all the services that take advantage of communication in distributed systems. In this area, the emerging technology of mobile agents aroused considerable interest. Mobile agents are applications that can move through the network for carrying out a given task on behalf of the user. In this work we present a platform called MAP (Mobile Agents Platform) for the development and the management of mobile agents. The language used both for developing the platform and for carrying out the agents is Java. The platform gives the user all the basic tools needed for creating some applications based on the use of agents. It enables us to create, run, suspend, resume, deactivate, reactivate local agents, to stop their execution, to make them communicate each other and migrate.||||
46|2||Performance analysis of video storage server under initial delay bounds|Previous studies on video storage servers focused on improving the disk throughput and reducing the server buffer size. However, the initial delay of a new request, one of the most important quality of service (QoS) parameters from the users' point of view, is almost neglected while designing a storage subsystem or evaluating its performance. For different types of video-on-demand (VOD) services such as interactive video game, digital library, or movie-on-demand system, the initial delay can vary from 0.5 s to 5 min. This criterion brings some impacts on designing a storage server for a particular VOD application. In this paper, we investigate the storage server design and the performance evaluation of VOD systems with different initial delay guarantees. We propose a new performance model on evaluating the efficiency of a video storage server so that a cost-effective configuration can be easily obtained under a specified initial delay bound.||||
46|2||A scheme for multiple on-chip signature checking for embedded SRAMS|Embedded read/write memories are integral parts of many VLSI chips designed for specific applications in the areas of computer communications, multimedia, and digital signal processing. Testing an embedded memory poses a challenge to a system test engineer, due to its limited controllability and observability. In this paper, we propose a pseudorandom built-in self test (BIST) scheme to solve this problem. Our technique is based on a test architecture known as multiple on-line signature checking (MOSC) which offers a very low aliasing probability and a high degree of confidence in testing. While the MOSC scheme is sufficiently general and applicable to any digital circuit, it can especially be optimized for circuits with embedded memories. We present interesting test scheduling algorithms that reduce the overhead of testing. On several industry-standard benchmark circuits, we report up to 35% savings in test area overhead.||||
46|2||A fault tolerant routing algorithm based on cube algebra for hypercube systems|We propose an approach to determine the shortest path between the source and the destination nodes in a faulty or a non-faulty hypercube. The number of faulty nodes and links may be rather large and if any path between the nodes exists, the developed algorithm determines it. To construct this algorithm, some properties of the cube algebra are considered and some transformations based on this algebra are developed.||||
46|3|http://www.sciencedirect.com/science/journal/13837621/46/3|An improved registerâtransfer level functional partitioning approach for testability1|||||
46|3||An extended-UIO-based method for protocol conformance testing|Verification of protocols is performed through conformance testing. The aim of this paper is to introduce the conformance test generation approach for protocols described by means of Finite State Machines. A functional fault model is adopted and the state discrimination is performed by applying an extended version of Unique Input Output Sequences (UIO), which, different from classical UIO, can always be found in any state. Both algorithms for efficient extended UIO sequence identification and for optimal test sequence generation are presented, together with the experimental results on different protocol descriptions.||||
46|3||Multiple context multithreaded superscalar processor architecture|Superscalar architecture is becoming the norm in today's high performance microprocessor design. However, achievable instruction level parallelism in programs limits the scalability of such architectures. In this paper, we introduce the Multiple Context Multithreaded Superscalar Processor (MCMS), which is an extension of conventional superscalar processor architecture to support multithreading. This is motivated by the enormous potential instruction level parallelism present in multithreaded programs. A hardware implementation of multithreaded constructs is also proposed. Results from trace-driven simulation show that with the MCMS, instruction level parallelism is indeed increased significantly. A MCMS processor with four hardware contexts can produce a speedup of up to 2.5 times over superscalar processor with similar hardware resources. We found that the primary limitation shifts from data dependencies in the superscalar processor to resource contentions in MCMS.||||
46|3||Impact of the memory interface structure in the memory-processor integrated architecture for computer vision|The memory-based processor array (MPA) was previously designed as an effective memory-processor integrated architecture. The MPA can be easily attached into any host system via memory interface. In this paper, the impact of the memory interface structure is analytically analyzed for computer vision tasks. An analytical model is constructed to describe the characteristics of the memory interface structure. Performance improvement for the memory interface model of the MPA system can be 6–40% for vision tasks consisting of sequential and data parallel tasks. Mapping algorithms to implement convolution and connected component labeling on the MPA are also presented. The asymptotic time complexities of the algorithms are evaluated to verify the cost-effectiveness and the efficiency of the MPA system.||||
46|3||Development of process visualization systems: An object-oriented approach|In this paper a process visualization development system and its associated development methodology are presented. This methodology is optimized to systems that have complex structure and are built of large number of components belonging to relatively small number of types. In order to handle the complexity, the input requirements of the method are as close to the “native language” of the application as possible. The elements of the native language are assumed to include engineering drawings and manuals describing the operation of the component types the system is built of. Graphics techniques are used to supply the engineering drawings into the development system while not only the required visual appearance is described but the structure of the underlying system is also defined. The elements of engineering drawings are dynamized to animate the graphics presentation, to reflect the current state of the monitored system. Component manuals are transformed to interface and state definitions from which a code generator generates a C++ class for each component type. This C++ class must be tuned to reflect the operation of a single component type. From these definitions the development system automatically builds up the complete visualization program, providing easy and fast application development.||||
46|3||Fault tolerant permutation mapping in multistage interconnection network|An efficient scheme for fault tolerant mapping of permutations is designed. The proposed algorithm uses extra passes through the network, instead of additional hardware.||||
46|4|http://www.sciencedirect.com/science/journal/13837621/46/4|Editorial|||||
46|4||The meaning and role of value in scheduling flexible real-time systems1|The real-time community is devoting considerable attention to flexible scheduling and adaptive systems. One popular means of increasing the flexibility, and hence effectiveness, of real-time systems is to use value-based scheduling. It is surprising however, how little attention has been devoted, in the scheduling field, to the actual assignment of value. This paper deals with value assignment and presents a framework for undertaking value-based scheduling and advises on the different methods that are available. A distinction is made between ordinal and cardinal value functions. Appropriate techniques from utility theory are reviewed. An approach based on constant value modes is introduced and evaluated via a case example.||||
46|4||Schedulability analysis of periodic and aperiodic tasks with resource constraints|In this paper, we address the problem of scheduling hybrid task sets consisting of hard periodic and soft aperiodic tasks that may share resources in exclusive mode in a dynamic environment, where tasks are scheduled based on their deadlines. Bounded blocking on exclusive resources is achieved by means of a dynamic resource access protocol which also prevents deadlocks and chained blocking. Aperiodic responsiveness is enhanced by an efficient servicing technique which assigns each aperiodic request a suitable deadline. Feasibility conditions are extended to handle tasks with deadlines different from periods and a reclaiming technique is presented to deal with early completions.||||
46|4||Complete worst-case execution time analysis of straight-line hard real-time programs|In this article, the problem of finding a tight estimate on the worst-case execution time (WCET) of a hard real-time program is addressed. The analysis is focused on straight-line code (without loops and recursive function calls) which is quite commonly found in synthesised code for embedded systems. A comprehensive timing analysis system covering both low-level (assembler instruction level) as well as high-level aspects (programming language level) is presented. The low-level analysis covers all speed-up mechanisms used for modern superscalar processors: pipelining, instruction-level parallelism and caching. The high-level analysis uses the results from the low-level to compute the final estimate on the WCET. This is done by a heuristic for searching the longest really executable path in the control flow, based on the functional dependencies between various program parts.||||
46|4||Techniques to increase the schedulable utilization of cache-based preemptive real-time systems1|Nowadays, cache memories are applicable to real-time systems with the help of tools that obtain the worst-case execution time (WCET) of cached programs. However, these tools do not allow preemption, because from the point of view of program analysis, the number of preemptions is unknown. To face this problem, the cache-related preemption cost can be considered in the schedulability analysis, or annulled by the use of private cache partitions. This paper comprises a number of techniques using the first or both solutions. This paper also explores the harmonic relationships among tasks to improve the estimation of the cache interference in the analysis.||||
46|4||Operating system support for the management of hard real-time disk traffic|Emerging applications like C3I systems, real-time databases, data acquisition systems and multimedia servers require access to secondary storage devices under timing constraints. In this paper, we focus on operating system support needed for managing real-time disk traffic with hard deadlines. We present the design and implementation of a preemptive deadline-driven disk I/O subsystem suitable for real-time disk traffic management. Preemptibility is achieved with a granularity that is automatically controlled by the I/O subsystem according to current workload and filesystem data layout. An admission control test checks the current resource availability for a given workload. We show that contiguous layout is necessary to maintain hard real-time guarantees and a reasonable level of disk throughput. Finally, we show how buffering can be used to obtain utilization factors close to the maximum disk bandwidth possible.||||
46|4||Issues and approaches to supporting timeliness and security in real-time database systems1|||||
46|5|http://www.sciencedirect.com/science/journal/13837621/46/5|On the performance of distributed objects|Early distributed shared memory systems used the shared virtual memory approach with fixed-size pages, usually 1–8 KB. As this does not match the variable granularity of sharing of most programs, recently the emphasis has shifted to distributed object-oriented systems. With small object sizes, the overhead of inter-process communication could be large enough to make a distributed program too inefficient for practical use. To support research in this area, we have implemented a user-level distributed programming testbed, DIPC, that provides shared memory, semaphores and barriers. We develop a computationally-efficient model of distributed shared memory using approximate queueing network techniques. The model can accommodate several algorithms including central server, migration and read-replication. These models have been carefully validated against measurements on our distributed shared memory testbed. Results indicate that for large granularities of sharing and small access bursts, central server performs better than both migration and read-replication algorithms. Read-replication performs better than migration for small and moderate object sizes for applications with high degree of read-sharing and migration performs better than read-replication for large object sizes for applications having moderate degree of read-sharing.||||
46|5||A case study of a distributed high-performance computing system for neurocomputing|||||
46|5||Improving cache performance with Full-Map Block Directory|There are two concurrent paths in a typical cache access – one through the data array and the other through the tag array. In most cases, the path through the tag array is significantly longer than that through the data array. In this paper, we propose a new scheme that exploits this imbalance in the tag and data paths to improve overall cache performance. Under this scheme, an additional tag directory, the Full-Map Block Directory, is used to provide an alternate tag path to speed up cache access for almost all the memory requests. This scheme is based on the observation that spatial locality exists on a cache line basis, i.e., cache lines near one another tend to be referenced together. Performance evaluation using a TPC-C-like benchmark and selected applications from the SPEC92 benchmark suite demonstrates that this scheme has the potential to improve overall system performance by more than 20%.||||
46|5||Markovian and analytical models for multiple bus multiprocessor systems with memory blockings|This paper deals with the performance evaluation of asynchronous multiple bus multiprocessor systems where memory modules are temporarily unavailable to processor requests. Such systems cannot be modelled with a product-form queueing network (QN) model because of memory blockings. In this context, we first propose an exact continuous-time Markovian QN model for analyzing small size systems. In order to study medium to large size systems, we also propose several approximate lumped Markovian models and two approximate analytical QN models. The robustness of these approximate models is studied when any memory module gets systematically blocked after each access to this memory module. Results are compared against those obtained either with the exact Markovian QN model or with a stochastic simulation model.||||
46|6|http://www.sciencedirect.com/science/journal/13837621/46/6|On the design of IP routers Part 1: Router architectures|Internet Protocol (IP) networks are currently undergoing transitions that mandate greater bandwidths and the need to prepare the network infrastructures for converged traffic (voice, video, and data). Thus, in the emerging environment of high performance IP networks, it is expected that local and campus area backbones, enterprise networks, and Internet Service Providers (ISPs) will use multigigabit and terabit networking technologies where IP routers will be used not only to interconnect backbone segments but also to act as points of attachments to high performance wide area links. Special attention must be given to new powerful architectures for routers in order to play that demanding role. In this paper, we describe the evolution of IP router architectures and highlight some of the performance issues affecting IP routers. We identify important trends in router design and outline some design issues facing the next generation of routers. It is also observed that the achievement of high throughput IP routers is possible if the critical tasks are identified and special purpose modules are properly tailored to perform them.||||
46|6||Systolic arrays architecture for computing the time-frequency spectrum|To solve the problem of detecting and displaying the changes in the spectrum of non-stationary signals, there are two possible approaches. Either, one uses the same estimators as for the stationary signals but with shorter length data blocks during which it is assumed to be stationary, or one can use the same length data and apply a time-varying (TV) spectrum estimator which accounts for the non-stationarity. A TV spectrum estimator called time-varying correlogram (TVC) is a well known estimator of the time-frequency (TF) spectrum of a non-stationary signal. In this paper, a VLSI architecture for computing the TVC is proposed.||||
46|6||The diagnosability of hypercubes with arbitrarily missing links|We study the problem of determining diagnosability for incomplete hypercubes that have arbitrarily distributed missing links, under the classic PMC diagnostic model and its variant, the BGM model. Based on the result proved in this paper, for both models, in most cases the diagnosability of an incomplete hypercube can be determined by simply checking the link degree of each node.||||
46|6||O(n) routing in rearrangeable networks|In (2n−1)-stage rearrangeable networks, the routing time for any arbitrary permutation is Î©(n2) compared to its propagation delay O(n) only. Here, we attempt to identify the sets of permutations, which are routable in O(n) time in these networks. We define four classes of self-routable permutations for Benes network. An O(n) algorithm is presented here, that identifies if any permutation P belongs to one of the proposed self-routable classes, and if yes, it also generates the necessary control vectors for routing P. Therefore, the identification, as well as the switch setting, both problems are resolved in O(n) time by this algorithm. It covers all the permutations that are self-routable by anyone of the proposed techniques. Some interesting relationships are also explored among these four classes of permutations, by applying the concept of ‘group-transformations’ [N. Das, B.B. Bhattacharya, J. Dattagupta, Hierarchical classification of permutation classes in multistage interconnection networks, IEEE Trans. Comput. (1993) 665–677] on these permutations. The concepts developed here for Benes network, can easily be extended to a class of (2n−1)-stage networks, which are topologically equivalent to Benes network. As a result, the set of permutations routable in a (2n−1)-stage rearrangeable network, in a time comparable to its propagation delay has been extended to a large extent.||||
46|6||Stripped mirroring RAID architecture|Redundant arrays of independent disks (RAID) provide an efficient stable storage system for parallel access and fault tolerance. The most common fault tolerant RAID architecture is RAID-1 or RAID-5. The disadvantage of RAID-1 lies in excessive redundancy, while the write performance of RAID-5 is only 1/4 of that of RAID-0. In this paper, we propose a high performance and highly reliable disk array architecture, called stripped mirroring disk array (SMDA). It is a new solution to the small-write problem for disk array. SMDA stores the original data in two ways, one on a single disk and the other on a plurality of disks in RAID-0 by stripping. The reliability of the system is as good as RAID-1, but with a high throughput approaching that of RAID-0. Because SMDA omits the parity generation procedure when writing new data, it avoids the write performance loss often experienced in RAID-5.||||
46|7|http://www.sciencedirect.com/science/journal/13837621/46/7|Low power architectures for digital signal processing|Low power architectures for digital signal processing algorithms requiring inner product computation are presented. In the first step a power efficient memory organization exploiting data reuse is determined. In the second step an order of evaluation of the partial products that reduces the switching activity at the inputs of the computational units is derived. Information related to both coefficients which are static and data which are dynamic, is used to drive the reordering of computation. Experimental results for several signal processing algorithms prove that the proposed techniques lead to significant savings in net switching activity and thus in power consumption.||||
46|7||A conditional abortable priority ceiling protocol for scheduling mixed real-time tasks|Priority Ceiling Protocol (PCP) is a well-known resource access protocol for hard real-time systems. However, it has a problem of ceiling blocking which imposes a great hindrance to task scheduling in mixed real-time systems where tasks may have different criticality. In this paper, a new resource access protocol called the Conditional Abortable Priority Ceiling Protocol (CA-PCP) is proposed. It resolves the problem of ceiling blocking by incorporating a conditional abort scheme into the PCP. The new protocol inherits all the desirable properties of the PCP and the Ceiling Abort Protocol (CAP) which is yet another modification of the PCP. In the proposed protocol, a condition is defined to control the abort of a job so that the schedulability of the system will not be affected. Performance study has been done to compare the CA-PCP with the PCP. The results indicate that CA-PCP can significantly improve the performance of a system if the lengths of the abortable critical sections are well chosen.||||
46|7||Algorithms for real-time scheduling of error-cumulative tasks based on the imprecise computation approach|This paper presents several algorithms for real-time scheduling that have been developed following an approach known as Imprecise Computation. This technique prevents timing faults (i.e., results not produced in time) by offering an approximate result of an acceptable quality whenever the exact result of the desired quality cannot be obtained in time. In this work we focused our attention on the problem of scheduling a set of real-time error-cumulative periodic tasks for which errors in different periods have cumulative effects, making it necessary to generate timely, precise results sometimes. This problem is particularly relevant in applications such as route tracking and real-time control of complex industrial plants. The adopted approach consists in applying a transformation of the set of error-cumulative tasks into a set of error-noncumulative tasks that can be easily scheduled by means of an optimal algorithm like the earliest deadline first. This approach permits to obtain scheduling algorithms for some specific, but quite significant, problems. The algorithms have a polynomial complexity, whereas most of the solutions found in literature are NP-hard.||||
46|7||Real-time multimedia standards in DQDB|In the present work, the necessary and sufficient conditions for the real-time schedulability of Dual Queue Dual Bus (DQDB) networks are formally proved, strictly adhering to the letter of the standard as it stands today. With those results, the quality of the standardized multimedia applications that a given DQDB network can carry is analysed.||||
46|7||Worst-case deadline failure probability in real-time applications distributed over controller area network|Real-time applications distributed over the controller area network (CAN) are generally characterised by stringent temporal and dependability constraints. Our goal is to take account of transmission errors in the design of such applications because the consequences of such disturbances are potentially disastrous. In this study, the concept of worst-case deadline failure probability (WCDFP) is introduced. The motivation of the probabilistic approach is that, in practice, the number of errors occurring during a given time period can with difficulty be bounded. To evaluate the WCDFP, we propose, on the one hand, a method of computing for each message the tolerable threshold of transmission errors under which timing constraints are guaranteed to be met. On the other hand, we also suggest an error model enabling us to consider both error frequency and error gravity. Our error model follows a generalized Poisson process and its stochastic parameters have been derived. We then propose a numerically efficient algorithm to compute the probabilities and apply the analysis to an industrial case-study of the automotive field.||||
46|7||Validation of SPEC6â¢ CFP95 traces for accurate performance evaluation of computer systems|Performance evaluation of computer systems and processors is necessary during design, procurement, and capacity planning. The increasing complexity of realistic benchmarks combined with the relatively sluggish rate of detailed performance analysis has resulted in an ever-increasing gap between the size of the workload/traces and the speed of analysis. In this paper, we present a methodology for generating and validating representative traces from SPEC™95 benchmark suite using R-metric and K-metric. Experimental results demonstrate the superiority of our proposed technique over the ubiquitous profile-driven methodology and the uniform sampling approach.||||
46|8|http://www.sciencedirect.com/science/journal/13837621/46/8|Heterogeneous distributed and parallel architectures: Hardware, software and design tools|||||
46|8||Techniques for mapping tasks to machines in heterogeneous computing systems|Heterogeneous computing (HC) is the coordinated use of different types of machines, net-works, and interfaces to maximize their combined performance and/or cost-effectiveness. HC systems are becoming a plausible technique for efficiently solving computationally intensive problems. The applicability and strength of HC systems are derived from their ability to match computing needs to appropriate resources. In an HC system, tasks need to be matched to machines, and the execution of the tasks must be scheduled. The goal of this invited keynote paper is to: (1) introduce the reader to some of the different distributed and parallel types of HC environments; and (2) examine some research issues for HC systems consisting of a network of different machines. The latter purpose is pursued by considering: (1) the quantification of heterogeneity; (2) the characterization of techniques for mapping (matching and scheduling) tasks on such systems; (3) an example HC resource management system; and (4) static and dynamic heuristics for mapping tasks to machines in such HC systems.||||
46|8||Efficient use of parallel libraries on heterogeneous Networks of Workstations|||||
46|8||Vertically-partitioned parallel signature file method|Recently, parallel signature file methods have been proposed for better retrieval performance in signature files. In this paper, we propose a vertically-partitioned parallel signature file (VPSF) method which can partition a signature file vertically. Our VPSF method uses an extendable hashing technique for dynamic environment and uses a frame-sliced signature file technique for efficient retrieval. Our VPSF method also can eliminate the data skew and the execution skew by allocating each frame to a processing node. To prove the efficiency of our VPSF method, we compare its performance with those of the conventional parallel signature file methods, i.e., HPSF and Hamming filter, in terms of retrieval time, storage overhead, and insertion time. The experimental result shows that our VPSF achieves about 40% better retrieval performance than the Hamming filter. In addition, we evaluate the performance of our VPSF methods on several normal distributions with half and double standard deviations of the real data. From the performance evaluation on record sets with half standard deviation, we show that our VPSF gains about 20–50% improvement in retrieval time, compared with the Hamming filter and the HPSF. Finally, we show that our VPSF generally outperforms the conventional parallel signature files on retrieval performance when the records of a database are uniform in size.||||
46|8||On parallel solvers for sparse triangular systems|In this paper we describe and compare two different methods for solving general sparse triangular systems in distributed memory multiprocessor architectures. The two methods involve some preprocessing overheads so they are primarily of interest in solving many systems with the same coefficient matrix. Both algorithms start off from the idea of the classical substitution method. The first algorithm we present introduces a concept of data driven flow and makes use of non-blocking communications in order to dynamically extract the inherent parallelism of sparse systems. The second algorithm uses a reordering technique for the unknowns, so the final system can be grouped in variable blocksizes where the rows are independent and can be solved in parallel. This latter technique is called level scheduling because of the way it is represented in the adjacency graph. These methods have been tested in the Fujitsu AP1000 and the Cray T3D and T3E multicomputers. The performance has been analysed using matrices from the Harwell-Boeing collection.||||
46|8||Achieving high degree of concurrency in multidatabase transaction scheduling: MTOS|A multidatabase system (MDBS), which is also called a heterogeneous distributed database system (HDDBS) or federated database system (FDBS), is a facility that allows users or applications to access data located in multiple local database systems (LDBSs), each of which is autonomously operated. Several practical MDBS concurrency control schemes have been proposed whilst maintaining global consistency without compromising local autonomy. However, they could degrade either local concurrency of local transactions or global concurrency of global transactions for the purpose of ensuring the global serializability. In this paper, we propose a new ticket-based global concurrency control scheme that employs multiple tickets at each site. The basic idea is that, with proper consideration of the potential cyclic global serialization orders, subtransactions with the possibility of the local indirect conflicts must access the same ticket at their sites, otherwise they are allowed to access the different tickets. This scheme alleviates the blockage on local resources, and, as a result, increases both global concurrency and local concurrency, whilst preserving the local autonomy.||||
46|8||Transaction multicasting scheme for resilient routing control in parallel cluster database systems|A disk cluster environment (DCE) refers to a distributed architecture for high performance transaction processing in which the computing nodes are locally coupled via a high-speed network and share a common database at the disk level. In the DCE, it is crucial to determine at which node the incoming transactions are processed. This is called transaction routing. The aim of disk sharing in DCE is not only to achieve high performance by distributing the workload among the processing nodes but also to obtain fault-tolerance against possible system failures, like a single node failure. Although a number of transaction routing schemes have been reported for DCE, it is true that most of them are not sufficiently resilient against system dynamics, which inevitably requires changing the routing information. In this paper, we propose a new dynamic transaction routing scheme for DCE, called multicast transaction routing scheme, MTR for short, that is able to change the transaction routing information in the presence of critical events without imposing too much overhead to the transaction processing system. In our scheme, when it is required to change the routing information dynamically, the routing algorithm sends multiple clones of a transaction to a group of candidate processing nodes and selects the processing node that first completes the multicasted transaction as a new processing node for re-routed transaction. The selected processing node is expected to be a best affinity node when the system load is evenly distributed, or a relatively unloaded processing node that is idle enough to process a transaction faster than other nodes. The novel aspect of MTR is that it automatically achieves an optimal balance between affinity-based routing and load balancing. The simulation study shows that MTR rapidly stabilizes the system and produces an optimal routing information so that it finally guarantees faster response time.||||
46|9|http://www.sciencedirect.com/science/journal/13837621/46/9|Testing and built-in self-test â A survey|As the density of VLSI circuits increases it becomes attractive to integrate dedicated test logic on a chip. This built-in self-test (BIST) approach not only offers economic benefits but also interesting technical opportunities with respect to hierarchical testing and the reuse of test logic during the application of the circuit.||||
46|9||Register bypassing in an asynchronous superscalar processor|Register bypassing, universally provided in synchronous processors, is more difficult to implement in an asynchronous design. Asynchronous bypassing requires synchronization between the forwarding and receiving units, with the danger that the advantages of asynchronous operation may be nullified by reintroducing the lock-step operation of synchronous processors. We present a novel implementation of register bypassing in an asynchronous processor architecture. Our technique of Decoupled Operand Forwarding provides centralized control over the bypassing operation, yet allows multiple execution units to function asynchronously. Our ideas are presented within the context of the development of Hades, a generic asynchronous processor architecture. We employ single-issue and dual-issue simulations of Hades to quantify the benefits of Decoupled Operand Forwarding and conclude that Decoupled Operand Forwarding yields significant speedups because of its success in removing register files from the critical timing path.||||
46|9||Multistage ring network: An interconnection network for large scale shared memory multiprocessors|Unidirectional ring-based networks are currently popular choices for high performance large scale shared memory multiprocessors. This class of networks is attractive for their simple hardware interfaces, high speed communication, wider data path, and easy addition of extra nodes. However, a single ring does not scale well due to the fixed bandwidth, and the hierarchical ring networks as a natural extension of a single ring show limited scalability due to their limited bandwidth near the root. In this paper we present a new interconnection network called the Multistage Ring Network (MRN). The MRN has a 2-level hierarchy of rings, and its interconnection of global rings forms a type of the multistage network. The architecture of the MRN is effective at diffusing the global traffic on the network to all global rings, and the bandwidth of the network increases proportionally with increases in the system size. Our results show that in a peak throughput, the MRN performs seven times better than the hierarchical ring network for system size of 1024.||||
46|9||On the design of hypermesh interconnection networks for multicomputers|Topology, routing algorithm, and router structure are among the most important factors that greatly influence network performance. This paper assesses the interaction of these elements on two related but distinct types of multicomputer networks, the binary n-cube (or cube) and the hypermesh. The analysis will show that the topological properties of the hypermesh confer an important advantage over the cube that makes the former a promising option for use in high-performance multicomputers. The hypermesh can use simple routing algorithms and cheap routers with little performance penalty. The cube, on the other hand, is constrained to the use of a specific routing algorithm and complex routers to take advantage of its rich connectivity.||||
46|9||Integrated dynamic scheduling of hard and QoS degradable real-time tasks in multiprocessor systems|Many time-critical applications require predictable performance and tasks in these applications have deadlines to be met. For tasks with hard deadlines, a deadline miss can be catastrophic while for Quality of Service (QoS) degradable tasks (soft real-time tasks) timely approximate results of poorer quality or occasional deadline misses are acceptable. Imprecise computation and (m,k)-firm guarantee are two workload models that quantify the trade-off between schedulability and result quality. In this paper, we propose dynamic scheduling algorithms for integrated scheduling of real-time tasks, represented by these workload models, in multiprocessor systems. The algorithms aim at improving the schedulability of tasks by exploiting the properties of these models in QoS degradation. We also show how the proposed algorithms can be adapted for integrated scheduling of multimedia streams and hard real-time tasks, and demonstrate their effectiveness in quantifying QoS degradation. Through simulation, we evaluate the performance of these algorithms using the metrics – success ratio (measure of schedulability) and quality. Our simulation results show that one of the proposed algorithms, multilevel degradation algorithm, outperforms the others in terms of both the performance metrics.||||
46|9||An atomic commit protocol for gigabit-networked distributed database systems|In the near future, different database sites will be interconnected via gigabit networks, forming a very powerful distributed database system. In such an environment, the propagation latency will be the dominant component of the overall communication cost while the migration of large amounts of data will not pose a problem. Furthermore, computer systems are expected to become even more reliable than today’s systems with long mean time between failures and short mean time to repair. In this paper, we present implicit yes-vote (IYV), a one-phase atomic commit protocol, that exploits these new domain characteristics to minimize the cost of distributed transaction commitment. IYV eliminates the explicit voting phase of the two-phase commit protocol, hence reducing the number of sequential phases of message passing during normal processing. In the case of a participant’s site failure, IYV supports the option of forward recovery by enabling partially executed transactions that are still active in the system to resume their execution when the failed participant is recovered.||||
||||||||
volume|issue|url|title|abstract||||
47|1|http://www.sciencedirect.com/science/journal/13837621/47/1|High-level co-simulation based on the extension of processor simulators|Hardware–software co-design is the cornerstone in the design of complex systems that involve both hardware and software. This paper presents a co-design approach where the co-simulation between hardware and software takes place early enough in the design cycle. The proposed platform is based on the extension of existing instruction set processor simulators in order to encapsulate hardware block description in the required and adequate accuracy level. The simplicity of the proposed technique as well as the use of homogeneous simulation environment, leads to a co-simulation alternative that is easy to implement and use. The applicability of the co-simulation environment developed is exhibited through the design and co-simulation, at various abstraction layers, of a telecommunication application. The latter, is based on the MAC layer and the RF-IF part of the Physical layer of the DECT protocol stack.||||
47|1||Symbolic two-dimensional minimization of strongly unspecified finite state machines|||||
47|1||Labeled rough partitions â a new general purpose representation for multiple-valued functions and relations|In this paper, we present a new data structure for representing multiple-valued (MV) relations (functions in particular), both completely and incompletely specified, and an associated set of manipulation algorithms. Relations are represented by labeled rough partitions, structure similar to rough partitions introduced in [T. Luba, Decomposition of multiple-valued functions, in: Proceedings of the 25th ISMVL, 1995, pp. 256–261] but extended with labels to store the full information about relations. We present experimental results from comparison of our data structure to binary decision diagrams (BDDs) on binary functions (MCNC benchmarks) showing its superiority in terms of memory requirements in 73% cases. The new representation can be used to a large class of MV, completely and incompletely specified functions and relations, typical for machine learning and complex finite state machine (FSM) controller optimization applications.||||
47|1||Secure communication protocols with discrete nonlinear chaotic maps|The discrete nonlinear chaotic maps (DNCMs) exploit a novel approach to encryption: the information is injected to a properly designed DNCM system and affects its dynamics. The evolution of a proper variable of this system composes the transmitted ciphertext. Consequently, this variable controls the dynamics of another DNCM system that acts as the decipher.||||
47|1||Congestion-free embedding of 2(nâk) spanning trees in an arrangement graph|The arrangement graph An,k is not only a generalization of star graph (n−k=1), but also more flexible. In this investigation, we elucidate the problem of embedding of multiple spanning trees in an arrangement graph with the objective of congestion-free. This result is to report how to exploit 2(n−k) edge disjoint spanning trees in an arrangement graph, where each congestion-free spanning tree's height is 2k−1. Our scheme is based on a subgraph-partitioning scheme. First, we construct 2(n−k) base spanning trees in every An−k+2,2. Then, we recursively construct 2(n−k) spanning trees from every An−k+2,2 up to An,k by a bottom-up approach. This is a near-optimal result since all of possible edges in the base subarrangement An−k+2,2 are fully utilized.||||
47|10|http://www.sciencedirect.com/science/journal/13837621/47/10|Behavioral test generation for the selection of BIST logic|The identification of the most suited BIST architecture is one of the bottlenecks in the actual application of self-testing techniques. The aim of this paper is the investigation of possible relations between the behavioral level specification of the circuit, and the structural level, where BIST logic is inserted. We propose to use behavioral test patterns to guide the selection of the most appropriate BIST architecture with respect to the given application as a trade-off between fault coverage and area overhead. The correlation between the behavioral analysis and the actual fault coverage of the inserted BIST logic has been shown on a number of benchmarks.||||
47|10||Studies of the SEMATECH IDDq test data|In the first part of this paper, we studied a few variations of a current signature method on the SEMATECH test data. We discovered two important facts: (1) many troublesome IDDq behaviors in the present and future technology were found in the SEMATECH data, (2) for those troublesome data, a relation was observed between the mean and the variance of the test data. Based on these findings, we proposed the second order current signature technique which considers both the first order mean and the second order variance information to provide a more robust and more effective mean of die selection. We examined the IDDq testing data from SEMATECH by the proposed second order current signature technique and compared the results to those of traditional single threshold and delta-IDDq techniques. We found that the proposed second order analysis may enable a more robust way to IDDq testing. In particular, the proposed method correctly identified all 12 known bad dies, while the delta-IDDq allows five of them to pass.||||
47|10||A prototype of a VHDL-based fault injection tool: description and application|This paper presents the prototype of an automatic and model-independent fault injection tool, to be used on an IBM-PC (or compatible) platform. The tool has been built around a commercial VHDL simulator and it is thought to implement different fault injection techniques. With this tool, a wide range of transient and permanent faults can be injected into medium-complexity models. Another remarkable aspect of the tool is the fact that it can analyse the results obtained from injection campaigns, in order to study the Error Syndrome of the system model and/or validate its fault-tolerance mechanisms. Some results of various fault injection campaigns carried out to validate the Dependability of a fault-tolerant microcomputer system are shown. We have analysed the pathology of the propagated errors, measured their latencies, and calculated both error detection and recovery latencies and coverages.||||
47|10||A new approach for critical area estimation in VLSI|||||
47|10||Quality-effective repair of multichip module systems|||||
47|10||A fault tolerant multistage interconnection network with partly duplicated switches|A multistage interconnection network (MIN) with partly duplicated stages is proposed in this paper, and network performance and fault tolerance are analyzed. The MIN is a hybrid of a non-redundant baseline network and a conventional fault-tolerant MIN called an ELMIN. In the case of a MIN with N input terminals and N output terminals, switching elements (SEs) in the first and nth stages are duplicated where n=log2N, and four-input two-output SEs and two-input four-output SEs are employed in the second and (n−1)th stages respectively. These extra SEs and links are useful in improving the fault tolerance and performance of the MIN and do not complicate the routing algorithm. A comparison of an ELMIN with the proposed MIN shows that this new approach is superior both in theoretical throughput and performance in faulty cases, even though it requires at most 1.33 times as many links and cross points in the SEs.||||
47|11|http://www.sciencedirect.com/science/journal/13837621/47/11|Folded-crossed hypercube: a complete interconnection network|In this paper, the complete folded-crossed hypercube FCQn is proposed based on the folded hypercube [IEEE Trans. Parall. Distrib. Syst. 2 (1) (1991) 31], the enhanced hypercube [IEEE Trans. Comput. 40 (3) (1991) 284] and the crossed hypercube CQn [IEEE Trans. Parall. Distrib. Syst. 3 (5) (1992) 513]. FCQn has such appealing properties as diameter of ⌈n/2⌉, degree of n+1, much short mean internode distance about 75–62.5% of that of n-cube for nâ©¾6, and very low message traffic density only about 0.667–0.625 for nâ©¾4. Therefore, FCQn is a high-performance-low-cost architecture. Additionally, a general formula of mean internode distance of the complete hypercube-type architecture is given in order to analyze different hypercube-type architectures easily.||||
47|11||Studies on striping and buffer caching issues for the software RAID file system|A software RAID file system is defined as a system that distributes data redundantly across an array of disks attached to each of the workstations connected on a high-speed network. This configuration provides higher throughput and availability compared to conventional file systems. In this paper, we consider two specific issues regarding the distribution of data among the cluster, namely, striping and buffer caching for such an environment. Through simulation studies we compare the performance of various striping methods and show that for effective striping in software RAID file systems, it must take advantage of its flexible nature. Further, for buffer caching, we show that conventional caching schemes developed for distributed systems are insufficient, and that the Exclusively Old Data and Parity scheme that is presented in this paper, overcomes the limitations of the previously proposed schemes.||||
47|11||A novel three-level architecture for large data warehouses|Classical architectures proposed so far for data warehouses show some drawbacks when adopted to work over large numbers of heterogeneous operational sources. In this paper we propose a variant of a three-level architecture for data warehouses that overcomes these drawbacks. However, in the application context under consideration, having a suitable architecture may be not enough for the design purposes. Indeed, data warehouse design in very large operational environments can be a quite hard problem to attack with traditional manual methodologies. In this paper, automatic techniques are also illustrated that are capable to produce the data warehouse design according to the proposed architecture, with a limited human intervention.||||
47|12|http://www.sciencedirect.com/science/journal/13837621/47/12|A global approach to improve conditional hardware reuse in high-level synthesis|The degree of conditional hardware reuse achieved after a high-level synthesis process depends on two factors: the number of mutually exclusive (mutex) operations pairs that an algorithm can detect and the description style used by the designer when specifying the system. In this paper, we propose a method that deals with both aspects. First, we propose a simple and homogeneous mechanism to analyze the input description and identify all the mutex operations pairs, independently of the conditional constructs (IF or CASE) used to specify the control flow of the system, and independently of the operators (logic or relational) needed to specify the conditions. Second, we provide a collection of formal transformations on the input description in order to overcome the “non-intended” design decisions (related to implicit hardware reuse) taken by the designer when writing the system description. Their application produces a specification of the same behavior that leads to improved implementations––in terms of the degree of conditional reuse that is achieved. Both facilities are possible thanks to the chosen internal representation mechanism, which is a mathematical model of the description, and to an underlying formal calculus that allows the description to be correctly manipulated. These ideas have been implemented in an algorithm that obtains better results than previous approaches.||||
47|12||NULL convention multiply and accumulate unit with conditional rounding, scaling, and saturation|Approaches for maximizing throughput of self-timed multiply–accumulate units (MACs) are developed and assessed using the NULL convention logic paradigm. In this class of self-timed circuits, the functional correctness is independent of any delays in circuit elements, through circuit construction, and independent of any wire delays, through the isochronic fork assumption  and , where wire delays are assumed to be much less than gate delays. Therefore self-timed circuits provide distinct advantages for System-on-a-Chip applications.||||
47|13|http://www.sciencedirect.com/science/journal/13837621/47/13|Dual-tree-based multicasting on wormhole-routed irregular switch-based networks|Recently, network of workstations (NOWs) is emerging as an inexpensive alternative to massively parallel processors (MPPs). The irregular switch-based networks are proposed to build NOWs for high performance parallel computing. In this paper, we address a dual-tree-based routing model and propose an efficient dual-tree-based multicasting algorithm with optimum level-based destination-switch partition strategy on irregular switch-based networks. The dual-tree-based routing scheme supports adaptive, distributed, and deadlock-free multicast on switch-based networks with double channels. We first describe a dual-tree structure established, based on the concept of network partitioning, from the irregular networks and prove that the multicasting based on this structure is deadlock-free. Then, an efficient multicast routing algorithm with optimum level-based partition strategy is proposed. Finally, the experimental results are given to show that the multicast performance based on tree-based multicasting scheme can be promoted by the dual-tree-based multicasting scheme with optimum level-based destination-switch partition strategy.||||
47|13||Architectural differences of efficient sequential and parallel computers|In this paper we try to conclude what kind of a computer architecture is efficient for executing sequential problems, and what kind of an architecture is efficient for executing parallel problems from the processor architect's point of view. For that purpose we analytically evaluate the performance of eight general purpose processor architectures representing widely both commercial and scientific processor designs in both single processor and multiprocessor setups. The results are interesting. The most efficient architecture for sequential problems is a two-level pipelined VLIW (very long instruction word) architecture with few parallel functional units. The most efficient architecture for parallel problems is a deeply inter-thread superpipelined architecture in which functional units are chained. Thus, designing a computer for efficient sequential computation leads to a very different architecture than designing one for efficient parallel computation and there exists no single optimal architecture for general purpose computation.||||
47|2|http://www.sciencedirect.com/science/journal/13837621/47/2|Interfaces for mixed-level simulation with sequential elements|It is well known that techniques such as performance modeling that can be used to effectively evaluate design alternatives early in the design process can greatly increase the quality of the ultimate implementation, while at the same time, decrease the design time. In order to gain the maximum benefit from performance modeling, it must be integrated into the design process such that the performance model can be directly refined into an implementation. This paper presents techniques for developing interfaces between abstract performance models and detailed behavioral models to enable this refinement process. These mixed-level modeling interfaces, as they are called, allow abstract performance models to be cosimulated with detailed behavioral models.||||
47|2||The architecture of a Gaussian mixture Bayes (GMB) robot position estimation system|Modelling and reducing uncertainty are two essential problems with mobile robot localisation. In this paper, a new robot position estimator, the Gaussian mixture of Bayes (GMB) which utilises a density estimation technique, is introduced in particular. The proposed system, namely the GMB robot position estimator, which allows a robot's position to be modelled as a probability distribution, and uses Bayes' theorem to reduce the uncertainty of its location. In addition, we describe, in this paper, how our proposed system is capable of dealing with multiple sensors, as well as a single sensor only. Nevertheless, it is known that such multiple sensors could be used to raise more robust than the single sensor, in terms of obtaining accurate estimate over a robot's position. The GMB position estimator mainly consists of four modules such as sonar-based, sensor selection, sensor fusion, and sensor selection improved by combining it with sensor fusion. The proposed system is also illustrated with respect to minimising the uncertainty of a robot's position, using the Nomad200 mobile robot shown in Fig. 1. Eventually, it was found that the proposed system was capable of constraining the position error of the robot by the modularity of the system.||||
47|2||Self-repairable GALs|This paper describes the concept of self-testable and self-repairable Generic Array Logic (GAL) devices for high security and safety applications. A design methodology is proposed for self-repairing of a GAL which is a kind of Electrically Programmable Logic Devices (EPLDs). The fault-locating and fault-repairing architecture with electrically re-configurable GALs is presented. It uses universal test sets, fault-detecting logic, and self-repairing circuits with spare devices. The design method allows to detect, diagnose, and repair of all multiple stuck-at faults which might occur on E2CMOS cells in programmable AND plane. A self-repairing methodology is presented, based on our design architecture. A “column replacement” method with extra columns is introduced that discards each faulty column entirely and replaces it with an extra column. The evaluation methodology proves that a self-repairable GAL will last longer in the field. It gives also information on how many extra columns a GAL needs to reach a lifetime goal, in terms of simulation looping time, until the GAL is not useful any more. Therefore, an ideal point could be estimated, where the maximum reliability can be reached with the minimum cost.||||
47|2||Functional decomposition with an efficient input support selection for sub-functions based on information relationship measures|||||
47|2||Validation of Patient Headache Care Education System (PHCES) using a software reuse reference model|The research goal in our study of educational medical information systems tools is to improve the performance of patient decision making by integrating medical professional information with computer-augmented information and to benefit patients for short and long periods of time. In part, this goal may be reached by cost-effective software development with validated reusable Patient Care Education Systems object design models, user interface, object-oriented approach for product line and with the help of a valid Patient Headache Care Education System (PHCES) derived from these models.||||
47|2||Reachability analysis of large circuits using disjunctive partitioning and partial iterative squaring|Reachability analysis is an orthogonal, state-of-the-art technique for the verification and validation of finite state machines (FSMs). Due to the state space explosion problem, it is currently limited to medium-small circuits, and extending its applicability is still a key issue. Among the factors that limit reachability analysis, let us list: the peak binary decision diagrams (BDD) size during image computation, the BDD size to represent state sets, and very high sequential depth. Following the promising trend of partitioning, we decompose a finite state machine into “functioning-modes”. We operate on a disjunctive partitioned transition relation. Decomposition is obtained heuristically based on complexity, i.e., BDD size, or functionality, i.e., dividing memory elements into “active” and “idle” ones. We use an improved iterative squaring algorithm to traverse high-depth subcomponents. The resulting methodology attacks the above problems, lowering intermediate peak BDD size, and dealing with high-depth subcomponents. Experiments on a few industrial circuits and on some large benchmarks show the feasibility of the approach.||||
47|2||Using fundamental electrical theory for varying time quantum uni-processor scheduling|Given the total number of instructions to be completed on a uni-processor system and the average cycle time per instruction we introduce a method of calculating time quantum allocation to individual fine grain tasks. The main theory behind our method is based on fundamental equations describing electrical phenomenon. We show how electric circuit analysis can be used to describe the fundamental scheduling problem, and provide a framework for defining more elaborate scheduling problems such as multiprocessor and multicomputer task scheduling. As a matter of physical soundness we demonstrate through unit derivation that our electrical analogy provides proper physical quantities that are supported by current literature. Our analysis shows that variable time round-robin scheduling (VTRR) provides a more appropriate means of scheduling fine-grain tasks than constant time round-robin scheduling (CTRR). We prove that, our VTRR scheduler always completes at least one task per cycle. We show through numerical comparisons some differences between VTRR and CTRR performance.||||
47|2||Conversations with fixed and potential participants|||||
47|3-4|http://www.sciencedirect.com/science/journal/13837621/47/3-4|Modern methods and tools in digital system design|||||
47|3-4||Quality-driven design in the system-on-a-chip era: Why and how?|Modern microelectronic technology enables implementation of a complete complex information processing system on a single chip. Progress in microelectronic technology is extremely fast and is outstripping the system designers' abilities to make use of the created opportunities. The complexity and quality of the microelectronics-based systems as well as their design and production cost and time tend to be more limited by the design methods and tools than by the microelectronic technology. Substantial improvement can only be achieved through development and application of a new generation of more suitable design paradigms, methods and tools. In this paper, some new opportunities and difficulties related to the system-on-a-chip technology are considered, the nature of the complex system design problems is analyzed and a quality-driven system design methodology is proposed and discussed. Some important implications are also considered of the modern quality concepts for design modeling, design exploration techniques and tools, design decision making, design reuse and design validation.||||
47|3-4||Grammar-based design of embedded systems|Grammars define syntax of languages and as such have not been commonly considered as methods for design, despite well-known applications in computer science. Only in recent years grammar-based design has become a promising research field and the first commercial tools have appeared on the market. This paper reviews the basic concepts of applying grammars to electronic design – in particular to the device driver synthesis of communication protocols for embedded software, to the design of custom-hardware, and to the virtual prototyping of DSP systems. The paper shows the power of these methods, presents the latest research results and discusses future developments in this field.||||
47|3-4||Constraints-driven design space exploration for distributed embedded systems|This paper presents a new method for design space exploration for distributed embedded systems. The method is based on constraint logic programming (CLP) and make it possible to model distributed embedded systems and design requirements using finite domain constraints. Design space exploration tools can then use this model to find different solutions satisfying constraints. An advantage of this method is its flexibility to defined design constraints and ability to mix both manual design decisions and automatic optimization methods. The solution of the set of constraints provides a final system implementation which, by definition, satisfies all imposed design constraints. Both algorithms guaranteeing optimal solutions and heuristic methods can be used in the optimization phase.||||
47|3-4||V-SAT: A visual specification and analysis tool for system-on-chip exploration|We describe V-SAT, a tool for performing design space exploration of system-on-chip (SOC) architectures. The key components of V-SAT include EXPRESSION, a language for specification of the architecture, SIMPRESS, a simulator generator for analysis/evaluation of the architecture, and the V-SAT GUI front-end for easy specification and detailed analysis. We give a brief overview of the components (EXPRESSION, SIMPRESS and GUI) and, using an example DLX architecture, demonstrate V-SAT's usefulness in exploration for an embedded SOC codesign flow by specifying and evaluating several modifications to the pipeline structure of the processor. We believe that V-SAT provides a powerful environment, both for early design space exploration, as well as for the detailed design of SOC architectures.||||
47|3-4||Kernel scheduling techniques for efficient solution space exploration in reconfigurable computing|Run-time reconfigurable systems have emerged as a flexible way of facing with a single device a wide range of applications. The adequate exploitation of the available hardware resources requires the development of design tools that support the design process. Task scheduling becomes a very critical issue in achieving the high performance that DSP and multimedia applications demand. This paper addresses task scheduling in reconfigurable computing for these kinds of applications. First, we discuss the main issues involved in the problem. Then, we propose a new methodology with some bounding and pruning techniques in order to produce an efficient exploration of the design space. This work has been developed for Morphosys, a coarse-grain multi-context reconfigurable architecture, and it can be successfully applied to systems with similar features.||||
47|3-4||High-level synthesis using hierarchical conditional dependency graphs in the CODESIS system|In previous work in behavioral high-level synthesis (HLS), data-flow and control-flow dominated descriptions are treated separately. A result of such a separation is that efficient techniques have been developed for the HLS of data-flow dominated behavioral descriptions. However, HLS of control-flow dominated descriptions still lags behind. To close this gap in this paper we propose a complete HLS framework based on an internal design representation where control- and data-flows are uniformly represented and disposes a formal foundation. Based on it conditional behaviors can be efficiently scheduled combining conditional resource sharing (CSR), speculative execution (SE) and other formal graph transformations. These techniques and tools covering all HLS activities have been organized in the CODESIS tool destined for both research and educational purposes.||||
47|3-4||An integrated system for developing regular array designs|This paper describes an integrated system for developing regular array designs based on the block description language Ruby. Ruby supports concise design description and formal verification. A parametrised Ruby description can be used in simulating, refining and visualising designs, and in compiling hardware implementations such as field programmable gate arrays. Our system enables rapid design production, while good design quality is achieved by (a) the efficient instantiation of device-specific libraries, (b) the size optimisation of bit-level components using the design refiner, and (c) the exploitation of regularity information at source level in the library composition process. The development and implementation of several median filters are used to illustrate the system.||||
47|3-4||Execution cost interval refinement in static software analysis|Embedded system software timing and power consumption or, in general, execution costs are state- and input-data dependent. Therefore, formal analysis of such dependencies leads to execution cost intervals rather than single values. These intervals depend on system concurrency, execution paths and process states, as well as on target architecture properties. This paper presents an approach to modeling and analysis of process behavior using intervals. Unlike other static software analysis approaches, it considers program properties and the execution context, i.e. the current state and input of a process. The example of an ATM switch component demonstrates significant improvements in analysis precision.||||
47|3-4||Design-for-testability to achieve complete coverage of delay faults in standard full scan circuits|We propose a testability enhancement technique for delay faults in standard scan circuits that does not involve modifications to the scan chain. Extra logic is placed on next-state variables, and if necessary, on primary inputs, and can be resynthesized with the circuit to minimize its hardware and performance overheads. The proposed technique allows us to achieve complete coverage of detectable delay faults by allowing any two-pattern test to be applied to the circuit through its functional path. In addition to the basic approach, we study the proposed procedure in the presence of a constraint that requires that extra logic would not be placed on the critical paths of the circuit.||||
47|3-4||POPS: A tool for delay/power performance optimization|Based on an incremental path search algorithm, this paper addresses the problem of performance-driven path classification by sizing selected gates on the shortest and the longest identified paths of the circuit. Delay and power/area constraints are managed using circuit path sizing alternatives defined through a realistic evaluation of gate power and delay. Implemented in the performance optimization by path selection (POPS) tool, the accuracy of this technique is compared to evaluation obtained from industrial tools [23] on examples of path enumeration and optimization evaluated on several ISCAS'85 benchmarks.||||
47|5|http://www.sciencedirect.com/science/journal/13837621/47/5|Theory and application of non-group cellular automata for message authentication|An ASIC based on cellular automata (CA) for data-authentication has been proposed in this paper. The scheme is designed from the analytical study of the state transition behaviour of non-group CA, and is significantly different from conventional approaches. Experimental studies confirm the superiority in terms of CPU time (50% reduction) of the software implementation of the proposed scheme compared to standard MD-5 algorithm, the most commonly used method. Regular, modular and cascadable structure of CA with local interconnections makes the scheme ideally suitable for VLSI implementation. The CA based hardware can achieve further speed improvement approximately by at least three order, depending on the message length. The proposed scheme supports existing IPv4 data rates on high bandwidth links (800 Mbps HiPPI). A register in the datapath of a processor can be easily converted to a CA to realize the hardware. The design has been specified in Verilog, simulated for functional correctness, and synthesized using the tool Synergy from Cadence.||||
47|5||Fundamental principles of modeling timing in hardware description languages|A fundamental property of digital hardware designs including VLSI designs is timing which underlies the occurrence of hardware activities and their relative ordering. In essence, timing constitutes the external manifestation of the causal relation between the relevant hardware activities. The constituent components of a hardware system are inherently concurrent and the relative time ordering of the hardware activities is critical to the correct functioning of complex hardware system. Hardware description languages (HDLs) are primarily developed for describing and simulating hardware systems faithfully and correctly and must, therefore, be capable of describing timing, accurately and precisely. This paper examines the fundamental nature of timing in hardware designs and develops, through reasoning, the basic principles of modeling timing in HDLs. This paper then traces the evolution of the key syntactic and semantic timing constructs in HDLs starting with CDL and up to the contemporary HDLs including ADLIB–SABLE, Verilog HDL, and VHDL, and critically examines them from the perspective of the basic principles of modeling timing in HDLs. Classical HDLs including CDL are limited to synchronous digital designs. In the contemporary hardware description languages including ADLIB–SABLE, CONLAN, Verilog, and VHDL, the timing models fail to encapsulate the true nature of hardware. While ADLIB and Verilog HDL fail to detect inconsistent events leading to the generation of potentially erroneous results, the concept of delta delay in VHDL which, in turn, is based on the BCL time model of CONLAN, suffers from a serious flaw.||||
47|5||Specification-based program slicing and its applications|More precise program slices could be obtained by considering the semantic relations between variables of interest. In this paper, we present specification-based slicing that allows a better decomposition of the program by taking a specification as its slicing criterion. A specification-based slice consists of a subset of program statements which preserve the behavior and the correctness of the original program with respect to a specification given by a pre–postcondition pair. Because specification-based slicing enables one to focus attention on only those program statements which realize the functional abstraction specified by the given specification, it can be widely used in many software engineering areas. In order to investigate its possible applications, we show how specification-based slicing can improve the process for extracting reusable functions from existing programs and restructuring complex programs for better maintainability.||||
47|5||Gray code clustering of wireless data for partial match queries|This paper proposes a broadcast data clustering method for partial match queries in mobile distributed systems. An effective broadcast data clustering method enables mobile clients to access the data in short latency. Our method utilizes the properties of the Gray coding scheme – Gray codewords have high locality. We describe the way the Gray code method (GCM) effectively clusters wireless data for partial match queries. And we analyze and evaluate the performance of the Gray code clustering method through comparison with other methods.||||
47|5||Analysing value substitution and confidence estimation for value prediction|Value Prediction is one of the newest techniques used to break down ILP limits. Despite being under continuous study during the last few years, a few aspects related to this emerging technique remain unanalysed in depth. Exhaustively investigated in the context of control speculation, confidence estimation has usually played a secondary role on value prediction and speculation. Closely linked to confidence estimation, value substitution also represents a relegated subject of research. This paper is focussed on analysing, in an isolated way, the respective impact on predictor performance of both confidence estimation and value substitution mechanisms. By using detailed pipeline-level simulations, we prove that improvements in these mechanisms are as important as reducing the predictor aliasing or even improving the prediction model.||||
47|6|http://www.sciencedirect.com/science/journal/13837621/47/6|Baldwinian learning utilizing genetic and heuristic algorithms for logic synthesis and minimization of incompletely specified data with Generalized ReedâMuller (ANDâEXOR) forms|This research applies a new heuristic combined with a genetic algorithm (GA) to the task of logic minimization for incompletely specified data, with both single and multi-outputs, using the Generalized Reed–Muller (GRM) equation form. The GRM equation type is a canonical expression of the Exclusive-Or Sum-of-Products (ESOPs) type, in which for every subset of input variables there exists not more than one term with arbitrary polarities of all variables. This AND–EXOR implementation has been shown to be economical, generally requiring fewer gates and connections than that of AND–OR logic. GRM logic is also highly testable, making it desirable for FPGA designs. The minimization results of this new algorithm tested on a number of binary benchmarks are given. This minimization algorithm utilizes a GA with a two-level fitness calculation, which combines human-designed heuristics with the evolutionary process, employing Baldwinian learning. In this algorithm, first a pure GA creates certain constraints for the selection of chromosomes, creating only genotypes (polarity vectors). The phenotypes (GRMs) are then learned in the environment and contribute to the GA fitness (which is the total number of terms of the best GRM for each output), providing indirect feedback as to the quality of the genotypes (polarity vectors) but the genotype chromosomes (polarity vectors) remain unchanged. In this process, the improvement in genotype chromosomes (polarity vectors) is the product of the evolutionary processes from the GA only. The environmental learning is achieved using a human-designed GRM minimization heuristic. As much previous research has presented the merit of AND–EXOR logic for its high density and testability, this research is the first application of the GRM (a canonical AND–EXOR form) to the minimization of incompletely specified data.||||
47|6||Strategies for solving the Boolean satisfiability problem using binary decision diagrams|The Boolean satisfiability (SAT) problem is the problem of finding a solution to the equation f=1, where f is a Boolean formula to be satisfied. Binary decision diagrams (BDDs) have been widely used to solve this problem; each of the individual output requirements of a multiple-output function is represented as a BDD and the conjunction of these requirements (product BDD) provides all satisfying solutions. However, these techniques suffer from BDD size explosion problems. This paper presents two BDD-based algorithms to solve the SAT problem that attempt to contain the growth of BDD size while identifying solutions quickly.||||
47|6||Efficient local memory sequence generation for data parallel programs using permutations|Generating local memory access sequence is a critical issue in distributed-memory implementations of data-parallel languages. In this paper, for arrays distributed block-cyclically on multiple processors, we introduce a novel approach to the local memory access sequence generation using the theory of permutation. By compressing the active elements in a block into an integer, called compress number, and exploiting the fact that there is a repeating pattern in the access sequence, we obtain the global block cycle. Then, we show that the local block cycle can be efficiently enumerated as closed forms using the permutation of global block cycle. After decompressing the compress number in the local block cycle, the local block patterns are restored and the local memory access sequence can be quickly generated. Unlike other works, our approach incurs no run-time overhead.||||
47|6||Efficient parallel timing simulation of synchronous models on networks of workstations|In this paper we address the parallel timing simulation of synchronous VLSI designs on a network of workstations (NOWs). We suggest combining cycle based and conventional timing simulation techniques to achieve fast timing simulation even on NOWs which are typically characterized by low bandwidth and high communication latency. In particular we execute a timing simulator on each node of the NOW and use cycle based simulation to produce synchronization information required by the timing simulators. As synchronization information is generated exclusively by the cycle based simulator there is no need for any communication between the timing simulators. To verify the feasibility and performance of our approach we simulated several circuits using our approach. The results show that a significant speedup can be achieved even for very small circuits.||||
47|6||Buffer management control in data transport network node|The application of analytical queuing theory results in behaviour analysis of a distributed computer network or mobile data system (data transport network). It belongs to the preferred method in comparison to the simulation method. The use of analytical methods allows us to calculate effectively various values of parameters in equilibrium, including the total input intensity of data units to every node of transport network even for the more realistic models than the M/M/1 systems. However, these results are derived assuming an infinite buffer in size at a given node. For practical application, we need to project the concrete number of buffers in every node. This paper describes the method of buffer management control for each decomposed network's node of a data transport network in two real cases. For this purpose, the linear dependence between buffer memory size and input queue size at each node of a data transport network was used.||||
47|6||Embedding of complete binary tree with 2-expansion in a faulty Flexible Hypercube|Although the embedding of complete binary trees in faulty hypercubes has received considerable attention, to our knowledge, no paper has demonstrated how to embed a complete binary tree in a faulty Flexible Hypercube. Therefore, this investigation presents an algorithm to facilitate the embedding job when the Flexible Hypercube contains faulty nodes. Of particular concern are the network structures of the Flexible Hypercube that balance the load before as well as after faults start to degrade the performance of the Flexible Hypercube. Furthermore, to obtain the replaceable node of the faulty node, 2-expansion is permitted such that up to (n−2) faults can be tolerated with congestion 1, dilation 4 and load 1.||||
47|6||Maximizing reliability of distributed computing system with task allocation using simple genetic algorithm|Reliability is one of the very important characteristics of the distributed computing system (DCS), and articles on task allocation (an NP-Hard problem) to maximize the reliability of DCS have appeared in the past [S. Kartik, C.S. Ram Murthy, IEEE Trans. Comput. 46 (6) (1997) 719; S.M. Shatz, Wang, Goto, IEEE Trans. Comput. 41 (90) (1992) 1156]. Genetic Algorithm (GA) has emerged as a successful tool for optimization purposes. We, in this work, have used a simple GA to optimize the reliability of DCS with task allocation.||||
47|7|http://www.sciencedirect.com/science/journal/13837621/47/7|Special issue on Evolutionary computing|||||
47|7||Scalable architecture for parallel distributed implementation of genetic programming on network of workstations|We present an approach for developing a scalable architecture for parallel distributed implementation of genetic programming (PDIGP). The approach is based on exploitation of the inherent parallelism among semi-isolated subpopulations in genetic programming (GP). Proposed implementation runs on cost-efficient configurations of networks on workstations in LAN and Internet environment. Developed architecture features single global migration broker and centralized manager of the semi-isolated subpopulations, which contribute to achieving quick propagation of the globally fittest individuals among the subpopulations, reducing the performance demands to the communication network, and achieving flexibility in system configurations by introducing dynamically scaling up opportunities. PDIGP exploits distributed component object model (DCOM) as a communication paradigm, which as a true system model offers generic support for the issues of naming, locating and protecting the distributed entities in proposed architecture of PDIGP. Experimentally obtained results of computational effort of proposed PDIGP are discussed. The results show that computational effort of PDIGP marginally differs from the computational effort in canonical panmictic GP evolving single large population. For PDIGP running on systems configurations with 16 workstations the computational effort is less than panmictic GP, while for smaller configurations it is insignificantly more. Analytically obtained and empirically proved results of the speedup of computational performance indicate that PDIGP features linear, close to ideal characteristics. Experimentally obtained results of PDIGP running on configurations with eight workstations show close to 8-fold overall speedup. These results are consistent with the anticipated cumulative effect of the insignificant increase of computational effort for the considered configuration and the close to linear speedup of computational performance.||||
47|7||Coevolving functions in genetic programming|In this paper we introduce a new approach to the use of automatically defined functions (ADFs) within genetic programming. The technique consists of evolving a number of separate sub-populations of functions which can be used by a population of evolving main programs. We present and refine a set of mechanisms by which the number and constitution of the function sub-populations can be defined and compare their performance on two well-known classification tasks. A final version of the general approach, for use explicitly on classification tasks, is then presented. It is shown that in all cases the coevolutionary approach performs better than traditional genetic programming with and without ADFs.||||
47|7||System identification using evolutionary Markov chain Monte Carlo|System identification involves determination of the functional structure of a target system that underlies the observed data. In this paper, we present a probabilistic evolutionary method that optimizes system architectures for the identification of unknown target systems. The method is distinguished from existing evolutionary algorithms (EAs) in that the individuals are generated from a probability distribution as in Markov chain Monte Carlo (MCMC). It is also distinguished from conventional MCMC methods in that the search is population-based as in standard evolutionary algorithms. The effectiveness of this hybrid of evolutionary computation and MCMC is tested on a practical problem, i.e., evolving neural net architectures for the identification of nonlinear dynamic systems. Experimental evidence supports that evolutionary MCMC (or eMCMC) exploits the efficiency of simple evolutionary algorithms while maintaining the robustness of MCMC methods and outperforms either approach used alone.||||
47|7||Conditions for the convergence of evolutionary algorithms|This paper presents a theoretical analysis of the convergence conditions for evolutionary algorithms. The necessary and sufficient conditions, necessary conditions, and sufficient conditions for the convergence of evolutionary algorithms to the global optima are derived, which describe their limiting behaviors. Their relationships are explored. Upper and lower bounds of the convergence rates of the evolutionary algorithms are given.||||
47|7||Evolving good hierarchical decompositions of complex systems|Many application areas represent the architecture of complex systems by means of hierarchical graphs containing basic entities with directed links between them, and showing the decomposition of systems into a hierarchical nested “module” structure. An interesting question is then: How best should such a complex system be decomposed into a hierarchical tree of nested “modules”? This paper describes an interesting complexity measure (based on an information theoretic minimum description length principle) which can be used to compare two such hierarchical decompositions. This is then used as the fitness function for a genetic algorithm (GA) which successfully explores the space of possible hierarchical decompositions of a system. The paper also describes the novel crosssover and mutation operators that are necessary in order to do this, and gives some examples of the system in practice.||||
47|7||Hybrid evolutionary motion planning using follow boundary repair for mobile robots|This paper presents a hybrid evolutionary motion planning simulation system for mobile robots operating in unstructured environments. We have designed a new obstacle representation method named cross-line, a follow boundary repair approach, and a hybrid evolutionary motion planning algorithm. A high-level navigator uses the approach and algorithms to implement mobile robot motion planning in various environments. An interactive graphical user interface is developed and the DLL technique is used to implement this system. A group of experiments was conducted. The results demonstrate that this system has high performance, effectiveness and flexibility.||||
47|7||Learning feed-forward and recurrent fuzzy systems: A genetic approach|In this paper, we present a new learning method for rule-based feed-forward and recurrent fuzzy systems. Recurrent fuzzy systems have hidden fuzzy variables and can approximate the temporal relation embedded in dynamic processes of unknown order. The learning method is universal i.e., it selects optimal width and position of Gaussian like membership functions and it selects a minimal set of fuzzy rules as well as the structure of the rules. A genetic algorithm (GA) is used to estimate the fuzzy systems which capture low complexity and minimal rule base. Optimization of the “entropy” of a fuzzy rule base leads to a minimal number of rules, of membership functions and of subpremises together with an optimal input/output (I/O) behavior. Most of the resulting fuzzy systems are comparable to systems designed by an expert but offers a better performance. The approach is compared to others by a standard benchmark (a system identification process). Different results for feed-forward and first-order recurrent fuzzy systems with symmetric and non-symmetric membership functions are presented.||||
47|7||Autotuning a PID controller: A fuzzy-genetic approach|A new method for tuning the parameters of the proportional integral derivative (PID) controller is presented in this paper. The technique adopted in this proposition is based on the format of dead-beat control. Fuzzy inference mechanism has been used here for predicting the future values of the controller output while crisp consequent values of the rulebase of the Takagi–Sugeno model are optimized using a genetic algorithm. The proposition is an extension of the work in R. Bandyopadhyay, D. Patranabis (A new autotuning algorithm for PID controllers using dead-beat format, ISA Trans., accepted for publication) where the rulebase was prepared based on the knowledge of process experts. The use of genetic algorithm for optimizing the crisp values of the rulebase has considerably improved the performance of the PID autotuner. The proposed algorithm seems to be a complete and generalized PID autotuner as can be seen from the simulated and experimental results. In all the cases the method shows substantial improvement over the controller tuned with Ziegler–Nichols formula and the PID controller proposed in (loc cit).||||
47|8|http://www.sciencedirect.com/science/journal/13837621/47/8|Host-diagnosis algorithms for parallel systems|This paper presents an off-line diagnosis strategy for parallel message-passing systems. This strategy, called host-diagnosis, allows a host system to perform centralized diagnosis of the system state, given results of distributed tests performed among the system processors. It is useful for a manufacturing or a maintenance test. Three algorithms that use the host-diagnosis strategy are proposed. Their performance are evaluated using a queuing network model and compared to those of a classic distributed self-diagnosis algorithm. Obtained results show important diagnosis latency savings in case of host-diagnosis algorithms in comparison with the self-diagnosis algorithm.||||
47|8||A robust stack folding approach for Java processors: an operand extraction-based algorithm|Data dependency in stack operations limits the performance of Java processors. To enhance Java's performance, existing literature suggests using stack operations folding. We extend this concept in a new folding algorithm that identifies principle operations in folding groups and extracts necessary operands from the bytecode queue. The proposed algorithm permits nested pattern folding and multiple issue of folding groups. Hence, the need for and therefore the limitations of a stack are eliminated. This paper discusses various aspects of the proposed algorithm and illustrates different folding scenarios as well as possible hazards. Benchmarking using SPECjvm98 shows excellent performance gains as compared to existing algorithms.||||
47|8||Adding static data dependence collapsing to a high-performance instruction scheduler|State-of-the-art processors achieve high performance by executing multiple instructions in parallel. However, the parallel execution of instructions is ultimately limited by true data dependencies between individual instructions. The objective of this paper is to present and quantify the benefits of static data dependence collapsing, a non-speculative technique for reducing the impact of true data dependencies on program execution time. Data dependence collapsing involves combining a pair of instructions when the second instruction is directly dependent on the first. The two instructions are then treated as a single entity and are executed together in a single functional unit that is optimised to handle functions with three input operands instead of the traditional two inputs. Dependence collapsing can be accomplished either dynamically at run time or statically at compile time. Since dynamic dependence collapsing has been studied extensively elsewhere, this paper concentrates on static dependence collapsing. To quantify the benefits of static dependence collapsing, we added a new dependence collapsing option to the Hatfield Superscalar Scheduler (HSS), a state-of-the-art instruction scheduler that targets the Hatfield Superscalar Architecture (HSA). We demonstrate that the addition of dependence collapsing to HSS delivers a significant performance increase of up to 15%. Furthermore, since HSA already executes over four instructions in each processor cycle without dependence collapsing, dependence collapsing enables 0.4 additional instructions to be executed in each processor cycle.||||
47|8||Modelling adaptive routing in circuit switched networks|A performance model of circuit switched k-ary n-cubes with deterministic routing has recently been proposed by Sharma and Varvarigos [IEEE Trans. Parallel Distrib. Syst. 8 (4) (1997) 349]. Many studies have revealed that using adaptive routing along with virtual channels considerably improves network performance over deterministic routing. This paper presents the first analytical model for calculating the message latency in circuit switched k-ary n-cube networks with adaptive routing. The main feature of the proposed model is the use of Markov chains to compute the path set-up time and to capture the effects of using virtual channels to reduce message blocking in the network. The mean waiting time that a message experiences at a source node is calculated using an M/G/1 queueing system. The validity of the model is demonstrated by comparing analytical results with those obtained through simulation experiments.||||
47|9|http://www.sciencedirect.com/science/journal/13837621/47/9|Segmenting endoscopic images using adaptive progressive thresholding: a hardware perspective|Hardware realization of a novel technique based on adaptive progressive thresholding (APT) for the real-time segmentation of endoscopic images is presented. The APT algorithm is mapped onto a linear array of processing elements with each element of a particular segment communicating with its nearest neighbours. The efficiency and hardware portability of this technique justifies its use in applications that require high performance in real-time.||||
47|9||An adaptive approach to achieving hardware and software fault tolerance in a distributed computing environment|||||
47|9||Unsafety vectors: a new fault-tolerant routing for the binary n-cube|This paper presents a new fault-tolerant routing algorithm for the binary n-cube which overcomes the limitations of the recently-proposed safety vectors algorithm (IEEE Trans. Parallel Distribut. Syst. 9 (4) (1998) 321). The algorithm is based on the concept of “unsafety vectors”. Each node A starts by computing a first level unsafety set, S1A, composed of the set of unreachable neighbours. It then performs (m−1) exchanges with its neighbours to determine the k-level unsafety set, SkA, for all 1â©½kâ©½m, where m is an adjustable parameter between 1 and n. SkA represents the set of all nodes at Hamming distance k from node A which are faulty or unreachable from A due to faulty nodes (or links). Equipped with these unsafety sets, each node calculates unsafety vectors, which are then used to achieve an efficient fault-tolerant routing in the binary n-cube. The kth element of the unsafety vector of node A represents a measure of the routing unsafety at distance k from A. We present an analytical study proving some properties of the proposed algorithm. We also conduct a comparative analysis through extensive simulation experiments that reveal the superiority of the proposed algorithm over the safety vectors algorithm (IEEE Trans. Parallel Distribut. Syst. 9 (4) (1998) 321) in terms of different performance measures, e.g. routing distances and percentage of reachability.||||
47|9||On the merits of hypermeshes and tori with adaptive routing|Most existing multicomputers employ the torus topology along with deterministic routing to ensure simple router implementation, and thus fast communication. Efficient adaptive routing algorithms with minimum implementation requirements have recently been proposed to overcome the limitations of deterministic routing. Such algorithms have been incorporated in the latest generation of multicomputers, e.g. the Cray T3E, which are still based on low-dimensional k-ary n-cubes. Our previous studies have shown that a hypergraph network, referred to as the distributed crossbar switch hypermesh (DCSH), has several topological and performance advantages over traditional k-ary n-cubes when deterministic routing is used. This paper evaluates the relative merits of the DCSH and a variant of k-ary n-cubes, the torus, in the context of adaptive routing. The evaluation takes into account the effects of increased switching delays due to adaptivity, and implementation costs for various technologies (e.g. VLSI and multiple-chip technology). The results reveal that the DCSH is a potential alternative as a future high-performance multicomputer network, which can fully exploit the benefits of adaptive wormhole routing. Even though the torus has higher bandwidth channels than its DCSH counterpart, due to its simpler interconnect structure, adaptivity cannot reduce its higher message blocking delays inherent in its topology.||||
47|9||A PCI bus simulation framework and some simulation results on PCI standard 2.1 latency limitations|||||
||||||||
volume|issue|url|title|abstract||||
48|1-3|http://www.sciencedirect.com/science/journal/13837621/48/1-3|Editorial Board|||||
48|1-3||An analytical POC stack operations folding for continuous and discontinuous Java bytecodes|The execution performance of a stack-based Java virtual machine (JVM) is limited by the true data dependency. To enhance the performance of the JVM, a stack operations folding mechanism for the picoJava-I/II processor was proposed by Sun Microsystems to fold 42.3% stack operations. By comparing the continuous bytecodes with pre-defined folding patterns in instruction decoder, the number of push/pop operations in between the operand stack and the local variable could be reduced. In this study, an enhanced POC (EPOC) folding model is proposed to further fold the discontinuous bytecodes that cannot be folded in continuous bytecodes folding mechanisms. By proposing a stack re-order buffer (SROB) to help the folding check processes, the EPOC folding model can fold the stack operations perfectly with a small size of SROB implementation. Statistical data shows that the four-foldable strategy of the EPOC folding model can eliminate 98.8% of push/pop operations with an instruction buffer size of 7 bytes and the SROB size of eight entries.||||
48|1-3||Virtual prototyping of PLC-based embedded system using object model of target and behavior model by converting RLL-to-statechart directly|A domain-specific virtual prototyping approach is proposed that can reduce the risks involved in programmable logic controllers (PLCs)-based embedded system programming. The proposed approach is based on an object-oriented real-time modeling concept, plus an algorithm is defined that can mechanically convert a PLC program, written in relay ladder logic (RLL), to a statechart model. In the field of virtual prototyping, statechart models are widely accepted as representing the behavior of target systems. Accordingly, the direct use of an RLL program enables a virtual prototype to be built quite easily, thereby eliminating complex behavior remodeling of the objects used in PLC embedded systems. As a case study, virtual prototyping of a target example was performed and analyzed to evaluate the benefit of the proposed approach.||||
48|1-3||Hypermeshes: implementation and performance|Common multicomputer networks, including the torus, mesh, and hypercube, are graph topologies where a channel interconnects exactly two nodes. Hypergraphs are generalisations of the graph model, where a channel interconnects an arbitrary number of nodes. The spanning-bus hypercube is a well-known network that belongs to the hypergraph family. Regular multidimensional hypergraphs, also known as hypermeshes, have been proposed as potential alternatives to traditional graph networks for future multicomputers due to their superior topological and performance features. This paper compares the different schemes that have been proposed in the literature for implementing the hypermesh. The results reveal that one particular version of the hypermesh, known as the distributed crossbar switch hypermesh, provides the best performance when implementation costs are taken into account.||||
48|1-3||Interface synthesis between software chip model and target board|This paper reports on the synthesis of interface between software chip model and target board in a behavioral emulation system called in-system algorithm verification engine (iSAVE). iSAVE performs in-system verification of the behavioral description of a chip in such high-level languages as C in the context of its application board at the early chip design stage. The interface between the target chip and the target board is implemented as two parts; software part running on a microprocessor in a multi-thread fashion and hardware part mapped into field programmable gate array logic. The proposed idea is validated by successfully demonstrating the behavioral emulation of MP3 decoder chip, i.e., running the MP3 decoding algorithm written in C along with the real MP3 player board minus the MP3 decoder chip itself through the proposed interface scheme.||||
48|1-3||Design and analysis of static memory management policies for CC-NUMA multiprocessors|In this paper, we characterize the performance of three existing memory management techniques, namely, buddy, round-robin, and first-touch policies. With existing memory management schemes, we find several cases where requests from different processors arrive at the same memory simultaneously. To alleviate this problem, we present two improved memory management policies called skew-mapping and prime-mapping policies. By utilizing the properties of skewing and prime, the improved memory management designs considerably improve the application performance of cache coherent non-uniform memory access multiprocessors. We also re-evaluate the performance of a multistage interconnection network using these existing and improved memory management policies. Our results effectively present the performance benefits of different memory management techniques based on the sharing patterns of applications. Applications with a low degree of sharing benefit from the data locality provided by first-touch. However, several applications with significant sharing degrees as well as those with single processor initialization routines benefit highly from the intelligent distribution of data provided by skew-mapping and prime-mapping schemes. Improvements due to the new schemes are found to be as high as 35% in stall time.||||
48|1-3||Building a dependable system from a legacy application with CORBA|This paper presents a dependability oriented, fault tolerance based system design, development, and deployment approach. The approach relies on an architectural framework, which allows legacy software modules to be reused as the basic building blocks of a distributed dependable application. Different levels of replication and alternative adjudication strategies are implemented behind a unified interface. These can be configured for achieving the optimal compromise between dependability and performance, according to application, deployment environment, and fault characteristics. The suggested solution can be implemented on top of any CORBA infrastructure. The architecture has been developed and tested. Experimental results are presented and discussed.||||
48|1-3||Genetic engineering versus natural evolution: Genetic algorithms with deterministic operators|Genetic algorithms (GA) have several important features that predestine them to solve design problems. Their main disadvantage however is the excessively long run-time that is needed to deliver satisfactory results for large instances of complex design problems. The main aims of this paper are (1) to demonstrate that the effective and efficient application of the GA concept to design problem solving requires substitution of the basic GAs natural evolution by genetic engineering (GE), (2) to propose and discuss the concept of a genetic engineering algorithm (GEA), and (3) to show how to apply the GEA to solve synthesis problems. In this paper, an effective and efficient GE scheme is proposed and applied to solve an important design problem: the minimal input support problem. In almost all cases, our GEA produces strictly optimal results and realizes a very good trade-off between effectiveness and efficiency. The experimental results clearly demonstrate that the proposed GE scheme is suitable for solving design problems and its application results in very effective and efficient GEAs.||||
48|11-12|http://www.sciencedirect.com/science/journal/13837621/48/11-12|Editorial board|||||
48|11-12||Permutation routing in optical MIN with minimum number of stages|In a hybrid optical multistage interconnection network (MIN), optical signals are routed by electronically controlled switches using directional couplers. A relevant design problem is to minimize the path-dependent loss of the optical signal, which is directly proportional to the number of couplers, i.e., the number of switches through which the signal has to pass. In general, given the network size and the type of the MIN, the number of stages is a constant. Hence, an input signal has to pass through a fixed number of couplers to reach the output.||||
48|11-12||On the topological properties of the arrangementâstar network|This paper proposes a new interconnection network, referred to as the arrangement–star network, which is constructed from the product of the star and arrangement networks. Studying this new network is motivated by the good qualities it exhibits over its constituent networks, the star and arrangement networks. The star network has been a research focus for quite a long time until recently when the algorithm development on the star network turned out to be cumbersome. The arrangement network as a generalized class for the star network offers no solution in that direction. The arrangement–star network, on the other hand, makes it possible to efficiently embed grids, pipelines, as well as other computationally important topologies in a very natural manner. Furthermore, the fact that the product of the star and arrangement networks comes with little increase in the network diameter and a better result on communication cost, motivates further investigation for this new alternative, the arrangement–star network.||||
48|11-12||A VLSI architecture for 3-D self-organizing map based color quantization and its FPGA implementation|Color quantization is the process of computing a color palette containing few best colors from a full color image and then associating to each pixel of the image, a color from the palette to yield a color quantized image that is close to the original image. It is of interest in applications such as digital display and image capture. To satisfy real time requirements, it is of vital importance to perform color quantization as fast as possible. This paper presents a novel architecture of a hardware unit for color quantization that is based on Kohonen’s self-organizing map. The proposed architecture is of the SIMD type and results in a scheme with linear time complexity (in the size of the image). The architecture has been implemented in Xilinx FPGA and results show that the proposed design achieves high speed taking only a few milliseconds for color quantization of images up to size of 512 × 512 with low area requirement.||||
48|11-12||Bandwidth constrained smoothing for multimedia streaming with scheduling support|Providing a satisfactory multimedia service in networking environments requires an effective media delivery mechanism. However, a common network such as the Internet does not provide a guaranteed network bandwidth to accommodate multimedia service in a reliable fashion. A typical approach to assist multimedia delivery is via buffer management and task scheduling in end-systems. Buffer management techniques are classified into two categories; one is to adapt the changes in network load and the other is to smooth the bandwidth requirement. The former may cause a serious loss of service quality whereas the latter is unable to adapt to the dynamic network condition. In this paper, we propose a bandwidth-adaptive media smoothing technique which smoothes the bandwidth requirement for media delivery at run time by considering the availability of network bandwidth. Meanwhile, the bandwidth smoothing technique still has the possibility of causing jitter because the policy runs on the application layer so that it cannot guarantee task completion in time. Thus, we also propose a task scheduling algorithm optimized for the bandwidth adaptive smoothing. This scheduling technique handles the media data appropriately in order to minimize jitter. Simulation results with prerecorded MPEG videos show that the quality of delivered video is improved with the proposed bandwidth adaptive smoothing and task scheduling mechanisms.||||
48|4-5|http://www.sciencedirect.com/science/journal/13837621/48/4-5|Editorial Board|||||
48|4-5||A fast and accurate delay dependent method for switching estimation of large combinational circuits|Assuming inertial gate delay model, the first-order temporal correlation and the structural dependencies, a probabilistic method to estimate the switching activity of a combinational circuit, is introduced. To capture the first temporal correlation a novel mathematical model and the associated new formulas are derived. Also, a modified boolean function, which describes the logic and timing behavior of each signal, is introduced. To capture the structural dependencies an efficient new method to partition a large circuit into small independent sub-circuits is proposed. Finally, an algorithm that evaluates the switching activity of any circuit node is presented.||||
48|4-5||On the design of low power BIST for multipliers with Booth encoding and Wallace tree summation|Low power dissipation (PD) during testing is emerging as one of the major objectives of a built-in self-test (BIST) designer. In this paper we examine the testability of multipliers based on Booth encoding and Wallace tree summation of the partial products and we present a methodology for deriving a low power BIST scheme for them. We propose several design rules for designing the Wallace tree in order to be fully testable under the cell fault model. The proposed low power BIST scheme for the derived multipliers is achieved by: (a) introducing suitable test pattern generators (TPGs), (b) properly assigning the TPG outputs to the multiplier inputs and (c) significantly reducing the test set length. Results indicate that the total power dissipated, the average power per test vector and the peak PD during testing can be reduced up to 73%, 27% and 36% respectively with respect to earlier schemes, depending on the implementation of the basic cells and the size of the multiplier. The test application time is also significantly reduced, while the introduced BIST scheme implementation area is small.||||
48|4-5||Increasing hardware data prefetching performance using the second-level cache|Techniques to reduce or tolerate large memory latencies are critical for achieving high processor performance. Hardware data prefetching is one of the most heavily studied solutions, but it is essentially applied to first-level caches where it can severely disrupt processor behavior by delaying normal cache requests, inducing cache pollution and occupying the heavily used bus to the second-level cache. In this article, we show that applying hardware data prefetching to the second level cache exhibits most of the benefits of first-level cache prefetching with almost none of its drawbacks. Moreover, we outline that second-level hardware data prefetching is particularly well suited to out-of-order (OoO) processors because it can hide the long memory latencies due to second-level cache misses while OoO execution of memory instructions can hide the lower latencies due to first-level cache misses that hit in the second-level cache. Finally, we show that when the full memory system is taken into account, especially bus traffic, first-level cache prefetching can actually degrade overall processor performance while second-level cache prefetching consistently improves overall performance. Our experimental results show that the instructions per cycle of floating-point programs (SPEC95) increases by 20% on a average using second-level cache hardware data prefetching while it decreases by 5% on a average using first-level cache hardware data prefetching.||||
48|4-5||Scheduling expression trees for delayed-load architectures|||||
48|6-7|http://www.sciencedirect.com/science/journal/13837621/48/6-7|Editorial Board|||||
48|6-7||Scheduling length for switching element disjoint multicasting in Banyan-type switching networks|Due to the stringent bit-error requirement in fiber optics, preventing the forthcoming crosstalk at switching elements (SEs) is very crucial in the large-scale photonic switches. In this paper, we consider the SE-disjoint multicasting for a photonic Banyan-type switching network. This ensures that at most, one connection holds each SE in a given time thus, neither photonic crosstalk nor link blocking will arise in the switching network implemented with optical devices such as directional couplers, splitters and combiners. Routing a set of connections under such constraint usually takes several routing rounds hence, it is desirable to keep the number of rounds (i.e., scheduling length) to a minimum. Unfortunately, finding the optimal length is NP-complete in general, we propose an algorithm that seeks an approximation solution less than double of the optimal upper bound. The bound permits the Banyan-type multicasting network to be rearrangeable nonblocking as well as crosstalk-free. It is given that the same bound guarantees wide-sense nonblocking for multicast connections, provided that the multicasting capability of the network is restricted to the second half of the whole stages, and strictly nonblocking for one-to-one connections, yet allowing to be free from the crosstalk. We also consider other crosstalk-free multicasting and the link-disjoint multicasting for the Banyan network. The theory developed in this paper gives a unified foundation on designing nonblocking and crosstalk-free photonic Banyan-type networks under the multicast connections.||||
48|6-7||Methods for distributed unicast in hypercubes|"Unicast algorithms in off-line routing have been used for one-to-one communication between a source node and a destination node in an n-dimensional hypercube, denoted as Hn. A node is called k-safe, where 0â©½kâ©½n, if it has at least k healthy neighbors, and Hn is called k-safe if every node in it is k-safe. A k-safe Hn is connected if the number of faulty nodes, |F|, does not exceed 2k(n−k)−1. In this paper, we propose two methods for distributed routing. The first method has been presented in [Proc. 7th Int. IEEE Conf. Electron., Circ. Syst., Jounieh, Lebanon, December, 2000, p. 194]. The second method that has not been addressed before, can be used for off-line routing. In the case of off-line routing we avoid the cost of collecting global information about the faulty nodes, and the cost of getting information about Hn, whether it is k-safe or not. The minimum requirements of the proposed methods is to have the path between the source and the destination connected. Hence, they may work when Hn is disconnected, which is an important advantage. The time cost of the first method may be O(mn4), and the expected length of the routing path between source and destination may be O(mn5), where 1â©½mâ©½n. The time cost of the second method may be O(enn/2), the space cost may be O(enn/2), and the expected length of the routing path between source and destination may be d(s,t)."||
48|6-7||Quantifying behavioral differences between multimedia and general-purpose workloads|Multimedia workloads are becoming increasingly more common on general-purpose computing systems. However, little quantitative results are available about the behavior of these workloads. This paper is a first step in quantifying and understanding the behavioral differences (if any) between multimedia and general-purpose workloads. This is done by comparing program characteristics of multimedia applications (coming from the MediaBench suite, the X benchmarks from the SimpleScalar distribution, plus a set of MPEG-4 like algorithms) and general-purpose applications (coming from the SPECint95 and the SPECint2000 benchmark suite). In addition to presenting a database of program characteristics, we conclude that (i) multimedia applications have less memory operations and less control operations in their instruction mix than general-purpose workloads, and thus are computationally intensive; (ii) multimedia and general-purpose applications exhibit comparable levels of instruction-level parallelism; (iii) multimedia applications also suffer from hard-to-predict branches and (iv) the instruction stream as well as the data stream of multimedia applications exhibit more spatial and more temporal locality than general-purpose applications. These results were obtained using statistical tests, namely the t-test and the Mann–Whitney test.||||
48|6-7||PET, a software monitoring toolkit for performance analysis of parallel embedded applications|Since the late 1980s a great deal of research has been dedicated to the development of software monitoring and visualization toolkits for parallel systems. These toolkits have traditionally been oriented to measuring and analyzing parallel scientific applications. However, nowadays, other types of parallel applications, using signal-processing or image-processing techniques, are becoming increasingly importance in the field of embedded computing. Such applications are executed on special parallel computers, normally known as embedded multiprocessors, and they exhibit structural and behavioral characteristics very different from scientific applications. Because of this, monitoring tools with specific characteristics are required for measuring and analyzing parallel embedded applications.||||
48|8-10|http://www.sciencedirect.com/science/journal/13837621/48/8-10|Editorial board|||||
48|8-10||Secure checkpointing|||||
48|8-10||Efficient communication sets generation for blockâcyclic distribution on distributed-memory machines|How to generate local memory access sequence and communication sets efficiently is an important issue in compiling a data-parallel language into a single program multiple data (SPMD) code for distributed-memory machines. Many methods have been developed for generating local memory access sequence. In this paper, we focus on the problem of communication sets generation. The local block distance between two active elements with the same offset and destination (source) in a processor will be investigated. We develop an algorithm for the sending phase and receive–execute phase, respectively. Our algorithms do not need to compute send and receive patterns and lose no communication sets while there exist incomplete blocks that cannot constitute a send or receive pattern. Experimental results showed that our method outperforms other previous work.||||
48|8-10||Clustering and reassignment-based mapping strategy for message-passing architectures|A fundamental issue affecting the performance of a parallel application running on message-passing parallel systems is the assignment of tasks to processors. In this paper we present a compilation-time two stage mapping strategy (denoted as Task Allocation by Clustering, Reassignment and Embedding, TACRE) used for mapping arbitrary programs (modeled as task interaction graphs) onto message-passing parallel systems. The first stage is based on task clustering and task reassignment algorithms that contract the original task graph. The second stage takes the contracted graph and tries to well match the physical properties of the target system. The results shown that TACRE provides a good trade-off between mapping quality and computational complexity.||||
48|8-10||New design methodology with efficient prediction of quality metrics for logic level design towards dynamic reconfigurable logic|The importance of efficient area and timing estimation is well established in high level synthesis (HLS) since it allows more efficient exploration of the design space while providing HLS tools with the capability of predicting the effects of technology specific tools on the design space. Much of the previous work has focused on estimation techniques that use very simple cost models based solely on functional units (FUs). Those models are not accurate enough to allow effective design space exploration since the effects of interconnects can indeed dominate the final design cost. The situation becomes even worst when the design is targeted to dynamically reconfigurable logic (DRL) technologies since the multiplexer delay may contribute heavily on the overall delay. In addition, large number of configurable logic blocks could be used for communication rather than for implementing FUs. In this paper we present a new HLS design flow, which performs an accurate estimation on area and timing for DRL circuits. It takes into account not only FUs area and delay, but also the interconnection and communication effects. We select our DRL LSI circuit [M. Meribout, M. Motomura, Method for compiling high level programs into hardware, Japanese Patent: JSP2000-313818, 2000; M. Motomura et al., An embedded DRAM-FPGA chip with instantaneous logic reconfiguration, in: Symposium on VLSI Circuits, July 1997, pp. 55–56] as our main concentration. We tested our method with several benchmarks and the results show that we receive good performance of the design, with area and timing estimated efficiently.||||
||||||||
volume|issue|url|title|abstract||||
49|1-2|http://www.sciencedirect.com/science/journal/13837621/49/1-2|Editorial board|||||
49|1-2||Fault-tolerance of Complete Josephus Cubes|The Complete Josephus Cube is proposed as a fault-tolerant node cluster architecture A reliable and cost-effective communications strategy is also presented. For a Complete Josephus Cube of dimension r, the strategy tolerates up to (r+1) encountered faults in its routes that are deadlock-free and livelock-free. The message is optimally (respectively, sub-optimally) delivered in at most r (respectively, 2r+1) hops. Message overhead is one of the lowest reported for the specified fault tolerance––with only a single (r+2)-bit routing vector accompanying the message to be communicated. Associated routing hardware may be implemented with standard logic.||||
49|1-2||An improved reconfiguration algorithm for degradable VLSI/WSI arrays|This paper discusses the NP-complete problem of reconfiguring a two-dimensional degradable VLSI/WSI array under the row and column routing constraints. A new strategy for row selection in the logical array is proposed and Low’s algorithm is simplified. Experimental results show that our algorithm is approximately 50% faster than the most efficient algorithm, cited in the literature, without loss of performance.||||
49|1-2||Multiple-path execution for chip multiprocessors|||||
49|1-2||Residue number system to binary converter for the moduli set (2nâ1,2nâ1,2n+1)|In many earlier publications, different authors have suggested residue to binary converters for the moduli sets: (2n,2n−1,2n+1) and (2n,2n−1,2n−1−1), where n is a positive integer. In this paper, we are introducing the moduli set (2n−1,2n−1,2n+1), which is one bit less in its dynamic range than that of (2n,2n−1,2n+1). However, it has a similar dynamic range to that of (2n,2n−1,2n−1−1). Closed form multiplicative inverses for the new set are introduced. Based on these inverses, a residue to binary converter design is proposed which requires less time and hardware than all published converters for the other two moduli sets.||||
49|1-2||Erratum to âBandwidth constrained smoothing for multimedia streaming with scheduling supportâ [Journal of Systems Architecture 48 (2003) 353â366]|||||
49|10-11|http://www.sciencedirect.com/science/journal/13837621/49/10-11|Editorial board|||||
49|10-11||Evolutions in parallel distributed and network-based processing|||||
49|10-11||HPC the easy way: new technologies for high performance application development and deployment|With the increase of both computing power available and computer application size and complexity, existing programming methodologies and technologies for parallel and distributed computing demonstrated their inadequacy. New techniques have therefore been designed and are currently being developed that aim at providing the user/programmer with higher level programming methodologies, environments and run time supports.||||
49|10-11||Automatic performance analysis of hybrid MPI/OpenMP applications|The EXPERT performance-analysis environment provides a complete tracing-based solution for automatic performance analysis of MPI, OpenMP, or hybrid applications running on parallel computers with SMP nodes. EXPERT describes performance problems using a high level of abstraction in terms of execution patterns that result from an inefficient use of the underlying programming model(s). The set of predefined problems can be extended to meet application-specific needs. The analysis is carried out along three interconnected dimensions: class of performance behavior, call tree, and thread of execution. Each dimension is arranged in a hierarchy so that the user can investigate the behavior on varying levels of detail. All three dimensions are interactively accessible using a single integrated view.||||
49|10-11||Supporting adaptive routing in IBA switches|InfiniBand is a new standard for communication between processing nodes and I/O devices as well as for interprocessor communication. The InfiniBand Architecture (IBA) supports distributed deterministic routing because forwarding tables store a single output port per destination ID. This prevents packets from using alternative paths when the requested output port is busy. Despite the fact that alternative paths could be selected at the source node to reach the same destination node, this is not effective enough to improve network performance. However, using adaptive routing could help to circumvent the congested areas in the network, leading to an increment in performance.||||
49|10-11||Architectural concerns in distributed and mobile collaborative systems|Organizations increasingly coordinate their product and service development processes to deliver their products and services as fast as possible, and to involve employees, customers, suppliers, and business partners seamlessly in different stages of the processes. These processes have to consider that their participants are increasingly on the move or distributed while they are working. Expertise needs to be shared across locations and different mobile devices. This paper describes a framework for distributed and mobile collaboration, defines a set of requirements for virtual communities, and discusses a mobile teamwork support software architecture that has been developed in the EU-project MOTION. The framework together with the architecture enables to enhance current collaboration approaches to include the dimension of mobile participants and virtual communities for distributed product development. This is achieved by integrating process and workspace management requirements with Peer-to-Peer Middleware, Publish-Subscribe, and Community and User Management components.||||
49|10-11||Video transmission adaptation on mobile devices|The development of multimedia streaming over wireless network is facing a lot of challenges. Taking into account mobility and highly variable bandwidth are the two major ones. Using scalable video content can solve the variable bandwidth problem only if the streaming architecture is able to react without latency. In this article, we present NetMoVie, an intermediate architecture based on real-time protocol which is able to adapt streams to the constraints of the wireless channel.||||
49|3|http://www.sciencedirect.com/science/journal/13837621/49/3|Editorial board|||||
49|3||Publisherâs note|||||
49|3||Parallel, distributed and network-based processing|||||
49|3||Optimization techniques for parallel irregular reductions|Different parallelization techniques have been proposed in the literature for irregular reductions in the context of shared memory multiprocessors. They may be classified into two broad families: those based on privatization of the reduction arrays and those based on the partitioning of the reduction arrays. Methods in the first family are simple but no data locality is exploited and their memory scalability is low. On the other hand, methods in the second family are more complex as they require an inspection phase but they exploit data locality and scale up better in memory. Focusing on partitioning-based methods, although they exhibit a good performance in a wide variety of irregular codes, some specific input data patterns may exist for which the performance is lowered. In particular these kind of access patterns may reduce the exploited parallelism by the method or introduce workload unbalances. In order to mitigate these negative effects, we propose three optimizations for a specific partitioning-based method (DWA–LIP). These optimizations try to increase the exploited parallelism, balance the workload and reduce the effect of high contention degree regions in the reduction arrays. Efficient implementations of the proposed optimizations for the DWA–LIP method have been tested experimentally, and compared with other methods for parallelizing irregular reductions.||||
49|3||Motion-compensated wavelet packet zerotree video coding on multicomputers|In this work we describe and analyze algorithms for advanced video coding on distributed memory MIMD architectures. In particular, we consider a wavelet packet based codec using the concept of zerotree encoding. The main contribution of this work is the design of a parallel motion-compensated video coder composed of a wavelet packet decomposition in conjunction with the best basis algorithm followed by zerotree coding. Whereas two sensible parallelization techniques can be employed for the wavelet packet decomposition (subband based partitioning and stripe partitioning), the zerotree coding and motion compensation stages only allow one reasonable parallelization method (stripe partitioning). We investigate the advantages and drawbacks of the resulting different overall data distribution strategies and show experimental results obtained on a Siemens hpcLine cluster and a Cray T3E.||||
49|3||Efficient implementation of reduce-scatter in MPI|We discuss the efficient implementation of a collective operation called reduce-scatter, which is defined in the MPI standard. The reduce-scatter is equivalent to the combination of a reduction on vectors of length n with a scatter of the resulting n-vector to all processors.||||
49|3||Incorporating memory layout in the modeling of message passing programs|One of the most fundamental tasks any automatic parallelization and optimization tool is confronted with is to find an optimal domain decomposition for an application at hand. For regular domain problems (such as simple matrix manipulations) this task may seem trivial. However, communication costs in message passing programs often significantly depend on the capabilities and particular behavior of the applied communication primitives. As a consequence, straightforward domain decompositions may deliver non-optimal performance.||||
49|4-6|http://www.sciencedirect.com/science/journal/13837621/49/4-6|Editorial board|||||
49|4-6||Special-issue on reconfigurable systems|||||
49|4-6||Configware and morphware going mainstream|The paper addresses a broad readership in information technology, computer science and related areas, and gives an introduction to fine grain and coarse grain morphware, reconfigurable computing, and its impact on classical computer science and business models. It points out trends driven by microelectronics technology, EDA, and the mind set of data-stream-based computing.||||
49|4-6||Polymorphous fabric-based systems: Model, tools, applications|A polymorphous fabric-based systems is a parameterized cellular architecture in which an array of computing cells communicates with an embedded processor through a global memory. This architecture is customizable to different classes of applications by functional unit, interconnect, and memory parameters, and can be instantiated efficiently on platform FPGAs. In previous work [IEEE Micro 22(5) (2002)], we have demonstrated the advantage of reconfigurable fabrics for image and signal processing applications. Recently, we have build a fabric generator (FG), a Java-based toolset that greatly accelerates construction of the fabrics. A module-generation library is used to define, instantiate, and interconnect cells’ datapaths. FG also generates customized sequencers for individual cells or collections of cells. We describe the fabric-based system model, the FG toolset, and concrete realizations of fabric architectures generated by FG on the Altera Excalibur ARM that can deliver 4.5 GigaMACs/s (8/16 bit data, multiply-accumulate).||||
49|4-6||Realization of wireless multimedia communication systems on reconfigurable platforms|Wireless multimedia communication systems become increasingly more computational intensive and demand for higher flexibility. The realization of these systems on reconfigurable hardware offers a good balance for these requirements. In this paper the suitability of commercially available reconfigurable hardware platforms for the target application domain is evaluated. Based on this evaluation a heterogeneous partly reconfigurable system-on-chip platform is identified as ideal implementation platform for the targeted systems. Systems from different target domains are analysed and different cases where the inclusion of reconfigurable hardware in their realizations would lead to improved quality in terms of implementation efficiency and flexibility are identified. Design methodology requirements for the realization of systems from the target application domain on the targeted platform are analysed and issues not covered by existing methodologies are identified. The principles of a methodology handling these open issues are described. Results from the prototyping of different systems are also presented and show the potentials of a reconfigurable hardware platform, which in the future will lead to reduced costs and increased flexibility of the wireless multimedia communication systems.||||
49|4-6||Functionally partitioned module-based programmable architecture for wireless base-band processing|A specialised reconfigurable architecture is targeted at wireless base-band processing. It is built to cater for multiple wireless standards. It has lower power consumption than the processor-based solution. It can be scaled to run in parallel for processing multiple channels. Test resources are embedded on the architecture and testing strategies are included.||||
49|4-6||Performance of reconfigurable architectures for image-processing applications|Reconfigurable architectures combine a programmable–visible interface and the high-level aspects of a computer’s design. The goal of this work is to explore the architectural behaviour of remote reconfigurable systems that are part of general-purpose computers. Our approach analyses various issues arising from the connection of processors with FPGA-based microarchitecture to an existing commodity microprocessor via a standard bus. The quantitative evaluation considers image-processing applications and shows that the maximum performance depends on the amount of data processed by the reconfigurable hardware. Taking images with 256 × 256 pixels, a moderate FPGA capacity of 1E+5 logic blocks provides two orders of magnitude of performance improvement over a Pentium III processor for most of our benchmarks. However, the performance benefits exhibited by reconfigurable architectures may be deeply influenced by some design parameters. This paper studies the impact of hardware capacity, reconfiguration time, memory organisation, and bus bandwidth on the performance achieved by FPGA-based systems. Those image-processing benchmarks that can exhibit high-performance improvement would require about 150 memory banks of 256 bytes each and a bus bandwidth as high as 30 GB/s. This quantitative approach can be applied to the design of high-performance reconfigurable coprocessors for multimedia applications.||||
49|4-6||The design and implementation of a reconfigurable processor for problems of combinatorial computation|The paper analyses different techniques that might be employed in order to solve various problems of combinatorial optimization and argues that the best results can be achieved by the use of software running on a general-purpose computer together with an FPGA-based reconfigurable co-processor. It suggests an architecture for a combinatorial co-processor that is based on hardware templates and consists of reconfigurable functional and control units. Finally the paper demonstrates how the co-processor can be applied to two practical applications formulated over discrete matrices, the Boolean satisfiability and covering problems.||||
49|4-6||Fast and compact sequential circuits for the FPGA-based reconfigurable systems|Reconfigurable systems fill the flexibility, performance, power dissipation, and development and fabrication cost gap between the application specific systems implemented with hard-wired application specific integrated circuits and systems based on the standard (general purpose) programmable microprocessors. During the last decade they became the mainstream implementation technology for custom computation and embedded system products in such fields as telecommunication, image processing, video processing, multimedia, DSP, cryptography, embedded control, etc. To efficiently develop, implement and use the reconfigurable systems, adequate computer-aided support tools are necessary. Since most reconfigurable systems are implemented using the look-up table (LUT) field programmable gate arrays (FPGA) technology, the circuit synthesis tools targeting this technology are of primary importance for their effective and efficient implementation. In this paper, a new sequential circuit synthesis methodology is discussed that targets LUT FPGAs and FPGA-based reconfigurable system-on-a-chip platforms. The methodology is based on the information-driven approach to circuit synthesis, general decomposition and theory of information relationship measures that we previously developed. Our synthesis methods considerably differ from all other known methods. The experimental results from the automatic circuit synthesis tools that implement our methods demonstrate that the information-driven approach consistently applied in the whole sequential circuit synthesis chain efficiently produces very fast and compact sequential circuits.||||
49|4-6||Effective and efficient FPGA synthesis through general functional decomposition|In this paper, a new information-driven circuit synthesis method is discussed that targets LUT-based FPGAs and FPGA-based reconfigurable system-on-a-chip platforms. The method is based on the bottom–up general functional decomposition and theory of information relationship measures that we previously developed. It differs considerably from all other known methods. The experimental results from the automatic circuit synthesis tool that implements the method clearly demonstrate that the information-driven general functional decomposition based on information relationship measures efficiently produces very fast and compact FPGA circuits.||||
49|4-6||Run-time support for dynamically reconfigurable computing systems|Reconfigurable computing systems normally consist of an instruction-set processor connected to a block of reconfigurable logic. The reconfigurable logic, for example, an field programmable gate arrays (FPGA), can usually be adapted during the run-time of an application to perform different tasks. This paper describes a novel FPGA support system (FSS) that facilitates the execution of hardware-based tasks on a reconfigurable Xilinx 6264 FPGA connected to an ARM 7 processor. The FSS provides the mechanisms to support the placement, execution, and removal of tasks on the FPGA. A key feature of the FSS is the ability to provide communication facilities between concurrently active hardware and software tasks during the run-time of an application. The design, implementation and status of the FSS are discussed, together with initial results based on the implementation of a wavelet image compression application. The paper concludes by considering how our experiences with this system have influenced the development of an enhanced FSS for the later generation of Xilinx Virtex FPGAs.||||
49|4-6||Evaluation of delay fault testability of LUTs for the enhancement of application-dependent testing of FPGAs|Testing delay faults in FPGAs differs significantly from testing delay faults in circuits whose combinational sections are represented as gate networks. Based on delay fault testability conditions, formulated in a form suitable for analysis of LUT-based FPGAs, a new method for the evaluation of delay fault testability of LUT functions has been developed. It relies on an indicator called delay fault activation profile. The proposed method supports an analysis and comparison of different procedures for the enhancement of detectability of FPGA delay faults that rely on transformations of user-defined functions of LUTs in the section under test. The effectiveness of the method is demonstrated by applying it to prove the efficiency and to optimize a specific procedure for the transformation of user-defined LUT functions.||||
49|7-9|http://www.sciencedirect.com/science/journal/13837621/49/7-9|Editorial board|||||
49|7-9||An optical switching architecture for hierarchical group communication|We show that the problem of finding the maximum passable subset of a permutation through a banyan is NP-complete. We then describe a two-level optical interconnection structure for groups or clusters of end-nodes (LANs or processors). At the higher level, an optical banyan is used to switch wavelength multiplexed packets from the groups. Technologically difficult switching of individual wavelengths is avoided by prearranging transmissions from the groups in a way that they can be switched in a wavelength insensitive manner. Further, by keeping the banyan conflict-free, we allow each of the groups to access the entire set of wavelengths for multiplexing, thus maximizing the bisection bandwidth. The proposed optical interconnect can support multiple multicast connections between each pair of groups, in which source nodes may simultaneously multicast several different packets to different subsets within a destination group. System-wide multicasts and broadcasts can be achieved through repetitive group-to-group transmissions. The network uses readily available components such as opto-electronic directional couplers, fixed wavelength transmitters, and diffraction-based parallel receivers while avoiding the use of relatively slow and expensive tunable components.||||
49|7-9||Use of embedded DRAMs in video and image computing|||||
49|7-9||A VLSI architecture for video object motion estimation using a novel 2-D hierarchical mesh|||||
49|7-9||On the weakest failure detector for hard agreement problems|Chandra and Toueg [J. ACM 43 (1996) 225] and Fromentin et al. [Proc. IEEE Internat. Conf. on Distrib. Comput., 1999, p. 470], respectively, stated that the weakest failure detector for any of non-blocking atomic commitment and terminating reliable broadcast is the perfect failure detector P. Recently, Guerraoui [IPL 79 (2001) 99] presented a counterexample of those results, exhibiting a failure detector called Marabout (M) that is incomparable to P and yet solves those problems.||||
49|7-9||A new cryptography system and its VLSI realization|In this paper, a new cryptography system (CS) is proposed and its VLSI architecture is designed and verified. The new system performs both random position permutation and random value transformation. The input data is processed by the swap and XOR/XNOR functions under the control of a binary sequence from a chaotic system. The scheme is analyzed and simulated by MATLAB to be high security. In order to maintain the requirement of real-time, the VLSI architecture with low hardware cost, high computing speeds, high modularity, and regularity is designed and implemented respectively with Altera FPGA and Avanti cell-library. According to the simulation result, the throughput rates of the proposed design with the two implementations are larger than 0.64 and 2.74 Gbps. Hence the proposed new cryptography system is strongly suitable for most of real-time video and audio applications.||||
49|7-9||A multi-queue TCP window control scheme with dynamic buffer allocation|Explicit transmission control protocol (TCP) window control through the modification of the receiver’s advertised window in ACK packets is one of the ways intermediate network elements can contribute to the end-to-end TCP control. The TCP receiver’s advertised window, which indicates the level of the receive buffer of a TCP connection, limits the maximum window and consequently the throughput that can be achieved by the TCP sender. Thus, appropriate reduction of the advertised window by intermediate network elements can control the number of packets sent from a TCP sender. This paper describes a TCP window control scheme for a shared memory system with multiple queues. A dynamic buffer threshold, computed using a simple recursive algorithm, is used to dynamically allocate buffer space to the queues.||||
49|7-9||Fast reconfigurable systolic hardware for modular multiplication and exponentiation|Modular multiplication and modular exponentiation are fundamental operations in most public-key cryptosystems such as RSA. In this paper, we propose a novel implementation of these operations using systolic arrays based architectures. For this purpose, we use the Montgomery algorithm to compute the modular product and the left-to-right binary exponentiation method to yield the modular power. In the proposed design, we invest hardware area in the hope of improving encryption/decryption throughput. Our implementation improves time requirement as well as the time area factor when compared that of Blum’s and Paar’s.||||
||||||||
volume|issue|url|title|abstract||||
50|1|http://www.sciencedirect.com/science/journal/13837621/50/1|Editorial board|||||
50|1||SAGE: an automatic analyzing system for a new high-performance SoC architectureââprocessor-in-memory|Continuous improvements in semiconductor fabrication density are supporting new classes of System-on-a-Chip (SoC) architectures that combine extensive processing logic/processor with high-density memory. Such architectures are generally called Processor-in-Memory (PIM) or Intelligent Memory (I-RAM) and can support high-performance computing by reducing the performance gap between the processor and the memory. The PIM architecture combines various processors in a single system. These processors are characterized by their computation and memory-access capabilities. Therefore, a novel strategy must be developed to identify their capabilities and dispatch the most appropriate jobs to them in order to exploit them fully. Accordingly, this study presents an automatic source-to-source parallelizing system, called statement-analysis-grouping-evaluation (SAGE), to exploit the advantages of PIM architectures. Unlike conventional iteration-based parallelizing systems, SAGE adopts statement-based analyzing approaches. This study addresses the configuration of a PIM architecture with one host processor (i.e., the main processor in state-of-the-art computer systems) and one memory processor (i.e., the computing logic integrated with the memory). The strategy of the SAGE system, in which the original program is decomposed into blocks and a feasible execution schedule is produced for the host and memory processors, is investigated as well. The experimental results for real benchmarks are also discussed.||||
50|1||Reducing disk I/O times using anticipatory movements of the disk head|Finding a good rest position for the disk head is very important for the performance of a hard disk. It has been shown in the past that rest positions obtained through anticipatory movements of the disk head can indeed improve response time, but practical algorithms have not been described yet. In this paper we describe a software technique for perfoming anticipatory movements of the disk head. In particular, we show that by partitioning the disk controller memory into a part used for caching and a part used for predictive movements, lower I/O times as compared with the usual read-ahead cache configurations are obtained. Through trace-driven simulations we show in fact that significant improvements in the disk I/O times can be obtained as compared to standard disk caching. Since the technique should be realized at the firmware level in the disk controller and no hardware modifications are needed, the implementation cost is low.||||
50|1||Switch fabric architecture analysis for a scalable bi-directionally reconfigurable IP router|This paper provides an in-depth analysis using six basic router functional requirements, a primary switch fabrics (SFs) selection criterion, and a semi-quantitative compliance scoring scheme for 10 SFs. The goal is to select candidates that can serve a hardware (HW)-wise scalable and bi-directionally reconfigurable Internet Protocol (IP) router. HW scalability and bi-directional HW reconfigurability for an IP router denote respectively its ability to (1) expand according to network traffic capacity growth; and (2) be functionally converted to perform in two conceptual directions on-demand: “downward” as “edge”, or “upward” as “hub” or “backbone” router according to the layer of the internet services provider’s network hierarchy it is targeted to serve at the moment. Overall result points to Hypercube, Multistage Interconnection Network (MIN), and 3-Dimensional Torus Mesh as potential candidates.||||
50|10|http://www.sciencedirect.com/science/journal/13837621/50/10|Editorial board|||||
50|10||Replication algorithms for the World-Wide Web|This paper addresses the two fundamental issues in replication, namely deciding on the number and placement of the replicas and the distribution of requests among replicas. We first introduce a centralized algorithm for replicating objects that can keep a balanced load on sites. In order to meet the requirement due to the dynamic nature of the Internet traffic and the rapid change in the access pattern of the World-Wide Web (Web), we also propose a distributed algorithm where each site relies on some collected information to decide on where to replicate and migrate objects to achieve good performance. The performance of the proposed algorithms is evaluated experimentally and a comparison of their measured performance is presented.||||
50|10||Efficiently tolerating failures in asynchronous real-time distributed systems|We present a proactive resource allocation algorithm, called BEA, for fault-tolerant asynchronous real-time distributed systems. BEA considers an application model where trans-node application timeliness requirements are expressed using benefit functions, and anticipated workload during future time intervals are expressed using adaptation functions. Furthermore, BEA considers an adaptation model where subtasks of application tasks are replicated at run-time for tolerating failures as well as for sharing workload increases. Given such models, the objective of the algorithm is to maximize the aggregate real-time benefit and the ability to tolerate host failures during the time window of adaptation functions. Since determining the optimal solution is computationally intractable, BEA heuristically computes suboptimal resource allocations in polynomial-time. We show that BEA can achieve almost the same fault-tolerance ability as full replication, and accrue most of real-time benefit that full replication can accrue. In the meanwhile, BEA requires much fewer replicas than full replication, and hence is cost effective.||||
50|10||Stabilizing ring clustering|In this paper, we first present simple stabilizing algorithms for finding clustering of ring networks on a distributed model of computation. Clustering is defined as partitioning of nodes of a network into non-overlapping sets of nodes based on certain criteria. Our criterion for partitioning the network is that the difference between the sizes of the largest cluster and the smallest cluster is minimal. We first present a uniform algorithm that evenly partitions the network into nearly the same size clusters. The clusters may continuously move in one direction while maintaining the difference of at most one between the size of the largest and the size of the smallest cluster. Then, we present a non-uniform self-stabilizing algorithm for the same problem that terminates after O(n2) moves. When resources are placed at cluster boundaries (or centers), the cost of sharing resources is minimized. The algorithms can withstand transient faults and do not require initialization. In addition, when the ring size changes, the proposed algorithms automatically identify the clusterings of the new ring. The paper includes correctness proofs of the algorithms. It concludes with remarks on issues such as open and related problems, and the application areas of the algorithm.||||
50|10||A parallel algorithm for constructing reduced visibility graph and its FPGA implementation|A central geometric structure in applications such as robotic path planning and hidden line elimination in computer graphics is the visibility graph. A new parallel algorithm to construct the reduced visibility graph in a convex polygonal environment is presented in this paper. The computational complexity is O(p2log(n/p)) where p is the number of objects and n is the total number of vertices. A key feature of the algorithm is that it supports easy mapping to hardware. The algorithm has been simulated (and verified) using C. Results of hardware implementation show that the design operates at high speed requiring only small space. In particular, the hardware implementation operates at approximately 53 MHz and accommodates the reduced visibility graph of an environment with 80 vertices in one XCV3200E device.||||
50|10||Guide for Authors|||||
50|11|http://www.sciencedirect.com/science/journal/13837621/50/11|Editorial board|||||
50|11||Software environment for integrating critical real-time control systems|In the recent few years, integration of multiple real-time control modules has gained increased acceptance in the industry. Such integration can achieve lower overall hardware costs and reduced level of spares by sharing hardware resources among multiple applications. Single contemporary CPU can now harbor several applications which have been traditionally running on several older and slower computing platforms. However, the integrated approach faces new challenges such as the reusability of existing software and the prevention of fault propagation. The reuse of legacy application code, with minimal modifications, is strongly desirable since the cost of application re-development can be prohibitive. Resource sharing introduces dependencies among applications and thus requires additional design precautions to ensure that the effect of a failure in one application will not spread and impact other applications. This paper describes a two-layer software architecture, which enables the integration of multiple real-time applications while maintaining strong spatial and temporal partitioning among application modules. At the lower layer, a system executive creates multiple virtual machines. Each module accommodates an application with its choice of a real-time operating system. This architecture allows the reusability of existent software modules by enabling the integration of applications written for different real-time operating systems. The paper also addresses some issues related to the inter-application communication and to the handling of I/O devices.||||
50|11||Using a serial cache for energy efficient instruction fetching|The design of a high performance fetch architecture can be challenging due to poor interconnect scaling and energy concerns. Way prediction has been presented as one means of scaling the fetch engine to shorter cycle times, while providing energy efficient instruction cache accesses. However, way prediction requires additional complexity to handle mispredictions.||||
50|11||A clocking technique for FPGA pipelined designs|This paper presents a clocking pipeline technique referred to as a single-pulse pipeline (PP-Pipeline) and applies it to the problem of mapping pipelined circuits to a Field Programmable Gate Array (FPGA). A PP-pipeline replicates the operation of asynchronous micropipelined control mechanisms using synchronous-orientated logic resources commonly found in FPGA devices. Consequently, circuits with an asynchronous-like pipeline operation can be efficiently synthesized using a synchronous design methodology. The technique can be extended to include data-completion circuitry to take advantage of variable data-completion processing time in synchronous pipelined designs. It is also shown that the PP-pipeline reduces the clock tree power consumption of pipelined circuits. These potential applications are demonstrated by post-synthesis simulation of FPGA circuits.||||
50|11||Optical transpose k-ary n-cube networks|This paper derives a number of results related to the topological properties of OTIS k-ary n-cube interconnection networks. The basic topological metrics of size, degree, shortest distance, and diameter are obtained. Then results related to embedding in OTIS k-ary n-cubes of OTIS k-ary (n−1)-cubes, cycles, meshes, cubes, and spanning trees are derived. The OTIS k-ary n-cube is shown to be Hamiltonian. Minimal one-to-one routing and optimal broadcasting algorithms are proposed. The OTIS k-ary n-cube is shown to be maximally fault-tolerant. These results are derived based on known properties of k-ary n-cube networks and general properties of OTIS networks.||||
50|11||Guide for Authors|||||
50|12|http://www.sciencedirect.com/science/journal/13837621/50/12|A tamper resistant hardware accelerator for RSA cryptographic applications|This paper presents an hardware accelerator which can effectively improve the security and the performance of virtually any RSA cryptographic application. The accelerator integrates two crucial security- and performance-enhancing facilities: an RSA processor and an RSA key-store. An RSA processor is a dedicated hardware block which executes the RSA algorithm. An RSA key-store is a dedicated device for securely storing RSA key-pairs. We chose RSA since it is by far the most widely adopted standard in public key cryptography. We describe the main functional blocks of the hardware accelerator and their interactions, and comment architectural solutions we adopted for maximizing security and performance while minimizing the cost in terms of hardware resources. We then present an FPGA-based implementation of the proposed architecture, which relies on a Commercial Off The Shelf (COTS) programmable hardware board. Finally, we evaluate the system in terms of performance and chip area occupation, and comment the design trade-offs resulting from different levels of parallelism.||||
50|12||Exploitation of parallelism to nested loops with dependence cycles|In this paper, we analyze the recurrences from the breakability of the dependence links formed in general multi-statements in a nested loop. The major findings include: (1) A sin k variable renaming technique, which can reposition an undesired anti-dependence and/or output-dependence link, is capable of breaking an anti-dependence and/or output-dependence link. (2) For recurrences connected by only true dependences, a dynamic dependence concept and the derived technique are powerful in terms of parallelism exploitation. (3) By the employment of global dependence testing, link-breaking strategy, Tarjan’s depth-first search algorithm, and a topological sorting, an algorithm for resolving a general multi-statement recurrence in a nested loop is proposed. Experiments with benchmark cited from Vector loops showed that among 134 subroutines tested, 3 had their parallelism exploitation amended by our proposed method. That is, our offered algorithm increased the rate of parallelism exploitation of Vector loops by approximately 2.24%.||||
50|12||Design of an efficient VLSI architecture for non-linear spatial warping of wide-angle camera images|Endoscopic images are subjected to spatial distortion due to the wide-angle configuration of the camera lenses. This barrel type of non-linear distortion should be corrected before these images are subjected to further analysis for diagnostic purposes. An efficient digital architecture suitable for an embedded system which can correct the barrel distortion in real-time is presented in this paper. The theoretical approach of this spatial warping technique is based on least-squares estimation. The images in the distorted image space are mapped onto the corrected image space by using a polynomial mapping model. The polynomial parameters include the expansion coefficients, back-mapping coefficients, distortion centre and corrected centre. Several experiments were conducted by applying the spatial warping algorithm on many endoscopic images. A digital architecture suitable for hardware implementation of the distortion correction technique is developed by mapping the algorithmic steps onto a linear array of processing modules. Each module of a particular unit communicates with its nearest neighbours. The spatial warping architecture implemented and simulated with Altera’s Quartus II software shows an overall computation time of 1.8 ms with 50 MHz clock for an image of size 256 × 192 pixels, which confirms that the spatial warping module could be mounted as a dedicated unit in an endoscopy system for real-time applications.||||
50|12||Author Index to Volume 50 (2004)|||||
50|12||Subject Index to Volume 50 (2004)|||||
50|12||Guide for Authors|||||
50|2-3|http://www.sciencedirect.com/science/journal/13837621/50/2-3|Editorial board|||||
50|2-3||Special issue on networks on chip|||||
50|2-3||Interconnect intellectual property for Network-on-Chip (NoC)|As technology scales down, the interconnect for on-chip global communication becomes the delay bottleneck. In order to provide well-controlled global wire delay and efficient global communication, a Network-on-Chip (NoC) architecture was proposed by different authors [Route packets, not wires: on-chip interconnection networks, in: Design Automation Conference, 2001, Proceedings, p. 684; Network on chip: an architecture for billion transistor era, in: Proceeding of the IEEE NorChip Conference, November 2000; Network on chip, in: Proceedings of the Conference Radio vetenskap och Kommunication, Stockholm, June 2002]. NoC uses Interconnect Intellectual Property (IIP) to connect different resources. Within an IIP, the switch has the central function. Depending on the network core of the NoC, the switch will have different architectures and implementations. This paper first briefly introduces the concept of NoC. It then studies NoC from an interconnect point of view and makes projections on future NoC parameters. At last, the IIP and its components are described, the switch is studied in more detail and a time–space–time (TST) switch designed for a circuit switched time-division multiplexing (TDM) NoC is proposed. This switch supports multicast traffic and is implemented with random access memory at the input and output. The input and output are then connected by a fully connected interconnect network.||||
50|2-3||Packetization and routing analysis of on-chip multiprocessor networks|Some current and most future systems-on-chips use and will use network architectures/protocols to implement on-chip communication. On-chip networks borrow features and design methods from those used in parallel computing clusters and computer system area networks. They differ from traditional networks because of larger on-chip wiring resources and flexibility, as well as constraints on area and energy consumption (in addition to performance requirements). In this paper, we analyze different routing schemes for packetized on-chip communication on a mesh network architecture, with particular emphasis on specific benefits and limitations of silicon VLSI implementations. A contention-look-ahead on-chip routing scheme is proposed. It reduces the network delay with significantly smaller buffer requirement. We further show that in the on-chip multiprocessor systems, both the instruction execution inside node processors, as well as data transaction between different processing elements, are greatly affected by the packetized dataflows that are transported on the on-chip networks. Different packetization schemes affect the performance and power consumption of multiprocessor systems. Our analysis is also quantified by the network/multiprocessor co-simulation benchmark results.||||
50|2-3||QNoC: QoS architecture and design process for network on chip|We define Quality of Service (QoS) and cost model for communications in Systems on Chip (SoC), and derive related Network on Chip (NoC) architecture and design process. SoC inter-module communication traffic is classified into four classes of service: signaling (for inter-module control signals); real-time (representing delay-constrained bit streams); RD/WR (modeling short data access) and block-transfer (handling large data bursts). Communication traffic of the target SoC is analyzed (by means of analytic calculations and simulations), and QoS requirements (delay and throughput) for each service class are derived. A customized Quality-of-Service NoC (QNoC) architecture is derived by modifying a generic network architecture. The customization process minimizes the network cost (in area and power) while maintaining the required QoS.||||
50|2-3||OCCN: a NoC modeling framework for design exploration|The On-Chip Communication Network (OCCN) project provides an efficient framework, developed within SourceForge, for the specification, modeling, simulation, and design exploration of network on-chip based on an object-oriented C++ library built on top of SystemC. OCCN is shaped by our experience in developing communication architectures for different System-on-Chip. OCCN increases the productivity of developing communication driver models through the definition of a universal Application Programming Interface (API). This API provides a new design pattern that enables creation and reuse of executable transaction level models across a variety of SystemC-based environments and simulation platforms. It also addresses model portability, simulation platform independence, interoperability, and high-level performance modeling issues.||||
50|2-3||Guide for Authors|||||
50|4|http://www.sciencedirect.com/science/journal/13837621/50/4|Editorial board|||||
50|4||A multiple disk failure recovery scheme in RAID systems|In this paper, we propose a practical disk error recovery scheme tolerating multiple simultaneous disk failures in a typical RAID system, resulting in improvement in availability and reliability. The scheme is composed of the encoding and the decoding processes. The encoding process is defined by making one horizontal parity and a number of vertical parities. The decoding process is defined by a data recovering method for multiple disk failures including the parity disks. The proposed error recovery scheme is proven to correctly recover the original data for multiple simultaneous disk failures regardless of the positions of the failed disks. The proposed error recovery scheme only uses exclusive OR operations and simple arithmetic operations, which can be easily implemented on current RAID systems without hardware changes.||||
50|4||Task migration in n-dimensional wormhole-routed mesh multicomputers|||||
50|4||Multi-mesh of trees with its parallel algorithms|In recent years the multi-mesh network [Proceedings of the Ninth International Parallel Processing Symposium, Santa Barbara, CA, April 25–28, 1995, 17; IEEE Trans. on Comput. 68 (5) (1999) 536] has created a lot of interests among the researchers for its efficient topological properties. Several parallel algorithms for various trivial and nontrivial problems have been mapped on this network. However, because of its O(n) diameter, a large class of algorithms that involves frequent data broadcast in a row or in a column or between the diametrically opposite processors, requires O(n) time on an n×n multi-mesh. In search of faster algorithms, we introduce, in this paper, a new network topology, called multi-mesh of trees. This network is built around the multi-mesh network and the mesh of trees. As a result it can perform as efficiently as a multi-mesh network and also as efficiently as a mesh of trees. Several topological properties, including number of links, diameter, bisection width and decomposition are discussed. We present the parallel algorithms for finding sum of n4 elements and the n2-point Lagrange interpolation both in O(logn) 1 time. The solution of n2-degree polynomial equation, n2-point DFT computation and sorting of n2 elements are all shown to run in O(logn) time too. The communication algorithms one-to-all, row broadcast and column broadcast are also described in O(logn) time. This can be compared with O(n) time algorithms on multi-mesh network for all these problems.||||
50|4||On the development of a communication-aware task mapping technique|Clusters have become a very cost-effective platform for high-performance computing. In these systems, although currently existing networks actually provide enough bandwidth for the existing applications and workstations, the trend is towards the interconnection network becoming the system bottleneck. Therefore, in the future, scheduling strategies will have to take into account the communication requirements of the applications and the communication bandwidth that the network can offer. One of the key issues in these strategies is the task mapping technique used when the network becomes the system bottleneck.||||
50|4||Optimal all-ports collective communication algorithms for the k-ary n-cube interconnection networks|The need for collective communication procedures such as One-to-All broadcast, All-to-All broadcast arises in many parallel or distributed applications. Many of these communication procedures have been studied for many topologies of interconnection networks such as hypercubes, meshes, De Bruijn, star graphs and butterflies. In this paper we propose a construction of multiple edge-disjoint spanning trees for the k-ary n-cube which can be used to derive an optimal and fault tolerant broadcasting algorithm. We propose also an optimal All-to-All broadcasting algorithm. We consider the k-ary n-cube as a point-to-point interconnection, using store-and-forward, all-port assumption and a linear communication model.||||
50|4||Guide for Authors|||||
50|5|http://www.sciencedirect.com/science/journal/13837621/50/5|Editorial board|||||
50|5||Guest editorial|||||
50|5||Efficient analysis of single event transients|The effects of charged particles striking VLSI circuits and producing single event transients (SETs) are becoming an issue for designers who exploit deep sub-micron technologies; efficient and accurate techniques for assessing their impact on VLSI designs are thus needed. This paper presents a new approach for generating the list of faults to be addressed during fault injection experiments tackling SET effects, which resorts to static timing analysis. Moreover, it proposes a simplified SET fault model, which is suitable for being adopted within a zero-delay fault simulation tool. Experimental results are reported on both standard benchmarks and real-life circuits assessing the effectiveness of the proposed techniques.||||
50|5||Matrix-based software test data decompression for systems-on-a-chip|This paper describes a new compression/decompression methodology for using an embedded processor to test the other components of a system-on-a-chip (SoC). The deterministic test vectors for each core are compressed using matrix-based operations that significantly reduce the amount of test data that needs to be stored on the tester. The compressed data is transferred from the tester to the processor's on-chip memory. The processor executes a program which decompresses the data and applies it to the scan chains of each core-under-test. The matrix-based operations that are used to decompress the test vectors can be performed very efficiently by the embedded processor thereby allowing the decompression program to be very fast and provide high throughput of the test data to minimize test time. Experimental results demonstrate that the proposed approach provides greater compression than previous methods.||||
50|5||Fast and energy-frugal deterministic test through efficient compression and compaction techniques|Conversion of the flip-flops of the circuit into scan cells helps ease the test challenge; yet test application time is increased as serial shift operations are employed. Furthermore, the transitions that occur in the scan chains during these shifts reflect into significant levels of circuit switching unnecessarily, increasing the power dissipated. Judicious encoding of the correlation among the test vectors and construction of a test vector through predecessor updates helps reduce not only test application time but also scan chain transitions as well. Such an encoding scheme, which additionally reduces test data volume, can be further enhanced through appropriately ordering and padding of the test cubes given. The experimental results confirm the significant reductions in test application time, test data volume and test power achieved by the proposed compression methodology.||||
50|5||A novel FPGA local interconnect test scheme and automatic TC derivation/generation|This paper presents a novel local interconnect testing scheme for field programmable gate arrays (FPGAs). To maximize parallel testing, error-detecting code is used for testing one portion of interconnects and functional test of D latch for another in a test configuration (TC). A polynomial run time algorithm is introduced for deriving a minimal set of TCs. An in-house CAD tool is developed to automate the generation of device configurations from the set of TCs.||||
50|5||Balanced dual-stage repair for dependable embedded memory cores|Advances in revolutionary system-on-chip (SoC) technology mainly depend on the high-performance ultra-dependable system core components. Among those core components, embedded memory system core, currently acquiring 54% of SoC area share, will continue its domination of SoC area share as it is anticipated to approach about 94% of SoC area share by the year 2014. Since memory cells are considered as more prone to defects and faults than logic cells, redundancy and repair have been extensively practiced for enhancing defect and fault tolerance. Unlike in legacy PCB (printed circuit board) or MCM (multichip module) based systems, embedded core components cannot be physically replaced once they are fabricated onto a SoC. To realize enhanced manufacturing yield and field reliability, both ATE (automated test equipment) and BISR (built-in-self-repair) are commonly utilized to allocate redundancy for embedded memory system cores. Since ATE (for repairing manufacturing defects) and BISR (for repairing field faults) share the given redundancy, probabilistic redundancy partitioning and utilization techniques are proposed in this paper to achieve optimal combination of yield and reliability of the embedded memory system core. Parametric simulation results are shown extensively.||||
50|5||IDDQ data analysis using neighbor current ratios|IDDQ test loses its effectiveness for deep sub-micron chips since it cannot distinguish between faulty and fault-free currents. The concept of current ratios, in which the ratio of maximum to minimum IDDQ is used to screen faulty chips, has been previously proposed. However, it is incapable of screening some defects. The neighboring chips on a wafer have similar fault-free properties and are correlated. In this paper, the use of spatial correlation in combination with current ratios is investigated. By differentiating chips based on their non-conformance to local IDDQ variation, outliers are identified. The analysis of SEMATECH test data is presented.||||
50|6|http://www.sciencedirect.com/science/journal/13837621/50/6|Editorial board|||||
50|6||Socket-based RR scheduling scheme for tightly coupled clusters providing single-name images|Server clusters for Internet services can yield both high performance and cost effectiveness. Contemporary approaches to cluster design must confront the tradeoff between dynamic load-balancing and efficiency when dispatching client requests to servers in the cluster. In this paper we describe a packet filtering-based RR scheduling scheme, intended to be an easily implemented scheme for hosting Internet services on a server cluster in a way transparent to clients. We take a non-centralized approach, in which client packets sent to the cluster's single IP address are broadcast to all of its servers. Packets requesting that a new TCP session be set up cause counters in each server to be incremented; if a server's counter matches its fixed unique ID, it takes charge of the session, else it ignores the packet. This yields a round-robin type algorithm. We describe this approach in detail, and present results of simulations that show it achieves higher performance (in terms of throughput and reliability) than similar approaches based on client-IP hashing and dispatcher-based RR.||||
50|6||Developing a reusable workflow engine|Every time a workflow solution is conceived there is a large amount of functionality that is eventually reinvented and redeveloped from scratch. Workflow management systems from academia to the commercial arena exhibit a myriad of approaches having as much in common as in contrast with each other. Efforts in standardizing a workflow reference model and the gradual endorsement of those standards have also not precluded developers from designing workflow systems tailored to specific user needs. This article is written in the belief that an appropriate set of common workflow functionality can be abstracted and reused in forthcoming systems or embedded in applications intended to become workflow-enabled. Specific requirements and a prototype implementation of such functionality, named Workflow Kernel, are discussed.||||
50|6||On the performance analysis of ABR in ATM LANs with Stochastic Petri Nets|In this paper we use Generalized Stochastic Petri Nets (GSPNs) and Stochastic Well-formed Nets (SWNs) for the performance analysis of Asynchronous Transfer Mode (ATM) Local Area Networks (LANs) that adopt the Available Bit Rate (ABR) service category in its Relative Rate Marking (RRM) version. We also consider a peculiar version of RRM ABR called Stop & Go ABR; this is a simplified ABR algorithm designed for the provision of best-effort services in low-cost ATM LANs, according to which sources can transmit only at two different cell rates, the Peak Cell Rate (PCR) and Minimum Cell Rate (MCR). Results obtained from the solution of GSPN models of simple ATM LAN setups comprising RRM or Stop & Go ABR users, as well as Unspecified Bit Rate (UBR) users, are first validated through detailed simulations, and then used to show that Stop & Go ABR is capable of providing good performance and fairness in a number of different LAN configurations. We also develop SWN models of homogeneous ABR LANs, that efficiently and automatically exploit system symmetries allowing the investigation of larger LAN configurations.||||
50|6||Performance evaluation of a Windows NT based PC cluster for high performance computing|In recent times the computational power of personal computers has remarkably increased and the use of groups of PCs and workstations, connected by a network and dedicated to parallel computations, is today frequent. Computing clusters are mainly based on UNIX workstations and Linux PCs but, in the last few years, different implementations of message passing systems were made available also for Microsoft Windows. In this work we test the performance of two implementations of MPI for Windows platforms, and we compare the results with those obtained from Linux systems.||||
50|6||Guide for Authors|||||
50|7|http://www.sciencedirect.com/science/journal/13837621/50/7|Editorial board|||||
50|7||Adaptable system/software architectures|||||
50|7||Relating evolving business rules to software design|In order to remain useful, it is important for software to evolve according to the changes in its business environment. Business rules, which can be used to represent both user requirements and conditions to which the system should conform, are considered as the most volatile part in today's software applications. Their changes bring high impact on both the business processes and the software itself. In this paper, we present an approach that considers business rules as an integral part of a software system and its evolution. The approach transcends the areas of requirements specification and software design. We develop the Business Rule Model to capture and specify business rules, and the Link Model to relate business rules to the metamodel level of software design elements. The aim is to improve requirements traceability in software design, as well as minimizing the efforts of software changes due to the changes of business rules. The approach is demonstrated using examples from an industrial application.||||
50|7||Assessing systems adaptability to a product family|In many cases, product families are established on top of a successful pilot product. While this approach provides an option to measure many concrete attributes like performance and memory footprint, adequateness and adaptability of the architecture of the pilot cannot be fully verified. Yet, these properties are crucial business enablers for the whole product family. In this paper, we discuss an architectural assessment of one such seminal system, intended for monitoring electronic subsystems of a mobile machine, which is to be extended to support a wide range of different types of products. This paper shows how well the assessment reveals possible problems and existing flexibilities in assessed system, and this way helps different stakeholders in their further decisions.||||
50|7||Measures for mobile users: an architecture|Software measures are important to evaluate software properties like complexity, reusability, maintainability, effort required, etc. Collecting such data is difficult because of the lack of tools that perform acquisition automatically. It is not possible to implement a manual data collection because it is error prone and very time expensive. Moreover, developers often work in teams and sometimes in different places using laptops. These conditions require tools that collect data automatically, can work offline and merge data from different developers working in the same project. This paper presents PROM (PRO Metrics), a distributed Java based tool designed to collect automatically software measures. This tool uses a distributed architecture based on plug-ins, integrated in most popular development tools, and the SOAP communication protocol.||||
50|7||A Software System evolutionary and adaptive framework: application to Agent-based systems|In this paper we present part of our current work: a proposal on a Software System evolutionary framework. This proposal is based mainly on previous work carried out by the GEDES (Group of Specification, Development and Evolution of Software) Research Group. Within this framework, we try to model the way a Software System can evolve, and especially, the evolution of Agent-based systems. We present the way systems evolve based on the application of operators and the understanding of definition of focusing on which should be these operators, and invariants in Agent-based systems, as well as introducing examples of actions and restrictions applied.||||
50|7||A model of runtime transformation for distributed systems based on directed acyclic graph model|This paper presents a formal model of runtime program transformation to optimize concurrent processes during executions based on a new representation of the scopes of names in distributed systems. We represent a site in a distributed system and the scopes of the local names in the site using a directed acyclic graph. It is possible to represent local names that their scope are not nested in a site in the model. Local names with overlapping scopes make possible to formalize folding transformation of process definitions for runtime program transformation. Computations in the system are represented with inferences on multisets of formulas of linear logic. We define rewriting system of directed acyclic graphs as the operational semantics of our model based on inference rules for linear logic. Each of the steps for runtime transformations are also represented using a rewriting for directed acyclic graphs.||||
50|7||Generative and incremental implementation for a scripting interface|||||
50|7||Guide for Authors|||||
50|8|http://www.sciencedirect.com/science/journal/13837621/50/8|Editorial board|||||
50|8||General decomposition of incompletely specified sequential machines with multi-state behavior realization|This paper is devoted to decomposition of sequential machines, discrete functions and relations. Sequential machine decomposition consists in representation of a given machine as a network of collaborating partial machines that together realize behavior of the given machine. A good understanding of possible decomposition structures and of conditions under which the corresponding structures exist is a prerequisite for any adequate circuit or system synthesis. The paper discusses the theory of general decomposition of incompletely specified sequential machines with multi-state behavior realization. The central point of this theory is a constructive theorem on the existence of the general decomposition structures and conditions under which the corresponding structures exist. The theory of general decomposition presented in this paper is the most general known theory of the binary, multi-valued and symbolic sequential and combinational discrete network structures. The correct circuit generator defined by the general decomposition theorem covers all other known structural models of sequential and combinational circuits as its special cases. Using this theory, in recent years we developed a number of effective and efficient methods and EDA tools for sequential and combinational circuit synthesis that consistently construct much better circuits than other academic and commercial state-of-the-art synthesis tools. This demonstrates the practical soundness of our theory. This theory can be applied to any sort of binary, multi-valued and symbolic systems expressed as networks of relations, functions or sequential machines, and can be very useful in such fields as circuit and architecture synthesis of VLSI systems, knowledge engineering, machine learning, neural network training, pattern analysis, etc.||||
50|8||Implementing a replicated service with group communication|Distributed computing systems are quickly pervading many aspects of everyday life. The public demand for high reliability of these systems can only grow, together with their penetration in critical application domains. Group communication is a technology that may greatly simplifying the deployment of reliable distributed applications, even in environments composed of off-the-shelf hardware and software components. In this paper we attempt to provide an introductory and unified view to group communication. To make the presentation concrete, we shall analyze in detail how group communication may help in implementing a highly-available service based on replication. We shall consider a service whose interface allows determining the outcome of a non-idempotent operation previously submitted to the service, e.g., an update, that returned prematurely at the client because of a communication error––a key practical problem. We shall also provide a discussion of the performance that can be obtained in practice with group communication platforms.||||
50|8||Guide for Authors|||||
50|9|http://www.sciencedirect.com/science/journal/13837621/50/9|Editorial board|||||
50|9||The use of economic agents under price driven mechanism in grid resource management|This paper presents multi-economic agent for grid resource management. A system model is described that allows agents representing various grid resources and grid users to interact without assuming priori cooperation. The system model consists of three layers. The lower layer is the underlying grid resource. The middle layer is the agent-based grid resource management system. It consists of three types of agent and market institution that allocates resources. The grid task agents buy resources to complete tasks. Grid resource agents charge the task agents for the amount of resource capacity allocated. Grid resource agents are registered with a Grid Manager. The third layer is the user layer at which grid request agents provide interfaces to the grid user' request. The three processes involved in grid resource management are given. A price-directed algorithm for solving the grid task agent resource allocation problem is presented. A basic performance evaluation is given. Finally, some conclusions are given.||||
50|9||A comparative evaluation of hardware-only and software-only directory protocols in shared-memory multiprocessors|The hardware complexity of hardware-only directory protocols in shared-memory multiprocessors has motivated many researchers to emulate directory management by software handlers executed on the compute processors, called software-only directory protocols.||||
50|9||On the performance of multicomputer interconnection networks|Several researchers have analysed the performance of k-ary n-cubes taking into account channel bandwidth constraints imposed by implementation technology, namely the constant wiring density and pin-out constraints for VLSI and multiple-chip technology respectively. For instance, Dally [IEEE Trans. Comput. 39(6) (1990) 775], Abraham [Issues in the architecture of direct interconnection networks schemes for multiprocessors, Ph.D. thesis, University of Illinois at Urbana-Champaign, 1992], and Agrawal [IEEE Trans. Parallel Distributed Syst. 2(4) (1991) 398] have shown that low-dimensional k-ary n-cubes (known as tori) outperform their high-dimensional counterparts (known as hypercubes) under the constant wiring density constraint. However, Abraham and Agrawal have arrived at an opposite conclusion when they considered the constant pin-out constraint. Most of these analyses have assumed deterministic routing, where a message always uses the same network path between a given pair of nodes. More recent multicomputers have incorporated adaptive routing to improve performance. This paper re-examines the relative performance merits of the torus and hypercube in the context of adaptive routing. Our analysis reveals that the torus manages to exploit its wider channels under light traffic. As traffic increases, however, the hypercube can provide better performance than the torus. Our conclusion under the constant wiring density constraint is different from that of the works mentioned above because adaptive routing enables the hypercube to exploit its richer connectivity to reduce message blocking.||||
50|9||Multi-node broadcasting in all-ported 3-D wormhole-routed torus using an aggregation-then-distribution strategy|In this paper, we investigate the multi-node broadcasting problem in a 3-D torus, where there are an unknown number of s source nodes located at unknown positions each intending to broadcast a message of size m bytes to the rest of the network. The torus is assumed to use the all-port model and the popular dimension-ordered routing. Existing congestion-free results are derived based on finding multiple edge-disjoint spanning trees in the network. This paper shows how to efficiently perform multi-node broadcasting in a 3-D torus. The main technique used in this paper is an aggregation-then-distribution strategy, which is characterized by the following features: (i) the broadcast messages are aggregated into some positions on the 3-D torus, then a number of independent subnetworks are constructed from the 3-D torus; and (ii) these subnetworks, which are responsible for distributing the messages, fully exploit the communication parallelism and the characteristic of wormhole routing. It is shown that such an approach is more appropriate than those using edge-disjoint trees for fixed-connection networks such as tori. Extensive simulations are conducted to evaluate this multi-broadcasting algorithm.||||
50|9||Guide for Authors|||||
||||||||
volume|issue|url|title|abstract||||
51|1|http://www.sciencedirect.com/science/journal/13837621/51/1|Hierarchical star: a new two level interconnection network|We propose a new two level interconnection network topology, hierarchical star networks, HSn, that uses the star graphs as building blocks. Two level networks have been previously proposed that use hypercube and its variants as building blocks; it has been shown that these two level networks are superior to the networks, that are used as building blocks, in terms of various performance metrics including diameter, cost, fault tolerance, fault diameter etc. Our results show that the proposed family of hierarchical star networks perform very competitively in comparison to star graphs; in addition, the proposed network outperforms all of the two level hierarchical networks proposed earlier that uses hypercubes (or its variations) as building blocks. Thus, our results further reinforce the notion that the star graphs are strong competitors of hypercubes for large multiprocessor design. We also investigate various topological properties of the network including embedding, mapping of parallel algorithms, fault tolerance and broadcasting algorithms.||||
51|1||Architecture optimization for multimedia application exploiting data and thread-level parallelism|The characteristics of multimedia applications when executed on general-purpose processors are not well understood. Such knowledge is extremely important in guiding the development of multimedia applications and the design of future processors.||||
51|1||An FPGA-based coprocessor for real-time fieldbus traffic schedulingâarchitecture and implementation|Distributed computer control systems used nowadays in the industry need often to meet requirements of on-line reconfigurability so they can adjust dynamically to changes in the application environment or to evolving specifications. The communication network connecting the computer nodes, commonly a fieldbus system, must use therefore dynamic scheduling strategies, together with on-line admission control procedures that test the validity of all changes in order to guarantee the satisfaction of real-time constraints. These are both very computationally demanding tasks, something that has precluded their wide adoption. However, these algorithms also embed sufficient levels of parallelism to grant them benefits from implementations in dedicated hardware.||||
51|1||A novel min-process checkpointing scheme for mobile computing systems|In distributed computing systems, processes in different hosts take checkpoints to survive failures. For mobile computing systems, due to certain new characteristics such as mobility, low bandwidth, disconnection, low power consumption and limited memory, conventional distributed checkpointing schemes need to be reconsidered. In this paper, a novel min-process coordinated checkpointing algorithm that makes full use of the computation ability and power of mobile support stations is proposed. During normal computation message transmission, the checkpoint dependency information among mobile hosts is recorded in the corresponding mobile support stations. When a checkpointing procedure begins, the initiator concurrently informs relevant mobile hosts, which minimizes the identifying time. Moreover, compared with the existing coordinated checkpointing schemes, our algorithm blocks the minimum number of mobile support stations during the identifying procedure, which leads to the improvement of the system performance. In addition, the proposed algorithm is a min-process, domino-free checkpointing algorithm, which is especially desirable for mobile computing systems. Quantitative analysis and experimental simulation show that our algorithm outperforms other coordinated checkpointing schemes in terms of the identifying time and the number of blocked mobile support stations and then can provide a better system performance for mobile computing systems.||||
51|1||The impact of x86 instruction set architecture on superscalar processing|Performance improvement of x86 processors is a relevant matter. From the point of view of superscalar processing, it is necessary to complement the studies on instruction use with analogous ones on data use and, furthermore, analyze the data flow graphs, as its dependencies are responsible for limitations on ILP. In this work, using instruction traces from common applications, quantitative analyses of implicit operands, memory addressing and condition codes have been performed, three sources of significant limitations on the maximum achievable parallelism in the x86 architecture. In order to get a deeper knowledge of these limitations, the data dependence graphs are built from traces. By means of graph matrix representation, potentially exploitable parallelism is quantified and parallelism distributions from the traces are shown. The method has also been applied to measure the impact of the use of condition codes. Results are compared with previous work and some conclusions are presented relating the obtained degree of parallelism with negative characteristics of x86 instruction set architecture.||||
51|1||Guide for Authors|||||
51|10-11|http://www.sciencedirect.com/science/journal/13837621/51/10-11|Switch fabric design for high performance IP routers: A survey|Traditionally, besides vendor product descriptions on high performance Internet Protocol (IP) router hardware (HW) architectures, materials on this subject area seldom appear in research literature. Recently, we introduced an architectural concept of HW scalability and bi-directional HW reconfigurability for high performance IP routers. Application of these two conceptual attributes enables router HW flexibility to adapt to today’s IP network environment with rapid changes in capacity and traffic characteristics. We analyzed 10 switch fabrics (SFs), selected and also presented brief survey of HW architectural techniques that enable the attributes for three candidates that can serve such a router. In this paper, we present a full survey of these 10 SFs. The intention is to provide background reference material on an area not yet frequently visited in formal literature.||||
51|10-11||Implementation and performance study of a hardware-VIA-based network adapter on Gigabit Ethernet|This paper presents the implementation and performance of a hardware-VIA-based network adapter on Gigabit Ethernet. VIA is a user–level communication interface for high performance PC clustering. The network adapter is a 64-bit/66 MHz PCI plug-in card containing an FPGA for the VIA Protocol Engine and a Gigabit Ethernet chip to construct a high performance system area network. The network adapter performs virtual-to-physical address translation, doorbell, RDMA write, and send/receive completion operations in hardware without kernel intervention. In particular, the Address Translation Table (ATT) is stored on the local memory of the network adapter, and the VIA Protocol Engine efficiently controls the address translation process by directly accessing the ATT. In addition, Address Prefetch Buffer is used to reduce the time of address translation process in the receiver. As a result, the communication overhead during send/receive transactions is greatly reduced. Our experimental results show a minimum latency of 8.2 Î¼s, and a maximum bandwidth of 112.1 MB/s. In terms of minimum latency, the hardware-VIA-based network adapter performs 2.8 times and 3.3 times faster than M-VIA, which is a software implementation of VIA, and TCP/IP, respectively, over Gigabit Ethernet. In addition, the maximum bandwidth of the hardware-VIA-based network adapter is 24% and 55% higher than M-VIA and TCP/IP, respectively. These results show that the performance of HVIA-GE is far better than that of ServerNet II, which is a hardware version of VIA developed by Tandem/Compaq.||||
51|10-11||A plane-based broadcast algorithm for multicomputer networks|Maximising the performance of parallel systems requires matching message-passing algorithms and application characteristics with a suitable underling interconnection network. Broadcast algorithms for wormhole-switched meshes have been widely reported in the literature. However, most of these algorithms handle broadcast in a sequential manner and do not scale well with the network size. As a consequence, many parallel applications cannot be efficiently supported using existing techniques. Motivated by these observations, this paper presents a new efficient broadcast algorithm for the mesh, called the Plane-Based (PB) algorithm. The main feature of this approach is its ability to perform broadcast operation with a high degree of scalability and parallelism. Furthermore, performance is insensitive to the network size, i.e., only three message-passing steps are required to implement a broadcast operation irrespective of the network size. Results from a comparative analysis demonstrate that the PB algorithm exhibits superior performance characteristics over those of the well-known Recursive Doubling and Extending Dominating Node algorithms.||||
51|10-11||Dynamic voltage scaling techniques for power efficient video decoding|||||
51|10-11||A low energy cache design for multimedia applications exploiting set access locality|An architectural technique is proposed to reduce power dissipation in conventional caches. Our technique is based on the observation of cache access locality: current access is likely to touch the same cache set including the tags as the last access. We show that considerable amount of power driving the cache tag and data banks can be saved if this cache access locality is fully exploited. This is achieved through buffering and accessing the last accessed cache set instead of driving the tag and data banks. Unlikely previous designs, our technique does not incur performance degradation. Experimental results carried out on 8 KB/16 KB/32 KB data and instruction caches have respectively shown 31%/35%/36% and 51%/58%/66% power savings.||||
51|10-11||Guide for Authors|||||
51|12|http://www.sciencedirect.com/science/journal/13837621/51/12|Publisherâs Note - New editorial system|||||
51|12||Time-constrained scheduling of large pipelined datapaths|||||
51|12||Heterogeneous system level co-simulation for the design of telecommunication systems|The advanced complexity and heterogeneity of modern telecommunication systems mostly lead to the incorporation of heterogeneous implementation technologies and design styles. Consequently, the design representation of such systems often requires the mixed use of distinct model of computations at different abstraction layers. Therefore, heterogeneous co-simulation is needed in order to enable the effective communication and interaction among the involved models of computation. This paper resolves this issue by proposing the heterogeneous co-modelling of telecom systems based on the combination of SDL semantics with C language running on an instruction set simulator, coupling in that way the specification and the first refinement steps of the co-design flow. The missing test link between the corresponding tools that support the SDL-C co-model is addressed by proposing a heterogeneous co-simulation scheme through the development of a mediator. Finally, the proposed methodology and the efficiency of the built environment are evaluated through a case study associated with the design of the MAC layer of the DECT telecom system.1,2||||
51|12||Author Index to Volume 51 (2005)|||||
51|12||Subject Index to Volume 51 (2005)|||||
51|12||Guide for Authors|||||
51|2|http://www.sciencedirect.com/science/journal/13837621/51/2|A new distributed storage scheme for cluster video server|For cluster video servers, it is very important to design a good distributed storage system with high performance. One of the important issues in designing a good distributed storage system is how to store multimedia data on many storage nodes. This issue includes two topics: the scheme of splitting an entire file into many clips, and the storage of these clips on many nodes. We have designed a new multimedia data storage scheme for cluster video server. In the new system, a novel multimedia file splitting scheme, named Owl, and a clips striping scheme have been proposed. In contrast with traditional media data splitting schemes based on fixed space length and constant time length, Owl is addressed with the consideration of spatial and temporal information. This scheme Owl has universality for every media format encoded based on time and makes cluster video servers work efficiently. Besides, the scheme Owl is feasible and easy to implement. With the data splitting scheme and the clips striping scheme, cluster video servers have good performance.||||
51|2||Dual and multiple token based approaches for load balancing|In distributed systems uneven arrivals of the tasks may overload a few hosts while some of the hosts may be lightly loaded. This load imbalance prevents distributed systems from delivering its performance to its capacity. Load balancing has been advocated as a means of improving performance and reliability of distributed systems. We propose a distributed load balancing algorithm LoGTra to deal with this problem. LoGTra uses load graph and token based policy. The extensions to LoGTra based on dual tokens DTLB and multiple tokens m-LoGTra are proposed in this paper. m-LoGTra allows host to generate multiple tokens. This allows system to search host for load balancing in multiple directions and can avoid starvation of remote hosts. Local maxima and local minima are responsible for initiating transfer and that limits the number of hosts generating tokens at a time. As overheads are kept under control by limiting number of tokens, the algorithm promises for improved performance.||||
51|2||A new NAND-type flash memory package with smart buffer system for spatial and temporal localities|This research is to design a high performance NAND-type flash memory package with a smart buffer cache that enhances the exploitation of spatial and temporal locality. The proposed buffer structure in a NAND flash memory package, called as a smart buffer cache, consists of three parts, i.e., a fully-associative victim buffer with a small page size, a fully-associative spatial buffer with a large page size, and a dynamic fetching unit. This new NAND-type flash memory package can achieve dramatically higher performance and lower power consumption compared with any conventional NAND-type flash memory module. Our results show that the NAND flash memory package with a smart buffer cache can reduce the miss ratio by around 70% and the average memory access time by around 67%, over the conventional NAND flash memory configuration. Also, the average miss ratio and the average memory access time of the package module with smart buffer cache for a given buffer space (e.g., 3 KB) can achieve better performance than package modules with a conventional direct-mapped buffer with eight times (e.g., 32 KB) or than a fully-associative configuration with twice as much space (e.g., 8 KB).||||
51|2||CORBA-based distributed and replicated resource repository architecture for hierarchically configurable home network|Home networks are typically ubiquitous computing networks consisting of various consumer devices and services. Although many middlewares have already been developed and used to implement partial services in home networks, none has yet emerged as a generic home network middleware architecture, as existing middlewares are designed to accommodate specific services, making them unable to cover all services or satisfy the desires of home users. Accordingly, the current paper presents a hierarchically configurable home network architecture as a generic and conceptual home network model, plus a middleware framework is proposed to realize the conceptual model. In the middleware framework, a distributed and replicated resource repository architecture is introduced to realize multi-services within a single home network platform. The proposed middleware was developed and implemented using CORBA within an IEEE1394-based home network, and its performance verified experimentally.||||
51|2||Optimal broadcasting on incomplete star graph interconnection networks|Broadcasting is an important collective communication operation in many applications which use parallel computing. In this paper, we focus on designing broadcasting algorithms for general incomplete star graphs. We propose two optimal one-to-all broadcasting algorithms for incomplete star graphs with a single-port communication model. An incomplete star graph with N nodes, where (n − 1)! < N < n!, is a subgraph of an n-dimensional star graph. The first scheme is single-message one-to-all broadcasting that takes O(n log n) steps. The second one is multi-message one-to-all broadcasting that takes O(n log n + m) steps.||||
51|2||Guide for Authors|||||
51|3|http://www.sciencedirect.com/science/journal/13837621/51/3|Instruction level redundant number computations for fast data intensive processing in asynchronous processors|Instruction level parallelism (ILP) is strictly limited by various dependencies. In particular, data dependency is a major performance bottleneck of data intensive applications. In this paper we address acceleration of the execution of instruction codes serialized by data dependencies. We propose a new computer architecture supporting a redundant number computation at the instruction level. To design and implement the scheme, an extended data-path and additional instructions are also proposed. The architectural exploitation of instruction level redundant number computations (IL-RNC) makes it possible to eliminate carry propagations. As a result execution of instructions which are serialized due to inherent data dependencies is accelerated. Simulations have been performed with data intensive processing benchmarks and the proposed architecture shows about a 1.2–1.35 fold speedup over a conventional counterpart. The proposed architecture model can be used effectively for data intensive processing in a microprocessor, a digital signal processor and a multimedia processor.||||
51|3||Multicast communication in wormhole-routed symmetric networks with hamiltonian cycle model|In this paper, we first introduce a new hamiltonian cycle model for exploiting the features of symmetric networks. Based on this model, we propose two efficient multicast routing algorithms, uniform multicast routing algorithm and fixed multicast routing algorithm, in symmetric networks with wormhole routing. The proposed multicast routing algorithms utilizes channels uniformly to reduce the path length of message worms, making the multicasting more efficient in symmetric networks. We present two symmetric networks, the torus and star graph, to illustrate the superiority of the proposed schemes. Simulations are conducted to show that the proposed routing schemes outperform the previous scheme.||||
51|3||A generalized fault-tolerant sorting algorithm on a product network|A product network defines a class of topologies that are very often used such as meshes, tori, and hypercubes, etc. This paper proposes a generalized algorithm for fault-tolerant parallel sorting in product networks. To tolerate r − 1 faulty nodes, an r-dimensional product network containing faulty nodes is partitioned into a number of subgraphs such that each subgraph contains at most one fault. Our generalized sorting algorithm is divided into two steps. First, a single-fault sorting operation is presented to correctly performed on each faulty subgraph containing one fault. Second, each subgraph is considered a supernode, and a fault-tolerant multiway merging operation is presented to recursively merge two sorted subsequences into one sorted sequence. Our generalized sorting algorithm can be applied to any product network only if the factor graph of the product graph can be embedding in a ring. Further, we also show the time complexity of our sorting operations on a grid, hypercube, and Petersen cube. Performance analysis illustrates that our generalized sorting scheme is a truly efficient fault-tolerant algorithm.||||
51|3||More on rearrangeability of combined (2n â 1)-stage networks|||||
51|3||Guide for Authors|||||
51|4|http://www.sciencedirect.com/science/journal/13837621/51/4|Generating cache hints for improved program efficiency|One of the new extensions in EPIC architectures are cache hints. On each memory instruction, two kinds of hints can be attached: a source cache hint and a target cache hint. The source hint indicates the true latency of the instruction, which is used by the compiler to improve the instruction schedule. The target hint indicates at which cache levels it is profitable to retain data, allowing to improve cache replacement decisions at run time. A compile-time method is presented which calculates appropriate cache hints. Both kind of hints are based on the locality of the instruction, measured by the reuse distance metric.||||
51|4||Evaluating IA-32 web servers through simics: a practical experience|Nowadays, the use of multiprocessor systems is not just limited to typical scientific applications, but these systems are increasingly being used for executing commercial applications, such as databases and web servers. Therefore, it becomes essential to study the behavior of multiprocessor architectures under commercial workloads. To accomplish this, we need simulators able to model not only the CPU, memory and interconnection network but also other aspects that are critical in the execution of commercial workloads, such as I/O subsystem and operating system. In this paper, we present our first experiences using Simics, a simulator which allows full-system simulation of multiprocessor architectures covering all the topics previously mentioned. Using Simics we carry out a detailed performance study of a static web content server, showing how changes in some architectural parameters, such as number of processors and cache size, affect final performance. The results we have obtained corroborate the intuition of increasing performance of a dual-processor web server opposite to a single-processor one, and at the same time, allow us to check out Simics limitations. Finally, we compare these results with those that are obtained on real machines.||||
51|4||On-chip short-time interval measurement system for high-speed signal timing characterization|In this paper, we present an on-chip short-time interval measurement circuit to characterize the ever-faster communication systems. The proposed circuit consists of time parameters extraction subcircuit and measurement subcircuit, which contain a charge pump, a comparator with hysteresis, a digital counter, and a capacitor. During the measurement, we control a current source and a current sink to charge or discharge a common capacitor, and record the time length ratio of the charging and discharging process. Using this method, we can easily measure most of the time-domain parameters. Also this method minimizes the impacts of the process variations to get high measurement accuracy.||||
51|4||Comments on âSign detection in residue arithmetic unitsâ [Journal of Systems Architecture 45 (1998) 251â258]|It has been shown that the hardware architecture of sign detector presented in [G. Alia, E. Martinelli, Sign detection in residue arithmetic units, Journal of Systems Architecture 45 (1998) 251] is incorrect because the interpretation and the derivation of the rank function is wrong.||||
51|4||Guide for Authors|||||
51|5|http://www.sciencedirect.com/science/journal/13837621/51/5|Generalized parallel divide and conquer on 3D mesh and torus|In this paper, we handle the problem of 1mapping divide-and-conquer idea to 3D mesh and torus interconnection networks. Binary tree is not an efficient computation structure, thus, we select the computation structure as binomial tree. We propose an algorithm for divide and conquer on 3D meshes/torus. After that we give dilation of this algorithm for any 3D mesh whose size is power of 2 and the congestion of this embedding is 1, since each binomial tree consists of two edge-disjoint binomial tree B(n − 1)s.||||
51|5||High-performance architecture for anisotropic filtering|Anisotropic filtering has been developed as a way of increasing the quality of texture mapping. However, a real hardware implementation of anisotropic filters implies approximations and simplifications that result in lower quality. In this paper we present a new, efficient hardware oriented anisotropic filtering technique. Specifically, the new algorithm we propose is based on the utilisation of a new distance computation method. This distance generation scheme permits simplification of the algorithm without reducing its quality. Additionally, a classification procedure for the identification of the position of each texel inside the footprint is proposed. This simple procedure permits the reduction of the computational requirements associated with the algorithm. We also present a new method for the computation of the coverage of a texel, based on the storage of a reduced set of possible coverage patterns. The new distance scheme obtains the patterns of all texels from a given texel through simple shift operations. The good quality results obtained with the filtering algorithm we propose, together with the low computational and storage requirements of the architecture we present, makes it a good candidate for hardware implementation.||||
51|5||Profiling soft-core processor applications for hardware/software partitioning|In this paper, we present an efficient approach to HW/SW partitioning of applications targeted for embedded soft-core SoPC and programmable logic. The methodology is based on the iterative performance analysis of the initial functional SW description and performance estimation of various HW/SW partitioning configurations. The main focus is on adequate profiling of arbitrary SW code regions (function or single instruction level) with clock-cycle accuracy without introducing additional execution overhead. In order to support the profiling for partitioning, we have developed the COMET Profiler tool. The performance analysis and estimation in the simulation and implementation domains are supported, necessitating no design and implementation of HW co-processing blocks for the partitioning evaluation. The design process is illustrated with two case studies.||||
51|5||Run-time analysis of time-critical systems|||||
51|5||Guide for Authors|||||
51|6-7|http://www.sciencedirect.com/science/journal/13837621/51/6-7|Reconfigurable embedded systems: Synthesis, design and application|||||
51|6-7||A configurable system-on-chip architecture for embedded and real-time applications: concepts, design and realization|This paper presents a Configurable System-on-Chip (CSoC) architecture that includes programmable and reconfigurable hardware to cope with the flexibility and real-time signal processing demands in future telecommunication and multimedia systems. A programmable micro Task Controller (mTC) with a small instruction set and a novel pipelined configuration technique with descriptors as configuration templates allows a dynamic use of physical processing resources. The CSoC architecture provides a micro-task based programming model, approves a library-based design approach to reduce developing time and costs and allows forward compatibility to other architecture families. It is shown to be easy scalable to future VLSI technologies where over a hundred processing cells on a single chip will be feasible to deal with the inherent dynamics of future applications and system requirements. Several mappings of commonly used signal processing algorithms and implementation results are given for a standard cell ASIC design realization in 0.18 Î¼m 6-layer UMC CMOS technology.||||
51|6-7||Distance-aware L2 cache organizations for scalable multiprocessor systems|||||
51|6-7||Multiple voltage and frequency scheduling for power minimization|The design description for an integrated circuit may be described in terms of three domains, namely: (1) behavioral domain, (2) structural domain and (3) physical domain. These domains may be hierarchically divided into several levels of abstraction. Classically, these level of abstraction are (1) Architectural or Functional level, (2) Register-transfer level, (3) Logic level and (4) Circuit level. Some of the design problems associated with VLSI circuit design are area, speed, reliability and power consumption. With the development of portable devices, power consumption has become a dominant design consideration in the modern VLSI design area. In each of these domains there are a number of design challenges to reduce power. For instance, at the behavioral level, the freedom to choose multiple voltages and frequencies to minimize power to meet the given hard time constraints is considered as an active field of research to minimize power. Various past researches have showed that higher the level of abstraction, better the ability to address the problems associated with the design. Therefore this work proposes an algorithm that allocates both voltage and frequency simultaneously to the operations of the directed flow graph to optimize power given the time constraints. The resources required for multiple voltage-frequency scheduling is derived using the classical force directed scheduling algorithm. This algorithm has been implemented and tested on High-Level synthesis benchmarks for both non-pipelined and pipeline instances.||||
51|6-7||Novel source-independent characterization methodology for embedded software energy estimation and optimization|In order to design a successful low-energy VLSI system, concurrent energy reduction at hardware and software levels is needed. The available techniques for embedded software energy estimation either provide unusable average-case results or require prohibitively complex hardware setups for cycle-accurate results. This paper introduces a new methodology for high-level software energy estimation for embedded systems. The methodology produces cycle-accurate results independent of the energy characterization process. The executed instructions as well as the transitions on the wires are taken into consideration for estimating the energy. This allows tradeoff between the accuracy and the complexity of the model. The methodology is generic and makes no assumptions about the measurement techniques or the architecture of the processor. The introduced methodology also allows successive improvements in the estimation accuracy with each step towards final silicon. The embedded ARM7TDMI RISC processor is modeled with this methodology and the errors are found to be less than 10%. For energy optimization, the model provides excellent relative accuracy too. Taking advantage of the relative accuracy, different code transformation techniques are discussed and employed to gain 32% energy savings.||||
51|6-7||Information-driven circuit synthesis with the pre-characterized gate libraries|The opportunities created by modern microelectronic technology cannot effectively be exploited, because of weaknesses in traditional circuit synthesis methods used in today’s CAD tools. In this paper, a new information-driven circuit synthesis method is discussed that targets combinational circuits implemented with gates from the pre-characterized gate libraries. The method is based on our original information-driven approach to circuit synthesis, bottom–up general functional decomposition and theory of information relationship measures. It differs considerably from all other known methods. The experimental results from the automatic circuit synthesis tool that implements the method demonstrate that the information-driven general decomposition produces very fast and compact gate-based circuits.||||
51|6-7||An application of functional decomposition in ROM-based FSM implementation in FPGA devices|Modern FPLD devices have very complex structure. They combine PLA like structures, as well as FPGA and even memory-based structures. However lack of appropriate synthesis methods do not allow fully exploiting the possibilities the modern FPLDs offer. The paper presents a general method for the synthesis targeted to implementation of sequential circuits using embedded memory blocks. The method is based on the serial decomposition concept and relies on decomposing the memory block into two blocks: a combinational address modifier and a smaller memory block. An appropriately chosen decomposition strategy may allow reducing the required memory size at the cost of additional logic cells for address modifier implementation. This makes possible implementation of FSMs that exceed available memory by using embedded memory blocks and additional programmable logic.||||
51|6-7||Design and FPGA implementation of an MPEG based video scalar with reduced on-chip memory utilization|A new algorithm and a novel architecture suitable for FPGA/ASIC implementation of a video scalar is presented in this paper. The scheme proposed here results in enormous savings of memory normally required, without compromising on the image quality. In the present work, SVGA compatible video sequence is scaled up to XGA format. The up scaling operation for a video sequence is carried out by scaling up the image input, followed by down scaling and filtering. The FPGA implementation of the proposed video-scaling algorithm is capable of processing high-resolution, color pictures of sizes of up to 1024 × 768 pixels at the real time video rate of 30 frames/s. The video scalar is capable of scaling down XGA format to SVGA format as well. The design has been realized by RTL compliant Verilog coding, and fits into a single chip with a gate count utilization of two million gates. For lower resolution pictures, the mapped device can be scaled down. The present FPGA implementation compares favorably with another ASIC implementation.||||
51|6-7||Guide for Authors|||||
51|8|http://www.sciencedirect.com/science/journal/13837621/51/8|Exploring the performance of split data cache schemes on superscalar processors and symmetric multiprocessors|Current technology continues providing smaller and faster transistors, so processor architects can offer more complex and functional ILP processors, because manufacturers can fit more transistors on the same chip area. As a consequence, the fraction of chip area reachable in a single clock cycle is dropping, and at the same time the number of transistors on the chip is increasing. However, problems related with power consumption and heat dissipation are worrying. This scenario is forcing processor designers to look for new processor organizations that can provide the same or more performance but using smaller sizes. This fact especially affects the on-chip cache memory design; therefore, studies proposing new smaller cache organizations while maintaining, or even increasing, the hit ratio are welcome. In this sense, the cache schemes that propose a better exploitation of data locality (bypassing schemes, prefetching techniques, victim caches, etc.) are a good example.||||
51|8||A pipeline architecture for computing the Euler number of a binary image|Euler number of a binary image is a fundamental topological feature that remains invariant under translation, rotation, scaling, and rubber-sheet transformation of the image. In this work, a run-based method for computing Euler number is formulated and a new hardware implementation is described. Analysis of time complexity and performance measure is provided to demonstrate the efficiency of the method. The sequential version of the proposed algorithm requires significantly fewer number of pixel accesses compared to the existing methods and tools based on bit-quad counting or quad-tree, both for the worst case and the average case. A pipelined architecture is designed with a single adder tree to implement the algorithm on-chip by exploiting its inherent parallelism. The architecture uses O(N) 2-input gates and requires O(N log N) time to compute the Euler number of an N × N image. The same hardware, with minor modification, can be used to handle arbitrarily large pixel matrices. A standard cell based VLSI implementation of the architecture is also reported. As Euler number is a widely used parameter, the proposed design can be readily used to save computation time in many image processing applications.||||
51|8||Functional test generation based on word-level SAT|Functional test generation coupled with symbolic simulation offers a good compromise between formal verification and numerical simulation for design validation. The generation of functional test vectors guided by miscellaneous coverage metrics can be posed as a satisfiability problem (SAT). While a number of efficient Boolean SAT engines have been developed for gate level designs, they are not directly applicable to behavioral and RTL designs containing significant arithmetic components. This paper presents two approaches that enhance the capability of functional test generation by preserving arithmetic operators in the design. They are based on word-level SAT techniques: (1) LPSAT, based on integer linear programming, and (2) CLP-SAT, based on constraint logic programming. The proposed SAT solvers allow to efficiently handle the designs with mixed word-level arithmetic operators and bit-level logic gates. The experimental results are quite encouraging compared to traditional CNF-based and BDD-based SAT solvers. The paper also suggests a method to build an integrated SAT solving framework where different SAT solvers work together to provide a more complete solution to functional test generation and other verification applications.||||
51|8||Guide for Authors|||||
51|9|http://www.sciencedirect.com/science/journal/13837621/51/9|Optimal sample length for efficient cache simulation|||||
51|9||A slot swapping protocol for time-critical internetworking|||||
51|9||Equation-based TCP-friendly congestion control under lossy environment|||||
51|9||Guide for Authors|||||
||||||||
volume|issue|url|title|abstract||||
52|1|http://www.sciencedirect.com/science/journal/13837621/52/1|A core generator for arithmetic cores and testing structures with a network interface|We present Eudoxus, a tool for generation of architectural variants for arithmetic soft cores and testing structures targeting a wide variety of functions, operand sizes and architectures. Eudoxus produces structural and synthesizable VHDL and/or Verilog descriptions for: (a) several arithmetic operations including addition, subtraction, multiplication, division, squaring, square rooting and shifting, and (b) several testing structures that can be used as test pattern generators and test response compactors. Interaction with the user is made through a network interface. Since the end user is presented with a variety of unencrypted structural cores, each one describing an architecture with its own area, delay and power characteristics, he can choose the one that best fits his specific needs which he can further optimize or customize. Therefore, designs utilizing these cores are completed in less time and with less effort.||||
52|1||Memory latency consideration for load sharing on heterogeneous network of workstations|With the development of cheap personal computer and high-speed network, heterogeneous network of workstation has become the trend in high performance computing. This paper focuses on the load sharing problem for heterogeneous network of workstations. Load sharing means even workloads of all coordinated computers in the heterogeneous system without leaving any computer idle. When some nodes suffer from heavy loading, it is necessary to migrate some processes to the nodes with light loading. However, most load sharing policies focus only on different CPU speed and/or memory capacity without taking the effect of memory access latencies into consideration. In the paper, we propose a new load sharing policy, CPU-Memory-Power-based policy, to improve CPU-Memory-based policy. In addition to CPU speed and memory capacity, this policy also puts emphasis on memory access latency. Experimental results show that this method performs better than the other policies, and that memory access latency is actually an important consideration in the design of load sharing policies on heterogeneous network of workstation.||||
52|1||Replacing media caches in streaming proxy servers|This paper presents a cache replacement policy which has specifically been developed for the efficient media caching in streaming media cache servers. For efficient media caching, the proposed policy takes into account the periodic patterns of users’ requests in addition to the parameters such as reference count, amount of media contents delivered to the clients, and reference time. These values are collected at run-time for each cached object. In order to adequately and promptly adopt to the changing characteristics of users preferences, the policy introduces, in particular, the concept of weighted-window for replacement with which higher priorities are given to more recently referenced media contents and consequently they are less likely to be replaced. We present and analyze the simulation results showing that the proposed policy has outperformed the conventional replacement policies such as LRU, LFU, and SEG in terms of hit ratio, byte-hit ratio, delayed start, and cache input.||||
52|1||Intelligent memory manager: Reducing cache pollution due to memory management functions|In this work, we show that data-intensive and frequently-used service functions such as memory allocation and de-allocation entangle with application’s working set and become a major cause for cache misses. We present our technique that transfers the allocation and de-allocation functions’ executions from main CPU to a separate processor residing on chip with DRAM (Intelligent Memory Manager). The results manifested in the paper state that, 60% of the cache misses caused by the service functions are eliminated when using our technique. We believe that cache performance of applications in computer system is poor due to their indulgence for the service functions.||||
52|1||Performance analysis of multistage interconnection networks with a new high-level net model|This paper introduces a new high-level net, named S-net, for modelling multistage interconnection networks. Based on the stochastic behaviour of GSPN and coupled with the flexibility and compactness of Coloured Petri net uses tokens for storage and manipulation of data besides modelling the flow of control. It requires exactly 3N/2 number of places to model an N × N multistage interconnection network irrespective of the number of stages and buddy properties of the network. A polynomial time algorithm is developed to check feasibility of mapping a permutation using the proposed S-net model of an MIN.||||
52|1||Guide for Authors|||||
52|10|http://www.sciencedirect.com/science/journal/13837621/52/10|Bidirectional liveness analysis, or how less than half of the Alphaâs registers are used|Interprocedural data flow analyses of executable programs suffer from the conservative assumptions that need to be made because no precise control flow graph is available and because registers are spilled onto the stack. This paper discusses the exploitation of calling-conventions in executable code data flow analyses to avoid the propagation of the conservative assumptions throughout a program. Based on this exploitation, the existing backward liveness analyses are improved, and a complementary forward liveness analysis is proposed. For the SPECint2000 programs compiled for the Alpha architecture, the combined forward and improved backward analysis on average finds 62% more free registers than the existing state-of-the-art liveness analysis. With the improved analysis, we are able to show that on an average less than half of the registers of the RISC Alpha architecture are used. This contradicts the common wisdom that compilers can exploit large, uniform register files as found on RISC architectures.||||
52|10||Dual-mode floating-point multiplier architectures with parallel operations|Although most modern processors have hardware support for double precision or double-extended precision floating-point multiplication, this support is inadequate for many scientific computations. This paper presents the architecture of a quadruple precision floating-point multiplier that also supports two parallel double precision multiplications. Since hardware support for quadruple precision arithmetic is expensive, a new technique is presented that requires much less hardware than a fully parallel quadruple precision multiplier. With this architecture, quadruple precision multiplication has a latency of three cycles and two parallel double precision multiplications have latencies of only two cycles. The multiplier is pipelined so that two double precision multiplications can begin every cycle or a quadruple precision multiplication can begin every other cycle. The technique used for the dual-mode quadruple precision multiplier is also applied to the design of a dual-mode double precision floating-point multiplier that performs a double precision multiplication or two single precision multiplications in parallel. Synthesis results show that the dual-mode double precision multiplier requires 43% less area than a conventional double precision multiplier. The correctness of all the multipliers presented in this paper is tested and verified through extensive simulation.||||
52|10||A schema version model for complex objects in object-oriented databases|In this paper, we propose a schema version model which allows to restructure complex object hierarchy in object-oriented databases. This model extends a schema version model, called RiBS, which is based on the concept of Rich Base Schema. In the RiBS model, each schema version is in the form of updatable class hierarchy view over one base schema, called the RiBS layer, which has richer schema information than any existing schema version in the database.||||
52|10||An XML data allocation method on disks|||||
52|10||A scalable VLSI speed/area tunable sorting network|This work presents a novel sorting network based on the “sorting by counting” algorithm. The proposed implementation of the algorithm is very regular. Further, its realization depends on a design parameter, that permits different tradeoffs between speed and area to be chosen. For example, we can fix this parameter to obtain a feasible SN with n inputs and O(log(n)) elaboration time with a reasonable multiplicative constant. Comparisons with previous works show that under some metrics for a wide range of values of n we obtain the best results.||||
52|10||Dynamic reuse of subroutine results|The paper discusses a concept of dynamic reuse of subroutine results. The described technique uses a hardware mechanism that caches the address of the called subroutine along with its arguments and returned value. When the same subroutine is called again using the same arguments, the returned value can be accurately predicted without an actual execution of the subroutine. Although this approach can be highly effective in some cases, it is limited to subroutines that do not use side effects, and use only by-value parameter passing. Since the proposed method requires that both the user and the compiler be aware of this mechanism, it might be more appropriate for specific computing-intensive applications, rather than standard all-purpose programming.||||
52|11|http://www.sciencedirect.com/science/journal/13837621/52/11|Preface|||||
52|11||Modeling and simulation of open source development using an agile practice|The goal of this work is to study the effects of the adoption of agile practices on open source development. In particular, we started to evaluate the effects of TDD (Test Driven Development) since it is easer to apply in a distributed environment than most other agile practices. In order to reach this goal we used the simulation modeling approach. We developed a simulation model of open source software development process. The model was tuned using data from a real FLOSS project: Apache HTTP Server. To introduce the TDD practice in our FLOSS simulation model, we made some assumptions based on empirical results. The two FLOSS development models (nonTDD and TDD) were compared. The one incorporating the agile practice yields better results in terms of code quality.||||
52|11||Integrating XP project management in development environments|Extreme Programming (XP) is an Agile Methodology (AM) which does not require any specific supporting tool for being successfully applied. Despite this starting observation, there are many reasons leading a XP team to adopt Web based tools to support XP practices. For example, such tools could be useful for process and product data collection and analysis or for supporting distributed development. In this article, we describe XPSuite, a tool composed of two parts: XPSwiki, a tool for managing XP projects and XP4IDE, a plug-in for integrating XPSwiki with an Integrated Development Environment (IDE). Moreover, we will show how the full Object Oriented implementation provides a powerful support for extracting all data represented in the model that the system implements.||||
52|11||FMESP: Framework for the modeling and evaluation of software processes|Nowadays, organizations face with a very high competitiveness and for this reason they have to continuously improve their processes. Two key aspects to be considered in the software processes management in order to promote their improvement are their effective modeling and evaluation. The integrated management of these key aspects is not a trivial task, the huge number and diversity of elements to take into account makes it complex the management of software processes. To ease and effectively support this management, in this paper we propose FMESP: a framework for the integrated management of the modeling and measurement of software processes. FMESP incorporates the conceptual and technological elements necessary to ease the integrated management of the definition and evaluation of software processes. From the measurement perspective of the framework and in order to provide the support for the software process measurement at model level a set of representative measures have been defined and validated.||||
52|11||Ontology-based multi-site software development methodology and tools|The disadvantages associated with remote communication rather than face-to-face communication is a key problem in the multi-site distributed software development environment. Awareness of what work has been done, what task has been misunderstood, what problems have been raised, what issues have been clarified, and understanding of why a team or a software engineer does not follow the project plan, and how to carry out a discussion over a multi-site distributed environment and to make a just-in-time decision are the challenge. Different teams might not be aware of what tasks are being carried out by others, potentially leading to problems such as two groups overlapping in some work or other work not being performed due to misinterpretation of the task. Wrong tasks may be carried out due to ignorance of who to contact to get the proper details. If everyone working on a certain project is located in the same area, then situational awareness is relatively straightforward but the overheads in communications to get together to discuss the problems, to raise issues, to make decisions and to find answers in a multi-site distributed environment can become very large. Consequently, these problems cause project delay and anxiety among teams and managers. Ontologies coupled with a multi-agents system allow greater ease of communication by aggregating the agreed knowledge about the project, the domain knowledge, the concepts of software engineering into a shared information resource platform and allow them to be shared among the distributed teams across the sites and enable the intelligent agents to use the ontology to carry out initial communication and classification with developers when the problem is raised in the first instance. In this paper, we present the key challenges in multi-site software engineering and the ontology representation of commonly shared conceptualisations in software development. We demonstrate the agent communication with developers in the form of man–machine interactions and the great potential of such a system to be used in the future for software engineering in multi-site environments.||||
52|11||Motivations and measurements in an agile case study|With the recent emergence of agile software development technologies, the software community is awaiting sound, empirical investigation of the impacts of agile practices in a live setting. One means of conducting such research is through industrial case studies. There are a number of influencing factors that contribute to the success of such a case study. In this paper, we describe a case study performed at Sabre Airline SolutionsTM evaluating the effects of adopting Extreme Programming (XP) practices with a team that had characteristically plan-driven risk factors. We compare the team’s business-related results (productivity and quality) to two published sources of industry averages. Our case study found that the Sabre team yielded above-average post-release quality and average to above-average productivity. We discuss our experience in conducting this case study, including specifics of how data was collected, the rationale behind our process of data collection, and what obstacles were encountered during the case study. We identify four factors that potentially impact the outcome of industrial case studies: availability of data, tool support, cooperative personnel and project status. Recognizing and planning for these factors is essential to conducting industrial case studies.||||
52|11||A non-invasive approach to product metrics collection|Software metrics are useful means in helping software engineers to develop large and complex software systems. In the past years, many software metrics have been proposed in order to represent several different concepts such as complexity, coupling, inheritance, reuse, etc. However, this requires the collection of large volumes of metrics and, without flexible and transparent tools, is nearly impossible to collect data accurately. This paper presents the design and the implementation of a tool for collecting and analyzing product metrics in a non-invasive way.||||
52|11||Managing non-invasive measurement tools|Software measures are of paramount importance to evaluate both processes and products. However, collecting such data requires a large effort and this activity produces benefits only in the long term. For these reasons, companies have to use automated tools able to collect data without affecting both the productivity and the process. This approach is useful to reduce the effort related to the metrics collection but managing the different tools required for a comprehensive acquisition is not an easy task and it requires a considerable effort. This paper presents some tools and techniques for managing large metrics collection projects based on the usage of the PROM metrics collection tool.||||
52|11||Discovering the software process by means of stochastic workflow analysis|||||
52|11||Using a role scheme to derive software project metrics|Roles’ playing is common in our lives. We play different roles with our family, at work as well as in other environments. Role allocation in software development projects is also accepted though it may be implemented differently by different software development methods. In a previous work [Y. Dubinsky, O. Hazzan, Roles in agile software development teams, in: 5th International Conference on Extreme Programming and Agile Processes in Software Engineering, 2004, pp. 157–165] we have found that personal roles may raise teammates’ personal accountability while maintaining the essence of the software development method. In this paper we present our role scheme, elaborate on its implementation and explain how it can be used to derive metrics. We illustrate our ideas by data gathered in student projects in the university.||||
52|11||Quantitative logic-based framework for agile methodologies|Agile methodologies and Extreme programming are the new and highly promising endeavors in Software Engineering. By addressing the important issue of dealing with continuously changing requirements we are faced with panoply of new problems and a genuine need to revisit some principles and classic models of software developments. When it comes to the management of software projects and a way in which we are looking at the software processes and underlying practices, it becomes apparent that in the management practices the issue of uncertainty needs to be quantified and fully addressed. Similarly, it becomes of interest to develop lightweight models of software quality and software processes that are easy to construct and modify as well are transparent to the developer and manager. Given these arguments, in this study we propose logic models based upon the mechanisms of multivalued and fuzzy logic. The realization of such models gives rise to so-called logic networks that are easy to construct, calibrate and interpret.||||
52|11||Guide for Authors|||||
52|12|http://www.sciencedirect.com/science/journal/13837621/52/12|Support for partial run-time reconfiguration of platform FPGAs|Run-time partial reconfiguration of programmable hardware devices can be applied to enhance many applications in high-end embedded systems, particularly those that employ recent platform FPGAs. The effective use of this approach is often hampered by the complexity added to the system development process and by limited tool support.||||
52|12||Evaluating the reliability of computational grids from the end userâs point of view|||||
52|12||Decentralized media streaming infrastructure (DeMSI): An adaptive and high-performance peer-to-peer content delivery network|Hosting an on-demand media content streaming service has been a challenging task mainly because of the outrageously enormous network and server bandwidth required to deliver large amount of content data to users simultaneously. We propose an infrastructure that helps online media content providers offload their server and network resources for media streaming. Using application level resource diversity together with the peer-to-peer resource-sharing model is a feasible approach to decentralize the content storage, server and network bandwidth. Each subscriber is responsible for only a small fraction of such resources. Most importantly, the cost of maintaining the service can also be shared amongst subscribers, especially when the subscriber base is large. As a result, subscribers can be benefit from lower subscription cost. There have been a few solutions out there that focused only on sharing the load of network bandwidth by division of a streaming task to be carried out by multiple sources. However, existing solutions require that the content to be replicated in full and stored in each source, which is impractical for a subscriber as the owner of the storage resource that is of consumer capacity. Our solution focuses on the division of responsibility on both the network bandwidth and content storage such that each subscriber is responsible for only a small portion of the content. We propose a light-weighted candidate peer selection strategy based on avoidance of network congestion and an adaptive re-scheduling algorithm in order to enhance smoothness of the aggregated streaming rate perceived at the consumer side. Experiments show that the performance of our peer-selection strategy out performs the traditional strategy based on end-to-end streaming bandwidth.||||
52|2|http://www.sciencedirect.com/science/journal/13837621/52/2|Parallel, distributed and network-based processing|||||
52|2||An abstraction model for a Grid execution framework|Computational Grids have been identified as one of the paradigms revolutionizing the discipline of distributed computing. The contributions within the Grid community have resulted in new Grid technologies and continuous improvements to Grid standards and protocols. Though crucial to the success of the Grid approach, such an incremental evolution of Grid standards has become a primary cause of frustration for scientific and commercial application communities aspiring to adopt the Grid paradigm. Motivated by our rich experience and the need to decouple the application development and the Grid technology development processes, we propose an abstraction-based Grid middleware layer as part of the Java CoG Kit. In this paper, we showcase our abstraction model and verify its extensibility by integrating it with an advanced quality-of-service-based execution framework.||||
52|2||Scheduling tasks sharing files on heterogeneous masterâslave platforms|This paper is devoted to scheduling a large collection of independent tasks onto heterogeneous clusters. The tasks depend upon (input) files which initially reside on a master processor. A given file may well be shared by several tasks. The role of the master is to distribute the files to the processors, so that they can execute the tasks. The objective for the master is to select which file to send to which slave, and in which order, so as to minimize the total execution time. The contribution of this paper is twofold. On the theoretical side, we establish complexity results that assess the difficulty of the problem. On the practical side, we design several new heuristics, which are shown to perform as efficiently as the best heuristics in [H. Casanova, A. Legrand, D. Zagorodnov, F. Berman, Heuristics for scheduling parameter sweep applications in Grid environments, in: Ninth Heterogeneous Computing Workshop, IEEE Computer Society Press, Silver Spring, MD, 2000, pp. 349–363; H. Casanova, A. Legrand, D. Zagorodnov, F. Berman, Using simulation to evaluate scheduling heuristics for a class of applications in Grid environments, Research Report RR-1999-46, LIP, ENS Lyon, France, 1999] although their cost is an order of magnitude lower.||||
52|2||The masterâslave paradigm on heterogeneous systems: A dynamic programming approach for the optimal mapping|We study the master–slave paradigm over heterogeneous systems. According to an analytical model, we develop a dynamic programming algorithm that allows to solve the optimal mapping for such paradigm. Our proposal considers heterogeneity due both to computation and also to communication. The optimization strategy used allows to obtain the set of processors for an optimal computation. The computational results show that considering heterogeneity also on the communication increases the performance of the parallel algorithm.||||
52|2||RDMA control support for fine-grain parallel computations|||||
52|2||Optimizing bus energy consumption of on-chip multiprocessors using frequent values|Chip multiprocessors (CMP) are a convenient way of leveraging from the technological trends to build high-end and embedded systems that are performance and power efficient, while exhibiting attractive properties such as scalability, reliability and ease of design. However, the on-chip interconnect for moving the data between the processors, and between the processors and memory subsystem, plays a crucial role in CMP design. This paper presents a novel approach to optimizing its power by exploiting the value locality in data transfers between processors. A communicating value cache (CVC) is proposed to reduce the number of bits transferred on the interconnect, and simulation results with several parallel applications show significant energy savings with this mechanism. Results show that the importance of our proposal will become even more significant in the future.||||
52|3|http://www.sciencedirect.com/science/journal/13837621/52/3|Improved composite confidence mechanisms for a perceptron branch predictor|In 2001, Jiménez and Lin [Dynamic branch prediction with perceptrons, Proceedings of the 7th International Symposium on High Performance Computer Architecture, 2001, pp. 197–206] introduced the perceptron branch predictor, the first dynamic branch predictor to successfully use neural networks. This simple neural network achieves higher accuracies (95% at a 4 KiB hardware budget) compared to other existing branch predictors and provides a free confidence level. In this paper, we first gain insight into this inherent confidence mechanism of the perceptron predictor and explain why (additional) counter based confidence strategies can complement it. Second, we evaluate several composite confidence estimation strategies and compare them to the described technique by Jiménez and Lin [Composite confidence estimators for enhanced speculation control, Tech. rep., Department of Computer Sciences, The University of Texas at Austin, 2002]. We conclude that our overruling AND-combination of perceptron confidence and resetting counter mechanism outperforms the previously proposed confidence scheme.||||
52|3||Circulating shared-registers for multiprocessor systems|||||
52|3||On-line evolvable fuzzy system for ATM cell-scheduling|The algorithms for solving ATM cell-scheduling problem include first-in-first-out (FIFO), static priority (SPR), dynamically weighted priority scheduling (DWPS) as well as other traditional schemes. However, these traditional algorithms lack flexibility. FIFO and SPR cannot adapt to changes in the cell flow environment. DWPS on the other hand, is more adaptable to changing traffic flow. But if the cell flow changes dramatically, the performance of this method is also not very good. In order to address these issues, we propose the framework of evolvable fuzzy system (EFS). The system is intrinsically evolvable and able to carry out on-line adaptation to meet the desired QoS requirement. The EFS is realizable as a form of evolvable fuzzy hardware (EFH) by means of a reconfigurable fuzzy inference chip (RFIC). With an implementation of the EFS as EFH which carries out intrinsic evolution and on-line adaptation, some open issues pertinent to evolvable hardware (EHW) can be addressed.||||
52|3||Class-based request control system in multimedia contents service|||||
52|3||Guide for Authors|||||
52|4|http://www.sciencedirect.com/science/journal/13837621/52/4|Pattern-driven prefetching for multimedia applications on embedded processors|Multimedia applications in general and video processing, such as the MPEG4 Visual stream decoders, in particular are increasingly popular and important workloads for future embedded systems. Due to the high computational requirements, the need for low power, high performance embedded processors for multimedia applications is growing very fast. This paper proposes a new data prefetch mechanism called pattern-driven prefetching. PDP inspects the sequence of data cache misses and detects recurring patterns within that sequence. The patterns that are observed are based on the notions of the inter-miss stride (memory address stride between two misses) and the inter-miss interval (number of cycles between two misses). According to the patterns being detected, PDP initiates prefetch actions to anticipate future accesses and hide memory access latencies. PDP includes a simple yet effective stop criterion to avoid cache pollution and to reduce the number of additional memory accesses. The additional hardware needed for PDP is very limited making it an effective prefetch mechanism for embedded systems. In our experimental setup, we use cycle-level power/performance simulations of the MPEG4 Visual stream decoders from the MoMuSys reference software with various video streams. Our results show that PDP increases performance by as much as 45%, 24% and 10% for 2KB, 4KB and 8KB data caches, respectively, while the increase in external memory accesses remains under 0.6%. In conjunction with these performance increases, system-level (on-chip plus off-chip) energy reductions of 20%, 11.5% and 8% are obtained for 2KB, 4KB and 8KB data caches, respectively. In addition, we report significant speedups (up to 160%) for various other multimedia applications. Finally, we also show that PDP outperforms stream buffers.||||
52|4||Dynamic feature selection for hardware prediction|Most hardware predictors are table based (e.g. two-level branch predictors) and have exponential size growth in the number of input bits or features (e.g. previous branch outcomes). This growth severely limits the amount of predictive information that such predictors can use. To avoid exponential growth we introduce the idea of “dynamic feature selection” for building hardware predictors that can use a large amount of predictive information. Based on this idea, we design the dynamic decision tree (DDT) predictor, which exhibits only linear size growth in the number of features. Our initial evaluation, in branch prediction, shows that the general-purpose DDT, using only branch-history features, is comparable on average to conventional branch predictors, opening the door to practically using large numbers of additional features.||||
52|4||The algorithm of pipelined gossiping|A family of gossiping algorithms depending on a parameter permutation is introduced, formalized, and discussed. Several of its members are analyzed and their asymptotic behaviour is revealed, including a member whose model and performance closely follows the one of hardware pipelined processors. This similarity is exposed. An optimizing algorithm is finally proposed and discussed as a general strategy to increase the performance of the base algorithms.||||
52|4||Guide for Authors|||||
52|5|http://www.sciencedirect.com/science/journal/13837621/52/5|Software-based self-testing of microprocessors|Hardware-based self-testing techniques have limitations in the performance and area overhead. Those can be eliminated using software-based self-testing. In this paper, we investigate capabilities of the microprocessor testing by software procedures taking into account system environment constraints. Special attention is paid to microarchitectural features of pipelined and superscalar processors. New test strategies are proposed combining deterministic and pseudorandom approaches supported by the available hardware mechanisms (test registers, on-chip monitoring circuitry, etc.), which improve testability features. The test effectiveness is studied using various test coverage measures (stimuli, circuit stressing), statistical and fault injection tools. To demonstrate the utility of the proposed methodology, it has been applied to commercial microprocessors and experimental results are presented in this paper.||||
52|5||Design of a compact reversible binary coded decimal adder circuit|Reversible logic is an emerging research area and getting remarkable interests over the past few years. Interest is sparked in reversible logic by its applications in several technologies, such as quantum, optical, thermodynamics and adiabatic CMOS. This paper represents a synthesis method to realize reversible binary coded decimal adder circuit. Firstly, a reversible full-adder circuit has been proposed that shows the improvement over the two existing circuits. A lower bound is also proposed for the reversible full-adder circuit on the number of garbage outputs (bits needed for reversibility, but not required for the output of the circuit). After that, a final improvement is presented for the reversible full-adder circuit. Finally, a new reversible circuit has been proposed, namely reversible binary coded decimal (BCD) adder, which is the first ever proposed in reversible logic synthesis. In the way to propose reversible BCD adder, a reversible n-bits parallel adder circuit is also shown. Lower bounds for the reversible BCD adder in terms of number of garbage outputs and number of reversible gates are also shown. Delay has also been calculated for each circuit.||||
52|5||The design and utility of the ML-RSIM system simulator|Execution-driven simulation has become the primary method for evaluating architectural techniques as it facilitates rapid design space exploration without the cost of building prototype hardware. To date, most simulation systems have either focused on the cycle-accurate modeling of user-level code while ignoring operating system and I/O effects, or have modeled complete systems while abstracting away many cycle-accurate timing details. The ML-RSIM simulation system presented here combines detailed hardware models with the ability to simulate user-level as well as operating system activity, making it particularly suitable for exploring the interaction of applications with the operating system and I/O activity. This paper provides an overview of the design of the simulation infrastructure and discusses its strengths and weaknesses in terms of accuracy, flexibility, and performance. A validation study using LMBench microbenchmarks shows a good correlation for most of the architectural characteristics, while operating system effects show a larger variability. By quantifying the accuracy of the simulation tool in various areas, the validation effort not only helps gauge the validity of simulation results but also allows users to assess the suitability of the tool for a particular purpose.||||
52|5||An optimal message routing algorithm for circulant networks|"A k-circulant network G(n; h1, h2, … , hk) is an undirected graph where the node set is Zn={0,1,…,n-1}Zn={0,1,…,n-1} and the edge set is the union of sets of unordered pairs Ei={(u,u+sign(i)∗h|i|(modn))|u∈Zn}, for i ∈ {−k, … , −1, 1, … , k}. We present an optimal (i.e. using shortest paths) dynamic two-terminal message routing algorithm for k-circulant networks, k â©¾ 2. Instead of computing the shortest paths in advance or using routing tables, our algorithm uses only the address of the final destination to determine the next node to which the message must be sent in order to stay on one of the shortest paths to its destination. We introduce the restricted shortest paths, which are used by our routing algorithm, and present an efficient algorithm for their construction in 2-circulant graphs."|
52|5||A power-efficient TCAM architecture for network forwarding tables|Stringent memory access and search speed requirements are two of the main bottlenecks in wire speed processing. Most viable search engines are implemented in content addressable memories (CAMs). CAMs have high operational speed advantage over other memory search algorithms. However, this performance advantage comes with a price of higher silicon area, and higher power consumption. Ternary CAMs (TCAM) are widely used for route lookup operations in networking applications. IP address prefix length distribution in the core routers shows a similar characteristic such that the prefixes with 24 or longer bits attract more than 50% of the traffic. Based on this statistical observation, we propose a TCAM architecture that can be used on top of the previously reported power saving techniques and it offers additional 30% reduction in power consumption. Furthermore, we model the dynamic power consumption in TCAM circuits due match, mismatch and don’t care activities.||||
52|5||Guide for Authors|||||
52|6|http://www.sciencedirect.com/science/journal/13837621/52/6|MEMPHIS: A mobile agent-based system for enabling acquisition of multilingual content and providing flexible format internet premium services|Mobile Agent technology is an interesting concept for large network-based system structures. In this paper we describe a Java-based agent environment which integrates agent execution platforms into World Wide Web servers promoting a world wide infrastructure for Mobile Agents. This system has been developed within the framework of MEMPHIS IST project, which involves the definition, development and validation of a new system for commercially significant multilingual premium services, focused on thin clients such as mobile phones, Personal Digital Assistants, etc. The concept relies on the extraction of multilingual information from various web-based content sources in different formats. The system is composed by three different Modules, namely the Acquisition, the Transformation and the Distribution Module. The content acquisition is based on dynamic user profiles. The acquired content is transformed into a meta-representation, which enables content storage independent both of source and destination format and language. The output format is based on the requirements of employed thin client terminals and the output document is distributed via the appropriate networks. The end-user receives the required information as premium services via the chosen output medium. The system supports both pull and push services. An evaluation of the system is also presented.||||
52|6||High-frequency pulse width modulation implementation using FPGA and CPLD ICs|Pulse width modulation (PWM) has been widely used in power converter control. Most high power level converters operate at switching frequencies up to 500 kHz, while operating frequencies in excess of 1 MHz at high power levels can be achieved using the planar transformer technology. The contribution of this paper is the development of a high-frequency PWM generator architecture for power converter control using FPGA and CPLD ICs. The resulting PWM frequency depends on the target FPGA or CPLD device speed grade and the duty cycle resolution requirements. The post-layout timing simulation results are presented, showing that PWM frequencies up to 3.985 MHz can be produced with a duty cycle resolution of 1.56%. Additionally, experimental results are also presented for low cost functional verification of the proposed architecture.||||
52|6||High-performance adaptive routing for networks with arbitrary topology|||||
52|6||Random early detection with flow number estimation and queue length feedback control|We evaluate effects of parameters in queue management on performance using 2k factorial designs. Among these systematic and environmental parameters, number of flows and Pmax are two dominant factors affecting RED performance. Then, we propose a novel active queue management scheme, NRED, employing flow number estimation and queue length feedback control, which are motivated from Bloom filter and control theory, respectively. By estimation of number of active flows, NRED is more scalable than FRED which employs per-active-flow accounting. Furthermore, fluctuation of queue length under NRED is smaller than BLUE, REM, SRED and DRED. Especially, NRED can stabilize queue length very fast even when a lot of flows suddenly crowd into the network. NRED is suitable to be deployed in today’s networks where hot spot frequently occurs.||||
52|6||Guide for Authors|||||
52|7|http://www.sciencedirect.com/science/journal/13837621/52/7|Deferred locking with shadow transaction for clientâserver DBMSs|Data-shipping systems that allow inter-transaction caching raise the need of a transactional cache consistency maintenance (CCM) protocol because each client is able to cache a portion of the database dynamically. Deferred locking (DL) is a new CCM scheme that is capable of reducing the communication overhead required for cache consistency checking. Due to its low communication overhead, DL could show a superior performance, but it tends to exhibit a high ratio of transaction abort. To cope with this drawback, we develop a new notion of shadow transaction, which is a backup-purpose one that is kept ready to replace an aborted transaction. This notion and the locking mechanism of DL have been incorporated into deferred locking with shadow transaction. Using a distributed database simulation model, we evaluate the performance of the proposed schemes under a wide range of workloads.||||
52|7||Analytical modeling of codes with arbitrary data-dependent conditional structures|Several analytical models that predict the memory hierarchy behavior of codes with regular access patterns have been developed. These models help understand this behavior and they can be used successfully to guide compilers in the application of locality-related optimizations requiring small computing times. Still, these models suffer from many limitations. The most important of them is their restricted scope of applicability, since real codes exhibit many access patterns they cannot model. The most common source of such kind of accesses is the presence of irregular access patterns because of the presence of either data-dependent conditionals or indirections in the code. This paper extends the Probabilistic Miss Equations (PME) model to be able to cope with codes that include data-dependent conditional structures too. This approach is systematic enough to enable the automatic implementation of the extended model in a compiler framework. Validations show a good degree of accuracy in the predictions despite the irregularity of the access patterns. This opens the possibility of using our model to guide compiler optimizations for this kind of codes.||||
52|7||Speedup of NULL convention digital circuits using NULL cycle reduction|A NULL Cycle Reduction (NCR) technique is developed to increase the throughput of NULL Convention Logic systems, by reducing the time required to flush complete DATA wavefronts, commonly referred to as the NULL cycle. The NCR technique exploits parallelism by partitioning input wavefronts, such that one circuit processes a DATA wavefront, while its duplicate processes a NULL wavefront. A NCR architecture is developed for both dual-rail and quad-rail circuits, using either full-word or bit-wise completion. To illustrate the technique, NCR is applied to case studies of a dual-rail non-pipelined 4-bit × 4-bit unsigned multiplier using full-word completion, a quad-rail non-pipelined 4-bit × 4-bit unsigned multiplier using full-word completion, and a dual-rail optimally-pipelined 4-bit × 4-bit unsigned multiplier using bit-wise completion. The application of NCR yields a speedup of 1.57, 1.55, and 1.34, respectively, over the standalone versions, while maintaining delay-insensitivity. Furthermore, NCR is applied to a single slow stage of two pipelined designs to boost the pipelines’ overall throughput by 20% and 26%, respectively.||||
52|7||An analytical model for hypercubes in the presence of multiple time-scale bursty traffic|||||
52|7||Distributed computing using Java: A comparison of two server designs|This paper proposes a new concurrent data structure, called parallel hash table, for synchronizing the access of multiple threads to resources stored in a shared buffer. We prove theoretically the complexity of the operations and the upper limit on the thread conflict probability of the parallel hash table. To empirically evaluate the proposed concurrent data structure, we compare the performance of a TCP multi-threaded parallel hash table-based server to a conventional TCP multi-threaded shared buffer-based server implemented in Java. The experimental results on a network of 36 workstations running Windows NT, demonstrate that the parallel hash table-based server outperforms the conventional multi-threaded server.||||
52|7||Guide for Authors|||||
52|8-9|http://www.sciencedirect.com/science/journal/13837621/52/8-9|Special issue on nature-inspired applications and systems|||||
52|8-9||Case studies for self-organization in computer science|Self-organization is bound to greatly affect computer science. The simplicity and yet power of self-organized models will allow researchers to propose efficient solutions to problems never before thought possible to be addressed efficiently. The published works in the field clearly demonstrate the potential of this approach. This paper first reviews a number of interesting self-organization phenomena found in nature, then it discusses their potential applicability in several computer science application scenarios.||||
52|8-9||A comprehensive review of nature inspired routing algorithms for fixed telecommunication networks|||||
52|8-9||ANSI: A swarm intelligence-based unicast routing protocol for hybrid ad hoc networks|We present a hybrid routing protocol for both pure and hybrid ad hoc networks which uses the mechanisms of swarm intelligence to select next hops. Our protocol, Ad hoc Networking with Swarm Intelligence (ANSI), is a congestion-aware routing protocol, which, owing to the self-organizing mechanisms of swarm intelligence, is able to collect more information about the local network and make more effective routing decisions than traditional MANET protocols. Once routes are found, ANSI maintains routes along a path from source to destination effectively by using swarm intelligence techniques, and is able to gauge the slow deterioration of a link and restore a path along newer links as and when necessary. ANSI is thus more responsive to topological fluctuations. ANSI is designed to work over hybrid ad hoc networks: ad hoc networks which consist of both lower-capability, mobile wireless devices and higher-capability, wireless devices which may or may not be mobile. In addition, ANSI works with multiple interfaces and with both wired and wireless interfaces.||||
52|8-9||A flocking based algorithm for document clustering analysis|Social animals or insects in nature often exhibit a form of emergent collective behavior known as flocking. In this paper, we present a novel Flocking based approach for document clustering analysis. Our Flocking clustering algorithm uses stochastic and heuristic principles discovered from observing bird flocks or fish schools. Unlike other partition clustering algorithm such as K-means, the Flocking based algorithm does not require initial partitional seeds. The algorithm generates a clustering of a given set of data through the embedding of the high-dimensional data items on a two-dimensional grid for easy clustering result retrieval and visualization. Inspired by the self-organized behavior of bird flocks, we represent each document object with a flock boid. The simple local rules followed by each flock boid result in the entire document flock generating complex global behaviors, which eventually result in a clustering of the documents. We evaluate the efficiency of our algorithm with both a synthetic dataset and a real document collection that includes 100 news articles collected from the Internet. Our results show that the Flocking clustering algorithm achieves better performance compared to the K-means and the Ant clustering algorithm for real document clustering.||||
52|8-9||Evolving classifiers on field programmable gate arrays: Migrating XCS to FPGAs|||||
52|8-9||Guide for Authors|||||
||||||||
volume|issue|url|title|abstract||||
53|1|http://www.sciencedirect.com/science/journal/13837621/53/1|Trace-based leakage energy optimisations at link time|Energy-aware compilers are becoming increasingly important for embedded systems due to the need to meet a variety of design constraints on time, code size and power consumption. This paper introduces for the first time a trace-based, link-time compiler framework on binaries for embedded systems and evaluates its potential benefits in supporting energy optimisations, especially those that exploit the interaction between compilers and architecture. We present two algorithms for reducing leakage energy in functional units and data caches, respectively. Both algorithms work uniformly at the granularity of optimisation regions that are formed by the hot traces of a program. Our experimental results using Mediabench benchmarks show that good leakage energy savings can be achieved at the cost of some small performance and code size penalties. Furthermore, by varying the granularity of optimisation regions, which is a tunable parameter, embedded application programmers can make the tradeoffs between energy savings and these associated costs.||||
53|1||Fault tolerant Web Services|Zwass suggested that middleware and message service is one of the five fundamental technologies used to realize Electronic Commerce (EC). The Simple Object Access Protocol (SOAP) is recognized as a more promising middleware for EC applications among other leading candidates such as CORBA. Many recent polls reveal however that security and reliability issues are major concerns that discourage people from engaging in EC transactions. We notice that the fault-tolerance issue is somewhat neglected in the current standard, i.e., SOAP 1.2. We therefore propose a fault tolerant Web Services called fault tolerant SOAP or FT-SOAP through which Web Services can be built with higher resilience to failure. FT-SOAP is based on our previous experience with an object fault tolerant service (OFS) and OMG’s fault tolerant CORBA (FT-CORBA). There are many architectural differences between SOAP and CORBA. One of the major contributions of this work is to discuss the impact of these architectural differences on FT-SOAP design. Our experience shows that Web Services built on a SOAP framework enjoy higher flexibility compared to those built on CORBA. We also point out the limitations of the current feature sets of SOAP 1.2, e.g. the application of the intermediary. In addition, we examine two implementation approaches; namely, one based on the SOAP 1.2’s intermediary, and the other on Axis handler. We conclude that the intermediary approach is infeasible due to the backward compatibility issue. We believe our experience is valuable not only to the fault-tolerance community, but also to other communities as well, in particular, to those who are familiar with the CORBA platform.||||
53|1||Resource consumption-aware QoS in cluster-based VOD servers|||||
53|1||An efficient variable partitioning approach for functional decomposition of circuits|||||
53|10|http://www.sciencedirect.com/science/journal/13837621/53/10|Editorial|||||
53|10||Effects of program compression|The size of the program code has become a critical design constraint in embedded systems, especially in handheld, battery operated devices. Large program codes require large memories, which increase the size and cost of the chip. In addition, the power consumption is increased due to higher memory I/O bandwidth. Program compression is one of the most often used methods to reduce the size of the program code. In this paper, two compression approaches, dictionary-based compression and instruction template-based compression, were evaluated on a customizable processor architecture with parallel resources. The effects on area and power consumption were measured. Dictionary-based compression reduced the area at best by 77% and power consumption by 73%. Instruction template-based compression resulted in increase in both area and power consumption and hence turned out to be impractical.||||
53|10||Hybrid functional- and instruction-level power modeling for embedded and heterogeneous processor architectures|In this contribution the concept of functional- level power analysis (FLPA) for power estimation of programmable processors is extended in order to model embedded as well as heterogeneous processor architectures featuring different embedded processor cores. The basic FLPA approach is based on the separation of the processor architecture into functional blocks like, e.g. processing unit, clock network, internal memory, etc. The power consumption of these blocks is described by parameterized arithmetic models. By application of a parser based automated analysis of assembler codes the input parameters of the arithmetic functions like e.g. the achieved degree of parallelism or the kind and number of memory accesses can be computed. For modeling an embedded general purpose processor (here, an ARM940T) the basic FLPA modeling concept had to be extended to a so-called hybrid functional-level and instruction-level (FLPA/ILPA) model in order to achieve a good modeling accuracy. In order to show the applicability of this approach even a heterogeneous processor architecture (OMAP5912) featuring an ARM926EJ-S core and a C55x DSP core has been modeled using the hybrid FLPA/ILPA technique described before. The approach is exemplarily demonstrated and evaluated applying a variety of basic digital signal processing tasks ranging from basic filters to complete audio decoders or classical benchmark suits. Estimated power figures for the inspected tasks are compared to physically measured values for both inspected processor architectures. A resulting maximum estimation error of 9% for the ARM940T and less than 4% for the OMAP5912 is achieved.||||
53|10||Simulated and measured performance evaluation of RISC-based SoC platforms in network processing applications|This paper presents results of a simulated performance evaluation of RISC-based SoC platforms for networking applications and compares them to measurement results on an FPGA prototype. We use our SystemC simulation environment, which is calibrated with a reference implementation. Starting with an analysis of the reference scenario, two approaches for improvements are investigated: at first, hardware assists are added, which offload the CPU from compute-intensive bit-level manipulations. Secondly, the concept of flexible processing paths as proposed in FlexPath NP with AutoRoute is evaluated, in which certain parts of the traffic can bypass the central CPU cluster. For each of the three scenarios we determine the maximum throughput and discuss the improvements and limitations of each solution. It can be shown that a FlexPath NP may achieve up to 2.5 times the throughput of the reference scenario. Simulation results are compared to additional measurements on the FPGA platform, which led to a further refinement of our system model. The investigations provide a deeper insight on the practical benefits and limitations of system-level performance simulations.||||
53|10||Exploration of distributed shared memory architectures for NoC-based multiprocessors|Multiprocessor system-on-chip (MP-SoC) platforms represent an emerging trend for embedded multimedia applications. To enable MP-SoC platforms, scalable communication-centric interconnect fabrics, such as networks-on-chip (NoCs), have been recently proposed. The shared memory represents one of the key elements in designing MP-SoCs to provide data exchange and synchronization support.||||
53|10||Efficient design space exploration for application specific systems-on-a-chip|A reduction in the time-to-market has led to widespread use of pre-designed parametric architectural solutions known as system-on-a-chip (SoC) platforms. A system designer has to configure the platform in such a way as to optimize it for the execution of a specific application. Very frequently, however, the space of possible configurations that can be mapped onto a SoC platform is huge and the computational effort needed to evaluate a single system configuration can be very costly. In this paper we propose an approach which tackles the problem of design space exploration (DSE) in both of the fronts of the reduction of the number of system configurations to be simulated and the reduction of the time required to evaluate (i.e., simulate) a system configuration. More precisely, we propose the use of Multi-objective Evolutionary Algorithms as optimization technique and Fuzzy Systems for the estimation of the performance indexes to be optimized. The proposed approach is applied on a highly parameterized SoC platform based on a parameterized VLIW processor and a parameterized memory hierarchy for the optimization of performance and power dissipation. The approach is evaluated in terms of both accuracy and efficiency and compared with several established DSE approaches. The results obtained for a set of multimedia applications show an improvement in both accuracy and exploration time.||||
53|10||Design space exploration of reliable networked embedded systems|||||
53|10||Chip size estimation for SOC design space exploration|At early design space exploration phases of architectures for Systems On a Chip (SOC) chip size estimation is of high interest. An accurate chip size estimation needs detailed knowledge of the transistor densities of a semiconductor process. This paper introduces a novel and simplified chip size estimator, which is independent of manufacturer specific process data. CMOS processes are characterized by only three parameters. These are the drawn gate length and the used numbers of metal layers for logic and for memories. The chip size estimator has been derived from a comprehensive analysis of realized VLSI chips. It has been investigated and confirmed either for published VLSIs as well as for latest SOC designs with 221 million transistors and 333 million transistors. The proposed model has been implemented as a web based tool and contributes to analytical modeling of cost and performance tradeoffs of SOC concepts.||||
53|10||FLUX interconnection networks on demand|In this paper, we introduce the FLUX interconnection networks, a scheme where the interconnections of a parallel system are established on demand before or during program execution. We present a programming paradigm which can be utilized to make the proposed solution feasible. We perform several experiments to show the viability of our approach and the potential performance gain of using the most suitable network configuration for a given parallel program. We experiment on several case studies, evaluate different algorithms, developed for meshes or trees, and map them on “grid”-like or reconfigurable physical interconnection networks. Our results clearly show that, based on the underlying network, different mappings are suitable for different algorithms. Even for a single algorithm different mappings are more appropriate, when the processing data size, the number of utilized nodes or the hardware cost of the processing elements changes. The implication of the above is that changing interconnection topologies/mappings (dynamically) on demand depending on the program needs can be beneficial.||||
53|11|http://www.sciencedirect.com/science/journal/13837621/53/11|Automated memory-aware application distribution for Multi-processor System-on-Chips|Mapping of applications on a Multi-processor System-on-Chip (MP-SoC) is a crucial step to optimize performance, energy and memory constraints at the same time. The problem is formulated as finding solutions to a cost function of the algorithm performing mapping and scheduling under strict constraints. Our solution is based on simultaneous optimization of execution time and memory consumption whereas traditional methods only concentrate on execution time. Applications are modeled as static acyclic task graphs that are mapped on MP-SoC with customized simulated annealing. The automated mapping in this paper is especially purposed for MP-SoC architecture exploration, which typically requires a large number of trials without human interaction. For this reason, a new parameter selection scheme for simulated annealing is proposed that sets task mapping specific optimization parameters automatically. The scheme bounds optimization iterations to a reasonable limit and defines an annealing schedule that scales up with application and architecture complexity. The presented parameter selection scheme compared to extensive optimization achieves 90% goodness in results with only 5% optimization time, which helps large-scale architecture exploration where optimization time is important. The optimization procedure is analyzed with simulated annealing, group migration and random mapping algorithms using test graphs from the Standard Task Graph Set. Simulated annealing is found better than other algorithms in terms of both optimization time and the result. Simultaneous time and memory optimization method with simulated annealing is shown to speed up execution by 63% without memory buffer size increase. As a comparison, optimizing only execution time yields 112% speedup, but also increases memory buffers by 49%.||||
53|11||Optimization decomposition approach for layered QoS scheduling in grid computing|The paper presents optimization decomposition based layered Quality of Service (QoS) scheduling for computational grid. Cross layer joint QoS scheduling is studied by considering the global problem as decomposed into three sub-problems: resource allocation at the fabric layer, service composing at the collective layer, and user satisfaction degree at the application layer. The paper proposes a complete solution from optimization modeling, Lagrange relaxation based decomposition, to solutions for each sub-problem Lagrange relaxation based decomposition. These aspects match the vertical organization of the considered grid system: each layer trade with adjacent layers to find a global optimum of the whole grid system. Through multi-layer QoS joint optimization approach, grid global QoS optimization can be achieved. The cross layer policy produces an optimal set of grid resources, service compositions, and user’s payments at the fabric layer, collective layer and application layer, respectively, to maximize global grid QoS. The owner of each layer obtains inputs from other layers, tries to maximize its own utility and provides outputs back to other layers. This iterative process lasts until presumably all layers arrive at the same solution.||||
53|11||Efficient segment-based video transcoding proxy for mobile multimedia services|To support various bandwidth requirements for mobile multimedia services for future heterogeneous mobile environments, such as portable notebooks, personal digital assistants (PDAs), and 3G cellular phones, a transcoding video proxy is usually necessary to provide mobile clients with adapted video streams by not only transcoding videos to meet different needs on demand, but also caching them for later use. Traditional proxy technology is not applicable to a video proxy because it is less cost-effective to cache the complete videos to fit all kinds of clients in the proxy. Since transcoded video objects have inheritance dependency between different bit-rate versions, we can use this property to amortize the retransmission overhead from transcoding other objects cached in the proxy. In this paper, we propose the object relation graph (ORG) to manage the static relationships between video versions and an efficient replacement algorithm to dynamically manage video segments cached in the proxy. Specifically, we formulate a transcoding time constrained profit function to evaluate the profit from caching each version of an object. The profit function considers not only the sum of the costs of caching individual versions of an object, but also the transcoding relationship among these versions. In addition, an effective data structure, cached object relation tree (CORT), is designed to facilitate the management of multiple versions of different objects cached in the transcoding proxy. Experimental results show that the proposed algorithm outperforms companion schemes in terms of the byte-hit ratios and the startup latency.||||
53|11||Accumulator-based pseudo-exhaustive two-pattern generation|In this paper, a novel scheme for the generation of pseudo-exhaustive two-pattern tests for combinational modules under test is presented. The proposed scheme utilizes an accumulator with 1’s complement adder to generate the patterns in time equal to the theoretical minimum. Since accumulators are commonly found in current, high-speed signal processing VLSI circuits, the presented scheme may prove a practical solution for the pseudo-exhaustive testing of such circuits for delay and stuck-open faults.||||
53|11||Static scheduling techniques for dependent tasks on dynamically reconfigurable devices|Dynamically reconfigurable hardware not only has high silicon reusability, but it can also deliver high performance for computation-intensive tasks. Advanced features such as run-time reconfiguration allow multiple tasks to be mapped onto the same device either simultaneously or multiplexed in time domain. These tasks need to be scheduled optimally or near optimally in order to efficiently utilize the device. It is a NP-hard problem, because task scheduling, allocation and configuration prefetching all need to be considered. In this paper, we target dependent task models and propose three static schedulers that use different problem solving strategies. The first is a heuristic approach developed from traditional list-based schedulers. It presents high efficiency but the least accuracy. The second is based on a full-domain search using constraint programming. It can guarantee to produce optimal solutions but requires significant searching effort. The last is a guided random search technique based on a genetic algorithm, which shows reasonable efficiency and much better accuracy than the heuristic approach.||||
53|11||Test data compression scheme based on variable-to-fixed-plus-variable-length coding|A test data compression scheme based on Variable-to-Fixed-Plus-Variable-Length (VTFPVL) coding is presented, by using which the test data can be compressed efficiently. In this scheme, code words are divided into fixed-length head section and variable-length tail section. In order to attain further compression, the highest bit of the tail is omitted from the code words, because all of the highest bits in the tail section of the code words are the same as 1. A special shift counter is also used, which further eases the control circuit. Experimental results of the MinTest fault sets which are part of ISCAS-89 benchmark circuits show that the proposed scheme is obviously better than traditional coding methods in the compression ratio and the implementation of decompression, such as Golomb, FDR, VIHC, v9C coding.||||
53|12|http://www.sciencedirect.com/science/journal/13837621/53/12|STAFF: A flash driver algorithm minimizing block erasures|Recently, flash memory is widely used in embedded applications since it has strong points: non-volatility, fast access speed, shock resistance, and low power consumption. However, due to its hardware characteristics, it requires a software layer called flash translation layer (FTL). The main functionality of FTL is to convert logical addresses from the host to physical addresses of flash memory. We propose a new FTL algorithm called state transition applied fast flash translation layer (STAFF). Compared to the previous FTL algorithms, STAFF shows higher performance and requires less memory. We provide performance results based on our implementation of STAFF and previous FTL algorithms.||||
53|12||Enhanced fault tolerant routing algorithms using a concept of âbalanced ringâ|||||
53|12||Dual actuator logging disk architecture and modeling|In this paper, we present a dual actuator logging disk architecture to minimize write access latencies. We reduce small synchronous write latency using the notion of logging writes, i.e. writing to free sectors near the current disk head location. However, we show through analytic models and simulations that logging writes by itself is not sufficient to reduce write access latencies, particularly in environments with writes to new data and intermixed reads and writes. Therefore, we augment the logging write method with the addition of a second disk actuator. Our models and simulations show that the addition of the second actuator offers significant performance benefits over a normal disk over a wide range of disk access patterns, and comparisons to strictly logging disk architectures show advantages over a range of disk access patterns.||||
53|12||Feasibility of decoupling memory management from the execution pipeline|In conventional architectures, the central processing unit (CPU) spends a significant amount of execution time allocating and de-allocating memory. Efforts to improve memory management functions using custom allocators have led to only small improvements in performance. In this work, we test the feasibility of decoupling memory management functions from the main processing element to a separate memory management hardware. Such memory management hardware can reside on the same die as the CPU, in a memory controller or embedded within a DRAM chip. Using Simplescalar, we simulated our architecture and investigated the execution performance of various benchmarks selected from SPECInt2000, Olden and other memory intensive application suites.||||
53|12||A platform-based SoC design and implementation of scalable automaton matching for deep packet inspection|||||
53|2-3|http://www.sciencedirect.com/science/journal/13837621/53/2-3|Embedded cryptographic hardware|||||
53|2-3||High-speed hardware implementations of Elliptic Curve Cryptography: A survey|For the last decade, Elliptic Curve Cryptography (ECC) has gained increasing acceptance in the industry and the academic community and has been the subject of several standards. This interest is mainly due to the high level of security with relatively small keys provided by ECC. To sustain the high throughput required by applications like network servers, high-speed implementations of public-key cryptosystems are needed. For that purpose, hardware-based accelerators are often the only solution reaching an acceptable performance-cost ratio. The fundamental question that arises is how to choose the appropriate efficiency–flexibility tradeoff.||||
53|2-3||Hardware acceleration of the Tate pairing on a genus 2 hyperelliptic curve|Many novel and interesting cryptographic protocols have recently been designed with bilinear pairings comprising their main calculation. The Î·T method for pairing calculation is an efficient computation technique based on a generalisation and optimisation of the Duursma–Lee algorithm for calculating the Tate pairing. The pairing can be computed very efficiently on hyperelliptic curves of genus 2. In this paper it is demonstrated that the Î·T method is ideally suited for hardware implementation since much of the more intensive arithmetic can be performed in parallel in hardware. A Tate pairing processor is presented and the architectures required for such a system are discussed. The processor returns a fast pairing computation when compared to the best results in the literature to date. Results are provided when the processor is implemented on an FPGA over the base field F2103F2103.||||
53|2-3||Fast hardware for modular exponentiation with efficient exponent pre-processing|||||
53|2-3||Efficient parallel multiplier in shifted polynomial basis|In this paper we study the multiplication in fields F2nF2n using the Shifted Polynomial Basis (SPB) representation of Fan and Dai [H. Fan, Y. Dai, Fast bit-parallel GF(2n) multiplier for all trinomials, IEEE Transactions on Computers 54 (4) (2005) 485–490]. We give a simpler construction than in Fan and Dai (2005) of the matrix associated to the SPB used to perform the field multiplication. We present also a novel parallel architecture to multiply in SPB. This multiplier have a smaller time complexity (for good field it is equal to TA + ⌈log2(n)⌉TX) than all previously presented architecture. For practical field F2nF2n, i.e., for n ≅ 163, this roughly improves the delay by 10%. On the other hand the space complexity is increased by 25%: the space complexity is a little greater than the time gain.||||
53|2-3||Scalable hardware implementing high-radix Montgomery multiplication algorithm|This paper presents a new scalable hardware implementing modular multiplication. A high radix Montgomery multiplication algorithm without final subtraction is used to perform this operation. An alternative proof for the final Montgomery multiplication by 1, removing the condition on the modulus, is given. This hardware fits in any chip area and is able to work with any size of modulus. Unlike other scalable designs only one cell is used. This cell contains standard and well optimized digit multiplier and adder. Time–area trade-offs are also available before hardware synthesis for differents sizes of internal data path. The pipeline architecture of the multiplier component increases the clock frequency and the throughput. Time–area trade-offs are analyzed in order to make the best choice for given time and area constraints. This architecture seems to provide a better time–area compromise than previous scalable hardware.||||
53|2-3||Multi-mode operator for SHA-2 hash functions|We propose an improved implementation of the SHA-2 hash family, with minimal operator latency and reduced hardware requirements. We also propose a high frequency version at the cost of only two cycles of latency per message. Finally we present a multi-mode architecture able to perform either a SHA-384 or SHA-512 hash or to behave as two independent SHA-224 or SHA-256 operators. Such capability adds increased flexibility for applications ranging from a server running multiple streams to independent pseudorandom number generation. We also demonstrate that our architecture achieves a performance comparable to separate implementations while requiring much less hardware.||||
53|2-3||Robust codes and robust, fault-tolerant architectures of the Advanced Encryption Standard|Hardware implementations of cryptographic algorithms are vulnerable to fault analysis attacks. Methods based on traditional fault-tolerant architectures are not suited for protection against these attacks. To detect these attacks we propose an architecture based on robust nonlinear systematic error-detecting codes. These nonlinear codes are capable of providing uniform error detecting coverage independently of the error distributions. They make no assumptions about what faults or errors will be injected by an attacker. Architectures based on these robust constructions have fewer undetectable errors than linear codes with the same n, k. We present the general properties and construction methods of these codes as well as their application for the protection of a cryptographic devices implementing the Advanced Encryption Standard.||||
53|4|http://www.sciencedirect.com/science/journal/13837621/53/4|The SegBus platform â architecture and communication mechanisms|In this study, we introduce the SegBus architecture, a synchronous segmented bus platform for systems on chip. We present the envisioned structure in detail, and also address aspects of communication on the platform. The motivation behind SegBus is the search for performance improvements, in several directions, such as global throughput, power consumption, modularity, adaptability. By means of an example, we illustrate the capabilities of the described architecture. The implementation strategy targets FPGA technology, and allows for the utilization of multiple clock domains. The platform emerges as a highly design-time configurable system, adaptable to various design constraints.||||
53|4||A comparison of two policies for issuing instructions speculatively|||||
53|4||Efficient FPGA hardware development: A multi-language approach|||||
53|4||An efficient immersion-based watershed transform method and its prototype architecture|This paper describes an improved immersion-based watershed algorithm to compute the watershed lines for segmentation of digital gray scale images and its hardware implementation. The proposed algorithm is devoid of certain disadvantages inherent in a conventional immersion-based algorithm originally proposed by Vincent and Soille. Flooding of catchment basins from pre-determined regional minima and conditional neighborhood comparisons while processing the eight neighboring pixels of a labeled center pixel ensures thin continuous watershed lines. Reduced computational complexity and increased throughput compared to the conventional algorithm occurs from simultaneous determination of labels of various neighboring pixels. The complexity of the proposed algorithm is analyzed. The results of running both the proposed and the conventional algorithm on different test images clearly establish the superiority of the proposed algorithm. A prototype architecture designed to implement the proposed watershed algorithm has been modelled in VHDL and synthesized for Virtex FPGA. The FPGA implementation results show acceptable performance of the proposed architecture.||||
53|4||Highly fault-tolerant cycle embeddings of hypercubes|||||
53|4||Hardware support for adaptive tessellation of BÃ©zier surfaces based on local tests|Bézier representations have been widely employed as a standard way of designing complex scenes with very good quality results. These surfaces are usually tessellated, in the software application, into triangle models to be rendered. Then, the final image is generated in the graphics card so that its triangle rendering capabilities are exploited.||||
53|5-6|http://www.sciencedirect.com/science/journal/13837621/53/5-6|Editorial|||||
53|5-6||Asynchronous arbiter for micro-threaded chip multiprocessors|||||
53|5-6||Supporting multiple-input, multiple-output custom functions in configurable processors|Configurable processors have emerged as a promising solution for high performance embedded systems. Many of these processors extend a RISC core with configurable functional units that execute dual-input, single-output (DISO) custom functions. Although studies have shown that supporting multiple-input, multiple-output (MIMO) custom functions can lead to significant speedups, mechanisms to efficiently achieve this have not been adequately addressed. The underlying reason is that a custom function is normally invoked by a single instruction, which usually transfers only two inputs and one output. Attempts to transfer more inputs and outputs in one instruction are impeded by the instruction length and the register file’s R/W ports. This paper proposes a simple extension to transfer multiple inputs and outputs of the custom functions using repeated instructions. While transferring the inputs and outputs may take a few extra cycles, our experiments show that the MIMO extension can still achieve an average 51% increase in speedup compared to a DISO extension and an average 27% increase in speedup compared to a multiple-input, single-output (MISO) extension.||||
53|5-6||A consistency-free memory architecture for sort-last parallel rendering processors|||||
53|5-6||Resource efficiency of the GigaNetIC chip multiprocessor architecture|In this article, we present the prototypical implementation of the scalable GigaNetIC chip multiprocessor architecture. We use an FPGA-based rapid prototyping system to verify the functionality of our architecture in a network application scenario before fabricating the ASIC in a modern CMOS standard cell technology. The rapid prototyping environment gives us the opportunity to test our multiprocessor architecture with Ethernet-based data streams in a real network scenario. Our system concept is based on a massively parallel processor structure. Due to its regularity, our architecture can be easily scaled to accommodate a wide range of packet processing applications with various performance and throughput requirements at high reliability. Furthermore, the composition based on predefined building blocks guarantees fast design cycles and simplifies system verification. We present standard cell synthesis results as well as a performance analysis for a firewall application with various couplings of hardware accelerators. Finally, we compare implementations of our architecture with state-of-the-art desktop CPUs. We use simple, general-purpose applications as well as the introduced packet processing tasks to determine the performance capabilities and the resource efficiency of the GigaNetIC architecture. We show that, if supported by the application, parallelism offers more opportunities than increasing clock frequencies.||||
53|5-6||Efficient control generation for mapping nested loop programs onto processor arrays|||||
53|5-6||A system architecture for high-speed deep packet inspection in signature-based network intrusion prevention|Pattern matching is one of critical parts of Network Intrusion Prevention Systems (NIPS). Pattern matching hardware for NIPS should find a matching pattern at wire speed. However, that alone is not good enough. First, pattern matching hardware should be able to generate sufficient pattern match information including the pattern index number and the location of the match found at wire speed. Second, it should support pattern grouping to reduce unnecessary pattern matches. Third, it should guarantee worst-case performance even if the number of patterns is increased. Finally it should be able to update patterns in a few minutes or seconds without stopping its operations. We propose a system architecture to meet the above requirements. Using Xilinx FPGA simulation, we show that the new system scales well to achieve a high speed over 10 Gbps and satisfies all of the above requirements.||||
53|5-6||High-speed, low-leakage integrated circuits: An evolutionary algorithm perspective|The markets today observe users having increasing demands on processing speed and energy consumption of their mobile devices. However, processing speed as well as functionality always comes at the expense of energy and thus limits, among other things, mobility and integration density. Recent technological developments allow for the simultaneous realization of slow, low-energy consuming as well as fast, high-energy consuming gates on the very same chip. In this respect, a particular design is an abstract optimization task for which this paper applies evolutionary algorithms. These algorithms are heuristic population-based search procedures that utilize certain mechanisms known from natural evolution. In comparison to currently available deterministic optimization procedures, the evolutionary algorithms achieved some energy savings of about 10–40% on standard ISCAS test problems, while still yielding the highest processing speed possible.||||
53|5-6||Energy consumption analysis for two embedded Java virtual machines|||||
53|7|http://www.sciencedirect.com/science/journal/13837621/53/7|Genetic algorithms for hardwareâsoftware partitioning and optimal resource allocation|A scheme for time and power efficient embedded system design, using hardware and software components, is presented. Our objective is to reduce the execution time and the power consumed by the system, leading to the simultaneous multi-objective minimization of time and power. The goal of suitably partitioning the system into hardware and software components is achieved using Genetic Algorithms (GA). Multiple tests were conducted to confirm the consistency of the results obtained and the versatile nature of the objective functions. An enhanced resource constrained scheduling algorithm is used to determine the system performance. To emulate the characteristics of practical systems, the influence of inter-processor communication is examined. The suitability of introducing a reconfigurable hardware resource over pre-configured hardware is explored for the same objectives. The distinct difference in the task to resource mapping with the variation in design objective is studied. Further, the procedure to allocate optimal number of resources based on the design objective is proposed. The implementation is constrained for power and time individually, with GA being used to arrive at the resource count to suit the objective. The results obtained are compared by varying the time and power constraints. The test environment is developed using randomly generated task graphs. Exhaustive sets of tests are performed on the set design objectives to validate the proposed solution.||||
53|7||A low-cost strategy to provide full QoS support in Advanced Switching networks|Advanced Switching (AS) is an open-standard fabric-interconnect technology that is built over the same physical and link layers as PCI Express technology. Moreover, it includes an optimized transaction layer to enable essential communication capabilities, including protocol encapsulation, peer-to-peer communications, mechanisms to provide quality of service (QoS), enhanced fail-over, high availability, multicast communications, and congestion and system management.||||
53|7||Efficient FPGA implementation of DWT and modified SPIHT for lossless image compression|In this paper, we present an implementation of the image compression technique set partitioning in hierarchical trees (SPIHT) in programmable hardware. The lifting based Discrete Wavelet Transform (DWT) architecture has been selected for exploiting the correlation among the image pixels. In addition, we provide a study on what storage elements are required for the wavelet coefficients. A modified SPIHT (Set Partitioning in Hierarchical Trees) algorithm is presented for encoding the wavelet coefficients. The modifications include a simplification of coefficient scanning process, use of a 1-D addressing method instead of the original 2-D arrangement for wavelet coefficients and a fixed memory allocation for the data lists instead of the dynamic allocation required in the original SPIHT. The proposed algorithm has been illustrated on both the 2-D Lena image and a 3-D MRI data set and is found to achieve appreciable compression with a high peak-signal-to-noise ratio (PSNR).||||
53|7||Efficient automatic gain control algorithm and architecture for wireless LAN receivers|The performance of a receiver front-end limits the quality and range of the given communication link. An appropriate design based on well-defined system parameters and architecture can make a huge difference in the performance, cost and marketability of the entire system. In particular, there is a need for improved digital automatic gain control (AGC) for use in multi-input multi-output orthogonal frequency division multiplexing (MIMO-OFDM) systems with application to wireless local area networks (WLANs), targeted for the upcoming 802.11n standard [Heejung Yu et al., IEEE 802.11 wireless LANs ETRI proposal specification for IEEE 802.11 TGn, IEEE 802.11 document, doc. No. 11-04-0923-00-000n, August, 2004; H. Yu, T. Jeon, S. Lee, Design of dual-band MIMO-OFDM system for next generation wireless LAN, in: IEEE International Conference on Communications (ICC), May, 2005]. In this paper, we propose an efficient algorithm and implementation of the digital AGC for next generation WLANs. The proposed AGC algorithm has two feedback loops for gain control to improve convergence speed, and at the same time maintains the stability of the AGC circuit. Also, a complete set of parameters for practical implementation is obtained by various experiments with fixed point constraints and accuracy requirements.||||
53|7||Hardware-oriented ant colony optimization|||||
53|7||Effectiveness of caching in a distributed digital library system|Today independent publishers are offering digital libraries with fulltext archives. In an attempt to provide a single user-interface to a large set of archives, the studied Article-Database-Service offers a consolidated interface to a geographically distributed set of archives. While this approach offers a tremendous functional advantage to a user, the fulltext download delays caused by the network and queuing in servers make the user-perceived interactive performance poor.||||
53|7||Systematic methodology for exploration of performance â Energy trade-offs in network applications using Dynamic Data Type refinement|Modern network applications require high performance and consume a lot of energy. Their inherent dynamic nature makes the dynamic memory subsystem a critical contributing factor to the overall energy consumption and to the execution time performance. This paper presents a novel, systematic methodology for generating performance-energy trade-offs by implementing optimal Dynamic Data Types, finely tuned and refined for network applications. Our systematic methodology is supported by a new, fully automated tool. We assess the effectiveness of the proposed approach in four representative, real-life case studies and provide significant energy savings and performance improvements compared to the original implementations.||||
53|7||Achieving multipoint-to-multipoint fairness with RCNWA|In current Internet, applications employ lots of sessions with multiple connections between multiple senders and receivers. Sessions or users with more connections gain higher throughput. To obtain more network resource, applications tend to create more connections. It causes unfair bandwidth allocation by per-connection TCP rate allocation and the network suffers lots of TCP overheads. In this paper, we explore the issue on fair share allocation of bandwidth among sessions or users. Various fairness definitions are discussed in this study. Then, we propose a novel distributed scheme to achieve various fairness definitions. Simulation results show that our distributed scheme could achieve fair allocation according to each fairness definition.||||
53|7||Quantum ternary parallel adder/subtractor with partially-look-ahead carry|Multiple-valued quantum circuits are a promising choice for future quantum computing technology since they have several advantages over binary quantum circuits. Binary parallel adder/subtractor is central to the ALU of a classical computer and its quantum counterpart is used in oracles – the most important part that is designed for quantum algorithms. Many NP-hard problems can be solved more efficiently in quantum using Grover algorithm and its modifications when an appropriate oracle is constructed. There is therefore a need to design standard logic blocks to be used in oracles – this is similar to designing standard building blocks for classical computers. In this paper, we propose quantum realization of a ternary full-adder using macro-level ternary Feynman and Toffoli gates built on the top of ion-trap realizable ternary 1-qutrit and Muthukrishnan–Stroud gates. Our realization has several advantages over the previously reported realization. Based on this realization of ternary full-adder we propose realization of a ternary parallel adder with partially-look-ahead carry. We also show the method of using the same circuit as a ternary parallel adder/subtractor.||||
53|8|http://www.sciencedirect.com/science/journal/13837621/53/8|Editorial|||||
53|8||Application of deterministic and stochastic Petri-Nets for performance modeling of NoC architectures|The design of appropriate communication architectures for complex Systems-on-Chip (SoC) is a challenging task. One promising alternative to solve these problems are Networks-on-Chip (NoCs). Recently, the application of deterministic and stochastic Petri-Nets (DSPNs) to model on-chip communication has been proven to be an attractive method to evaluate and explore different communication aspects. In this contribution the modeling of basic NoC communication scenarios featuring different processor cores, network topologies and communication schemes is presented. In order to provide a testbed for the verification of modeling results a state-of-the-art FPGA-platform has been utilized. This platform allows to instantiate a soft-core processor network which can be adapted in terms of communication network topologies and communication schemes. It will be shown that DSPN modeling yields good communication performance prediction results at low modeling effort. Different DSPN modeling aspects in terms of accuracy and computational effort are discussed.||||
53|8||Benchmarking mesh and hierarchical bus networks in system-on-chip context|The performance and area of a System-on-Chip depend on the utilized communication method. This paper presents simulation-based comparison of generic, synthesizable single bus, hierarchical bus, and 2-dimensional mesh on-chip networks. Performance of the network depends heavily on the application and therefore six test cases with multiple parameter values are used. Furthermore, two versions of each network topology are compared. The results show that hierarchical bus scales well to large number of agents and offers a good performance and area trade-off although it has smaller aggregate bandwidth and area than mesh. Hierarchical HIBI bus achieves runtimes comparable to 2-dimensional cut-through mesh with about 50% smaller network logic. However, depending on the test case, the runtime can be reduced by 20–50% when wider bus links are utilized.||||
53|8||Exploiting program phase behavior for energy reduction on multi-configuration processors|||||
53|8||Ultra fast cycle-accurate compiled emulation of inorder pipelined architectures|Emulation of one architecture on another is useful when the architecture is under design, when software must be ported to a new platform or is being developed for systems which are still under development, or for embedded systems that insufficient resources to support the software development process. Emulation using an interpreter is typically slower than normal execution by up to 3 orders of magnitude. Our approach instead translates the program from the original architecture to another architecture while faithfully preserving its semantics at the lowest level. The emulation speeds are comparable to, and often faster than, programs running on the original architecture. Partial evaluation of architectural features is used to achieve such impressive performance, while permitting accurate statistics collection. Accuracy is at the level of the number of clock cycles spent executing each instruction (hence the description cycle-accurate).||||
53|8||Rapid implementation and optimisation of DSP systems on FPGA-centric heterogeneous platforms|The emergence of programmable logic devices as single-chip heterogeneous processing platforms for digital signal processing applications poses challenges concerning rapid implementation and high level optimisation of algorithms on these platforms. This paper describes Abhainn, a rapid implementation methodology and toolsuite for translating an algorithmic expression of the system to a working solution on FPGA-centric embedded platforms. Two particular focuses for Abhainn are the automated but configurable realisation of inter-processor communication fabrics, and the establishment of novel dedicated hardware component design methodologies allowing algorithm level transformation for system optimisation. This paper outlines the approaches employed in these areas and demonstrates their effectiveness on high end signal processing beamforming applications.||||
53|8||A scalable embedded JPEG 2000 architecture|The latest image compression standard, JPEG 2000 is well tuned for diverse applications, thus raising various throughput demands on its building blocks. Therefore, a JPEG 2000 encoder with the feature of scalability is favorable for its ability of meeting different throughput requirements. On the other hand, the large amounts of data streams underline the importance of bandwidth optimization in designing the encoder. The initial specification, especially in terms of loop organization and array indices, describes the data manipulations and, subsequently, influences the outcome of the architecture implementation. Therefore, there is a clear need for the exploiting support, and we believe the emphasis should lie on the loop level steering. In this paper, we apply loop transformation techniques to a scalable embedded JPEG 2000 encoder design during the architectural exploration stage, considering not only the balance of throughput among different blocks, but also the reduction of data transfer. The architecture is prototyped onto Xilinx FPGA.||||
53|8||Optimizing data structures at the modeling level in embedded multimedia|Traditional design techniques for embedded systems apply transformations on the source code to optimize hardware-related cost factors. Unfortunately, such transformations cannot adequately deal with the highly dynamic nature of today’s multimedia applications. Therefore, we go one step back in the design process. Starting from a conceptual UML model, we first transform the model before refining it into executable code. This paper presents: various model transformations, an estimation technique for the steering cost parameters, and three case studies that show how our model transformations result in factors improvement in memory footprint and performance with respect to the initial implementation.||||
53|9|http://www.sciencedirect.com/science/journal/13837621/53/9|Designing layoutâtiming independent quantum-dot cellular automata (QCA) circuits by global asynchrony|The concept of clocking for QCA, referred to as the four-phase clocking, is widely used. However, inherited characteristics of QCA, such as the way to hold state, the way to synchronize data flows, and the way to power QCA cells, make the design of QCA circuits quite different from VLSI and introduce a variety of new design challenges and the most severe challenges are due to the fact that the overall timing of a QCA circuit is mainly dependent upon its layout. This fact is commonly referred to as the “layout = timing” problem. To circumvent the problem, a novel self-timed QCA circuit design methodology referred to as the Globally Asynchronous, Locally Synchronous (GALS) Design for QCA is proposed in this paper. The proposed technique can significantly reduce the layout–timing dependency from the global network of QCA devices in a circuit; therefore, considerably flexible QCA circuit design and floorplanning will be possible.||||
53|9||Low power data processing system with self-reconfigurable architecture|||||
53|9||A new Mixed Radix Conversion algorithm MRC-II|In this paper, we present an efficient and simplified algorithm for the Residue Number System (RNS) conversion to weighted number system which in turn will simplify the implementation of RNS sign detection, magnitude comparison, and overflow detection. The algorithm is based on the Mixed Radix Conversion (MRC). The new algorithm simplifies the hardware implementation and improves the speed of conversion by replacing a number of multiplication operations with small look-up tables. The algorithm requires less ROM size compared to those required by existing algorithms. For a moduli set consisting of eight moduli, the new algorithm requires seven tables to do the conversion with a total table size of 519 bits, while Szabo and Tanaka MRC algorithm [N.S. Szabo, R.I. Tanaka, Residue Arithmetic and its Application to Computer Technology, McGraw-Hill, New York, 1967; C.H. Huang, A fully parallel mixed-radix conversion algorithm for residue number applications, IEEE Transactions on Computers c-32 (4) (1983)] requires 28 tables with a total table size of 8960 bits; and Huang MRC algorithm (Huang, 1983) requires 36 tables with a total table size of 5760 bits.||||
53|9||Speculative trivialization point advancing in high-performance processors|||||
53|9||Real-time shape description system based on MPEG-7 descriptors|||||
53|9||A heuristic fault-tolerant routing algorithm in mesh using rectilinear-monotone polygonal fault blocks|A new, rectilinear-monotone polygonally shaped fault block model, called Minimal-Connected-Component (MCC), was proposed in [D. Wang, A rectilinear-monotone polygonal fault block model for fault-tolerant minimal routing in mesh, IEEE Trans. Comput. 52 (3) (2003) 310–320] for minimal adaptive routing in mesh-connected multiprocessor systems. This model refines the widely used rectangular model by including fewer non-faulty nodes in fault blocks. The positions of source/destination nodes relative to faulty nodes are taken into consideration when constructing fault blocks. Adaptive routing algorithm was given in Wang (2003), that constructs a minimal “Manhattan” route avoiding all fault blocks, should such routes exist. However, if there are no minimal routes, we still need to find a route, preferably as short as possible. In this paper, we propose a heuristic algorithm that takes a greedy approach, and can compute a nearly shortest route without much overhead. The significance of this algorithm lies in the fact that routing is a frequently performed task, and messages need to get to their destinations as soon as possible. Therefore one would prefer to have a fast answer about which route to take (and then take it), rather than spend too much time working out an absolutely shortest route.||||
53|9||Tornado: A self-reconfiguration control system for core-based multiprocessor CSoPCs|In this work we present a self-reconfiguration control focused on multiprocessor core-based systems implemented on FPGA technology. An infrastructure of signals, protocols, interfaces and a controller is exposed to perform safe hardware/software reconfigurations. This infrastructure is part of the Tornado framework that includes other elements such as a multi-context assembler for a reconfigurable processor or a custom design flow developed for the Wishbone IP-Core interconnection specification. We present two applications where the presented control system has been applied, and it is compared with other available approaches.||||
53|9||A multi-channel architecture for high-performance NAND flash-based storage system|Many mobile devices demand a large-capacity and high-performance storage system in order to store, retrieve, and process large multimedia data quickly. In this paper, we present a high-performance NAND flash-based storage system based on a multi-channel architecture. The proposed system consists of multiple independent channels, where each channel has multiple NAND flash memory chips. On this hardware, we investigate three optimization techniques to exploit I/O parallelism: striping, interleaving, and pipelining. By combining all the optimization techniques carefully, our system has shown 3.6 times higher overall performance compared to the conventional single-channel architecture.||||
53|9||Multiprogrammed non-blocking checkpoints in support of optimistic simulation on myrinet clusters|CCL (checkpointing and communication library) is a software layer in support of optimistic parallel discrete event simulation (PDES) on myrinet-based COTS clusters. Beyond classical low latency message delivery functionalities, this library implements CPU offloaded, non-blocking (asynchronous) checkpointing functionalities based on data transfer capabilities provided by a programmable DMA engine on board of myrinet network cards. These functionalities are unique since optimistic simulation systems conventionally rely on checkpointing implemented as a synchronous, CPU-based data copy. Releases of CCL up to v2.4 only support monoprogrammed non-blocking checkpoints. This forces re-synchronization between CPU and DMA activities, which is a potential source of overhead, each time a new checkpoint request must be issued at the simulation application level while the last issued one is still being carried out by the DMA engine. In this paper we present a redesigned release of CCL (v3.0) that, exploiting hardware capabilities of more advanced myrinet clusters, supports multiprogrammed non-blocking checkpoints. The multiprogrammed approach allows higher degree of concurrency between checkpointing and other simulation specific operations carried out by the CPU, with benefits on performance. We also report the results of the experimental evaluation of those benefits for the case of a Personal Communication System (PCS) simulation application, selected as a real world test-bed.||||
||||||||
volume|issue|url|title|abstract||||
54|1-2|http://www.sciencedirect.com/science/journal/13837621/54/1-2|Announcing JSA embedded software design|||||
54|1-2||TSB: A DVS algorithm with quick response for general purpose operating systems|DVS is becoming an essential feature of state-of-the-art mobile processors. Interval-based DVS algorithms are widely employed in general purpose operating systems thanks to their simplicity and transparency. Such algorithms have a few problems, however, such as delayed response, prediction inaccuracies, and underestimation of the performance demand. In this paper we propose TSB (time slice based), a new DVS algorithm that takes advantage of the high transition speeds available in state-of-the-art processors. TSB adjusts processor performance at every context switch in order to match the performance demand of the next scheduled task. The performance demand of a task is predicted by analyzing its usage pattern in the previous time slice. TSB was evaluated and compared to other interval-based power management algorithms on the Linux kernel. The results show that TSB achieved similar or better energy efficiency compared to existing interval-based algorithms. In addition, TSB dramatically reduced the side effect of prolonging short-term execution times. For a task requiring 50 ms to run without a DVS algorithm, TSB prolonged the execution time by only 6% compared to results of 136% for CPUSpeed and 20% for Ondemand.||||
54|1-2||Formal verification of ASMs using MDGs|We present a framework for the formal verification of abstract state machine (ASM) designs using the multiway decision graphs (MDG) tool. ASM is a state based language for describing transition systems. MDG provides symbolic representation of transition systems with support of abstract sorts and functions. We implemented a transformation tool that automatically generates MDG models from ASM specifications. Then formal verification techniques provided by the MDG tool, such as model checking or equivalence checking, can be applied on the generated models. We illustrate this work with the case study of an ATM switch controller, in which behavior and structure were specified in ASM and, using our ASM-MDG facility, are successfully verified with the MDG tool.||||
54|1-2||Processor array architectures for flexible approximate string matching|||||
54|1-2||A novel caching mechanism for peer-to-peer based media-on-demand streaming|In recent years, peer-to-peer networks and application-level overlays without dedicated infrastructure have been widely proposed to provide on-demand media services on the Internet. However, the scalability issue, which is caused by the asynchronism and the sparsity of the online peers, is a major problem for deploying P2P-based MoD systems, especially when the media server’s capacity is limited. In this paper, we propose a novel probabilistic caching mechanism for P2P-based MoD systems. Theoretical analysis is presented to show that by engaging our proposed mechanism with a flexible system parameter, better scalability could be achieved by a MoD system with less workload imposed on the server, and the service capacity of the MoD system could be tradeoff with the peers’ gossip cost. We verify these properties with simulation experiments. Moreover, we show by simulation results that our proposed caching mechanism could improve the quality of the streaming service conceived by peers when the capacity of the server is limited, but will not cause notable performance degradation under highly lossy network environments, compared with the conventional continuous caching mechanism.||||
54|1-2||RISC: A resilient interconnection network for scalable cluster storage systems|The explosive growth of data generated by information digitization has been identified as the key driver to escalate storage requirements. It is becoming a big challenge to design a resilient and scalable interconnection network which consolidates hundreds even thousands of storage nodes to satisfy both the bandwidth and storage capacity requirements. This paper proposes a resilient interconnection network for storage cluster systems (RISC). The RISC divides storage nodes into multiple partitions to facilitate the data access locality. Multiple spare links between any two storage nodes are employed to offer strong resilience to reduce the impact of the failures of links, switches, and storage nodes. The scalability is guaranteed by plugging in additional switches and storage nodes without reconfiguring the overall system. Another salient feature is that the RISC achieves a dynamic scalability of resilience by expanding the partition size incrementally with additional storage nodes along with associated two network interfaces that expand resilience degree and balance workload proportionally. A metric named resilience coefficient is proposed to measure the interconnection network. A mathematical model and the corresponding case study are employed to illustrate the practicability and efficiency of the RISC.||||
54|1-2||Nash equilibria in bandwidth allocation for non-cooperative peer-to-peer networks|In peer-to-peer networks, peers act as clients and servers, i.e., they can download files from others and allow others to download from them, at the same time. Since the bandwidth of a peer acting as server is shared among all its clients, the download rate experienced by a peer depends on the server choices of the other peers.||||
54|1-2||Pipelined circuit switching: Analysis for the torus with non-uniform traffic|Pipelined circuit switching (PCS) has been suggested as an efficient switching method for supporting interprocessor communication in multicomputer networks due to its ability to preserve both communication performance and fault-tolerant demands in such networks. The torus has been the underlying topology for a number of practical multicomputers. Analytical models of fully adaptive routing have recently been proposed for PCS in the torus under the uniform traffic pattern. However, there has not been any similar analytical model of PCS under the non-uniform traffic pattern, such as that generated by the presence of hot spots in the network. This paper proposes a new analytical model of PCS in the torus operating under the hot spot traffic pattern. Results from simulation experiments show close agreement with those predicted by the analytical model.||||
54|1-2||Optimal load distribution in nondedicated heterogeneous cluster and grid computing environments|||||
54|1-2||Analyzing concurrency in streaming applications|We present a concurrency model that allows reasoning about concurrency in executable specifications of streaming applications. It provides measures for five different concurrency properties. The aim of the model is to provide insight into concurrency bottlenecks in an application and to provide global direction when performing implementation-independent concurrency optimization. The model focuses on task-level concurrency. A concurrency optimization method and a prototype implementation of a supporting analysis tool have been developed. We use the model and tool to optimize the concurrency in a number of multimedia applications. The results show that the concurrency model allows target-architecture-independent concurrency optimization.||||
54|1-2||A new architecture for efficient hybrid representation of terrains|Interactive visualization of highly detailed terrain models is a challenging problem as the size of the data sets can easily exceed the capabilities of current hardware. Hybrid representation of terrains tries to solve the problem by combining the advantages in terms of reduced size associated with multiple levels-of-detail of digital elevation models with the high quality associated with the triangulated irregular networks. However, as the measurements of both representations have different origins, a direct representation of the hybrid system would result in discontinuities.||||
54|1-2||A small data cache for multimedia-oriented embedded systems|This paper proposes a data cache with small space for low power, but high performance on multimedia applications. The basic architecture is a split-cache consisting of a direct-mapped cache with small block size (DMC) and a fully-associative buffer with large block size (FAB). To overcome the disadvantage caused by small cache areas, two hardware mechanisms are enhanced considering the operational behaviors of multimedia applications: an adaptive multi-block prefetching to initiate various fetch sizes for FAB and an efficient block filtering to remove the data likely to be rarely reused for DMC. The simulations on MediaBench show that the proposed 5kB cache can achieve up to 57% and 50% of power saving while providing almost equal and better performance compared with the 16kB 4-way set associative cache and 17kB stream caches, respectively.||||
54|1-2||Configurable folded array for FIR filtering|The synthesis of configurable bit-plane processing array for FIR filtering is described in this paper. Possibilities for configuration are explored and encompassed by application of folding technique. The proposed folded architecture supports on-the-fly configuration of number of taps and coefficient length. This is achieved by dynamic operations mapping on the different hardware units in array structure. Dynamic operations mapping, involved in application of folding technique, allows recognition of user defined parameters, such as number of coefficients and coefficient length on implemented array size. The architecture provides flexible computations and offers the possibility of increasing the folded system throughput, by reducing the number of operations performed on a single functional unit, at cost of decreasing the coefficient number or length. Effects of folding technique application to architecture configuration capabilities are presented. The configurable folded FIR filter synthesis process is presented in detail. The obtained folded system architecture is described by block diagram, DFG, functional block diagram and the data flow diagram. The method of operation and operations mapping on the processing units are described. The algorithms for data reordering are given. With the aim to illustrate the functionality, configuration capabilities, and “trade-offs” relating to occupation of the chip resources and achieved throughputs of synthesized folded architecture, we present results of FPGA prototyping. The proposed configurable folded array is used for H.264/AVC deblocking filter implementation with extremely low-gate count that is achieved at the cost of time, but the design meets the requirement for real-time deblocking in mobile embedded computing platforms.||||
54|1-2||Architectural designs for a scalable reconfigurable IP router|We present two flexible Internet Protocol (IP) router hardware (HW) architectures that enable a router to (1) readily expand in capacity according to network traffic volume growth; and (2) reconfigure its functionalities according to the hierarchical network layer in which it is placed. Reconfigurability is effectuated by a novel method called Methodology for Hardware Unity and an associated functional unit: Special Processing Agent, which can be built using state-of-the-art technology. More specifically, reconfiguration between an “edge” and a “hub” or “backbone” router can be done rapidly via a simple “OPEN/CLOSE” connection approach. Such architectures may, among other benefits, significantly extend the intervals between router HW upgrades for Internet services providers. They can serve as the basis for development of the next generation IP routers, and are directly applicable to the emerging concept of a single-layer IP network architecture.||||
54|1-2||Applying neural networks to performance estimation of embedded software|||||
54|1-2||Combinatorial performance modelling of toroidal cubes|||||
54|1-2||Design of a hardware accelerator for path planning on the Euclidean distance transform|||||
54|1-2||A Java processor architecture for embedded real-time systems|Architectural advancements in modern processor designs increase average performance with features such as pipelines, caches, branch prediction, and out-of-order execution. However, these features complicate worst-case execution time analysis and lead to very conservative estimates. JOP (Java Optimized Processor) tackles this problem from the architectural perspective – by introducing a processor architecture in which simpler and more accurate WCET analysis is more important than average case performance.||||
54|1-2||System level design of telecom systems using formal model refinement: Applying the B method/language in practice|The increasing complexity of modern telecommunication systems is one of the main issues encountered in most telecom products. Despite the plethora of methods and tools for efficient system design, verification and validation phases are still consuming significant part of the overall design time. The proposed approach outlines the use of the B method/language for producing correct-by-construction implementations of telecommunication systems. The method described is supported by appropriate tools that automate the process of proving that system properties are maintained during the various design stages. The feasibility of the latter is evaluated in practice through the design of a real world telecom application, borrowed from the domain of wireless telecommunication networks.||||
54|1-2||Improving stability for peer-to-peer multicast overlays by active measurements|The instability of the tree-like multicast overlay caused by nodes’ abrupt departures is considered as one of the major problems for peer-to-peer (P2P) multicast systems. In this paper, we present a protocol for improving the overlay’s stability by actively estimating the nodes’ lifetime model, and combining the nodes’ lifetime information with the overlay’s structural properties. We use the shifted Pareto distribution to model the nodes’ lifetimes in designing our protocol. To support this model, we have measured the residual lifetimes of the nodes in a popular IPTV system named PPLive [PPLive. http://www.pplive.com], and have formally analyzed the relationships between the distribution of the nodes’ lifetimes, ages and their residual lifetimes under the shifted Pareto distribution model. We evaluate the overlay construction strategies, which are essential in improving the overlay’s stability in our protocol, by comparing them with a number of other strategies in simulation. The experimental results indicate that our proposed protocol could improve the overlay’s stability considerably, with informative but not necessarily accurate lifetime model estimation, and with limited overhead imposed on the network as well as negligible sacrifice regarding the end-to-end service latencies for the nodes on the overlay.||||
54|1-2||DS2IS: Dictionary-based segmented inversion scheme for low power dynamic bus design|||||
54|1-2||A request distribution method for clustered VOD servers considering buffer sharing effects|||||
54|10|http://www.sciencedirect.com/science/journal/13837621/54/10|FPGA implementation of high performance elliptic curve cryptographic processor over |In this paper, we propose a high performance elliptic curve cryptographic processor over GF(2163)GF(2163), one of the five binary fields recommended by National Institute of Standards and Technology (NIST) for Elliptic Curve Digital Signature Algorithm (ECDSA). The proposed architecture is based on the López–Dahab elliptic curve point multiplication algorithm and uses Gaussian normal basis for GF(2163)GF(2163) field arithmetic. To achieve high throughput rates, we design two new word-level arithmetic units over GF(2163)GF(2163) and derive parallelized elliptic curve point doubling and point addition algorithms with uniform addressing based on the López–Dahab method. We implement our design using Xilinx XC4VLX80 FPGA device which uses 24,263 slices and has a maximum frequency of 143 MHz. Our design is roughly 4.8 times faster with two times increased hardware complexity compared with the previous hardware implementation proposed by Shu et al. Therefore, the proposed elliptic curve cryptographic processor is well suited to elliptic curve cryptosystems requiring high throughput rates such as network processors and web servers.||||
54|10||Design and implementation of a vehicle interface protocol using an IEEE 1394 network|||||
54|10||Optimum RNS sign detection algorithm using MRC-II with special moduli set|In this paper, we present a generic sign detection algorithm based on mixed radix conversion algorithm, MRC-II [M. Akkal, P. Siy, A new mixed radix conversion algorithm MRC-II, Journal of System Architecture (2006)] and also we present an optimum algorithm for sign detection based on a special moduli set where mn is even. The described algorithm requires only one comparison for sign detection. A new moduli set will also be presented which simplifies MRC-II conversion algorithm by eliminating the need for table lookup normally used in MRC hardware implementation. The algorithm does not require ROM table like other algorithms. For a moduli set of four moduli that satisfies the special moduli set conditions, 0 tables are needed to do the conversion, while Szabo and Tanaka MRC algorithm [N.S. Szabo, R.I. Tanaka, Residue Arithmetic and Its Application to Computer Technology, McGraw-Hill, New York, 1967] requires 6 tables with a total table size of 4608 bits; and Huang MRC algorithm [C.H. Huang, A fully ParallelMixed-radix conversion algorithm for residue number applications, IEEE Transactions on Computers c-32 (4) (1983)] requires 10 tables with a total table size of 3840 bits.||||
54|10||TTPM â An efficient deadlock-free algorithm for multicast communication in 2D torus networks|A torus network has become increasingly important to multicomputer design because of its many features including scalability, low bandwidth and fixed degree of nodes. A multicast communication is a significant operation in multicomputer systems and can be used to support several other collective communication operations. This paper presents an efficient algorithm, TTPM, to find a deadlock-free multicast wormhole routing in two-dimensional torus parallel machines. The introduced algorithm is designed such that messages can be sent to any number of destinations within two start-up communication phases; hence the name Torus Two Phase Multicast (TTPM) algorithm. An efficient routing function is developed and used as a basis for the introduced algorithm. Also, TTPM allows some intermediate nodes that are not in the destination set to perform multicast functions. This feature allows flexibility in multicast path selection and therefore improves the performance. Performance results of a simulation study on torus networks are discussed to compare TTPM algorithm with a previous algorithm.||||
54|10||An efficient architecture for designing reverse converters based on a general three-moduli set|In this paper, a high-speed, low-cost and efficient design of reverse converter for the general three-moduli set {2Î±, 2Î² − 1, 2Î² + 1} where Î± < Î² is presented. The simple proposed architecture consists of a carry save adder (CSA) and a modulo adder. As a result it can be efficiently implemented in VLSI circuits. The values of Î± and Î² are set in order to provide the desired dynamic range and also to obtain a balanced moduli set. Based on the above, two new moduli sets {2n+k, 22n − 1, 22n + 1} and {22n−1, 22n+1 − 1, 22n+1 + 1}, which are the special cases of the moduli set {2Î±, 2Î² − 1, 2Î² + 1} are proposed. The reverse converters for these new moduli sets are derived from the proposed general architecture with better performance compared to the other reverse converters for moduli sets with similar dynamic range.||||
54|10||PORCE: An efficient power off recovery scheme for flash memory|Flash memory is now replacing hard disk in many embedded applications including cellular phones, digital cameras, car navigation systems, and so on. However, because flash memory has its own characteristics such as “erase-before-write” and wear-leveling, a software layer called FTL (flash translation layer) should be provided. However, most FTL algorithms did not include the power off recovery module though it is very important in portable devices. In this paper, we suggest an efficient power off recovery scheme for flash memory called PORCE (Power Off Recovery sChEme for flash memory). PORCE is tightly coupled to FTL operations and minimizes performance degradation during normal operations by storing recovery information as small as possible. Additionally, PORCE provides cost-based reclamation protocols which include the wear-leveling module. Our empirical study shows that PORCE is an efficient recovery protocol.||||
54|10||Execution coordination in mobile agent-based distributed job workflow execution|Mobile agent-based distributed job workflow execution requires the use of execution coordination techniques to ensure that an agent executing a subjob can locate its predecessors’ execution results. This paper describes the classification, implementation, and evaluation of execution coordination techniques in the mobile agent-based distributed job workflow execution system. First, a classification of the existing execution coordination techniques is developed for mobile agent systems. Second, to put the discussion into perspective, our framework for mobile agent-based distributed job workflow execution over the Grid (that is, MCCF: Mobile Code Collaboration Framework) is described. How the existing coordination techniques can be applied in the MCCF is also discussed. Finally, a performance study has been conducted to evaluate three coordination techniques using real and simulated job workflows. The results are presented and discussed in the paper.||||
54|10||Modular array structure for non-restoring square root circuit|Square root is an operation performed by the hardware in recent generations of processors. The hardware implementation of the square root operation is achieved by different means. One of the popular methods is the non-restoring algorithm. In this paper, the classical non-restoring array structure is improved in order to simplify the circuit. This reduction is done by eliminating a number of circuit elements without any loss in the precision of the square root or the remainder. For a 64-bit non-restoring circuit the area of the suggested circuit is about 44% smaller than that of a conventional non-restoring array circuit. Furthermore, in order to create an environment for modular design of the non-restoring square root circuit, a number of modules are suggested. Using these modules it is possible to construct any square root circuit with an arbitrary number of input bits. The suggested methodology results in an expandable design with reduced-area. Analytical and simulation results show that the delay of the proposed circuit, for a 64-bit radicand, is 80% less than that of a conventional non-restoring array circuit.||||
54|10||Some topological and combinatorial properties of WK-recursive mesh and WK-pyramid interconnection networks|The WK-recursive mesh and WK-pyramid networks are recursively defined hierarchical interconnection networks with excellent properties which well idealize them as alternatives for mesh and traditional pyramid interconnection topologies. They have received much attention due to their favorable attributes such as small diameter, large connectivity, and high degree of scalability and expandability. In this paper, we deal with pancyclicity and surface area of these networks. These properties are of great importance in the implementation of a variety of parallel algorithms in multicomputers. We show that WK-recursive mesh network is 1-partially pancyclic, i.e. any cycle of length 3, 4, 6,…, and N can be constructed in the WK-recursive mesh. Also, we prove that the WK-pyramid is pancyclic, that is all cycles of length 3, 4, … , and N can be formed in a WK-pyramid. It is also proved that two link-disjoint Hamiltonian paths/cycles can be embedded in the WK-recursive mesh/WK-pyramid. We then study the surface area of WK-recursive mesh and WK-pyramid networks and put forth some equations for calculating the surface area in these networks.||||
54|10||Synthesis of quaternary reversible/quantum comparators|Multiple-valued quantum circuits are promising choices for future quantum computing technology, since they have several advantages over binary quantum circuits. Quaternary logic has the advantage that classical binary functions can be very easily represented as quaternary functions by grouping two bits together into quaternary values. Grover’s quantum search algorithm requires a sub-circuit called oracle, which takes a set of inputs and gives an output stating whether a given search condition is satisfied or not. Equality, less-than, and greater-than comparisons are widely used as search conditions. In this paper, we show synthesis of quaternary equality, less-than, and greater-than comparators on the top of ion-trap realizable 1-qudit gates and 2-qudit Muthukrishnan–Stroud gates.||||
54|10||A novel hardware-oriented Kohonen SOM image compression algorithm and its FPGA implementation|Kohonen self-organizing map (K-SOM) has proved to be suitable for lossy compression of digital images. The major drawback of the software implementation of this technique is its very computational intensive task. Fortunately, the structure is fairly easy to convert into hardware processing units executing in parallel. The resulting hardware system, however, consumes much of a microchip’s internal resources, i.e. slice registers and look-up table units. This results in utilising more than a single microchip to realize the structure in pure hardware implementation. Previously proposed K-SOM realizations were mainly targetted on implementing on an application specific integrated circuit (ASIC) with low restriction on resource utilization. In this paper, we propose an alternative architecture of K-SOM suitable for moderate density FPGAs with acceptable image quality and frame rate. In addition, its hardware architecture and synthesis results are presented. The proposed K-SOM algorithm compromises between the image quality, the frame rate throughput, the FPGA’s resource utilization and, additionally, the topological relationship among neural cells within the network. The architecture has been proved to be successfully synthesized on a single moderate resource FPGA with acceptable image quality and frame rate.||||
54|10||COMPASS â A tool for evaluation of compression strategies for embedded processors|A major concern of embedded system architects is the design for low power. We address one aspect of the problem in this paper, namely the effect of executable code compression. There are two benefits of code compression – firstly, a reduction in the memory footprint of embedded software, and secondly, potential reduction in memory bus traffic and power consumption. Since decompression has to be performed at run time it is achieved by hardware. We describe a tool called COMPASS which can evaluate a range of strategies for any given set of benchmarks and display compression ratios. Also, given an execution trace, it can compute the effect on bus toggles, and cache misses for a range of compression strategies. The tool is interactive and allows the user to vary a set of parameters, and observe their effect on performance. We describe an implementation of the tool and demonstrate its effectiveness. To the best of our knowledge this is the first tool proposed for such a purpose.||||
54|10||A unified architecture for a public key cryptographic coprocessor|This paper presents a unified architecture for public key cryptosystems that can support the operations of the Rivest–Shamir–Adleman cryptogram (RSA) and the elliptic curve cryptogram (ECC). A hardware solution is proposed for operations over finite fields GF(p)GF(p) and GF(2p)GF(2p). The proposed architecture presents a unified arithmetic unit which provides the functions of dual-field modular multiplication, dual-field modular addition/subtraction, and dual-field modular inversion. A new adder based on the signed-digit (SD) number representation is provided for carry-propagated and carry-less operations. The critical path of the proposed design is reduced compared with previous full adder implementation methods. Experimental results show that the proposed design can achieve a clock speed of 1 GHz using 776 K gates in a 0.09 Î¼m CMOS standard cell technology, or 150 MHz using 5227 CLBs in a Xilinx Virtex 4 FPGA. While the different technologies, platforms and standards make a definitive comparison difficult, based on the performance of our proposed design, we achieve a performance improvement of between 30% and 250% when compared with existing designs.||||
54|11|http://www.sciencedirect.com/science/journal/13837621/54/11|Editorial|||||
54|11||OpenMP-based parallelization on an MPCore multiprocessor platform â A performance and power analysis|In this contribution, the potential of parallelized software that implements algorithms of digital signal processing on a multicore processor platform is analyzed. For this purpose various digital signal processing tasks have been implemented on a prototyping platform i.e. an ARM MPCore featuring four ARM11 processor cores. In order to analyze the effect of parallelization on the resulting performance-power ratio, influencing parameters like e.g. the number of issued program threads have been studied. For parallelization issues the OpenMP programming model has been used which can be efficiently applied on C-level. In order to elaborate power efficient code also a functional and instruction level power model of the MPCore has been derived which features a high estimation accuracy. Using this power model and exploiting the capabilities of OpenMP a variety of exemplary tasks could be efficiently parallelized. The general efficiency potential of parallelization for multiprocessor architectures can be assembled.||||
54|11||Exploration methodology of dynamic data structures in multimedia and network applications for embedded platforms|In the last years, there is a trend towards network and multimedia applications to be implemented in portable devices. These applications usually contain complex dynamic data structures. The appropriate selection of the dynamic data type (DDT) combination of an application affects the performance and the energy consumption of the whole system. Thus, DDT exploration methodology is used to perform trade-offs between design factors, such as performance and energy consumption. In this paper we provide a new approach to the DDT exploration procedure, based on a new library of DDTs which remedies the limitations of an existing solution and allows the DDT optimization of a wider range of application domains. Using the new library, we performed DDT exploration in network and multimedia benchmarks and achieved performance and energy consumption improvements up to 22% and 5.8%, respectively.||||
54|11||Efficiency measures for SOC concepts|This paper discusses efficiency measures for the evaluation of high performance systems on a chip (SOC), considering a throughput rate R, chip size A, power dissipation P, and a flexibility criterion F. Based on the analysis of recently published multimedia chips, the paper shows equivalences between the ratio of R over AP, a weighted sum on 1/R, A, P, and a fuzzy multicriteria analysis on R, A, P. The paper indicates the fuzzy multicriteria analysis as generalization of the other efficiency measures, which can be easily extended to multiple cost and performance criteria. Because of the application of fuzzy set theory, the multicriteria approach supports quantitative criteria with a physical background as well as qualitative criteria by linguistic variables. Uncertainties at early conceptual phases are considered by a possibilistic measure, which is based on fuzzy set theory.||||
54|11||Improving evolutionary exploration to area-time optimization of FPGA designs|This paper presents a new methodology based on evolutionary multi-objective optimization (EMO) to synthesize multiple complex modules on reprogrammable devices. It starts from a behavioral description written in a common high-level language (for instance C) to automatically produce the register-transfer level (RTL) design in a hardware description language (e.g. Verilog). Since all high-level synthesis problems (scheduling, resource allocation and binding) are notoriously NP-complete and interdependent, these problems should be considered simultaneously. This drives to a wide design space, that needs to be thoroughly explored to obtain solutions able to satisfy the design constraints (e.g. area and performance).||||
54|11||Resource conflict detection in simulation of function unit pipelines|Processor simulators are important parts of processor design toolsets in which they are used to verify and evaluate the properties of the designed processors. While simulating architectures with independent function unit pipelines using simulation techniques that avoid the overhead of instruction bit-string interpretation, such as compiled simulation, the simulation of function unit pipelines can become one of the new bottlenecks for simulation speed. This paper evaluates several resource conflict detection models, commonly used in compiler instruction scheduling, in the context of function unit pipeline simulation. The evaluated models include the conventional reservation table based-model, the dynamic collision matrix model, and an finite state automata (FSA) based model. In addition, an improvement to the simulation initialization time by means of lazy initialization of states in the FSA-based approach is proposed. The resulting model is faster to initialize and provides comparable simulation speed to the actively initialized FSA.||||
54|11||Secure communication in microcomputer bus systems for embedded devices|The protection of the microcomputer bus system in embedded devices is essential to prevent eavesdropping and the growing number of todays hardware hacking attacks. This contribution presents a hardware solution to ensure microcomputer bus systems via the Tree Parity Machine Rekeying Architecture (TPMRA). For this purpose a scalable TPMRA IP-core is designed and implemented in order to meet adaptability, low cost terms and variable bus performance requirements. It allows the authentication of different bus participants as well as the encryption of chip-to-chip buses from a single primitive. The solution is transparent and easy applicable to an arbitrary microcomputer bus system for embedded devices on the market. A proof of concept implementation shows the applicability of the TPMRA in the standardized Advanced Microprocessor Bus Architecture (AMBA) by implementing the IP-core extension into the peripheral AMBA bus-to-bus interface. It will be shown that the proposed solution is latency free and can be easy implemented into the AMBA bus interface bridge in order to protect the ARM bus system with a low hardware overhead considering all AMBA bus features.||||
54|12|http://www.sciencedirect.com/science/journal/13837621/54/12|FPGA implementations of elliptic curve cryptography and Tate pairing over a binary field|Elliptic curve cryptography (ECC) and Tate pairing are two new types of public-key cryptographic schemes that become popular in recent years. ECC offers a smaller key size compared to traditional methods without sacrificing security level. Tate pairing is a bilinear map commonly used in identity-based cryptographic schemes. Therefore, it is more attractive to implement these schemes by using hardware than by using software because of its computational expensiveness. In this paper, we propose field programmable gate array (FPGA) implementations of the elliptic curve point multiplication in Galois field GF(2283)GF(2283) and Tate pairing computation in GF(2283)GF(2283). Experimental results demonstrate that, compared with previously proposed approaches, our FPGA implementations of ECC and Tate pairing can speed up by 31.6 times and 152 times, respectively.||||
54|12||DLL-conscious instruction fetch optimization for SMT processors|Simultaneous multithreading (SMT) processors can issue multiple instructions from distinct processes or threads in the same cycle. This technique effectively increases the overall throughput by keeping the pipeline resources more occupied at the potential expense of reducing single thread performance due to resource sharing. In the software domain, an increasing number of dynamically linked libraries (DLL) are used by applications and operating systems, providing better flexibility and modularity, and enabling code sharing. It is observed that a significant amount of execution time in software today is spent in executing standard DLL instructions, that are shared among multiple threads or processes. However, for an SMT processor with a virtually-indexed cache implementation, existing instruction fetching mechanisms can induce unnecessary false I-TLB and I-Cache misses caused by the DLL-based instructions that are intended to be shared. This problem is more prominent when multiple independent threads are executing concurrently on an SMT processor.||||
54|12||A low-complexity microprocessor design with speculative pre-execution|Current superscalar architectures strongly depend on an instruction issue queue to achieve multiple instruction issue and out-of-order execution. However, the issue queue requires a centralized structure and mainly causes globally broadcasting operations to wakeup and select the instructions. Therefore, a large issue queue ultimately results in a low clock rate along with a high circuit complexity. In other words, the increasing demands for a larger issue queue correspondingly impose a significant burden on achieving a higher clock speed.||||
54|12||A recursive method for synthesizing quantum/reversible quaternary parallel adder/subtractor with look-ahead carry|Multiple-valued quantum logic circuits are a promising choice for future quantum computing technology since they have several advantages over binary quantum logic circuits. Adder/subtractor is the major component of the ALU of a computer and is also used in quantum oracles. In this paper, we propose a recursive method of hand synthesis of reversible quaternary full-adder circuit using macro-level quaternary controlled gates built on the top of ion-trap realizable 1-qudit quantum gates and 2-qudit Muthukrishnan–Stroud quantum gates. Based on this quaternary full-adder circuit we propose a reversible circuit realizing quaternary parallel adder/subtractor with look-ahead carry. We also show the way of adapting the quaternary parallel adder/subtractor circuit to an encoded binary parallel adder/subtractor circuit by grouping two qubits together into quaternary qudit values.||||
54|12||An area-efficient VLSI implementation for programmable FIR filters based on a parameterized divide and conquer approach|In this paper, we propose an optimal VLSI implementation for a class of programmable FIR filters with binary coefficients, whose architecture is based on a parameterized divide and conquer approach. The proposed design is shown to be easily extendable to FIR filters with multibit coefficients of arbitrary sign. The area efficiency achieved in comparison to direct form realization is demonstrated by VLSI implementation examples, synthesized in TSMC 0.18-Î¼m single poly six metal layer CMOS process using state-of-art VLSI EDA tools. The possible saving in average power consumption is estimated using gate-level power analysis. Suggestions for applications and topics for further research conclude the paper.||||
54|12||Dual-mode floating-point adder architectures|Most modern microprocessors provide multiple identical functional units to increase performance. This paper presents dual-mode floating-point adder architectures that support one higher precision addition and two parallel lower precision additions. A double precision floating-point adder implemented with the improved single-path algorithm is modified to design a dual-mode double precision floating-point adder that supports both one double precision addition and two parallel single precision additions. A similar technique is used to design a dual-mode quadruple precision floating-point adder that implements the two-path algorithm. The dual-mode quadruple precision floating-point adder supports one quadruple precision and two parallel double precision additions. To estimate area and worst-case delay, double, quadruple, dual-mode double, and dual-mode quadruple precision floating-point adders are implemented in VHDL using the improved single-path and the two-path floating-point addition algorithms. The correctness of all the designs is tested and verified through extensive simulation. Synthesis results show that dual-mode double and dual-mode quadruple precision adders designed with the improved single-path algorithm require roughly 26% more area and 10% more delay than double and quadruple precision adders designed with the same algorithm. Synthesis results obtained for adders designed with the two-path algorithm show that dual-mode double and dual-mode quadruple precision adders requires 33% and 35% more area and 13% and 18% more delay than double and quadruple precision adders, respectively.||||
54|12||Design space exploration of an open-source, IP-reusable, scalable floating-point engine for embedded applications|This paper describes an open-source and highly scalable floating-point unit (FPU) for embedded systems. Our FPU is fast and efficient, due to the high parallelism of its architecture: the functional units inside the datapath can operate in parallel and independently from each other. A comparison between different versions of the FPU has been made to highlight how performance scales accordingly. Logic synthesis results show that our FPU requires 105 Kgates and runs at 400 MHz on a low-power 90 nm std-cells low-power technology, and requires 20 K Logic Elements running at 67 MHz of an Altera Stratix FPGA. The proposed FPU is supported by a software tool suite which compiles programs written using the C/C++ language. A set of DSP and 3D graphics algorithms have been benchmarked, showing that using our FPU the amount of clock cycles required to perform each algorithm is one order of magnitude smaller than what is required by its corresponding software implementation.||||
54|12||Optimizing CAM-based instruction cache designs for low-power embedded systems|Energy consumption and power dissipation are important concerns in the design of embedded systems and they will become even more crucial with finer process geometry, higher frequencies, deeper pipelines and wider issue designs. In particular, the instruction cache consumes more energy than any other processor module, especially with commonly used highly associative CAM-based implementations.||||
54|12||Unicast-based fault-tolerant multicasting in wormhole-routed hypercubes|A unicast-based fault-tolerant multicasting method is proposed for hypercubes, which can still work well when the system contains enough faults. A multicast message may be unable to reach a destination if Hamming distance between the destination and the multicast source is large enough. A multicast message fails if any one of the destinations is unreachable from the source. An effective destination ordering scheme of the destinations is proposed for one-port systems first, it is extended to all-port systems for unicast-based fault-tolerant multicasting. Unreachable destinations from the source based on the local safety information are forwarded to a reachable destination, where the multicast message can be routed reliably. Destination ordering is completed based on Hamming distance. A multiple round p-cube routing scheme is presented for a deadlock-free fault-tolerant routing for each unicast step in hypercubes, where the same virtual channel is used for each round of p-cube routing. Sufficient simulation results are presented by comparing with the previous methods.||||
54|3-4|http://www.sciencedirect.com/science/journal/13837621/54/3-4|Editorial|||||
54|3-4||Quality-driven model-based architecture synthesis for real-time embedded SoCs|The recent spectacular progress in modern microelectronics created a big stimulus towards development of embedded systems. Unfortunately, it also introduced unusual complexity which results in many serious issues that cannot be resolved without new more adequate development methods and electronic design automation tools for the system-level design. This paper discusses the problem of an efficient model-based multi-objective optimal architecture synthesis for complex hard real-time embedded systems, when using as an example a system-level architecture exploration and synthesis method that we developed.||||
54|3-4||Analyzing composability of applications on MPSoC platforms|Modern day applications require use of multi-processor systems for reasons of performance, scalability and power efficiency. As more and more applications are integrated in a single system, mapping and analyzing them on a multi-processor platform becomes a multi-dimensional problem. Each possible set of applications that can be concurrently active leads to a different use-case (also referred to as scenario) that the system has to be verified and tested for. Analyzing the feasibility and resource utilization of all possible use-cases becomes very demanding and often infeasible.||||
54|3-4||Designing efficient irregular networks for heterogeneous systems-on-chip|Networks-on-chip will serve as the central integration platform in future complex systems-on-chip (SoC) designs, composed of a large number of heterogeneous processing resources. Most researchers advocate the use of traditional regular networks like meshes, tori or trees as architectural templates which gained a high popularity in general-purpose parallel computing. However, most SoC platforms are special-purpose tailored to the domain-specific requirements of their application. They are usually built from a large diversity of heterogeneous components which communicate in a very specific, mostly irregular way.||||
54|3-4||A monitoring-aware network-on-chip design flow|Networks-on-chip (NoC) are a scalable interconnect solution for systems on chip and are rapidly becoming reality. Monitoring is a key enabler for debugging or performance analysis and quality-of-service techniques. The NoC design problem and the NoC monitoring problem cannot be treated in isolation. We propose a monitoring-aware NoC design flow able to take into account the monitoring requirements in general. We illustrate our flow with a debug driven monitoring case study of transaction monitoring. By treating the NoC design and monitoring problems in synergy, the area cost of monitoring can be limited to 3–20% in general. We also investigate run-time configuration options for the NoC monitoring system resulting in acceptable configuration times.||||
54|3-4||Resource-efficient routing and scheduling of time-constrained streaming communication on networks-on-chip|Network-on-chip-based multiprocessor systems-on-chip are considered as future embedded systems platforms. One of the steps in mapping an application onto such a parallel platform involves scheduling the communication on the network-on-chip. This paper presents different scheduling strategies that minimize resource usage by exploiting all scheduling freedom offered by networks-on-chip. It also introduces a technique to take the dynamism in applications into account when scheduling the communication of an application on the network-on-chip while minimizing the resource usage. Our experiments show that resource-utilization is improved when compared to existing techniques.||||
54|3-4||Deadlock free routing algorithms for irregular mesh topology NoC systems with rectangular regions|The simplicity of regular mesh topology Network on Chip (NoC) architecture leads to reductions in design time and manufacturing cost. A weakness of the regular shaped architecture is its inability to efficiently support cores of different sizes. A proposed way in literature to deal with this is to utilize the region concept, which helps to accommodate cores larger than the tile size in mesh topology NoC architectures. Region concept offers many new opportunities for NoC design, as well as provides new design issues and challenges. One of the most important among these is the design of an efficient deadlock free routing algorithm. Available adaptive routing algorithms developed for regular mesh topology cannot ensure freedom from deadlocks. In this paper, we list and discuss many new design issues which need to be handled for designing NoC systems incorporating cores larger than the tile size. We also present and compare two deadlock free routing algorithms for mesh topology NoC with regions. The idea of the first algorithm is borrowed from the area of fault tolerant networks, where a network topology is rendered irregular due to faults in routers or links, and is adapted for the new context. We compare this with an algorithm designed using a methodology for design of application specific routing algorithms for communication networks. The application specific routing algorithm tries to maximize adaptivity by using static and dynamic communication requirements of the application. Our study shows that the application specific routing algorithm not only provides much higher adaptivity, but also superior performance as compared to the other algorithm in all traffic cases. But this higher performance for the second algorithm comes at a higher area cost for implementing network routers.||||
54|3-4||Energy reduction through crosstalk avoidance coding in networks on chip|||||
54|3-4||Dependable design technique for system-on-chip|A technique for highly reliable digital design for two FPGAs under a processor control is presented. Two FPGAs are used in a duplex configuration system design, but better dependability parameters are obtained by the combination of totally self-checking blocks based on a parity predictor. Each FPGA can be reconfigured when a SEU fault is detected. This reconfiguration is controlled by a control unit implemented in a processor. Combinational circuit benchmarks have been considered in all our experiments and computations. All our experimental results are obtained from a XILINX FPGA implementation using EDA tools. The dependability model and dependability calculations are presented to document the improved reliability parameters.||||
54|3-4||Mixed hierarchical-functional fault models for targeting sequential cores|||||
54|5|http://www.sciencedirect.com/science/journal/13837621/54/5|Speedups from extending embedded processors with a high-performance coarse-grained reconfigurable data-path|In this paper, an embedded system that extends microprocessor cores with a high-performance coarse-grained reconfigurable data-path is introduced. The data-path have been previously introduced by the authors. It is composed by computational resources able to realize complex operations which aid in improving the performance of time critical application parts, called kernels. A compilation flow is defined for mapping high-level software descriptions to the microprocessor system. The kernel code is mapped using a properly developed mapping algorithm for the reconfigurable data-path, while the non-critical segments are executed on the microprocessor. Extensive exploration is performed by mapping four real-life applications on six different instances of the system. The results show that the speedup from executing kernels on the reconfigurable logic ranges from 6.3 to 154.3, relative to the software execution on the processor since the available processing elements of the data-path are efficiently utilized. Important overall application speedups, due to the kernels’ acceleration, have been reported for the four applications. These overall performance improvements range from 1.70 to 3.70 relative to an all-processor execution. Furthermore, the experiments show that the proposed data-path achieves faster kernels’ execution compared with other high-performance data-paths.||||
54|5||A versatile timing unit for traffic shaping, policing and charging in packet-switched networks|Timing has a key role in several traffic control functions encountered in modern packet-switched networks. In order to be effective, a timing unit must provide fine resolution, be simple to implement and scale well with the number of controlled traffic streams. This paper addresses the design, implementation and evaluation of a timing unit that can support accurate and efficient implementations of traffic shaping, policing and charging in packet-switched networks. The timing unit is implemented in hardware and, therefore, overcomes constraints associated with software-based timers. It accommodates a pool of independently-clocked timers and counters, organised in timing blocks, and, consequently, is able to support, in parallel, traffic streams with diverse timing requirements. The design supports shaping and policing through token buckets, leaky buckets and a scheme, variation of the token bucket, that aims at providing statistical quality of service guarantees by exploiting the effective rate concept. Charging is supported by dedicated counters that measure the utilization of the effective rate. The granularity of the timing unit is adjustable in run-time to adapt to changes in the rate parameters of the shaping and policing functions. The validation of the timing unit is done through the development of a prototype board consisting of programmable hardware and embedded software blocks. The temporal resolution of the timing unit and the advantages of the hardware/software co-design are experimentally evaluated.||||
54|5||Using supplier locality in power-aware interconnects and caches in chip multiprocessors|Conventional snoopy-based chip multiprocessors take an aggressive approach broadcasting snoop requests to all nodes. In addition each node checks all received requests. This approach reduces the latency of cache to cache transfer misses at the expense of increasing power. In this paper we show that a large portion of interconnect/cache transactions are redundant as many snoop requests miss in the remote nodes.||||
54|5||Task scheduling in multiprocessing systems using duplication|"Task scheduling continues to be one of the most challenging problems in both parallel and distributed computing environments. In this paper, we present a task scheduling algorithm, which uses duplication, to optimally schedule any application represented in the form of a directed acyclic graph (DAG). It has a time complexity of O(d|V|3)O(d|V|3), where â£Vâ£ represents the number of tasks and d the maximum indegree of tasks."
54|5||A multilevel partitioning approach for efficient tasks allocation in heterogeneous distributed systems|This work addresses the problem of allocating parallel application tasks to heterogeneous distributed computing resources, such as multiclusters or Grid environments. The proposed allocation scheme is based on a multilevel graph partitioning and mapping approach. The objective is to find an efficient allocation that minimizes the application completion time, subject to the specified constraints pertinent to the application and system environment. The allocation scheme consists of three phases; the clustering phase, the initial mapping phase and the refinement and remapping phase. The scheme introduces an efficient heuristic in the clustering phase for contracting (coarsening) large size application graphs to the number of processors, called the VHEM method. An initial mapping technique based on a tabu-search approach has been introduced as a basis for the process of refinement and remapping phase. The simulation study shows that the VHEM coarsening heuristic can achieve optimal or near-optimal communication, compared to the HEM method, when the ratio of the number of tasks to the number of processors exceeds a threshold value. The simulation study shows that those optimal or near-optimal VHEM-coarsened graphs have an effect of generating very efficient mappings, when they are compared to the HEM-coarsened graphs.||||
54|6|http://www.sciencedirect.com/science/journal/13837621/54/6|Editorial|||||
54|6||Model-typed component interfaces|Component based software engineering (CBSE) allows to design and develop reusable software components that can be assembled to construct software systems via well defined interfaces. However, designing such reusable components for data intensive business logic often requires heavy data transfer between components over interfaces. Static interface definitions using basic data types or structures of such lead to large interfaces susceptible to modifications. The goal of this paper is to present model-typed interfaces based on generic interface parameters, which allows to transfer complex structured data between components. Providing such generic, model-defined types (MDT) with data models specifying the parameter structure supports compatibility checks of model-typed interfaces at platform independent system design time. The methodology is described platform independently and the coherency with our system development process is discussed. Moreover, a technology mapping to IDL and the CORBA component model (CCM) is illustrated.||||
54|6||Architecting reconfigurable component-based operating systems|Dynamic reconfiguration allows modifying a system during its execution, and can be used to apply patches and updates, to implement adaptive systems, dynamic instrumentation, or to support third-party modules. Dynamic reconfiguration is important in embedded systems, where one does not necessarily have the luxury to stop a running system. While several proposals have been presented in the literature supporting dynamic reconfiguration in operating system kernels, these proposals in general hardwire a fixed reconfiguration mechanism, which may be far from optimal in certain configurations.||||
54|6||A product management challenge: Creating software product value through requirements selection|It is important for a software company to maximize value creation for a given investment. The purpose of requirements engineering activities is to add business value that is accounted for in terms of return on investment of a software product. This paper provides insight into the release planning processes used in the software industry to create software product value, by presenting three case studies. It examines how IT professionals perceive value creation through requirements engineering and how the release planning process is conducted to create software product value. It also presents to what degree the major stakeholders’ perspectives are represented in the decision-making process. Our findings show that the client and market base of the software product represents the most influential group in the decision to implement specific requirements. This is reflected both in terms of deciding the processes followed and the decision-making criteria applied when selecting requirements for the product. Furthermore, the management of software product value is dependant on the context in which the product exists. Factors, such as the maturity of the product, the marketplace in which it exists, and the development tools and methods available, influence the criteria that decide whether a requirement is included in a specific project or release.||||
54|6||A model for service-oriented communication systems|Using innovative protocols at the transport or network layer is difficult today. Even if such protocols become available, most applications are not able to utilize them because usage of TCP/IP is hard coded into the application. Service-oriented communication systems (SOCS) aim to decouple applications from lower level protocols. Therefore, a service-oriented interface between applications and the transport layer is introduced. A broker mediates transport service requests to appropriate configurations of transport service providers. A flexible and protocol independent specification schema for defining service requirements and offers is regarded as a key element for such an interface. The specification schema enables short and simple descriptions as well as detailed and sophisticated descriptions and can thus scale with information available about service providers, network status, as well as application and user requirements.||||
54|6||Authentication in stealth distributed hash tables|Most existing DHT algorithms assume that all nodes have equal capabilities. This assumption has previously been shown to be untrue in real deployments, where the heterogeneity of nodes can actually have a detrimental effect upon performance. We now acknowledge that nodes on the same overlay may also differ in terms of their trustworthiness. However, implementing and enforcing security policies in a network where all nodes are treated equally is a non-trivial task. We therefore extend our previous work on Stealth DHTs to consider the differentiation of nodes based on their trustworthiness rather than their capabilities alone.||||
54|7|http://www.sciencedirect.com/science/journal/13837621/54/7|A methodology to design arbitrary failure detectors for distributed protocols|Nowadays, there are many protocols able to cope with process crashes, but, unfortunately, a process crash represents only a particular faulty behavior. Handling tougher failures (e.g. sending omission failures, receive omission failures, arbitrary failures) is a real practical challenge due to malicious attacks or unexpected software errors. This is usually achieved either by changing, in an ad hoc manner, the code of a crash resilient protocol or by devising a new protocol from scratch. This paper proposes an alternative methodology to detect processes experiencing arbitrary failures. On this basis, it introduces the notions of liveness failure detector and safety failure detector as two independent software components. With this approach, the nature of failures experienced by processes becomes transparent to the protocol using the components. This methodology brings a few advantages: it makes possible to increase the resilience of a protocol designed in a crash failure context without changing its code by concentrating only on the design of a few well-specified components, and second, it clearly separates the task of designing the protocol from the task of detecting faulty processes, a methodological improvement. Finally, the feasibility of this approach is shown, by providing an implementation of liveness failure detectors and of safety failure detectors for two protocols: one solving the consensus, and the second solving the problem of global data computation.||||
54|7||High-performance computing of  for a vector of inputs  on Alpha and IA-64 CPUs|The modern microprocessors have become more sophisticated, the performance of software on modern architectures has grown more and more difficult to dissect and prognosticate. The execution of a program nowadays entails the complex interaction of code, compiler and processor micro-architecture. The built-in functions to compute 1/x or exp(±x)exp(±x) of math library and hardware are often incapable of achieving the challenging performance of high-performance numerical computing. To meet this demand, the current trend in constructing high-performance numerical computing for specific processors Alpha 21264 & 21364, and IA-64 has been optimized for 1/xi and exp(±xi)exp(±xi) for a vector of inputs xixi which is significantly faster than optimized library routines. A detailed deliberation of how the processor micro-architecture as well as the manual optimization techniques improve the computing performance has been developed.||||
54|7||Evaluation and optimization of a peer-to-peer video-on-demand system|Video-on-demand (VoD) is increasingly popular with internet users. However, VoD is costly due to the load placed on video servers. Peer-to-peer (P2P) techniques are an approach to alleviating server load through peer-assisted sharing. Existing studies on P2P VoD are mostly based on simulation and focus on areas such as overlay topology, but little is known about the effectiveness of P2P in a real VoD system.||||
54|7||A proposal for managing ASI fabrics|||||
54|7||A quantitative analysis of the .NET common language runtime|Microsoft’s .NET platform has been developed to simplify development of Windows applications. The execution environment at the heart of this platform is a virtual machine known as the common language runtime (or CLR). The goal of this paper is to present a comprehensive behavioral analysis of the CLR instruction set and the high level language support. This will aid in the development of a hardware implementation of the CLR, similar to techniques applied to the Java virtual machine. The pertinent data is extracted using a profiling application while executing a benchmark application. We have analyzed this data with respect to access patterns for data types, addressing modes, instruction set utilization, execution time requirements, method invocation behavior and the effects of object orientation. Conclusions and recommendations are presented that will aid in the future development of a hardware implementation.||||
54|7||Optimized reversible binary-coded decimal adders|Babu and Chowdhury [H.M.H. Babu, A.R. Chowdhury, Design of a compact reversible binary coded decimal adder circuit, Journal of Systems Architecture 52 (5) (2006) 272–282] recently proposed, in this journal, a reversible adder for binary-coded decimals. This paper corrects and optimizes their design. The optimized 1-decimal BCD full-adder, a 13 × 13 reversible logic circuit, is faster, and has lower circuit cost and less garbage bits. It can be used to build a fast reversible m-decimal BCD full-adder that has a delay of only m + 17 low-power reversible CMOS gates. For a 32-decimal (128-bit) BCD addition, the circuit delay of 49 gates is significantly lower than is the number of bits used for the BCD representation. A complete set of reversible half- and full-adders for n-bit binary numbers and m-decimal BCD numbers is presented. The results show that special-purpose design pays off in reversible logic design by drastically reducing the number of garbage bits. Specialized designs benefit from support by reversible logic synthesis. All circuit components required for optimizing the original design could also be synthesized successfully by an implementation of an existing synthesis algorithm.||||
54|7||A predecoding technique for ILP exploitation in Java processors|Java processors have been introduced to offer hardware acceleration for Java applications. They execute Java bytecodes directly in hardware. However, the stack nature of the Java virtual machine instruction set imposes a limitation on the achievable execution performance. In order to exploit instruction level parallelism and allow out of order execution, we must remove the stack completely. This can be achieved by recursive stack folding algorithms, such as OPEX, which dynamically transform groups of Java bytecodes to RISC like instructions. However, the decoding throughputs that are obtained are limited. In this paper, we explore microarchitectural techniques to improve the decoding throughput of Java processors. Our techniques are based on the use of a predecoded cache to store the folding results, so that it could be reused. The ultimate goal is to exploit every possible instruction level parallelism in Java programs by having a superscalar out of order core in the backend being fed at a sustainable rate. With the use of a predecoded cache of 2 × 2048 entries and a 4-way superscalar core we have from 4.8 to 18.3 times better performance than an architecture employing pattern based folding.||||
54|8|http://www.sciencedirect.com/science/journal/13837621/54/8|Cost-driven repair optimization of reconfigurable nanowire crossbar systems with clustered defects|With the recent development of nanoscale materials and assembly techniques, it is envisioned to build high-density reconfigurable systems which have never been achieved by the photolithography. Various reconfigurable architectures have been proposed based on nanowire crossbar structure as the primitive building block. Unfortunately, high-density systems consisting of nanometer-scale elements are likely to have many imperfections and variations; thus, defect tolerance is considered as one of the most exigent challenges. In this paper, we evaluate three different logic mapping algorithms with defect tolerance to circumvent clustered defective crosspoints in nanowire reconfigurable crossbar architectures. The effectiveness of inherited redundancy and configurability utilization is demonstrated through extensive parametric simulations. Then, costs associated with the repair process are analyzed and a method to find the most cost-effective repair solution is presented.||||
54|8||Enhanced-functionality multipliers|High-speed arithmetic units in modern processors are expected to support multiplication operations with integers, fractions, and floating-point numbers. This paper presents hardware designs that can perform three modes of multiplication: (1) A double-width multiplication that returns a 2n2n-bit product. (2) A single-width integer multiplication that returns the n least-significant product bits and an overflow signal. (3) A truncated-fractional multiplication that returns the n most-significant product bits. The presented multipliers achieve up to 50% reduced power dissipation in integer and truncated-fractional multiplication modes of operation. For 16-bit or greater operand sizes the enhanced-functionality multiplier (EFM) designs use less than 10% more hardware compared to the conventional multipliers.||||
54|8||A unified fault-tolerant routing scheme for a class of cluster networks|||||
54|8||A software defined approach for common baseband processing|We present a novel software defined approach for designing and implementing common baseband processing tasks. Our focus is on exploring the algorithmic and architectural design spaces of 3G and 4G systems to identify the computational and geometric structures shared by diverse coding schemes, services and hardware platforms, and the efficient and flexible integration of these structures on innovative extensible hardware. With an existing protocol processor design framework as our starting point, we add flexibility to the physical layer of the radio application domain by defining a methodology and a hardware platform for designing programmable open wireless architecture-enabled device instances. The proposed methodology executes in two phases: (a) initial design, which is done to a single standard using our design principles and methods, and (b) extension phase where the system upgrade is done component by component. The approach standardizes control structures, component abstractions, implementation of the architecture itself as well as methods for interactive optimization. Thus, in both design phases there is only a need to consider changes in component functionality and connectivity. We demonstrate the approach by initially targeting digital television, and then extending the system with minimal effort to support GSM. The costs of the GSM extension in the system were an area increase of 2.4%, a power increase of 2.7% and four days in hardware design and verification.||||
54|8||Exploring the performance impact of stripe size on network attached storage systems|Network attached storage (NAS) integrates redundant array of independent disks (RAID) subsystem that consists of multiple disk drives to aggregate storage capacity, I/O performance and reliability based on data striping and distribution. Traditionally, the stripe size is an important parameter that has a great influence on the RAID subsystem performance, whereas the performance impact has been changed due to the development of disk drive technologies and some I/O optimization methods. Based on disk drive access time, this paper constructs a performance analysis model to exploit the impact of some I/O optimization approaches including sub-commands combination, storage interface augment, and I/O scatter/gather on the stripe size of NAS. The analysis results and experimental validation indicate that due to the evolution of hardware and software, the stripe size has a negligible performance impact on NAS when the disk drives involved are organized in a RAID0 pattern.||||
54|8||On the effectiveness of phase based regression models to trade power and performance using dynamic processor adaptation|Microarchitecture optimizations, in general, exploit the gross program behavior for performance improvement. Programs may be viewed as consisting of different “phases” which are characterized by variation in a number of processor performance metrics. Previous studies have shown that many of the performance metrics remain nearly constant within a “phase”. Thus, the change in program “phases” may be identified by observing the change in the values of these metrics. This paper aims to exploit the time varying behavior of programs for processor adaptation. Since the resource usage is not uniform across all program “phases”, the processor operates at varying levels of underutilization. During phases of low available Instruction Level Parallelism (ILP), resources may not be fully utilized while in other phases, more resources may be required to exploit all the available ILP. Thus, dynamically scaling the resources based on program behavior is an attractive mechanism for power–performance trade-off. In this paper we develop per-phase regression models to exploit the phase behavior of programs and adequately allocate resources for a target power–performance trade-off. Modeling processor performance–power using such a regression model is an efficient method to evaluate an architectural optimization quickly and accurately. We also show that the per-phase regression model is better suited than an “unified” regression model that does not use phase information. Further, we describe a methodology to allocate processor resources dynamically by using regression models which are developed at runtime. Our simulation results indicate that average energy savings of 20% can be achieved with respect to a maximally configured system with negligible impact on performance for most of the SPEC-CPU and MEDIA benchmarks.||||
54|8||Memory hierarchy performance measurement of commercial dual-core desktop processors|As chip multiprocessor (CMP) has become the mainstream in processor architectures, Intel and AMD have introduced their dual-core processors. In this paper, performance measurement on an Intel Core 2 Duo, an Intel Pentium D and an AMD Athlon 64 × 2 processor are reported. According to the design specifications, key derivations exist in the critical memory hierarchy architecture among these dual-core processors. In addition to the overall execution time and throughput measurement using both multi-program-med and multi-threaded workloads, this paper provides detailed analysis on the memory hierarchy performance and on the performance scalability between single and dual cores. Our results indicate that for better performance and scalability, it is important to have (1) fast cache-to-cache communication, (2) large L2 or shared capacity, (3) fast L2 to core latency, and (4) fair cache resource sharing. Three dual-core processors that we studied have shown benefits of some of these factors, but not all of them. Core 2 Duo has the best performance for most of the workloads because of its microarchitecture features such as the shared L2 cache. Pentium D shows the worst performance in many aspects due to its technology-remap of Pentium 4 without taking the advantage of on-chip communication.||||
54|8||A BCD-based architecture for fast coordinate rotation|||||
54|9|http://www.sciencedirect.com/science/journal/13837621/54/9|Parallel, distributed and network-based processing|||||
54|9||A business process monitor for a mobile phone recharging system|Dependable (i.e. accurate and timely) monitoring is a key aspect of business process management, since it provides information which is crucial for determining the actual Quality of Service (QoS) delivered to individual parties, and for promptly handling off-plan deviations. This paper describes a business process monitor for the recharging system of a mobile phone network provider. The monitored system is currently in operation for the major mobile phone company in Italy, namely Telecom Italia Mobile (TIM). Due to the amazingly high throughput of the monitored system, meeting the performance requirements for the monitor was a challenging issue. A buffer-based implementation of the monitor system failed to meet such requirements. In this paper, we propose a stream-based architecture, which exceeds the performance requirements imposed by the monitored application. The paper provides a detailed description of the monitor system architecture, including a discussion of technology choices, and an experimental evaluation of the performance boost achieved by resorting to a streaming approach. The proposed solution also exploits grammar-based pluggable parsers for rapid and seamless integration of heterogeneous data feeds.||||
54|9||Simulation-based development of Peer-to-Peer systems with the RealPeer methodology and framework|In the process of developing Peer-to-Peer (P2P) systems, simulation has proved to be an essential tool for the evaluation of existing and conceived P2P systems. So far, in practice, there has been a clear separation between a simulation model of a P2P system and a real P2P system that operates on a real physical network. This separation hinders the transition of models to real systems and the evaluation of already deployed systems by means of simulation.||||
54|9||Failure-tolerant distributed storage with compressed (1 out-of N) codes|||||
54|9||Securing skeletal systems with limited performance penalty: The muskel experience|Algorithmic skeletons have been exploited to implement several parallel programming environments, targeting workstation clusters as well as workstation networks and computational grids. When targeting non-dedicated clusters, workstation networks and grids, security has to be taken adequately into account in order to guarantee both code and data confidentiality and integrity. However, introducing security is usually an expensive activity, both in terms of the effort required to managed security mechanisms and in terms of the time spent performing security related activities at run time.||||
54|9||An agent based platform for task distribution in virtual environments|This paper focuses on automatic mechanisms for task distribution and execution in virtual and mobile environments. In particular, the goal is the implementation of Utility Computing services that enable users to submit their source code and to have their applications executed without concerning about resource allocation, task distribution, and load-balancing.||||
54|9||Activity pre-scheduling for run-time optimization of grid workflows|The capability to support resource sharing between different organizations and high-level performance are noteworthy features of grid computing. Applications require significant design effort and complex coordination of resources to define, deploy and execute components on heterogeneous and often unknown resources. A common trend today aims at diffusing workflow management techniques to reduce the complexity of grid systems through model-driven approaches that significantly simplify application design through the composition of distributed services often belonging to different organizations. With this approach, the adoption of efficient workflow enactors becomes a key aspect to improve efficiency through run-time optimizations, so reducing the burden for the developer, who is only responsible of defining the functional aspects of complex applications since he/she has only to identify the activities that characterize the application and the causal relationships among them. This paper focuses on performance improvements of grid workflows by presenting a new pattern for workflow design that ensures activity pre-scheduling at run-time through a technique that generates fine-grained concurrency with a couple of concepts: asynchronous invocation of services and continuation of execution. The technique is implemented in a workflow enactment service that dynamically optimizes process execution with a very limited effort for application developer.||||
||||||||
volume|issue|url|title|abstract||||
55|1|http://www.sciencedirect.com/science/journal/13837621/55/1|Rapid design of area-efficient custom instructions for reconfigurable embedded processing|RISPs (Reconfigurable Instruction Set Processors) are increasingly becoming popular as they can be customized to meet design constraints. However, existing instruction set customization methodologies do not lend well for mapping custom instructions on to commercial FPGA architectures. In this paper, we propose a design exploration framework that provides for rapid identification of a reduced set of profitable custom instructions and their area costs on commercial architectures without the need for time consuming hardware synthesis process. A novel clustering strategy is used to estimate the utilization of the LUT (Look-Up Table) based FPGAs for the chosen custom instructions. Our investigations show that the area costs computations using the proposed hardware estimation technique on 20 custom instructions are shown to be within 8% of those obtained using hardware synthesis. A systematic approach has been adopted to select the most profitable custom instruction candidates. Our investigations show that this leads to notable reduction in the number of custom instructions with only marginal degradation in performance. Simulations based on domain-specific application sets from the MiBench and MediaBench benchmark suites show that on average, more than 25% area utilization efficiency (performance/area) can be achieved with the proposed technique.||||
55|1||Improving energy efficiency for flash memory based embedded applications|The JFFS2 file system for flash memory compresses files before actually writing them into flash memory. Because of this, multimedia files, for instance, which are already compressed in the application level go through an unnecessary and time-consuming compression stage and cause energy waste. Also, when reading such multimedia files, the default use of disk cache results in unnecessary main memory access, hence an energy waste, due to the low cache hit ratio. This paper presents two techniques to reduce the energy consumption of the JFFS2 flash file system for power-aware applications. One is to avoid data compression selectively when writing files, and the other is to bypass the page caching when reading sequential files. The modified file system is implemented on a PDA running Linux and the experiment results show that the proposed mechanism effectively reduces the overall energy consumption when accessing continuous and large files.||||
55|1||Protocol offload analysis by simulation|In the last years, diverse network interface designs have been proposed to cope with the link bandwidth increase that is shifting the communication bottleneck towards the nodes in the network. The main point behind some of these network interfaces is to reach an efficient distribution of the communication overheads among the different processing units of the node, thus leaving more host CPU cycles for the applications and other operating systems tasks. Among these proposals, protocol offloading searches for an efficient use of the processing elements in the network interface card (NIC) to free the host CPU from network processing. The lack of both, conclusive experimental results about the possible benefits and a deep understanding of the behavior of these alternatives in their different parameter spaces, have caused some controversy about the usefulness of this technique.||||
55|1||A comparative evaluation of hybrid distributed shared-memory systems|Distributed Shared-Memory (DSM) systems are shared-memory multiprocessor architectures in which each processor node contains a partition of the shared memory. In hybrid DSM systems coherence among caches is maintained by a software-implemented coherence protocol relying on some hardware support. Hardware support is provided to satisfy every node hit (the common case) and software is invoked only for accesses to remote nodes.||||
55|1||Exploiting an abstract-machine-based framework in the design of a Java ILP processor|Abstract machines bridge the gap between the high-level of programming languages and the low-level mechanisms of a real machine. The paper proposed a general abstract-machine-based framework (AMBF) to build instruction level parallelism processors using the instruction tagging technique. The constructed processor may accept code written in any (abstract or real) machine instruction set, and produce tagged machine code after data conflicts are resolved. This requires the construction of a tagging unit which emulates the sequential execution of the program using tags rather than actual values. The paper presents a Java ILP processor by using the proposed framework. The Java processor takes advantage of the tagging unit to dynamically translate Java bytecode instructions to RISC-like tag-based instructions to facilitate the use of a general-purpose RISC core and enable the exploitation of instruction level parallelism. We detailed the Java ILP processor architecture and the design issues. Benchmarking of the Java processor using SpecJVM98 and Linpack has shown the overall ILP speedup improvement between 78% and 173%.||||
55|1||IP Routing table compaction and sampling schemes to enhance TCAM cache performance|Routing table lookup is an important operation in packet forwarding. This operation has a significant influence on the overall performance of the network processors. Routing tables are usually stored in main memory which has a large access time. Consequently, small fast cache memories are used to improve access time. In this paper, we propose a novel routing table compaction scheme to reduce the number of entries in the routing table. The proposed scheme has three versions. This scheme takes advantage of ternary content addressable memory (TCAM) features. Two or more routing entries are compacted into one using don’t care elements in TCAM. A small compacted routing table helps to increase cache hit rate; this in turn provides fast address lookups. We have evaluated this compaction scheme through extensive simulations involving IPv4 and IPv6 routing tables and routing traces. The original routing tables have been compacted over 60% of their original sizes. The average cache hit rate has improved by up to 15% over the original tables. We have also analyzed port errors caused by caching, and developed a new sampling technique to alleviate this problem. The simulations show that sampling is an effective scheme in port error-control without degrading cache performance.||||
55|1||Multicast communication in wormhole-routed 2D torus networks with hamiltonian cycle model|||||
55|2|http://www.sciencedirect.com/science/journal/13837621/55/2|Using age registers for a simple loadâstore queue filtering|One of the main challenges of modern processor design is the implementation of a scalable and efficient mechanism to detect memory access order violations as a result of out-of-order execution. Traditional age-ordered associative load and store queues are complex, inefficient, and power-hungry. In this paper, we introduce two new LSQ filtering mechanisms with different design tradeoffs, but both explicitly rely on timing information as a primary instrument to rule out dependence violation and enforce memory dependences. Our timing-centric design operates at a fraction of the energy cost of an associative LQ and SQ with no performance degradation.||||
55|2||Power saving and fault-tolerance in real-time critical embedded systems|In this paper, a method with the double purpose of reducing the consumption of energy and giving a deterministic guarantee on the fault tolerance of real-time embedded systems operating under the Rate Monotonic discipline is presented. A lower bound exists on the slack left free by tasks being executed at their worst-case execution time. This deterministic slack can be redistributed and used for any of the two purposes. The designer can set the trade-off point between them. In addition, more slack can be reclaimed when tasks are executed in less than their worst-case time. Fault-tolerance is achieved by using the slack to recompute the faulty task. Energy consumption is reduced by lowering the operating frequency of the processor as much as possible while meeting all time-constraints. This leads to a multifrequency method; simulations are carried out to test it versus two single frequency methods (nominal and reduced frequencies). This is done under different trade-off points and rates of faults’ occurrence. The existence of an upper bound on the overhead caused by the transition time between frequencies in Rate Monotonic scheduled real-time systems is formally proved. The method can also be applied to multicore or multiprocessor systems.||||
55|2||Dependability assessment of by-wire control systems using fault injection|This paper is focused on the validation by means of physical fault injection at pin-level of a time-triggered communication controller: the TTP/C versions C1 and C2. The controller is a commercial off-the-shelf product used in the design of by-wire systems. Drive-by-wire and fly-by-wire active safety controls aim to prevent accidents. They are considered to be of critical importance because a serious situation may directly affect user safety. Therefore, dependability assessment is vital in their design.||||
55|2||An embedded implementation of the Common Language Infrastructure|||||
55|2||A framework for low energy data management in reconfigurable multi-context architectures|In this paper, we present an approach to the problem of low energy data scheduling for reconfigurable architectures targeting digital signal processing (DSP) and multimedia applications. The main goal is the reduction of the energy consumed by these applications through the integration of the proposed data management framework within a compilation tool specifically conceived for these architectures. Two levels of on-chip data storage are assumed to be available in the reconfigurable architecture. Then, the data manager tries to optimally exploit this storage hierarchy by saving data transfers among on-chip and external memories, so reducing the energy consumption. To do that, specific algorithms for finding the data shared among the different computation kernels of the application have been developed. Also, a data placement and replacement policy has been designed. We also show how an adequate data scheduling could decrease the number of operations required to implement the dynamic reconfiguration of the system.||||
55|2||Conditional diagnosability of hypercubes under the comparison diagnosis model|Processor fault diagnosis plays an important role in multiprocessor systems for reliable computing, and the diagnosability of many well-known networks has been explored. Lai et al. proposed a novel measure of diagnosability, called conditional diagnosability, by adding an additional condition that any faulty set cannot contain all the neighbors of any vertex in a system. We make a contribution to the evaluation of diagnosability for hypercube networks under the comparison model and prove that the conditional diagnosability of n-dimensional Hypercube Qn is 3(n − 2) + 1 for n â©¾ 5. The conditional diagnosability of Qn is about three times larger than the classical diagnosability of Qn.||||
55|3|http://www.sciencedirect.com/science/journal/13837621/55/3|Guest Editorâs introduction|||||
55|3||Implementation and evaluation of a microthread architecture|Future many-core processor systems require scalable solutions that conventional architectures currently do not provide. This paper presents a novel architecture that demonstrates the required scalability. It is based on a model of computation developed in the AETHER project to provide a safe and composable approach to concurrent programming. The model supports a dynamic approach to concurrency that enables self-adaptivity in any environment so the model is quite general. It is implemented here in the instruction set of a dynamically scheduled RISC processor and many such processors form a microgrid. Binary compatibility over arbitrary clusters of such processors and an inherent scalability in both area and performance with concurrency exploited make this a very promising development for the era of many-core chips. This paper introduces the model, the processor and chip architecture and its emulation on a range of computational kernels. It also estimates the area of the structures required to support this model in silicon.||||
55|3||An implementation of the SANE Virtual Processor using POSIX threads|The SANE Virtual Processor (SVP) is an abstract concurrent programming model that is both deadlock free and supports efficient implementation. It is captured by the Î¼TC programming language. The work presented in this paper covers a portable implementation of this model as a C++ library on top of POSIX threads. Programs in Î¼TC can be translated to the standard C++ syntax and linked with this library to run on conventional systems. The motivation for this work was to provide an early implementation on conventional processors as well as supporting work from programming FPGA chips to Grids.||||
55|3||Coordinated management of hardware and software self-adaptivity|Self-adaptivity is the capability of a system to adapt itself dynamically to achieve its goals. Self-adaptive systems will be widely used in the future both to efficiently use system resources and to ease the management of complex systems. The frameworks for self-adaptivity developed so far usually concentrate either on self-adaptive software or on self-adaptive hardware, but not both.||||
55|3||Applying inherent capabilities of quantum-dot cellular automata to design: D flip-flop case study|Nowadays, quantum cellular automata (QCA) has been considered as the pioneer technology in next generation computer designs. QCA provides the computer computations at nano level using molecular components as computation units. Although the QCA technology provides smaller chip area and eliminates the spatial constraints than earlier CMOS technology, but different characteristics and design limitations of QCA architectures have led to essential attentions in replacement of traditional structures with QCA ones. Inherent information flow control, limited wire length, and consumed area are of such features and restrictions. In this paper, D flip-flop structure has been considered and we have proposed two new D flip-flop structures which employ the inherent capabilities of QCA in timing and data flow control, rather than ordinary replacement of CMOS elements with equivalent QCA ones. The introduced structures involve small number of cells in contrast to earlier proposed ones in presence of the same or even lower input to output delay. The proposed structures are simulated using the QCADesigner and the validity of them has been proved.||||
55|3||Exploiting selective instruction reuse and value prediction in a superscalar architecture|||||
55|3||Broadcast filtering: Snoop energy reduction in shared bus-based low-power MPSoCs|In multiprocessor system-on-a-chips (MPSoCs) that use snoop-based cache coherency protocols, a miss in the data cache triggers the broadcast of coherency request to all the remote caches, to keep all data coherent. However, the majority of these requests are unnecessary because remote caches do not have the matching blocks and so their tag lookups fail. Both the coherency requests and the tag lookups corresponding to a remote miss consume unnecessary energy.||||
55|4|http://www.sciencedirect.com/science/journal/13837621/55/4|Editorial|||||
55|4||Model-driven business process security requirement specification|Various types of security goals, such as authentication or confidentiality, can be defined as policies for service-oriented architectures, typically in a manual fashion. Therefore, we foster a model-driven transformation approach from modelled security goals in the context of process models to concrete security implementations. We argue that specific types of security goals may be expressed in a graphical fashion at the business process modelling level which in turn can be transformed into corresponding access control and security policies. In this paper we present security policy and policy constraint models. We further discuss a translation of security annotated business processes into platform specific target languages, such as XACML or AXIS2 security configurations. To demonstrate the suitability of this approach an example transformation is presented based on an annotated process.||||
55|4||Security architecture for virtual organizations of business web services|Virtual organizations (VO) temporarily aggregate resources of different domains to achieve a common goal. Web services are being positioned as the technological framework for achieving this aggregation in the context of cross-organizational business applications. Numerous architectures have been proposed for securing VOs, mostly for scientific research, such that they do not address all the requirements of business-oriented applications. This paper describes these additional requirements and proposes a novel architecture and approach to managing VO access control policies. Business users can focus on designing business processes, exposing web services and managing their VO partnerships, while the architecture supports and secures the web service interactions involved.||||
55|4||Secure web services using two-way authentication and three-party key establishment for service delivery|With the advance of web technologies, a large quantity of transactions have been processed through web services. Service Provider needs encryption via public communication channel in order that web services can be delivered to Service Requester. Such encryptions can be realized using secure session keys. Traditional approaches which can enable such transactions are based on peer-to-peer architecture or hierarchical group architecture. The former method resides on two-party communications while the latter resides on hierarchical group communications. In this paper, we will use three-party key establishment to enable secure communications for Service Requester and Service Provider. The proposed protocol supports Service Requester, Service Broker, and Service Provider with a shared secret key established among them. Compared with peer-to-peer architecture and hierarchical group architecture, our method aims at reducing communication and computation overheads.||||
55|4||Landscape-aware location-privacy protection in location-based services|Mobile network providers have developed a variety of location-based services (LBSs), such as friend-finder, point of interest services, emergency rescue and many other safety and security services. The protection of location-privacy has consequently become a key aspect to the success of LBSs, since users consider their own physical location and movements highly privacy-sensitive, and demand for solutions able to protect such an information in a variety of environments. The idea behind location-privacy protection is that the individual should be able to set the level at which the location information is released to avoid undesired exploitation by a potential attacker: one of the approaches to this problem is given by the application of spatial obfuscation techniques, actuated by a trusted agent, and consisting in artificial perturbations of the location information collected by sensing technologies, before its disclosure to third parties. In many situations, however, landscape/map information can help a third party to perform Bayesian inference over spatially obfuscated data and to refine the user’s location estimate up to a violation of the original user’s location-privacy requirements. The goal of this paper is to provide a map-dependent obfuscation procedure that enables the release of the maximum possible user’s location information, that does not lead to a violation of the original user’s location-privacy requirements, even when refined through map-based inference.||||
55|4||Qualitative trust modeling in SOA|Trust among cooperating agents is an essential precondition for every e-business transaction. It is becoming increasingly vital in service oriented architectures (SOAs), where services from various administration domains are deployed. Traditional hard security mechanisms with different techniques of authorization, access control and information security services give a solid foundation, but they fail when cooperating entities act deceitfully. Trust as a soft social security mechanism can protect against such threats and consequently improves the quality of services and reliability of service providers. This paper presents an abstract trust model that applies complementary qualitative methodology which addresses the core of trust as socio-cognitive phenomenon. The model complements existing quantitative methodologies and is applied in the web services environment that enables trust management in SOAs.||||
55|4||Enforcing role based access control model with multimedia signatures|Recently ubiquitous technology has invaded almost every aspect of the modern life. Several application domains, have integrated ubiquitous technology to make the management of resources a dynamic task. However, the need for adequate and enforced authentication and access control models to provide safe access to sensitive information remains a critical matter to address in such environments. Many security models were proposed in the literature thus few were able to provide adaptive access decisions based on the environmental changes. In this paper, we propose an approach based on our previous work [B.A. Bouna, R. Chbeir, S. Marrara, A multimedia access control language for virtual and ambient intelligence environments, In Secure Web Services (2007) 111–120] to enforce current role based access control models [M.J. Moyer, M. Ahama, Generalized role-based access control, in: Proceedings of International Conference on Distributed Computing Systems (ICDCS), Phoenix, Arizona, USA, 2001, pp. 391–398] using multimedia objects in a dynamic environment. In essence, multimedia objects tend to be complex, memory and time consuming nevertheless they provide interesting information about users and their context (user surrounding, his moves and gesture, people nearby, etc.). The idea behind our approach is to attribute to roles and permissions, multimedia signatures in which we integrate conditions based on users’ context information described using multimedia objects in order to limit role activation and the abuse of permissions in a given environment. We also describe our architecture which extends the known XACML [XACML, XACML Profile for Role Based Access Control (RBAC), <http://docs.oasis-open.org/xacml/cd-xacml-rbac-profile-01.pdf>, 2008] terminology to incorporate multimedia signatures. We provide an overview of a possible implementation of the model to illustrate how it could be valuable once integrated in an intelligent environment.||||
55|4||A security policy framework for context-aware and user preferences in e-services|In today’s dynamic and distributed markets a large spectrum of services is delivered through information and communication technologies. Emerging markets of e-services lie at the intersection of non-traditional user behaviour, and cyber-partnerships of enterprises to deliver innovative services. Current approaches to manage and control security demonstrate lacks in terms of security policy matching and integration in heterogeneous e-service environments. In this paper, we introduce a framework to support role-based access control for distributed services focusing on the integration of customer preferences. The framework aims to collect and generate policy-based security measures in cross-organisational scenarios. In addition to catering to specifications of security and business policies, the ability to integrate contextual information and user preferences make the role-based framework flexible and express a variety of access policies that provide a just-in-time permission activation.||||
55|4||Towards the homogeneous access and use of PKI solutions: Design and implementation of a WS-XKMS server|Nowadays, there exists certain important scenarios where different WS-* security related protocols and technologies are being used, such as e-commerce, resource control, or secure access to grid nodes. Additionally, most of these scenarios require the interaction with a trust management infrastructure (such as a PKI -Public Key Infrastructure-), usually to validate the digital certificates provided by communication peers belonging, in most cases, to different administrative domains. For doing this with WS-enabled technologies the W3C proposed the XKMS (XML Key Management Specification) standard a few years ago. However, few implementations exist so far of this standard, and most of them with important limitations. This paper presents an open-source WS-enabled implementation of the XKMS standard named Open XKMS, certain key scenarios where it can be used and the details of how it has been designed and implemented. This paper tries to motivate and foster the use of the XKMS standard and describe a software solution that can help to designers and developers of WS-based security scenarios.||||
55|5-6|http://www.sciencedirect.com/science/journal/13837621/55/5-6|An architectural co-synthesis algorithm for energy-aware Network-on-Chip design|Network-on-Chip (NoC) has been proposed to overcome the complex on-chip communication problem of System-on-Chip (SoC) design in deep sub-micron. A complete NoC design contains exploration on both hardware and software architectures. The hardware architecture includes the selection of Processing Elements (PEs) with multiple types and their topology. The software architecture contains allocating tasks to PEs, scheduling of tasks and their communications. To find the best hardware design for the target tasks, both hardware and software architectures need to be considered simultaneously. Previous works on NoC design have concentrated on solving only one or two design parameters at a time. In this paper, we propose a hardware–software co-synthesis algorithm for a heterogeneous NoC architecture. The design goal is to minimize energy consumption while meeting the real-time requirements commonly seen in embedded applications. The proposed algorithm is based on Simulated-Annealing (SA). To compare the solution quality and efficiency of the proposed algorithm, we also implement the branch-and-bound and iterative algorithm to solve the hardware–software co-synthesis problem of a heterogeneous NoC. With the given synthetic task sets, the experimental results show that the proposed SA-based algorithm achieves near-optimal solution in a reasonable time, while the branch-and-bound algorithm takes a very long time to find the optimal solution, and the iterative algorithm fails to achieve good solution quality. When applying the co-synthesis algorithms to a real-world application with PE library that has little variation in PE performance and energy consumption, the iterative algorithm achieves solution quality comparable to that of the proposed SA-based algorithm.||||
55|5-6||FPGA/DSP-based implementation of a high-performance multi-channel counter|A high-performance configurable multi-channel counter is presented. The system has been implemented on a small-size and low-cost Commercial-Off-The-Shelf (COTS) FPGA/DSP-based board, and features 64 input channels, a maximum counting rate of 45 MHz, and a minimum integration window (time resolution) of 24 Î¼s with a 23 b counting depth. In particular, the time resolution depends on both the selected counting bit-depth and the number of acquisition channels: indeed, with a 8 b counting depth, the time resolution reaches the value of 8 Î¼s if all the 64 input channels are enabled, whereas it lowers to 378 ns if only 2 channels are used. Thanks to its flexible architecture and performance, the system is suitable in highly demanding photon counting applications based on SPAD arrays, as well as in many other scientific experiments. Moreover, the collected counting results are both real-time processed and transmitted over a high-speed IEEE 1394 serial link. The same link is used to remotely set up and control the entire acquisition process, thus giving the system a even higher degree of flexibility. Finally, a theoretical model of general use which immediately provides the overall system performance is described. The model is then validated by the reported experimental results.||||
55|5-6||Dual-Mode Execution Environment for active network|||||
55|5-6||A survey of Flash Translation Layer|Recently, flash memory is widely adopted in embedded applications as it has several strong points, including its non-volatility, fast access speed, shock resistance, and low power consumption. However, due to its hardware characteristics, specifically its “erase-before-write” feature, it requires a software layer known as FTL (Flash Translation Layer). This paper surveys the state-of-the-art FTL software for flash memory. It defines the problems, addresses algorithms to solve them, and discusses related research issues. In addition, the paper provides performance results based on our implementation of each FTL algorithm.||||
55|5-6||Reducing message-length variations in resource-constrained embedded systems implemented using the Controller Area Network (CAN) protocol|The Controller Area Network (CAN) protocol is widely used in low-cost embedded systems. CAN uses “Non Return to Zero” (NRZ) coding and includes a bit-stuffing mechanism. Whilst providing an effective mechanism for clock synchronization, the bit-stuffing mechanism causes the CAN frame length to become (in part) a complex function of the data contents: variations in frame length can have a detrimental impact on the real-time behaviour of systems employing this protocol. In this paper, two software-based mechanisms for reducing the impact of CAN bit stuffing are considered and compared. The first approach considered is a modified version of a technique described elsewhere (e.g. Nolte et al. [T. Nolte, H.A. Hansson, C. Norström, Minimizing CAN response-time jitter by message manipulation, in: Proceedings of the Eighth IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS 2002), San Jose, California, 2002]). The second approach considered is a form of software bit stuffing (SBS). In both cases, not only the impact on message-length variations is addressed but also the implementation costs (including CPU and memory requirements) involved in creating practical implementation of each technique on a range of appropriate hardware platforms. It is concluded that the SBS technique is more effective in the reduction of message-length variations, but at the cost of an increase in CPU time and memory overheads and a reduction in the available data bandwidth. The choice of the most appropriate technique will, therefore, depend on the application requirements and the available resources.||||
55|7-9|http://www.sciencedirect.com/science/journal/13837621/55/7-9|An efficient signed digit montgomery multiplication for RSA|In this paper we present an efficient Montgomery multiplier using the signed digit number representation suitable for modular exponentiation, which is the main operation of RSA. The multiplier consists of one level of signed digit adder plus multiplexer through a precomputation. We design the multiplier with the improved signed digit adder using SAMSUNG STD 130 0.18Î¼m 1.8 V CMOS Standard Cell Library and compare to multipliers with other previous adders. The proposed modular multiplier can be applied to public key cryptosystems based on integer arithmetic such as RSA, DSA or ECC.||||
55|7-9||Computation and communication schedule optimization for data-sharing tasks on uniprocessor|||||
55|7-9||Quantitative analysis of packet-processing applications regarding architectural guidelines for network-processing-engine development|This paper presents a simulation-based profile-driven quantitative analysis of packet-processing applications. In this domain, demands for increasing the performance and the ongoing development of network protocols both call for flexible and performance-optimized engines. Based on the achieved profiling results, we introduce platform-independent analysis that locates the performance bottlenecks and architectural challenges of a packet-processing engine. Finally based on these results, we extract helpful architectural guidelines for design of a flexible and high-performance embedded processor that is optimized for packet-processing operations in high-performance and cost-sensitive network embedded applications.||||
55|7-9||Evaluating the energy consumption and the silicon area of on-chip interconnect architectures|Sophisticated on-chip interconnects using packet and circuit switching techniques were recently proposed as a solution to non-scalable shared-bus schemes currently used in Systems-on-Chip (SoCs) implementation. Different interconnect architectures have been studied and adapted for SoCs to achieve high throughput, low latency and energy consumption, and efficient silicon area. Recently, a new on-chip interconnect architecture by adapting the WK-recursive network topology structure has been introduced for SoCs. This paper analyses and compares the energy consumption and the area requirements of Wk-recursive network with five common on-chip interconnects, 2D Mesh, Ring, Spidergon, Fat-Tree and Butterfly Fat-Tree. We investigated the effects of load and traffic models and the obtained results show that the traffic models and load that ends processing elements has a direct effect on the energy consumption and area requirements. In these results, WK-recursive interconnect generally has a higher energy consumption and silicon area requirements in heavy traffic load.||||
55|7-9||Maintaining constraints of UML models in distributed collaborative environments|||||
55|7-9||A platform-based design framework for joint SW/HW multiprocessor systems design|We present P-Ware, a framework for joint software and hardware modelling and synthesis of multiprocessor embedded systems. The framework consists of (1) component-based annotated transaction-level models for joint modelling of parallel software and multiprocessor hardware, and (2) exploration-driven methodology for joint software and hardware synthesis. The methodology has the advantage of combining real-time requirements of software with efficient optimization of hardware performance. We describe and apply the methodology to synthesize a scheduler of a H264 video encoder on the Cake multiprocessor. Moreover, experiments show that the framework is scalable while achieving rapid and efficient designs.||||
||||||||
volume|issue|url|title|abstract||||
56|1|http://www.sciencedirect.com/science/journal/13837621/56/1|On reducing load/store latencies of cache accesses|Effective address calculations for load and store instructions need to compete for ALU with other instructions and hence extra latencies might be incurred to data cache accesses. Fast address generation is an approach proposed to reduce cache access latencies. This paper presents a fast address generator that can eliminate most of the effective address computations by storing computed effective addresses of previous load/store instructions in a dummy register file. Experimental results show that this fast address generator can reduce effective address computations of load and store instructions by about 74% on average for SPECint2000 benchmarks and cut the execution times by 8.5%. Furthermore, when multiple dummy register files are deployed, this fast address generator eliminates over 90% of effective address computations of load and store instructions and improves the average execution times by 9.3%.||||
56|1||An analytical performance model for the Spidergon NoC with virtual channels|The Spidergon Network-on-Chip (NoC) was proposed to address the demand for a fixed and optimized communication infrastructure for cost-effective multi-processor Systems-on-Chip (MPSoC) development. To deal with the increasing diversity in quality of service requirements of SoC applications, the performance of this architecture needs to be improved. Virtual channels have traditionally been employed to enhance the performance of the interconnect networks. In this paper, we present analytical models to evaluate the message latency and network throughput in the Spidergon NoC and investigate the effect of employing virtual channels. Results obtained through simulation experiments show that the model exhibits a good degree of accuracy in predicting average message latency under various working conditions. Moreover an FPGA implementation of the Spidergon has been developed to provide an accurate analysis of the cost of employing virtual channels in this architecture.||||
56|1||An architecture of a high-speed digital hologram generator based on FPGA|In this paper, a hardware architecture to generate a computer-generated hologram (CGH) in a real-time is proposed and implemented in FPGAs. The algorithm that generates digital hologram is reinterpreted and rearranged for higher operation speed. In order to optimize the hardware architecture and performance, the precision is analyzed using fixed-point simulation. The bit-width inside the hardware is obtained by numerical and visual precision analysis. The structure of the basic calculational unit (CGH Cell), an arrangement of these cells (CGH Kernel) to calculate a row of a hologram, and a processor (CGH Processor) with the kernels to perform the modified CGH algorithm are proposed.||||
56|1||A coarse-grain reconfigurable architecture for multimedia applications supporting subword and floating-point calculations|Signal processors exploiting ASIC acceleration suffer from sky-rocketing manufacturing costs and long design cycles. FPGA-based systems provide a programmable alternative for exploiting computation parallelism, but the flexibility they provide is not as high as in processor-oriented architectures: HDL or C-to-HDL flows still require specific expertise and a hardware knowledge background. On the other hand, the large size of the configuration bitstream and the inherent complexity of FPGA devices make their dynamic reconfiguration not a very viable approach. Coarse-grained reconfigurable architectures (CGRAs) are an appealing solution but they pose implementation problems and tend to be application specific. This paper presents a scalable CGRA which eases the implementation of algorithms on field programmable gate array (FPGA) platforms. This design option is based on two levels of programmability: it takes advantage of performance and reliability provided by state-of-the-art FPGA technology, and at the same time it provides the user with flexibility, performance and ease of reconfiguration typical of standard CGRAs. The basic cell template provides advanced features such as sub-word SIMD integer and floating-point computation capabilities, as well as saturating arithmetic. Multiple reconfiguration contexts and partial run-time reconfiguration capabilities are provided, tackling this way the problem of high reconfiguration overhead typical of FPGAs. Selected instances of the proposed architecture have been implemented on an Altera Stratix II EP2S180 FPGA. On this system, we mapped some common DSP, image processing, 3D graphics and audio compression algorithms in order to validate our approach and to demonstrate its effectiveness by benchmarking the benefits achieved.||||
56|1||A soft multi-core architecture for edge detection and data analysis of microarray images|As configurable processing advances, elements from the traditional approaches of both hardware and software development can be combined by incorporating customized, application-specific computational resources into the processor’s architecture, especially in the case of field-programmable-gate-array-based systems with soft-processors, so as to enhance the performance of embedded applications. This paper explores the use of several different microarchitectural alternatives to increase the performance of edge detection algorithms, which are of fundamental importance for the analysis of DNA microarray images. Optimized application-specific hardware modules are combined with efficient parallelized software in an embedded soft-core-based multi-processor. It is demonstrated that the performance of one common edge detection algorithm, namely Sobel, can be boosted remarkably. By exploiting the architectural extensions offered by the soft-processor, in conjunction with the execution of carefully selected application-specific instruction-set extensions on a custom-made accelerating co-processor connected to the processor core, we introduce a new approach that makes this methodology noticeably more efficient across various applications from the same domain, which are often similar in structure. With flexibility to update the processing algorithms, an improvement reaching one order of magnitude over all-software solutions could be obtained. In support of this flexibility, an effective adaptation of this approach is demonstrated which performs real-time analysis of extracted microarray data; the proposed reconfigurable multi-core prototype has been exploited with minor changes to achieve almost 5× speedup.||||
56|1||Experimental evaluation of slack management in real-time control systems: Coordinated vs. self-triggered approach|Effective slack management, i.e., management of unused computing resources, for real-time control tasks mandates to redistribute the available resources between controllers as a function of the state of the controlled plants. Slack can be allocated to control tasks to alter their rate of progress via e.g., the controllers’ period, in order to adapt their behavior to changes in the computing platform and in the environment. This paper presents an experimental evaluation of two representative slack redistribution policies for multitasking real-time control systems: “coordinated” vs. “self-triggered”. In the coordinated policy a resource manager is responsible for modifying each control task progress. Alternatively, in the self-triggered policy, each control task decides its progress. The demands that each policy poses to the computing platform are analyzed and different operating system architectures providing flexibility and adaptivity are discussed. A proof-of-concept implementation including the real-time control of three double integrator plants in the form of electronic circuits is presented, and a complete performance analysis is reported.||||
56|1||Erratum to âAccess region cache with register guided memory reference partitioningâ [Journal of Systems Architecture 55 (2009) 434â445]|||||
56|10|http://www.sciencedirect.com/science/journal/13837621/56/10|Embedded software debugging using virtual filesystem abstractions|We present a scalable technique to simplify the software debugging process for embedded systems that is based on the use of distributed filesystem abstractions. In this technique, the principal building blocks within embedded systems comprising “system-on-chip” (SoC) devices export debugging interfaces realized with filesystem abstractions. These filesystem interfaces are composed in a manner consistent with the hardware hierarchy and provide a portable interface to concurrently debug software executing in the various processing elements within the embedded system. We illustrate application of the model to support the various software debugging requirements unique to SoCs, many of which traditional methods are ill-equipped to deal with. We also present an architecture that may be used to realize distributed filesystem abstractions within resource constrained embedded architectures.||||
56|10||CoDBT: A multi-source dynamic binary translator using hardwareâsoftware collaborative techniques|For implementing a dynamic binary translation system, traditional software-based solutions suffer from significant runtime overhead and are not suitable for extra complex optimization. This paper proposes using hardware–software collaboration techniques to create an high efficient dynamic binary translation system, CoDBT, which emulates several heterogeneous ISAs (Instruction Set Architectures) on a host processor without changing to the existing processor. We analyze the major performance bottlenecks via evaluating overhead of a pure software-solution DBT. Guidelines are provided for applying a suitable hardware–software partition process to CoDBT, as are algorithms for designing hardware-based binary translator and code cache management. An intermediate instruction set is introduced to make multi-source translation more practicable and scalable. Meantime, a novel runtime profiling strategy is integrated into the infrastructure to collect program hot spots information to supporting potential future optimizations. The advantages of using co-design as an implementation approach for DBT system are assessed by several SPEC benchmarks. Our results demonstrate that significant performance improvements can be achieved with appropriate hardware support choices. CoDBT could be an efficient and cost-effective solution for situations where the usual methods of performance acceleration for dynamic binary translation are inappropriate.||||
56|10||Implementing the new Ada 2005 timing event and execution time control features on the AVR32 architecture|This paper describes how the new Ada 2005 timing event and execution time control features were implemented for the GNAT bare-board Ravenscar run-time environment on the Atmel AVR32 architecture. High accuracy for execution time measurement was achieved by accounting for the effects of interrupts and executing entries by proxy. The implementation of timing events was streamlined by using a single alarm mechanism both for timing events and waking up tasks. Test results on the overhead and accuracy of the implemented features are presented. While the implementation is for the AVR32, it may serve as a blueprint for implementations on other architectures. It is also discussed how the presented design could be transferred to other systems such as C/POSIX and RTSJ.||||
56|10||RETRACTED: Specification and verification of dynamic evolution of software architectures|||||
56|10||Performance-asymmetry-aware scheduling for Chip Multiprocessors with static core coupling|||||
56|11|http://www.sciencedirect.com/science/journal/13837621/56/11|Design flows and system architectures for adaptive computing on reconfigurable platforms|||||
56|11||Model-based platform-specific co-design methodology for dynamically partially reconfigurable systems with hardware virtualization and preemption|To facilitate the development of the dynamically partially reconfigurable system (DPRS), we propose a model-based platform-specific co-design (MPC) methodology for DPRS with hardware virtualization and preemption. For DPRS analysis and validation, a model-based verification and estimation framework is proposed to make model-driven architecture (MDA) more realistic and applicable to the DPRS design. Considering inherent characteristics of DPRS and real-time system requirements, a semi-automatic model translator converts the UML models of DPRS into timed automata models with transition urgency semantics for model checking. Furthermore, a UML-based hardware/software co-design platform (UCoP) can support the direct interaction between the UML models and the real hardware architecture. Compared to the existing estimation methods, UCoP can provide accurate and efficient platform-specific verification and estimation. We also propose a hierarchical design that consists of a hardware virtualization mechanism for dynamically linking the device nodes, kernel modules, and on-demand reconfigurable hardware functions and a hardware preemption mechanism for further increasing the utilization of hardware resources per unit time. Further, we realize a dynamically partially reconfigurable network security system (DPRNSS) to show the applicability and practicability of the MPC methodology. The DPRNSS cannot only dynamically adapt some of its hardware functions at run-time to meet different system requirements, but also determine which mechanism will be used. Our experiments also demonstrate that the hardware virtualization mechanism can save the overall system execution time up to 12.8% and the hardware preemption mechanism can reduce up to 41.3% of the time required by reconfiguration-based methods.||||
56|11||Dynamically reconfigurable dataflow architecture for high-performance digital signal processing|In this paper a dataflow architecture is introduced that maps efficiently onto multi-FPGA platforms and is composed of communication channels which can be dynamically adapted to the dataflow of the algorithm. The reconfiguration of the topology can be accomplished within a single clock cycle while DSP operations are in progress. Finally, the programmability and scalability of the proposed architecture is demonstrated by a high-performance parallel FFT implementation.||||
56|11||Fine grain pipeline architecture for high performance phase-based optical flow computation|Accurate motion analysis of real life sequences is a very active research field due to its multiple potential applications. Currently, new technologies offer us very fast and accurate sensors that provide a huge quantity of data per second. Processing these data streams is very expensive (in terms of computing power) for general purpose processors and therefore, is beyond processing capabilities of most current embedded devices. In this work, we present a specific hardware architecture that implements a robust optical flow algorithm able to process input video sequences at a high frame rate and high resolution, up to 160 fps for VGA images. We describe a superpipelined datapath of more than 85 stages (some of them configured with superscalar units able to process several data in parallel). Therefore, we have designed an intensive parallel processing engine. System speed (frames per second) produces fine optical flow estimations (by constraining the actual motion ranges between consecutive frames) and the phase-based method confers the system robustness to image noise or illumination changes. In this work, we analyze the architecture of different frame rates and input image noise levels. We compare the results with other approaches in the state of the art and validate our implementation using several hardware platforms.||||
56|11||FPGA-implementation of atan(Y/X) based on logarithmic transformation and LUT-based techniques|This paper presents an architecture for the computation of the atan(Y/X) operation suitable for broadband communications systems where a throughput between 20 and 40 MHz is required. The proposed architecture implements a division operation of two inputs by means of a logarithmic transformation, in which the division can be performed with a subtraction. A combination of non-uniform segmentation and multipartite LUT technique is proposed for the arctangent of the logarithm approximation. The architecture was implemented in a Xilinx FPGA device achieving higher throughput than the approach based on CORDIC algorithm and lower area than previous LUT-based approaches.||||
56|11||High level modeling and automated generation of heterogeneous SoC architectures with optimized custom reconfigurable cores and on-chip communication media|In this paper we propose a framework for modeling and automated generation of heterogeneous SoC architectures with emphasis on reconfigurable component integration and optimized communication media. In order to facilitate rapid development of SoC architectures, communication-centric platforms for data intensive applications, high level modeling of reconfigurable components for quick simulation and a tool for generation of complete SoC architectures is presented. Four different communication-centric platforms based on traditional bus, crossbar, hierarchical bus and novel hybrid communication media are proposed. These communication-centric platforms are proposed to cater for the different communication requirement of future SoC architectures. Multi-Standard telecommunication application is chosen as our target application domain and a case study of WiMAX is used as a real world example to demonstrate the effectiveness of our approach. A system consisting of an ARM processor, reconfigurable FFT and reconfigurable Viterbi decoder is considered with the option of system scalability for future upgrades. Behavior of system with different communication platforms is analyzed for its throughput and power characteristics with different reconfigurable scenarios to show the effectiveness of our approach.||||
56|11||NDE system for railway wheel inspection in a standard FPGA|A special NDE system built into a low-cost FPGA has been developed for detecting railway wheelflats. The system operates with the train moving at low-speed over a measuring rail. Ultrasonic surface wave pulses are sent at regular intervals and echoes are acquired and processed by the system. The variations in the round trip time-of-flight (RTOF) of the ultrasonic pulse allow to detect and quantify the flats size.||||
56|11||Efficient task scheduling for runtime reconfigurable systems|Recent research indicates the promising performance of employing reconfigurable systems to accelerate multimedia and communication applications. Nonetheless, they are yet to be widely adopted. One reason is the lack of efficient operating system support for these platforms. In this paper, we address the problem of runtime task scheduling as a main part of the operating systems. To do so, a new task replacement parameter, called Time-Improvement, is proposed for compiler assisted scheduling algorithms. In contrast with most related approach, we validate our approach using real application workload obtained from an application for multimedia test remotely taken by students. The proposed online task scheduling algorithm outperforms previous algorithms and accelerates task execution from 4% up to 20%.||||
56|11||Content-based image retrieval algorithm acceleration in a low-cost reconfigurable FPGA cluster|The SMILE project main aim is to build an efficient low-cost cluster based on FPGA boards in order to take advantage of its reconfigurable capabilities. This paper shows the cluster architecture, describing: the SMILE nodes, the high-speed communication network for the nodes and the software environment. Simulating complex applications can be very hard, therefore a SystemC model of the whole system has been designed to simplify this task and provide error-free downloading and execution of the applications in the cluster. The hardware–software co-design process involved in the architecture and SystemC design is presented as well. The SMILE cluster functionality is tested executing a real complex Content-Based Information Retrieval (CBIR) parallel application and the performance of the cluster is compared (time, power and cost) with a traditional cluster approach.||||
56|12|http://www.sciencedirect.com/science/journal/13837621/56/12|ComboFTL: Improving performance and lifespan of MLC flash memory using SLC flash buffer|Multi-level cell (MLC) flash memory has lower bit cost compared to single-level cell (SLC) flash memory. However, there are several obstacles to the wide use of MLC flash memory, including slow write performance and shorter lifespan. To improve the performance and lifespan of MLC flash memory, we propose an FTL (flash translation layer) for MLC flash memory, called ComboFTL. By exploiting the SLC mode of MLC flash memory, ComboFTL manages a small SLC region for hot data and a large MLC region for cold data. To provide the performance and lifespan similar to those of SLC flash memory, ComboFTL identifies the hotness/coldness of data effectively. It can also adjust its several policies based on workload changes. Our experimental results showed that ComboFTL improves the write performance and lifespan of MLC flash memory significantly.||||
56|12||Computing the correct Increment of Induction Pointers with application to loop unrolling|||||
56|12||Gather/scatter hardware support for accelerating Fast Fourier Transform|As we enter the multi-core era, seeking methods to boost the performance of single-threaded applications remains critical. Achieving gains in processor performance by increasing the operating frequency has begun to meet more obstacles. However, significant performance improvements can be achieved by extending the capability of the processor with the addition of hardware support, which makes much more effective use of the available transistors. This paper presents a novel hardware support called, DistTree, to speed up processor performance. The DistTree hardware automates gather and scatter operations for applications with complex but predictable memory access patterns like the Fast Fourier Transform (FFT). With this hardware support integrated with a modern microprocessor (the Alpha architecture in our experiments), the FFT performance can reap a more than twofold increase when compared against the FFTW library, a state-of-the-art implementation. The DistTree hardware support enables the processor to spend the majority of processor cycles on executing the computations of an algorithm by reducing both the arithmetic and address computation overhead. Therefore, the performance of many single-threaded applications can be significantly increased.||||
56|12||Stack filter: Reducing L1 data cache power consumption|The L1 data cache is one of the most frequently accessed structures in the processor. Because of this and its moderate size it is a major consumer of power. In order to reduce its power consumption, in this paper a small filter structure that exploits the special features of the references to the stack region is proposed. This filter, which acts as a top – non-inclusive – level of the data memory hierarchy, consists of a register set that keeps the data stored in the neighborhood of the top of the stack. Our simulation results show that using a small Stack Filter (SF) of just a few registers, 10–25% data cache power savings can be achieved on average, with a negligible performance penalty.||||
56|2-3|http://www.sciencedirect.com/science/journal/13837621/56/2-3|A scalable organization for distributed directories|Although directory-based cache-coherence protocols are the best choice when designing chip multiprocessors with tens of cores on-chip, the memory overhead introduced by the directory structure may not scale gracefully with the number of cores. Many approaches aimed at improving the scalability of directories have been proposed. However, they do not bring perfect scalability and usually reduce the directory memory overhead by compressing coherence information, which in turn results in extra unnecessary coherence messages and, therefore, wasted energy and some performance degradation. In this work, we present a distributed directory organization based on duplicate tags for tiled CMP architectures whose size is independent on the number of tiles of the system up to a certain number of tiles. We demonstrate that this number of tiles corresponds to the number of sets in the private caches. Additionally, we show that the area overhead of the proposed directory structure is 0.56% with respect to the on-chip data caches. Moreover, the proposed directory structure keeps the same information than a non-scalable full-map directory. Finally, we propose a mechanism that takes advantage of this directory organization to remove the network traffic caused by replacements. This mechanism reduces total traffic by 15% for a 16-core configuration compared to a traditional directory-based protocol.||||
56|2-3||UML-based hardware/software co-design platform for dynamically partially reconfigurable network security systems|The dynamic partial reconfiguration technology of FPGA has made it possible to adapt system functionalities at run-time to changing environment conditions. However, this new dimension of dynamic hardware reconfigurability has rendered existing CAD tools and platforms incapable of efficiently exploring the design space. As a solution, we proposed a novel UML-based hardware/software co-design platform (UCoP) targeting at dynamically partially reconfigurable network security systems (DPRNSS). Computation-intensive network security functions, implemented as reconfigurable hardware functions, can be configured on-demand into a DPRNSS at run-time. Thus, UCoP not only supports dynamic adaptation to different environment conditions, but also increases hardware resource utilization. UCoP supports design space exploration for reconfigurable systems in three folds. Firstly, it provides reusable models of typical reconfigurable systems that can be customized according to user applications. Secondly, UCoP provides a partially reconfigurable hardware task template, using which users can focus on their hardware designs without going through the full partial reconfiguration flow. Thirdly, UCoP provides direct interactions between UML system models and real reconfigurable hardware modules, thus allowing accurate time measurements. Compared to the existing lower-bound and synthesis-based estimation methods, the accurate time measurements using UCoP at a high abstraction level can more efficiently reduce the system development efforts.||||
56|2-3||Multi-level reconfigurable architectures in the switch model|||||
56|2-3||FPGA schemes for minimizing the power-throughput trade-off in executing the Advanced Encryption Standard algorithm|Today most research involving the execution of the Advanced Encryption Standard (AES) algorithm falls into three areas: ultra-high-speed encryption, very low power consumption, and algorithmic integrity. This study’s focus is on how to lower the power consumption of an FPGA-based encryption scheme with minimum effect on performance. Three novel FPGA schemes are introduced and evaluated. These schemes are compared in terms of architectural and performance differences, as well as the power consumption rates. The results show that the proposed schemes are able to reduce the logic and signal power by 60% and 27%, respectively on a Virtex 2 Pro FPGA while maintaining a high level of throughput.||||
56|2-3||A Force-Directed Scheduling based architecture generation algorithm and design tool for FPGAs|The derivation of efficient, custom architectures for implementing algorithms on Field Programmable Gate Array platforms presents several research challenges. We focus on the derivation of efficient streaming architectures from dataflow graphs, targeting multi-cycle, fully pipelined functional units. In this paper, we present a Force-Directed Scheduling based algorithm for deriving area-efficient architectures from dataflow graphs based on replication and critical path relaxation. We have implemented this algorithm in a design tool called CHARGER, which integrates schedule generation with post-schedule communications infrastructure generation and Hardware Description Language generation. We compare the performance of our algorithm against that of a traditional Force-Directed Scheduling approach by generating architectures from algorithms selected from embedded computing and scientific computing.||||
56|2-3||Composition-based Cache simulation for structure reorganization|Finding the best data layout has been an ultimate goal of memory optimization. Even with data access profile, heuristic algorithms are needed to reorganize data layout for better locality. The best layout could be found by running the given application with all possible data layouts and selecting the best performing layout. This approach, however, can incur too much overhead, particulary when the number of possible layouts are too many. In this paper, we present a composition-based cache simulation for structure reorganization. Instead of running all possible layouts, we simulate only the primary subsets of layouts and compose the cache misses for all layouts by summing up the cache misses of component subsets. Our experiment with the composition-based cache simulation shows that the differences in the cache misses are within 10% of the full cache simulation for 4-way and 8-way set associative caches. In addition to the cache miss estimation, our heuristic algorithm takes account of the extra instruction overhead incurred by structure reorganization. Our experiment with several structure intensive benchmarks shows the 37% reduction in the L1D read misses and the 28% reduction in the L2 read misses. As a result, the execution times are also reduced by 19% on average.||||
56|4-6|http://www.sciencedirect.com/science/journal/13837621/56/4-6|Improving cache locking performance of modern embedded systems via the addition of a miss table at the L2 cache level|To confer the robustness and high quality of service, modern computing architectures running real-time applications should provide high system performance and high timing predictability. Cache memory is used to improve performance by bridging the speed gap between the main memory and CPU. However, the cache introduces timing unpredictability creating serious challenges for real-time applications. Herein, we introduce a miss table (MT) based cache locking scheme at level-2 (L2) cache to further improve the timing predictability and system performance/power ratio. The MT holds information of block addresses related to the application being processed which cause most cache misses if not locked. Information in MT is used for efficient selection of the blocks to be locked and victim blocks to be replaced. This MT based approach improves timing predictability by locking important blocks with the highest number of misses inside the cache for the entire execution time. In addition, this technique decreases the average delay per task and total power consumption by reducing cache misses and avoiding unnecessary data transfers. This MT based solution is effective for both uniprocessors and multicores. We evaluate the proposed MT-based cache locking scheme by simulating an 8-core processor with 2 levels of caches using MPEG4 decoding, H.264/AVC decoding, FFT, and MI workloads. Experimental results show that in addition to improving the predictability, a reduction of 21% in mean delay per task and a reduction of 18% in total power consumption are achieved for MPEG4 (and H.264/AVC) by using MT and locking 25% of the L2. The MT results in about 5% delay and power reductions on these video applications, possibly more on applications with worse cache behavior. For the FFT and MI (and other) applications whose code fits inside the level-1 instruction (I1) cache, the mean delay per task increases only by 3% and total power consumption increases by 2% due to the addition of the MT.||||
56|4-6||A capacity sharing and stealing strategy for open real-time systems|||||
56|4-6||Shifted gray encoding to reduce instruction memory address bus switching for low-power embedded systems|Gray code bus encoding is a simple approach to reduce instruction address bus switching. It requires little encoding hardware and no additional bus lines. Our analytical study reveals that with Gray encoding the address bus switching can be reduced by nearly 50%, for long, sequentially accessed code.||||
56|4-6||A reconfigurable platform for evaluating the performance of QoS networks|Nowadays, high performance System and Local Area Networks (SAN/LAN) have to serve heterogeneous traffic consisting of information flows with different bandwidth and latency requirements. This makes it necessary to provide Quality of Service (QoS) and optimize the design of network components.||||
56|4-6||Buffer flush and address mapping scheme for flash memory solid-state disk|The flash memory solid-state disk (SSD) is emerging as a killer application for NAND flash memory due to its high performance and low power consumption. To attain high write performance, recent SSDs use an internal SDRAM write buffer and parallel architecture that uses interleaving techniques. In such architecture, coarse-grained address mapping called superblock mapping is inevitably used to exploit the parallel architecture. However, superblock mapping shows poor performance for random write requests. In this paper, we propose a novel victim block selection policy for the write buffer considering the parallel architecture of SSD. We also propose a multi-level address mapping scheme that supports small-sized write requests while utilizing the parallel architecture. Experimental results show that the proposed scheme improves the I/O performance of SSD by up to 64% compared to the existing technique.||||
56|7|http://www.sciencedirect.com/science/journal/13837621/56/7|From systems to networks on chip: A promising research area in the Hardware/Software co-design|||||
56|7||Combining mapping and partitioning exploration for NoC-based embedded systems|Networks on Chip (NoC) have emerged as the key paradigm for designing a scalable communication infrastructure for future Systems on Chip (SoC). An important issue in NoC design is how to map an application on this architecture and how to determine the hardware/software partition that satisfies the performance, cost and flexibility requirements. In this paper, we propose an approach that concurrently optimizes the mapping and the partitioning of streaming applications. The proposed approach exploits multiobjective evolutionary algorithms that are fed by execution performances scores corresponding to the evaluated mappings and partitioning ability to pipeline execution of the streaming application. As result, most promising solutions are highlighted for mapping multimedia applications onto a SoC architecture interconnecting 16 nodes through 2D-Mesh and Ring NoC.||||
56|7||Communication-aware task assignment algorithm for MPSoC using shared memory|In a Multi-Processor System-on-a-Chip (MPSoC) based on Network-on-Chip (NoC), which processes massive data in a distributed fashion, communication is concentrated on shared memory. This paper proposes an assignment algorithm that can minimize the total power consumption for data communication in executing application programs and a switch structure that can reduce communication congestion resulting from simultaneous accesses to the shared memory. The proposed assignment algorithm gives higher priority to the tasks transferring a larger amount of data to shared memory, so that these tasks can be assigned to the PEs close to shared memory. The proposed switch structure was designed to support multi-port memory, which is often used for shared memory. The ports of the proposed switch are dedicated to be connected with in/out ports of shared memory in order to increase communication bandwidth between PEs and shared memories. By adopting the proposed scheme, the congestion caused by the concentrated requests to the memory can be reduced. Experimental results show that power consumption for transferring data in High-Definition (HD) H.264 decoder, Motion-JPEG decoder, MP3 decoder and 2D Wavelet transform codes has been reduced by 23.9% on the average, when compared with the cases of applying the well-known FC, BN and SA algorithms. The area has been slightly increased by 1.7% compared to conventional NoC structures.||||
56|7||Communication-aware heuristics for run-time task mapping on NoC-based MPSoC platforms|Efficient run-time mapping of tasks onto Multiprocessor System-on-Chip (MPSoC) is very challenging especially when new tasks of other applications are also required to be supported at run-time. In this paper, we present a number of communication-aware run-time mapping heuristics for the efficient mapping of multiple applications onto an MPSoC platform in which more than one task can be supported by each processing element (PE). The proposed mapping heuristics examine the available resources prior to recommending the adjacent communicating tasks on to the same PE. In addition, the proposed heuristics give priority to the tasks of an application in close proximity so as to further minimize the communication overhead. Our investigations show that the proposed heuristics are capable of alleviating Network-on-Chip (NoC) congestion bottlenecks when compared to existing alternatives. We map tasks of applications onto an 8 × 8 NoC-based MPSoC to show that our mapping heuristics consistently leads to reduction in the total execution time, energy consumption, average channel load and latency. In particular, we show that energy savings can be up to 44% and average channel load is improved by 10% for some cases.||||
56|7||EDXY â A low cost congestion-aware routing algorithm for network-on-chips|In this paper, an adaptive routing algorithm for two-dimensional mesh network-on-chips (NoCs) is presented. The algorithm, which is based on Dynamic XY (DyXY), is called Enhanced Dynamic XY (EDXY). It is congestion-aware and more link failure tolerant compared to the DyXY algorithm. On contrary to the DyXY algorithm, it can avoid the congestion when routing from the current switch to the destination whose X position (Y position) is exactly one unit apart from the switch X position (Y position). This is achieved by adding two congestion wires (one in each direction) between each two cores which indicate the existence of congestion in a row (column). The same wires may be used to alarm a link failure in a row (column). These signals enable the routing algorithm to avoid these paths when there are other paths between the source and destination pair. To assess the latency of the proposed algorithm, uniform, transpose, hotspot, and realistic traffic profiles for packet injection are used. The simulation results reveal that EDXY can achieve lower latency compared to those of other adaptive routing algorithms across all workloads examined, with a 20% average and 30% maximum latency reduction on SPLASH-2 benchmarks running on a 49-core CMP. The area of the technique is about the same as those of the other routing algorithms.||||
56|7||CA-MPSoC: An automated design flow for predictable multi-processor architectures for multiple applications|||||
56|7||Scalable mpNoC for massively parallel systems â Design and implementation on FPGA|The high chip-level integration enables the implementation of large-scale parallel processing architectures with 64 and more processing nodes on a single chip or on an FPGA device. These parallel systems require a cost-effective yet high-performance interconnection scheme to provide the needed communications between processors. The massively parallel Network on Chip (mpNoC) was proposed to address the demand for parallel irregular communications for massively parallel processing System on Chip (mppSoC). Targeting FPGA-based design, an efficient mpNoC low level RTL implementation is proposed taking into account design constraints. The proposed network is designed as an FPGA based Intellectual Property (IP) able to be configured in different communication modes. It can communicate between processors and also perform parallel I/O data transfer which is clearly a key issue in an SIMD system. The mpNoC RTL implementation presents good performances in terms of area, throughput and power consumption which are important metrics targeting an on chip implementation. mpNoC is a flexible architecture that is suitable for use in FPGA-based parallel systems. This paper introduces the basic mppSoC architecture. It mainly focuses on the mpNoC flexible IP based design and its implementation on FPGA. The integration of mpNoC in mppSoC is also described. Implementation results on a Stratix II FPGA device are given for three data-parallel applications ran on mppSoC. The obtained good performances justify the effectiveness of the proposed parallel network. It is shown that the mpNoC is a lightweight parallel network making it suitable for both small as well as large FPGA-based parallel systems.||||
56|7||Reconfigurable Networks on Chip: DRNoC architecture|To cover the complexity of future systems, where thousands and hundreds of heterogeneous cores have to be interconnected, new on-chip communication solutions are being searched. In this context, Networks on Chip (NoCs) have been studied as bus alternative. However, the inclusion of NoCs’ broad design space increases even more the complexity of design flows. Additionally, today electronic industry demands drastic time to market reduction and improved device diversity. On the other side, reconfigurable devices have had an impressive evolution and now, they are complex heterogeneous platforms which include a broad variety of embedded cores. Furthermore, today it is possible to embed reconfigurable arrays in application-specific integrated circuits and thus create highly flexible systems. These tendencies provide support to the idea of reconfigurable on-chip communication, which can reduce the system design time and permit to adapt their characteristics to currently running applications. This paper overviews some reconfigurable NoCs’ state of the art solutions and describes a reconfigurable on-chip communication approach, called DRNoC, which explores the highest possible flexibility and is not limited to NoCs. An important aspect considered in this paper is real implementations and therefore, all the solutions discussed along it, including DRNoC, have been validated on FPGAs. The paper also highlights some technology restrictions of currently available FPGA reconfiguration techniques that do not permit to test real-live examples on such systems.||||
56|8|http://www.sciencedirect.com/science/journal/13837621/56/8|Recent advances in Hardware/Software co-design|||||
56|8||Efficient architectures for 3D HWT using dynamic partial reconfiguration|This paper presents the design and implementation of three dimensional (3D) Haar wavelet transform (HWT) with transpose based computation and dynamic partial reconfiguration (DPR) mechanism on field programmable gate array (FPGA). Due to the separability property of the multi-dimensional HWT, the proposed architecture has been implemented using a cascade of three N  -point one dimensional (1D) HWT and two transpose memories for a 3D volume of N×N×NN×N×N suitable for real-time 3D medical imaging applications. These applications require continuous hardware servicing, hence DPR has been introduced. Two architectures were synthesised using VHDL and implemented on Xilinx Virtex-5 FPGAs. Experimental results and comparisons between different configurations using partial and non-partial reconfiguration processes and a detailed performance analysis of the area, power consumption and maximum frequency are analysed in this paper.||||
56|8||Hardware/software co-design of a real-time kernel based tracking system|The probabilistic visual tracking methods using color histograms have been proven to be robust to target model variations and background illumination changes as shown by the recent research. However, the required computational cost is high due to intensive image data processing. The embedded solution of such algorithms become challenging due to high computational power demand and algorithm complexity. This paper presents a hardware/software co-design architecture for implementation of the well-known kernel based mean shift tracking algorithm. The design uses color histogram of the target as tracking feature. The target is searched in the consecutive images by maximizing the statistical match of the color distributions. The target localization is based on gradient based iterative search instead of exhaustive search which makes the system capable of achieving frame rate up to hundreds of frames per second while tracking multiple targets. The design, which is fully standalone, is implemented on a low-cost medium-size field programmable gate array (FPGA) device. The hardware cost of the design is compared with some other tracking systems. The performance of the system in terms of speed is evaluated and compared with the software based implementation. It is expected that the proposed solution will find its utility in applications like embedded automatic video surveillance systems.||||
56|8||Towards real time implementation of reconstructive signal processing algorithms using systolic arrays coprocessors|Reconstructive signal processing algorithms encompass a broad spectrum of computational methods. Fortunately, most of the methods fall into the classes of the matrix algebraic calculations, convolution, or transform type algorithms. These algorithms possess common properties such as regularity, locality and recursiveness. Considering such general class of reconstructive signal processing (SP) techniques, in this paper we propose a new Hardware/Software (HW/SW) co-design paradigm for the implementation of reconstructive SP algorithms via efficient systolic arrays integrated as digital SP coprocessors units. In particular, the selected matrix–matrix and matrix–vector multiplication algorithms are implemented in a systolic computing fashion that meets the real time SP system requirements when employing the developed Hardware/Software Co-Design method oriented at the use of a Xilinx Field Programmable Gate Array (FPGA) XC4VSX35-10ff668.||||
56|8||Selecting profitable custom instructions for reconfigurable processors|||||
56|8||Transition-aware DVS algorithm for real-time systems using tree structure analysis|||||
56|8||Two versions of architectures for dynamic implied addressing mode|The complexity of today’s embedded applications increases with various requirements such as execution time, code size or power consumption. To satisfy these requirements for performance, efficient instruction set design is one of the important issues because an instruction customized for specific applications can make better performance than multiple instructions in aspect of fast execution time, decrease of code size, and low power consumption. Limited encoding space, however, does not allow adding application specific and complex instructions freely to the instruction set architecture. To resolve this problem, conventional architectures increases free space for encoding by trimming excessive bits required beyond the fixed word length. This approach however shows severe weakness in terms of the complexity of compiler, code size and execution time. In this paper, we propose a new instruction encoding scheme based on the dynamic implied addressing mode (DIAM) to resolve limited encoding space and side-effect by trimming. We report our two versions of architectures to support our DIAM-based approach. In the first version, we use a special on-chip memory to store extra encoding information. In the second version, we replace the memory by a small on-chip buffer along with a special instruction. We also suggest a code generation algorithm to fully utilize DIAM. In our experiment, the architecture augmented with DIAM shows about 8% code size reduction and 18% speed up on average, as compared to the basic architecture without DIAM.||||
56|8||Hardware transactional memory: A high performance parallel programming model|||||
56|8||Hardware/software support for adaptive work-stealing in on-chip multiprocessor|During the past few years, embedded digital systems have been requested to provide a huge amount of processing power and functionality. A very likely foreseeable step to pursue this computational and flexibility trend is the generalization of on-chip multiprocessor platforms (MPSoC). In that context, choosing a programming model and providing optimized hardware support to it on these platforms is a challenging task. To deal in a portable way with MPSoCs having a different number of processors running possibly at different frequencies, work-stealing (WS) based parallelization is a current research trend.||||
56|9|http://www.sciencedirect.com/science/journal/13837621/56/9|Open-architecture system based on a reconfigurable hardwareâsoftware multi-agent platform for CNC machines|New generation of manufacturing systems endows their intelligence and reconfigurability to the computerized numerical controller (CNC) machines. This paper presents an open-architecture platform based on multi-agent hardware–software units, by developing a novel Multi-Agent Distributed CONtroller (MADCON) system. This system intends to fulfill the requirements of reconfigurability for the next generation of intelligent machines. The design of intelligent drives for this system follows a hardware–software co-design approach using a simple and intuitive structure. The hardware units of the proposed system integrate control and monitoring functions providing an FPGA-based open architecture for reconfigurable applications. On the other hand, software components were developed utilizing the XML structure for system description files, gathering features like a flowchart descriptive language and a graphic user-interface. MADCON was applied to a retrofitted to CNC lathe for control and monitoring in order to validate the proposed architecture towards the development of new generation intelligent manufacturing systems.||||
56|9||Compressed tag architecture for low-power embedded cache systems|Processors in embedded systems mostly employ cache architectures in order to alleviate the access latency gap between processors and memory systems. Caches in embedded systems usually occupy a major fraction of the implemented chip area. The power dissipation of cache system thus constitutes a significant fraction of the power dissipated by the entire processor in embedded systems. In this paper, we propose the compressed tag architecture to reduce the power dissipation of the tag store in cache systems. We introduce a new tag-matching mechanism by using a locality buffer and a tag compression technique. The main power reduction feature of our proposal is the use of small tag space matching instead of full tag matching, with modest additional hardware costs. The simulation results show that the proposed model provides a power and energy-delay product reduction of up to 27.8% and 26.5%, respectively, while still providing a comparable level of system performance to regular cache systems.||||
56|9||Exploiting address compression and heterogeneous interconnects for efficient message management in tiled CMPs|High performance processor designs have evolved toward architectures that integrate multiple processing cores on the same chip. As the number of cores inside a Chip MultiProcessor (CMP) increases, the interconnection network will have significant impact on both overall performance and energy consumption as previous studies have shown. Moreover, wires used in such interconnect can be designed with varying latency, bandwidth and power characteristics. In this work, we show how messages can be efficiently managed in tiled CMP, from the point of view of both performance and energy, by combining both address compression with a heterogeneous interconnect. In particular, our proposal is based on applying an address compression scheme that dynamically compresses the addresses within coherence messages allowing for a significant area slack. The arising area is exploited for wire latency improvement by using a heterogeneous interconnection network comprised of a small set of very-low-latency wires for critical short-messages in addition to baseline wires. Detailed simulations of a 16-core CMP show that our proposal obtains average improvements of 10% in execution time and 38% in the energy-delay2 product of the interconnect. Additionally, the sensitivity analysis shows that our proposal performs well when either OoO cores or caches with higher latencies are considered.||||
56|9||Multi-port abstraction layer for FPGA intensive memory exploitation applications|We describe an efficient, high-level abstraction, multi-port memory-control unit (MCU) capable of providing data at maximum throughput. This MCU has been developed to take full advantage of FPGA parallelism. Multiple parallel processing entities are possible in modern FPGA devices, but this parallelism is lost when they try to access external memories. To address the problem of multiple entities accessing shared data we propose an architecture with multiple abstract access ports (AAPs) to access one external memory. Bearing in mind that hardware designs in FPGA technology are generally slower than memory chips, it is feasible to build a memory access scheduler by using a suitable arbitration scheme based on a fast memory controller with AAPs running at slower frequencies. In this way, multiple processing units connected through the AAPs can make memory transactions at their slower frequencies and the memory access scheduler can serve all these transactions at the same time by taking full advantage of the memory bandwidth.||||
56|9||Quarter Load Threshold (QLT) flow control for wormhole switching in mesh-based Network-on-Chip|The fact that latency increases abruptly once the on-chip network is saturated indicates that it is necessary to devise an effective flow control strategy. Through tracing the status of the network buffer space we found that the payload of the on-chip network cannot get beyond an upper bound to avoid vicious congestion. Specifically, quarter of the total network buffer space is such a threshold, which is termed Quarter Load Threshold (QLT). Based on this fact we present the Quarter Load Threshold (QLT) flow control strategy. The performance of the proposed strategy is evaluated by the open source simulator Noxim [Noxim: Network-on-Chip Simulator, http://sourceforge.net/projects/noxim, 2008]. Simulation results show that the on-chip network runs smoothly and no serious congestion is encountered any more.||||
56|9||Online adaptive utilization control for real-time embedded multiprocessor systems|Many embedded systems have stringent real-time constraints. An effective technique for meeting real-time constraints is to keep the processor utilization on each node at or below the schedulable utilization bound, even though each task’s actual execution time may have large uncertainties and deviate a lot from its estimated value. Recently, researchers have proposed solutions based on Model Predictive Control (MPC) for the utilization control problem. Although these approaches can handle a limited range of execution time estimation errors, the system may suffer performance deterioration or even become unstable with large estimation errors. In this paper, we present two online adaptive optimal control techniques, one is based on Recursive Least Squares (RLS) based model identification plus Linear Quadratic (LQ) optimal controller; the other one is based on Adaptive Critic Design (ACD). Simulation experiments demonstrate both the LQ optimal controller and ACD-based controller have better performance than the MPC-based controller and the ACD-based controller has the smallest aggregate tracking errors.||||
56|9||Finding the best compromise in compiling compound loops to Verilog|||||
||||||||
volume|issue|url|title|abstract||||
57|1|http://www.sciencedirect.com/science/journal/13837621/57/1|Special issue on: On-chip parallel and network-based systems|||||
57|1||Hierarchical opto-electrical on-chip network for future multiprocessor architectures|Importance of power dissipation in NoCs, along with power reduction capability of on-chip optical interconnects, offers optical network-on-chip as a new technology solution for on-chip interconnects. In this paper, we extract analytical models for data transmission delay, power consumption, and energy dissipation of optical and traditional NoCs. Utilizing extracted models, we compare optical NoC with electrical one and calculate lower bound limit on the optical link length below which optical on-chip network loses its efficiency. Based on this constraint, we propose a novel hierarchical on-chip network architecture, named as H2NoC, which benefits from optical transmissions in large scale SoCs and overcomes the scalability problem resulted from lower bound limit on the optical link length. Performing a series of simulation-based experiments, we study efficiency of H2NoC along with its power and energy consumption and data transmission delay. Furthermore, the impact of network size, traffic pattern, and packet size distribution on the prominence of the proposed architecture over traditional NoC and non-hierarchical ONoC is addressed in this paper. Our experimental results verify that the proposed hierarchical architecture outperforms non-hierarchical ONoC for moderate and large scale MPSoCs, while its prominence degrades for small number of processing cores.||||
57|1||Area and power-efficient innovative congestion-aware Network-on-Chip architecture|This paper proposes a novel Network-on-Chip architecture that not only enhances network transmission performance while maintaining a feasible implementation cost, but also provides a power-efficient solution for interconnection network scenarios. Diagonally-linked mesh NoC that uses wormhole packet switching technique implements a high-performance NoC platform to meet both cost and power consumption requirements. The proposed architecture uses an adaptive quasi-minimal routing algorithm so that it can improve average latency and saturation traffic load owing to its flexibility and adaptiveness. Based on these features, a congestion-aware routing algorithm is proposed to balance traffic load so as to alleviate congestion caused by high throughput network activities. Simulation results show that saturation load is improved dramatically for various traffic patterns. Implementation results also show that employing diagonal links is a more area-efficient method for improving network performance than using large buffers. It is shown that congestion-aware router requires negligible cost overhead but provides better throughput. Finally, simulation results also reveal that power consumption in the proposed architecture outperforms traditional mesh networks.||||
57|1||Modeling and evaluation of ring-based interconnects for Network-on-Chip|A popular network topology for Network-on-Chip (NoC) implementations is the two-dimensional mesh, which has its drawbacks in the communication latency scalability, and the concentration of the traffic in the center of the mesh. In this paper, we consider the addition of simple and hierarchical rings to the mesh network. We propose several composite topologies that use the ring networks to reduce hop counts and latencies of global (long distance) traffic. Furthermore, we study two alternative ring architectures. The first is a slotted ring architecture that is suitable for NoC implementation due to its simplicity and low cost. The second uses wormhole routing and virtual channels, providing increased flexibility and better performance. Simulation results show that the composite architectures decrease the latencies and hop counts incurred by global traffic, thereby validating our claim that the use of hierarchical rings for global routing can in fact increase the scalability of the normal mesh network used for NoC implementations.||||
57|1||Investigation of transient fault effects in synchronous and asynchronous Network on Chip router|This paper presents comparison of transient fault effects in an asynchronous NoC router and a synchronous one. The experiment is based on simulation-based fault injection method to assess the fault-tolerant behavior of both architectures. The effort has been accomplished by employing fault injector signal (FIS) in asynchronous design and synchronous one. Different fault models such as Crosstalk, SEU, and SET have been applied in both architectures to evaluate their robustness. Glitch fault model has also been injected through the asynchronous scheme. The experimental results have been considered in different aspects to estimate the NoC router’s robustness. Although asynchronous designs seems inherently fault-tolerant due to applying handshaking signals, up to 55% of the injected faults result in failure, and about 44% of injected faults are replaced by new values before turning into errors. Less than 1% of injected faults treated as latent error. Moreover, the failure rate of token generation is higher than token consumption effects. Furthermore, experiments show that asynchronous NoC router is more robust than the synchronous one by preventing the fault propagation.||||
57|1||New heuristic algorithms for energy aware application mapping and routing on mesh-based NoCs|Ever shrinking technologies in VLSI era made it possible to place several IP (Intellectual Property) blocks onto a single die. This technology improvement also brought the challenge of inventing new communication methods since traditional bus-based systems suffer from signal propagation delays, signal integrity, and scalability on these large platforms. Network-on-Chip (NoC) is the biggest step towards the solution of this communication bottleneck of System-on-Chip (SoC) architectures. Topology selection is the very first step of designing NoC systems and mesh topology is the commonly accepted topology type for NoCs. However, mapping applications represented by the weighted task graphs onto the mesh architectures is an NP-hard problem. In this paper, we present a new low complexity heuristic algorithm, CastNet, for the application mapping and bandwidth constrained routing algorithm for mesh-based NoC architectures aiming to minimize the energy consumption. We compared the presented approach with the one that generates the optimal solutions and two existing heuristic methods. Our experiments on multimedia benchmarks show that the proposed mapping algorithm obtains optimal or, in worst case, within 6% to the optimal solutions in very short times.||||
57|1||Customized computer-aided application mapping on NoC infrastructure using multi-objective optimization|||||
57|1||Network-on-Chip interconnect for pairing-based cryptographic IP cores|On-chip data traffic in cryptographic circuits often consists of very long words or large groups of smaller words exchanged between processing elements. The resulting wide cross-chip buses exhibit power, congestion and scalability problems. In this paper, two case study cryptographic IP cores with demanding interconnect requirements are implemented on 65 nm CMOS. Lightweight, custom bus-replacement Networks-on-Chip (NoCs) have been developed for both cores. Results show that eliminating the 251-bit-wide cross-chip cryptographic buses dramatically improves the quality of physical implementation. The results have applicability to wire-constrained designs in other domains.||||
57|1||A generic adaptive path-based routing method for MPSoCs|Several unicast routing protocols have been presented for unicast traffic in MPSoCs. Exploiting the unicast routing algorithms for multicast traffic increases the likelihood of deadlock and congestion. In order to avoid deadlock for multicast traffic, the Hamiltonian path strategy was introduced. The traditional Hamiltonian path routing protocols supporting both unicast and multicast traffic are based on deterministic models, leading to lower performance. In this paper, we propose an adaptive routing protocol for both unicast and multicast traffic without using virtual channels. The proposed method maximizes the degree of adaptiveness of the routing functions which are based on the Hamiltonian path while guaranteeing deadlock freedom. Furthermore, both unicast and multicast aspects of the presented method have been widely investigated separately. Results obtained in both synthetic and real traffic models show that the proposed adaptive method for multicast and unicast aspects has lower latency and power dissipation compared to previously proposed path-based multicasting algorithms with negligible hardware overhead.||||
57|1||Efficient dynamic program monitoring on multi-core systems|Dynamic program execution monitors allow programmers to observe and verify an application while it is running. Instrumentation-based dynamic program monitors often incur significant performance overhead due to instrumentation. Special hardware supports have been proposed to reduce this overhead. However, most of these supports often target specific monitoring requirements, and thus have limited applicability. Recently, with multi-core systems becoming mainstream, executing the monitored program and the monitor simultaneously on separate cores has emerged as an attractive option. However, due to the large amount of information forwarded to the monitor, existing approaches of dynamic monitoring on multi-core still suffers from significant performance overhead, unless adapt special-purpose hardware support.||||
57|1||Compiler-directed memory management for heterogeneous MPSoCs|Advances in semiconductor technique enable multiple processor cores to be integrated into a single chip. Heterogeneous multiprocessor system-on-a-chip (MPSoC) becomes important platforms to accelerate applications. However, compilation techniques for memory management on MPSoCs still lag behind. This paper presents an automatic memory management framework to orchestrate the data movement between local memory and off-chip memory. In our framework, data alignment, hierarchically data distribution, communication generation, loop tiling, and loop splitting are employed. Moreover, a communication optimization approach is proposed to improve data reuse. These techniques can reduce off-chip memory access and exploit data locality. Experimental results on Cell BE show that our data management framework can generate efficient code for the program.||||
57|1||An analytical network performance model for SIMD processor CSX600 interconnects|||||
57|1||Transactional memories for multi-processor FPGA platforms|Programming efficiency of heterogeneous concurrent systems is limited by the use of lock-based synchronization mechanisms. Transactional memories can greatly improve the programming efficiency of such systems. In field-programmable computing machines, a conventional fixed transactional memory becomes inefficient use of the silicon. We propose configurable transactional memory (CTM) as a mechanism to implement application specific synchronization that utilizes the field-programmability of such devices to match with the requirements of an application. The proposed configurable transactional memory is targeted at embedded applications and is area efficient compared to conventional schemes that are implemented with cache-coherent protocols. In particular, the CTM is designed to be incorporated in to compilation and synthesis paths of either high-level languages or during system creation process using tools such as Xilinx EDK. The proposed system supports an OpenMP-based programming paradigm for the efficient use of transactional memories. In addition, the conflict detection scheme can be configured to work either in lazy or in eager mode, depending on the application requirements. We study the impact of deploying a CTM using both micro-benchmarks and real applications as compared to a lock-based synchronization scheme. We have implemented the proposed scheme in a Xilinx Virtex4 device and found that the CTM can provide both higher programming efficiency, lower energy consumption and higher speedup than a fine-grained lock-based scheme.||||
57|1||A design scheme for a reconfigurable accelerator implemented by single-flux quantum circuits|||||
57|10|http://www.sciencedirect.com/science/journal/13837621/57/10|Preface to the special issue on: Emerging Applications of Embedded Systems Research|||||
57|10||Weapon classification and shooter localization using distributed multichannel acoustic sensors|A wireless sensor network-based wearable countersniper system prototype is presented. The sensor board is connected to a small helmet-mounted microphone array that uses time of arrival (ToA) estimates of the ballistic shockwave and the muzzle blast to compute the angle of arrival (AoA) of both acoustic events. A low-power radio is used to form an ad-hoc multihop network that shares the detections among the nodes. Utilizing all available ToA and AoA data, a novel sensor fusion algorithm then estimates the shooter position, bullet trajectory, miss distance, caliber, and weapon type. A single sensor relying only on its own detections is able determine the shooter position when both the shockwave and the muzzle blast are detected by at least three microphones each. Even with just one shockwave and one muzzle blast detection, the miss distance and range can be accurately estimated by a single sensor. The system has been tested multiple times at the US Army Aberdeen Test Center and the Nashville Police Academy. The demonstrated performance is 1-degree trajectory precision, over 95% caliber estimation accuracy, and close to 100% weapon estimation accuracy for 4 out of the 6 guns tested.||||
57|10||Soft core based embedded systems in critical aerospace applications|There is an increasing interest in the aerospace industry to reduce the cost of the systems by means of using Commercial Off The Shelf (COTS) devices. The engineering of novel microsatellites and nanosatellites are clear examples of this new trend. However, the use of sub-micron technologies has led to greater sensitivity of these devices to radiation-induced transient faults, limiting the exploitation of this approach in critical systems. This paper presents an innovative application of soft-core microprocessor based embedded systems, to design dependable and reduced-cost critical systems with COTS reconfigurable devices (flash based FPGAs). To make this possible, it is necessary to fine-tune the protection strategy by combining selectively fault mitigation techniques based on hardware or software. In this way, the resultant system not only fulfills both the design constraints and the dependability requirements, but also avoids the cost provoked by excessive use of protection mechanisms. A case study is presented in which the design space exploration between hardware and software protection techniques permits to find the best trade-offs among performance, reliability, memory size and hardware cost in a dependable subsystem.||||
57|10||Reliable and energy optimized WSN design for a train application|In the near past, several applications have been envisioned for WSNs because of the need to collect, interpret and act on real-time data. Data collecting and processing using typical wired sensor network has always been expensive, especially in already-operational systems. For this reason, it is obviously advantageous to improve the functionality of these systems using WSNs. Trains are long life means of transport whose electrical system cannot be changed easily. In this paper, it is demonstrated that a WSN can be a suitable solution to add more services to the current operational trains. A reliable algorithm for these applications has been designed and simulated in a self-developed simulator, and then optimized in terms of energy consumption and response time. The bounded delay guarantees on packet delivery allows that this algorithm can be used in limited response time situations as emergency cases.||||
57|10||Wagyromag: Wireless sensor network for monitoring and processing human body movement in healthcare applications|Human body movement can be monitored through a wireless network composed of inertial sensors. This work presents the development of Wagyromag (Wireless Accelerometer, GYROscope and MAGnetometer), a wireless Inertial Measurement Unit (IMU) composed of a triaxial accelerometer, gyroscope and magnetometer. Communication is based on a 802.15.4 network. Furthermore, calibration, signal conditioning and signal processing algorithms are presented throughout this work. Wagyromag’s high potential permits its application in a wide range of medical applications such as telerehabilitation, nocturnal epilepsy seizure detection, fall detection and other applications in the field of sport science.||||
57|10||Bringing pervasive embedded networks to the service cloud: A lightweight middleware approach|The emergence of novel pervasive networks that consist of tiny embedded nodes have reduced the gap between real and virtual worlds. This paradigm has opened the Service Cloud to a variety of wireless devices especially those with sensorial and actuating capabilities. Those pervasive networks contribute to build new context-aware applications that interpret the state of the physical world at real-time. However, traditional Service-Oriented Architectures (SOA), which are widely used in the current Internet are unsuitable for such resource-constraint devices since they are too heavy. In this research paper, an internetworking approach is proposed in order to address that important issue. The main part of our proposal is the Knowledge-Aware and Service-Oriented (KASO) Middleware that has been designed for pervasive embedded networks. KASO Middleware implements a diversity of mechanisms, services and protocols which enable developers and business processing designers to deploy, expose, discover, compose, and orchestrate real-world services (i.e. services running on sensor/actuator devices). Moreover, KASO Middleware implements endpoints to offer those services to the Cloud in a REST manner. Our internetworking approach has been validated through a real healthcare telemonitoring system deployed in a sanatorium. The validation tests show that KASO Middleware successfully brings pervasive embedded networks to the Service Cloud.||||
57|10||An energy-efficient delay reduction technique for supporting WLAN-based VoIP in SmartPhone|For non-real time data such as Web or e-mail, the 802.11 PSM scheme can be a useful mechanism for reducing power consumption effectively. However, there are some limitations when these are used for voice communication in which the main traffic is composed of delay-sensitive data like voice or call signaling. In this paper, in order to overcome the limitations, we present an efficient power saving scheme which can minimize power consumption while guaranteeing the delay constraint during call signaling and talk time. Furthermore, in order to illustrate the aims of the proposed approach, the terminal systems are implemented and evaluated by measuring average call connection delay and power consumption. The experimental results show that our approach can minimize traffic delay and power consumption, and find an optimal sleep threshold value according to network condition changes.||||
57|2|http://www.sciencedirect.com/science/journal/13837621/57/2|Dynamic and adaptive SPM management for a multi-task environment|In this paper, we present a dynamic and adaptive scratchpad memory (SPM) management strategy targeting a multi-task environment. It can be applied to a contemporary embedded processor that maps the physically addressed SPM into a virtual space with the help of an integrated memory management unit (MMU). Based on mass-count disparity, we introduce a hardware memory reference sampling unit (MRSU) that samples the memory reference stream with very low probability. The captured address is considered as one of the memory addresses contained in a frequently referenced memory block. A hardware interruption is generated by the MRSU, and the identified frequently accessed memory block is placed into the SPM space by software. The software also modifies the page table so that the follow-up memory accesses to the memory block will be redirected to the SPM. With no dependence on compiler and profiling information, our proposed strategy is specifically adequate for SPM management in a multi-task environment. In such an environment, a real-time operating system (RTOS) is usually hosted, and the behavior of the memory accesses cannot be predicted by static analysis or profiling. We evaluate our SPM allocation strategy by running several tasks on a tiny RTOS with preemptive scheduling. Experimental results show that our approach can achieve 10% reduction in energy consumption on average, with 1% performance degradation at runtime compared with a cache-only reference system.||||
57|2||A portable, efficient inter-core communication scheme for embedded multicore platforms|Multicore processor designs have become increasingly popular for embedded applications in recent years, but diversified inter-core communication mechanisms have led to the difficulties in software development, integration and migration. A unified, portable, and efficient inter-core communication mechanism would have helped reduce these difficulties significantly, but such a solution did not exist today. We proposed a scheme called MSG, which provides users with a set of essential message-passing programming interfaces adopted from MPI and MCAPI, including blocking and non-blocking point-to-point communications, one-sided communications, and collective operations. We experimented and evaluated our design methodology with case studies on two heterogeneous multicore platforms: IBM CELL and ITRI PAC DUO. On the CELL platform, our MSG library fitted in the 256 KB local memory on each individual processor core and outperformed two existing communication libraries, DaCS and CML. On the second case study, we were able to port MSG onto the PAC DUO platform within two weeks upon receiving the platform. With a systematic approach, we showed how optimizations could be done to improve the performance of the MSG libraries. Hopefully, our experiences help the design and development of communication libraries for existing and future multicore platforms.||||
57|2||I2CSec: A secure serial Chip-to-Chip communication protocol|This paper presents a secured I2C (Inter-Integrated Circuit) protocol for Chip-to-Chip communications. The well known AES-GCM cryptographic and authentication algorithm is used to secure this low speed serial communication protocol. This securization allows the use of this standard into applications where security is an issue and the computation resources are constrained. Both the hardware architecture and the protocol are presented. The implementation of the I2CSec presented in the paper has been optimized for FPGA devices.||||
57|2||Architectures and optimization methods of flash memory based storage systems|Flash memory is a non-volatile memory which can be electrically erased and reprogrammed. Its major advantages such as small physical size, no mechanical components, low power consumption, and high performance have made it likely to replace the magnetic disk drives in more and more systems. However, flash memory has four specific features which are different to the magnetic disk drives, and pose challenges to develop practical techniques: (1) Flash memory is erased in blocks, but written in pages. (2) A block has to be erased before writing data to the block. (3) A block of flash memory can only be written for a specified number of times. (4) Writing pages within a block should be done sequentially. This survey presents the architectures, technologies, and optimization methods employed by the existing flash memory based storage systems to tackle the challenges. I hope that this paper will encourage researchers to analyze, optimize, and develop practical techniques to improve the performance and reduce the energy consumption of flash memory based storage systems, by leveraging the existing methods and solutions.||||
57|3|http://www.sciencedirect.com/science/journal/13837621/57/3|Editorial|||||
57|3||Secure service orchestration in open networks|Service Oriented Computing is a paradigm for creating a fully compositional service infrastructure. Compositionality makes security issues difficult to establish. As a matter of fact, defining global security properties on distribute, large-scale network seems to have little or even no sense at all.||||
57|3||Security services architecture for Secure Mobile Grid Systems|Mobile Grid, is a full inheritor of the Grid with the additional feature that it supports mobile users and resources. Security is an important aspect in Grid based systems, and it is more complex to ensure this in a mobile platform owing to the limitations of resources in these devices. A Grid infrastructure that supports the participation of mobile nodes and incorporates security aspects will thus play a significant role in the development of Grid computing. The idea of developing software through systematic development processes to improve software quality is not new. However, many information systems such as those of Grid Computing are still not developed through methodologies which have been adapted to their most differentiating features. The lack of adequate development methods for this kind of systems in which security is taken into account has encouraged us to build a methodology to develop them, offering a detailed guide for their analysis, design and implementation. It is important to use software V&V techniques, according to IEEE Std. 1012 for Software Verification and Validation, to ensure that a software system meets the operational needs of the user. This ensures that the requirements for the system are correct, complete, and consistent, and that the life-cycle products correctly design and implement system requirements. This paper shows part of a development process that we are elaborating for the construction of information systems based on Grid Computing, which are highly dependent on mobile devices in which security plays a highly important role. In the design activity of the process, we design a security architecture which serves as a reference for any mobile Grid application that we wish to build since this security architecture defines a complete set of security services which will be instantiated depending on the requirements and features found in previous activities of the process. A V&V task is also defined in the design activity to validate and verify both the architecture built and the traceability of the artifacts generated in this activity. In this paper, we will present the service-oriented security architecture for Mobile Grid Systems which considers all possible security services that may be required for any mobile Grid application.||||
57|3||Efficient file fuzz testing using automated analysis of binary file format|Fuzz testing is regarded as the most useful technique in finding serious security holes in a software system. It inserts unexpected data into the input of the software system and finds the system’s bugs or errors. However, one of the disadvantages that fuzz testing executed using binary files has is that it requires a large number of fault-inserted files to cover every test case, which could be up to 28×FILESIZE28×FILESIZE files. In order to overcome this drawback, we propose a novel algorithm that efficiently reduces the number of fault-inserted files, yet still maintain the maximum test case coverage. The proposed approach enables the automatic analysis of fields of binary files by tracking and analyzing stack frames, assembly codes, and registers as the software system parses the files. We evaluate the efficacy of the new method by implementing a practical tool, the Binary File Analyzer and Fault Injector (BFAFI), which traces the program execution and analyzes the fields in binary file format. Our experiments demonstrate that the BFAFI reduced the total number of fault-inserted files with maximum test case coverage as well as detected approximately 14 times more exceptions than did the general fuzzer. Also, the BFAFI found 11 causes of exceptions; five of them were found only by BFAFI. Ten of the 11 causes of exceptions that we found were generated by a graphic rendering engine (GDI32.dll); the other was generated by the system library (kernel32.dll) in Windows XP SP2.||||
57|3||Verifying security properties of internet protocol stacks: The split verification approach|We propose a novel method to construct user-space internet protocol stacks whose security properties can be formally explored and verified. The proposed method allows construction of protocol stacks using a C++ subset. We define a formal state-transformer representation of protocol stacks in which the protocol stack is specified in terms of three primary operations, which are constructed from sub-operations, in a compositional manner. We also define a Kripke model that captures the sequencing and attributes of stack operations. We propose a novel approach, called split verification, which combines theorem-proving and model-checking to establish properties for a protocol stack specification. In split verification, properties to be established for the stack are expressed as a combination of properties for primitive operations to be established via theorem-proving as well as temporal properties on operation sequencing, called promotion conditions, to be established via model-checking on the stack operations model. We use abstract Z specifications to represent operation properties and computational tree logic (CTL) formulae to represent promotion conditions. Operation properties are established by checking whether the operation(s) under consideration are correct refinements of the abstract Z specification(s). Our conclusion is that split verification: (a) avoids scalability issues caused by state-space explosion in model-checking and long unwieldy proofs in theorem-proving, and, (b) lowers cost of proof maintenance for localized changes in the stack.||||
57|3||Component-oriented verification of noninterference|Component-based software engineering often relies on libraries of trusted components that are combined to build dependable and secure software systems. Resource dependences, constraint conflicts, and information flow interferences arising from component combination that may violate security requirements can be revealed by means of the noninterference approach to information flow analysis. However, the security of large component-based systems may be hard to assess in an efficient and systematic way. In this paper, we propose a component-oriented formulation of noninterference that enables compositional security verification driven by system topology. This is realized by implementing scalable noninterference checks in the formal framework of a process algebraic architectural description language equipped with equivalence checking techniques.||||
57|3||Using complexity, coupling, and cohesion metrics as early indicators of vulnerabilities|Software security failures are common and the problem is growing. A vulnerability is a weakness in the software that, when exploited, causes a security failure. It is difficult to detect vulnerabilities until they manifest themselves as security failures in the operational stage of software, because security concerns are often not addressed or known sufficiently early during the software development life cycle. Numerous studies have shown that complexity, coupling, and cohesion (CCC) related structural metrics are important indicators of the quality of software architecture, and software architecture is one of the most important and early design decisions that influences the final quality of the software system. Although these metrics have been successfully employed to indicate software faults in general, there are no systematic guidelines on how to use these metrics to predict vulnerabilities in software. If CCC metrics can be used to indicate vulnerabilities, these metrics could aid in the conception of more secured architecture, leading to more secured design and code and eventually better software. In this paper, we present a framework to automatically predict vulnerabilities based on CCC metrics. To empirically validate the framework and prediction accuracy, we conduct a large empirical study on fifty-two releases of Mozilla Firefox developed over a period of four years. To build vulnerability predictors, we consider four alternative data mining and statistical techniques – C4.5 Decision Tree, Random Forests, Logistic Regression, and Naïve-Bayes – and compare their prediction performances. We are able to correctly predict majority of the vulnerability-prone files in Mozilla Firefox, with tolerable false positive rates. Moreover, the predictors built from the past releases can reliably predict the likelihood of having vulnerabilities in the future releases. The experimental results indicate that structural information from the non-security realm such as complexity, coupling, and cohesion are useful in vulnerability prediction.||||
57|3||Towards accuracy of role-based access control configurations in component-based systems|||||
57|4|http://www.sciencedirect.com/science/journal/13837621/57/4|Saving register-file static power by monitoring instruction sequence in ROB|Modern information technology (IT) applications make microprocessors require not only high performance, but also low power-consumption. To enhance computational performance, many instruction level parallelism techniques have been implemented in current microprocessors, e.g., data forwarding, out-of-order execution, register renaming etc. The reorder buffer (ROB) and the register file are the two most critical components to implement these features. The cooperation of them, however, causes serious static-power consumption on a physical register file which stores a large amount of speculative and committed temporary values. In this paper, we use the Pentium 4-like processor as the baseline architecture and propose a runtime approach to save the physical register file’s static power. In this approach, a monitoring mechanism is built in the ROB and the register file to identify the timing of usage for each register. This mechanism can be integrated with a DVS approach on the datapath to power down (or up) the supply voltages to a register when it is idle (or active). Simulation results show that by this monitoring mechanism and a low-cost DVS design, a 128-entry register file can save at least 50% register file power consumption.||||
57|4||Real-time scheduling on heterogeneous system-on-chip architectures using an optimised artificial neural network|Today’s integrated circuit technologies allow the design of complete systems on a single chip which execute complex applications specified as a set of tasks. The tasks are managed by an Operating System whose main role consists in defining the resource allocation and the temporal scheduling. One of the main characteristics of these architectures is the heterogeneity of their execution resources which makes this scheduling complex.||||
57|4||A comprehensive study of energy efficiency and performance of flash-based SSD|Use of flash memory as a storage medium is becoming popular in diverse computing environments. However, because of differences in interface, flash memory requires a hard-disk-emulation layer, called FTL (flash translation layer). Although the FTL enables flash memory storages to replace conventional hard disks, it induces significant computational and space overhead. Despite the low power consumption of flash memory, this overhead leads to significant power consumption in an overall storage system. In this paper, we analyze the characteristics of flash-based storage devices from the viewpoint of power consumption and energy efficiency by using various methodologies. First, we utilize simulation to investigate the interior operation of flash-based storage of flash-based storages. Subsequently, we measure the performance and energy efficiency of commodity flash-based SSDs by using microbenchmarks to identify the block-device level characteristics and macrobenchmarks to reveal their filesystem level characteristics.||||
57|4||Modular software architecture for flexible reservation mechanisms on heterogeneous resources|Management, allocation and scheduling of heterogeneous resources for complex distributed real-time applications is a challenging problem. Timing constraints of applications may be fulfilled by the proper use of real-time scheduling policies, admission control and enforcement of timing constraints. However, it is not easy to design basic infrastructure services that allow for easy access to the allocation of multiple heterogeneous resources in a distributed environment.||||
57|4||Rapid functional modelling and simulation of coarse grained reconfigurable array architectures|||||
57|4||P2PVR: A playback offset aware multicast tree for on-demand video streaming with VCR functions|Tree-based peer-to-peer multicast overlays have been widely proposed to provide on-demand video streaming services. However, alleviating the server load in large-scale, dynamic environments such as the Internet remains a major challenge. Accordingly, this paper proposes a new peer-to-peer on-demand video streaming system, designated as peer-to-peer personal video recorder (P2PVR), which facilitates essential VCR functions in an efficient manner. In the proposed architecture, the peers are organized into a playback offset aware tree-based overlay, in which they share streaming data with other peers having a similar playback offset. In addition, a semi-decentralized directory service is developed to assist peers in searching only those parts of the tree known to contain eligible parent nodes, i.e. nodes which possess the expected streaming data. The performance of P2PVR is evaluated by performing a series of experiments on the PlanetLab platform. The results show that P2PVR yields a significant reduction in the server stream stress and achieves a reasonably low playback discontinuity under dynamic network conditions whilst simultaneously granting a low control overhead and startup latency.||||
57|4||An ASIC implementation of a low power robust invisible watermarking processor|Digital watermarking is the process of embedding data called watermark into a multimedia object so that it can be detected or extracted later to make an assertion about the object. Several software implementations of watermarking algorithms are available, but very few attempts have been made for hardware implementation. The objective of this research paper is to implement “low power robust invisible binary image watermarking processor” in an Application Specific Integrated Circuit (ASIC) using Hardware Description Language (HDL). An 8-bit processor has been used since it consumes less power than other higher order bit (16-bit, 32-bit, etc.) processors. The proposed invisible watermarking algorithm is implemented in spatial domain. The proposed algorithm is prototyped (i) using XILINX FPGA (ii) using ASIC. To the best of our knowledge this is the first low power binary image watermarking processor implemented in ASIC which uses 8-bit processor with no limitation on input size. The algorithm is tested in Virtex E (xcv50e-8-cs144) Field Programmable Gate Arrays (FPGA) and implemented in an ASIC.||||
57|4||The Policy Machine: A novel architecture and framework for access control policy specification and enforcement|The ability to control access to sensitive data in accordance with policy is perhaps the most fundamental security requirement. Despite over four decades of security research, the limited ability for existing access control mechanisms to generically enforce policy persists. While researchers, practitioners and policy makers have specified a large variety of access control policies to address real-world security issues, only a relatively small subset of these policies can be enforced through off-the-shelf technology, and even a smaller subset can be enforced by any one mechanism. In this paper, we propose an access control framework, referred to as the Policy Machine (PM) that fundamentally changes the way policy is expressed and enforced. Employing PM helps in building high assurance enforcement mechanisms in three respects. First, only a relatively small piece of the overall access control mechanism needs to be included in the host system (e.g., an operating system or application). This significantly reduces the amount of code that needs to be trusted. Second, it is possible to enforce the precise policies of resource owners, without compromise on enforcement or resorting to less effective administrative procedures. Third, the PM is capable of generically imposing confinement constraints that can be used to prevent leakage of information to unauthorized principals within the context of a variety of policies to include the commonly implemented Discretionary Access Control and Role-Based Access Control models.||||
57|4||Provenance security guarantee from origin up to now in the e-Science environment|The e-Science environment provides science researchers with an online laboratory. Objects, including research data and related information, are transferred and shared in electronic form easily in an e-Science environment. Provenance, as a complete record of the changes applied to an object, provides a basis to trust an object. At this point, this paper proposes the “Provenance Security from Origin up to Now (PSecON)(PSecON)” scheme to solve the problem of how to trust provenance which is used for object trust. Through PSecON, while tracing the real source of an object transferred between e-Science environments, researchers can ensure the integrity of the object and its provenance, and confirm its actual origin. PSecON satisfies transparent audits and audit availability as well as the confidentiality and privacy of data and its provenance. These are provided by the history pool as an open board and by information hashing, respectively. PSecON supports forgery prevention and detection for provenance as well via two-way certification. Moreover, PSecON is scalable and ensures efficient and fast tracking of provenance. Based on the detailed description of PSecON in the e-Science domain, this paper demonstrates the soundness of PSecON to ensure provenance security and its easy applicability to real-world systems by analyzing the time, space and transmission overhead.||||
57|4||Intrusion-tolerant fine-grained authorization for Internet applications|||||
57|4||Dynamic cohesion measures for object-oriented software|Most of the object-oriented cohesion metrics proposed in the literature define static cohesion at class level. Measurement of object-level dynamic cohesion however gives better insight into the behavioural aspects of the system. In this paper, dynamic cohesion metrics are introduced which provide scope of cohesion measurement up to object level and take into account important and widely used object-oriented features such as inheritance, polymorphism and dynamic binding during measurement. A theoretical framework is introduced before defining the measures and a theoretic validation of the proposed measures is carried out to make them more meaningful. A dynamic analyser tool is developed using aspect-oriented programming (AOP) to perform dynamic analysis of Java applications for the purpose of collecting run-time data for computation of the proposed dynamic cohesion measures. Further, an experiment is carried out for the proposed dynamic cohesion metrics using 20 Java programs and this study shows that the proposed dynamic cohesion metrics are more accurate and useful in comparison to the existing cohesion metrics. Moreover, the proposed dynamic cohesion metrics are validated empirically using source code APIs of Java Development Kit (JDK) and the proposed metrics are found to be better indicators of change-proneness of classes than the existing cohesion metrics.||||
57|4||Attestation of integrity of overlay networks|Security of overlay networks requires that the integrity of the software stack of a node is attested not only when a node joins an overlay but continuously, to discover updates of its configuration due to malware. We present a framework that integrates an initial attestation and a continuous node monitoring that strongly separates the software of a node from the attestation system by running them in two virtual machines (VMs). The Monitored VM (Mon-VM) runs the applicative software while the Assurance VM (A-VM) exploits virtual machine introspection to access the status of the Mon-VM to attest and monitor the integrity of its software stack. Before a node can join an overlay, the A-VM of one overlay node interacts with the A-VM of the joining node to attest the integrity of the Mon-VM. After this start-up attestation, the A-VM continuously monitors the behavior of the Mon-VM of its node to detect the injection of malware. Monitoring strategies range from the evaluation of assertions on memory areas of the OS to the comparison of the application behavior against the expected one. The expected behavior is defined by the overlay security policy or computed by applying static tools to the application source code. To define a root-of-trust for the measurements, each node includes a TPM to attest the integrity of the A-VM and of the underlying VMM.||||
57|4||A study on the uncertainty inherent in class cohesion measurements|Software metrics are essential for component certification and for the development of high quality software in general. Accordingly, research in the area of software metrics has been active and a wide range of metrics has been proposed. However, the variety of metrics proposed for measuring the same quality attribute suggests that there may be some sort of inconsistencies among the measurements computed using these metrics. In this paper, we report a case study considering class cohesion as a quality attribute of concern. We present the results of our empirical investigation to confirm that prominent object-oriented class cohesion metrics provide inconsistent measurements. We also present an analysis of the uncertainty that should be considered in these class cohesion measurements due to their inter-inconsistencies. Considering such uncertainty, we provide a framework for associating a probability distribution of error to the measurements computed by each metric; thus enabling the assessment of the degree of reliability of measurements of each metric when used to rank a set of classes with regard to their cohesion quality. The error probability distribution would be useful in practice where it is seldom feasible to use a large set of metrics and rather a single metric is used.||||
57|5|http://www.sciencedirect.com/science/journal/13837621/57/5|Multiprocessor real-time scheduling|||||
57|5||Tests for global EDF schedulability analysis|Several schedulability tests have been proposed for global EDF scheduling on identical multiprocessors. All these tests are sufficient, rather than exact. These different tests were, for the most part, independently developed. The relationships among such tests have not been adequately investigated, so that it is difficult to understand which test is most appropriate in a particular given scenario. This paper represents an attempt to remedy this, by means of three major contributions. First, we summarize the main existing results for the schedulability analysis of multiprocessor systems scheduled with global edf, showing, when possible, existing dominance relations. We compare these algorithms taking into consideration different aspects, namely, run-time complexity, average performances over randomly generated workloads, sustainability properties and speedup factors. Second, based on this comparative evaluation we propose a recommended approach to schedulability analysis, that suggests a particular order in which to apply preexisting tests, thereby accomplishing both good provable performance and good behavior in practice. And finally, we propose a further improvement to one of these preexisting tests to improve its run-time performance by an order of magnitude, while completely retaining its ability to correctly identify schedulable systems.||||
57|5||Global EDF-based scheduling with laxity-driven priority promotion|This paper presents an algorithm, called Earliest Deadline Critical Laxity (EDCL), for scheduling sporadic task systems on multiprocessors. EDCL is a derivative of the Earliest Deadline Zero Laxity (EDZL) algorithm. Each job is assigned a priority based on the well-known Global Earliest Deadline First (G-EDF) algorithm, as long as its laxity – the amount of time from the earliest possible time of job completion to the deadline of job – is above a certain value. The priority is however promoted to the highest level once the laxity falls below this certain value in order to meet the deadline. Priority promotions are aligned with arrivals and completions of jobs under EDCL to avoid additional scheduler invocations, while EDZL can promote priorities arbitrarily. As compared with EDZL, EDCL reduces runtime overhead and implementation cost, but still strictly dominates G-EDF in schedulability. Schedulability tests for EDCL are derived through theoretical analysis, and sustainability properties are also considered. Our simulation results demonstrate that EDCL is competitive to EDZL in schedulability with a smaller number of scheduler invocations, and it also outperforms traditional EDF-based algorithms.||||
57|5||Job vs. portioned partitioning for the earliest deadline first semi-partitioned scheduling|In this paper, we focus on the semi-partitioned scheduling of sporadic tasks with constrained deadlines and identical processors. We study two cases of semi-partitioning: (i) the case where the worst case execution time (WCET) of a job can be portioned, each portion being executed on a dedicated processor, according to a static pattern of migration; (ii) the case where the jobs of a task are released on a processor, 1 time out of p, where p is an integer at most equal to the number of processors, according to a round-robin migration pattern. The first approach has been investigated in the state-of-the-art by migrating a job at its local deadline, computed from the deadline of the task it belongs to. We study several local deadline assignment heuristics (fair, based on processor utilization and based on the minimum acceptable local deadline for a job on a processor). In both cases, we propose feasibility conditions for the schedulability of sporadic tasks scheduled using earliest deadline first (EDF) semi-partitioned scheduling. We show that the load function used for global scheduling to establish the feasibility of sporadic task sets exhibits interesting properties in the semi-partitioning context. We carry out simulations to study the performance of the two approaches in terms of success rate and number of migrations, for platforms composed of four and eight processors. We compare the performance of these semi-partitioned heuristics with the performance of classical partitioned scheduling algorithms and with a global scheduling heuristic which is currently considered to have good performances.||||
57|5||Schedulability analysis for non-preemptive fixed-priority multiprocessor scheduling|Non-preemptive scheduling is usually considered inferior to preemptive scheduling for time critical systems, because the non-preemptive block would lead to poor task responsiveness. Although this is true in single-processor scheduling, we found by empirical simulation experiments that it is not necessarily the case in multiprocessor scheduling. Additionally, non-preemptive scheduling enjoys other benefits like lower implementation complexity and run-time overhead. So non-preemptive scheduling may be a better alternative compared to preemptive scheduling for a considerable part of real-time applications on multiprocessor/multi-core platforms.||||
57|5||Thermal-aware global real-time scheduling and analysis on multicore systems|As the power density of modern electronic circuits increases dramatically, systems are prone to overheating. Thermal management has become a prominent issue in system design. This paper explores thermal-aware scheduling for sporadic real-time tasks to minimize the peak temperature in a homogeneous multicore system, in which heat might transfer among some cores. By deriving an ideally preferred speed for each core, we propose global scheduling algorithms which can exploit the flexibility of multicore platforms at low temperature. We perform simulations to evaluate the performance of the proposed approach.||||
57|5||Exact schedulability tests for real-time scheduling of periodic tasks on unrelated multiprocessor platforms|In this paper, we study the global scheduling of periodic task systems on unrelated multiprocessor platforms. We first show two general properties which are well known for uniprocessor platforms and which are also true for unrelated multiprocessor platforms: (i) under few and not so restrictive assumptions, we prove that feasible schedules of periodic task systems are periodic starting from some point in time with a period equal to the least common multiple of the task periods and (ii) for the specific case of synchronous periodic task systems, we prove that feasible schedules repeat from their origin. We then present our main result: we characterize, for task-level fixed-priority schedulers and for asynchronous constrained or arbitrary deadline periodic task models, upper bounds of the first time-instant where the schedule repeats. For task-level fixed-priority schedulers, based on the upper bounds and the predictability property, we provide exact schedulability tests for asynchronous constrained or arbitrary deadline periodic task sets. Finally, we provide an exact schedulability test as well for the job-level fixed-priority Earliest Deadline First (EDF) scheduler, for which such an upper bound is unknown.||||
57|6|http://www.sciencedirect.com/science/journal/13837621/57/6|Special issue: Design and optimization for embedded and real-time computing systems and applications|||||
57|6||Platform synthesis and partitioning of real-time tasks for energy efficiency|||||
57|6||Simultaneous thermal and timeliness guarantees in distributed real-time embedded systems|||||
57|6||Supporting component-based failover units in middleware for distributed real-time and embedded systems|Although component middleware is increasingly used to develop distributed, real-time and embedded (DRE) systems, it poses new fault-tolerance challenges, such as the need for efficient synchronization of internal component state, failure correlation across groups of components, and configuration of fault-tolerance properties at the component granularity level. This paper makes three contributions to R&D on component-based fault-tolerance. First, it describes the COmponent Replication based on Failover Units (CORFU) component middleware, which provides fail-stop behavior and fault correlation across groups of components treated as an atomic unit in DRE systems. Second, it describes how CORFU’s Components with HEterogeneous State Synchronization (CHESS) module provides mechanisms for real-time aware state transfer and synchronization in CORFU. Third, we empirically evaluate the client failover and group shutdown capabilities of CORFU and its CHESS module and compare/contrast it with existing object-oriented fault-tolerance methods. Our results show that component middleware (1) has acceptable fault-tolerance performance for DRE systems, (2) allows timely recovery while considering failure location, size, and functional topology of the group, and finally (3) eases the burden of application development by providing middleware support for fault-tolerance at the component level.||||
57|6||An efficient algorithm for parametric WCET calculation|||||
57|6||Branch target buffers: WCET analysis framework and timing predictability|One step in the verification of hard real-time systems is to determine upper bounds on the worst-case execution times (WCET) of tasks. To obtain tight bounds, a WCET analysis has to consider micro-architectural features like caches, branch prediction, and branch target buffers (BTB).||||
57|6||An overview of interrupt accounting techniques for multiprocessor real-time systems|||||
57|6||A co-commitment based secure data collection scheme for tiered wireless sensor networks|Tiered wireless sensor networks (WSNs) have many advantages over traditional WSNs. However, they are vulnerable to security attacks, especially the attacks to the storage nodes that buffer and process the data readings from sensors. In this paper, we propose a secure data collection protocol SDC to support time-based queries in tiered WSNs. With small overhead introduced to data communication, SDC protects both data confidentiality and data integrity. In particular it employs data co-commitment scheme such that it can detect the seriousness of data losing and estimate the value of lost data in the network.||||
57|6||Energy reduction for scheduling a set of multiple feasible interval jobs|Time-critical jobs in many real-time applications have more than one feasible interval. Such jobs can be executed in any of their feasible intervals. Given a schedulable set of multiple feasible interval (MFI) jobs, energy can be saved by carefully selecting the executing interval for each job. In this paper, we explore the energy minimization problem for real-time systems in which jobs have more than one feasible interval. The static and dynamic energy management schemes are both investigated to minimize the energy consumption while preserving the system’s feasibility. Focusing on EDF scheduling algorithm, we first study reducing the dynamic power consumption of a single CPU. We show that the static optimal speed assignment problem is NP-Hard and propose a simulated annealing (SA) based approach to solve it. Then, we develop several on-line greedy algorithms to exploit run-time slack by reselecting a job’s executing interval on-the-fly. In addition, a leakage-aware version is discussed to improve the overall energy efficiency. Simulation results show that all proposed schemes achieve significant improvements of energy efficiency while the system remains schedulable.||||
57|7|http://www.sciencedirect.com/science/journal/13837621/57/7|Preface to the special issue on worst-case execution-time analysis|||||
57|7||WCET analysis of instruction cache hierarchies|With the advent of increasingly complex hardware in real-time embedded systems (processors with performance enhancing features such as pipelines, caches, multiple cores), most embedded processors use a hierarchy of caches. While much research has been devoted to the prediction of Worst-Case Execution Times (WCETs) in the presence of a single level of cache (instruction caches, data caches, impact of cache replacement policies), very little research has focused on WCET estimations in the presence of cache hierarchies.||||
57|7||Improving the WCET computation in the presence of a lockable instruction cache in multitasking real-time systems|In multitasking real-time systems it is required to compute the WCET of each task and also the effects of interferences between tasks in the worst case. This is very complex with variable latency hardware, such as instruction cache memories, or, to a lesser extent, the line buffers usually found in the fetch path of commercial processors. Some methods disable cache replacement so that it is easier to model the cache behavior. The difficulty in these cache-locking methods lies in obtaining a good selection of the memory lines to be locked into cache. In this paper, we propose an ILP-based method to select the best lines to be loaded and locked into the instruction cache at each context switch (dynamic locking), taking into account both intra-task and inter-task interferences, and we compare it with static locking. Our results show that, without cache, the spatial locality captured by a line buffer doubles the performance of the processor. When adding a lockable instruction cache, dynamic locking systems are schedulable with a cache size between 12.5% and 50% of the cache size required by static locking. Additionally, the computation time of our analysis method is not dependent on the number of possible paths in the task. This allows us to analyze large codes in a relatively short time (100 KB with 10651065 paths in less than 3 min).||||
57|7||Cache-related preemption delay via useful cache blocks: Survey and redefinition|||||
57|7||Identifying irreducible loops in the Instrumentation Point Graph|||||
57|8|http://www.sciencedirect.com/science/journal/13837621/57/8|A three-tier middleware architecture supporting bidirectional location tracking of numerous mobile nodes under legacy WSN environment|In the mobile asset management services applicable to warehouse, hospital, etc. a low-cost and practical bidirectional location tracking of mobile asset is one of the most important technical issue must be solved. Due to the complexity and heavy traffic of the legacy location-awareness techniques, simultaneous locationing and tracking of numerous mobile nodes in real-time is not easy. To address this problem, we propose the three-tier middleware architecture called uMATI (ubiquitous Mobile Asset Tracking Infra). In the uMATI, all nodes (stationary and mobile) commonly use the IEEE 802.15.4 MAC protocol to guarantee the compatibility with the legacy wireless sensor network (WSN) despite of mobile-stationary nodes co-existence network. To solve the bidirectional tracking in spite of the free mobility of the numerous mobile nodes, we firstly suggest a simple bidirectional location protocol called BLIDx (bidirectional location ID exchange) and its implementation into both the mobile and stationary nodes. In addition, to prevent the traffic overflow due to the concentration of excessive mobile nodes into a single location, we propose adding a specially designed stationary node called virtual sink (VS) node and installing related middleware components into the node. Results from the experimental evaluation prove that the proposed architecture enhanced the practicability by effectively supporting the mobility and managing the traffic in the real-time mobile asset tracking applications.||||
57|8||A new approach to evaluating internal Xilinx FPGA resources|In this paper, a new approach of application test process is presented aimed at verifying internal Xilinx FPGA (field programmable gate array) resources using a multi-load bitstream system. Basically, the new system comprises an algorithmic part, running on a PC (the software aspect), and an ad hoc hardware architecture. The bitstreams necessary for testing FPGA internal resources are automatically generated on a PC using a sequential algorithm, which varies according to the FPGA chip to be evaluated, and are subsequently downloaded onto the hardware architecture. Next, a customized application, also run on a PC, downloads the previously generated bitstreams consecutively, using the Xilinx Impact tool.||||
57|8||Fast placement and routing by extending coarse-grained reconfigurable arrays with Omega Networks|Reconfigurable computing architectures are commonly used for accelerating applications and/or for achieving energy savings. However, most reconfigurable computing architectures suffer from computationally demanding placement and routing (P&R) steps. This problem may disable their use in systems requiring dynamic compilation (e.g., to guarantee application portability in embedded systems). Bearing in mind the simplification of P&R steps, this paper presents and analyzes a coarse-grained reconfigurable array (CGRA) extended with global multistage interconnect networks, specifically Omega Networks. We show that integrating one or two Omega Networks in a CGRA permits to simplify the P&R stage resulting in both low hardware resource overhead and low performance degradation (18% for an 8 × 8 array). We compare the proposed CGRA, which integrates one or two Omega Networks, with a CGRA based on a grid of processing elements with reach neighbor interconnections and with a torus topology. The execution time needed to perform the P&R stage for the two array architectures shows that the array using two Omega Networks needs a far simpler and faster P&R. The P&R stage in our approach completed on average in about 16× less time for the 17 benchmarks used. Similar fast approaches needed CGRAs with more complex interconnect resources in order to allow most of the benchmarks used to be successfully placed and routed.||||
57|8||Fast and efficient FPGA implementation of connected operators|The Connected Component Tree (CCT)-based operators play a central role in the development of new algorithms related to image processing applications such as pattern recognition, video-surveillance or motion extraction. The CCT construction, being a time consuming task (about 80% of the application time), these applications remain far-off mobile embedded systems. This paper presents its efficient FPGA implementation suited for embedded systems. Three main contributions are discussed: an efficient data structure proposal adapted to representing the CCT in embedded systems, a memory organization suitable for FPGA implementation by using on-chip memory and a customizable hardware accelerator architecture for CCT-based applications.||||
57|8||Temporal partitioning of data flow graph for dynamically reconfigurable architecture|In this paper, we present a novel temporal partitioning algorithm that temporally partitions a data flow graph on reconfigurable system. Our algorithm can be used to resolve the temporal partitioning problem at the behaviour level. Our algorithm optimizes the whole latency of the design; this aim can be reached by minimizing the latency of the graph and the number of partitions at the same time. Consequently, our algorithm starts by the lowest possible number of partitions; and next it uses the eigenvectors of the graph to find the best schedule of nodes that minimizes the latency of the graph. The proposed methodology was tested on several examples on reconfigurable architecture based on Xilinx Vertex-II XC2V1000 FPGA device. The results show significant reduction in the design latency compared to famous related algorithms used in this field.||||
57|9|http://www.sciencedirect.com/science/journal/13837621/57/9|Dynamic hinting: Collaborative real-time resource management for reactive embedded systems|The increasing complexity of today’s reactive embedded applications can rapidly result in reduced real-time capabilities of the underlying hard and software. As an example for this paper we’ll refer to the specific and growing demands on the severely resource constrained sensor nodes in sensor/actuator networks (SANet). While preemptive operating systems are one way to retain acceptable reactivity within highly dynamic environments, their concurrency paradigm commonly leads to severe resource management problems, caused by the coexistence of tasks with interfering and even varying requirements. To counteract these problems, we present the novel Dynamic Hinting approach for maintaining good reactivity in typically resource constrained sensor/actuator systems by efficient combination of preemptive task scheduling and collaborative resource allocation. With respect to task priorities, our technique significantly improves classical methods for handling priority inversions (and deadlocks where required) under both short- and long-term resource allocations. Furthermore, we facilitate compositional software design by providing independently developed tasks with runtime information for yet collaborative and reflective resource sharing – e.g. by means of time-utility-functions. In many cases this even allows to reduce blocking delays as otherwise imposed by bounded priority inversion.||||
57|9||Repetitive model refactoring strategy for the design space exploration of intensive signal processing applications|||||
57|9||An efficient diversity-driven selective forwarding approach for replicated data queries in wireless sensor networks|This study considers a wireless sensor network (WSN) designed to track specified objects of interest such as bird-calls, insect-images, and so forth. An assumption is made that the sensors in the WSN are capable of analyzing and identifying detected objects and are pre-loaded with the features of the tracked objects before they are deployed. The features associated with the tracked objects are referred to as “model tuples”. When a sensor subsequently detects an object, it extract features from the detected object and then compares it with the tuples stored in its memory in order to determine whether or not the detected object is the tracked object. Since the sensors have only limited memory and storage space, it is impossible to store all the tuples on a single sensor. Furthermore, the sensors are battery operated, and thus the stored tuples are irretrievably lost once the sensor’s energy resources have been consumed. As a result, the network no longer has a complete knowledge of all the tracked information. Accordingly, the present study proposes four tuple dispatching schemes for distributing the tracked information amongst the sensors in such a way as to mitigate the effects of sensor energy depletion, namely sequential dispatching, sequential dispatching with overlap, fixed distance dispatching, and balanced incomplete block dispatching. In addition, an efficient diversity-driven selective forwarding scheme is proposed to resolve the problem where the detected object fails to match the tuples held at the local sensor. In the approach, the local sensor applies the correlation between the sensor identifier and the indexes of the tuples stored at the various sensors to deliver the feature of the object along the paths with the highest diversity. The simulation presents a series of experimental results to benchmark the performance of the proposed forwarding approach for each of the dispatching schemes against that of a blind flooding approach.||||
57|9||Resource allocation robustness in multi-core embedded systems with inaccurate information|Multi-core technologies are widely used in embedded systems and the resource allocation is vita to guarantee Quality of Service (QoS) requirements for applications on multi-core platforms. For heterogeneous multi-core systems, the statistical characteristics of execution times on different cores play a critical role in the resource allocation, and the differences between the actual execution time and the estimated execution time may significantly affect the performance of resource allocation and cause system to be less robust. In this paper, we present an evaluation method to study the impacts of inaccurate execution time information to the performance of resource allocation. We propose a systematic way to measure the robustness degradation of the system and evaluate how inaccurate probability parameters may affect the performance of resource allocations. Furthermore, we compare the performance of three widely used greedy heuristics when using the inaccurate information with simulations.||||
57|9||Slack computation for DVS algorithms in fixed-priority real-time systems using fluid slack analysis|This work presents a scheduling algorithm to reduce the energy of hard real-time tasks with fixed priorities assigned in a rate-monotonic policy. Sets of independent tasks running periodically on a processor with dynamic voltage scaling (DVS) are considered as well. The proposed online approach can cooperate with many slack-time analysis methods based on low-power work demand analysis (lpWDA) without increasing the computational complexity of DVS algorithms. The proposed approach introduces a novel technique called low-power fluid slack analysis (lpFSA) that extends the analysis interval produced by its cooperative methods and computes the available slack in the extended interval. The lpFSA regards the additional slack as fluid and computes its length, such that it can be moved to the current job. Therefore, the proposed approach provides the cooperative methods with additional slack. Experimental results show that the proposed approach combined with lpWDA-based algorithms achieves more energy reductions than do the initial algorithms alone.||||
||||||||
volume|issue|url|title|abstract||||
58|1|http://www.sciencedirect.com/science/journal/13837621/58/1|Tokenisation and compression of Java class files|Method calls in object oriented languages, such as Java, are bound at run-time, making the method binding technique very important for the performance of the language. Efficient implementations can rely on having additional memory and/or processing power available either to store lookup tables or to allow for the construction of caches or rewriting of instructions during runtime. These are luxuries not always available on mobile devices such as mobile phones, tablets, etc. In this paper we describe a novel way of tokenising and compressing method dispatch tables to provide an efficient dispatch process which could be implemented in hardware in only a few operations. We demonstrate this in the context of Java, also showing a significant reduction in size for the resulting class files.||||
58|1||Register allocation for write activity minimization on non-volatile main memory for embedded systems|Non-volatile memories are good candidates for DRAM replacement as main memory in embedded systems and they have many desirable characteristics. Nevertheless, the disadvantages of non-volatile memory co-exist with its advantages. First, the lifetime of some of the non-volatile memories is limited by the number of erase operations. Second, read and write operations have asymmetric speed or power consumption in non-volatile memory. This paper focuses on the embedded systems using non-volatile memory as main memory. We propose register allocation technique with re-computation to reduce the number of store instructions. When non-volatile memory is applied as the main memory, reducing store instructions will reduce write activities on non-volatile memory. To re-compute the spills effectively during register allocation, a novel potential spill selection strategy is proposed. During this process, live range splitting is utilized to split certain long live ranges such that they are more likely to be assigned into registers. In addition, techniques for re-computation overhead reduction is proposed on systems with multiple functional units. With the proposed approach, the lifetime of non-volatile memory is extended accordingly. The experimental results demonstrate that the proposed technique can efficiently reduce the number of store instructions on systems with non-volatile memory by 33% on average.||||
58|1||Run-time generation of partial FPGA configurations|||||
58|1||Analysis and redesign of the âTTCâ and âTTHâ schedulers|The Time-Triggered Cooperative (TTC) and TT Hybrid (TTH) schedulers have previously been described in the literature as highly predictable static schedulers with very low overheads, which are suitable for use in resource-constrained embedded software applications. Although several previous papers have considered these schedulers to a certain degree of depth, to date there has not been (i) a serious comparative investigation of the scheduler properties or (ii) the attempt to apply mainstream scheduling theory within their frameworks. As designs based around the TTC framework seem to be increasing in popularity, with operating systems based around these schedulers now commercially available, these points should be investigated; this is the focus of the current paper. The investigation reveals that although it can be said that the schedulers are predictable, their underlying operation seems to be dynamic – in the regular sense of task priorities – and employs the Earliest Activation First (EAF) priority assignment rule. In addition, with their present design the TTC/TTH scheduling overheads do not scale well, and can be excessive in comparison to alternative techniques. Motivated by these observations, the paper goes onto describe: (i) a re-design of the schedulers which allows the overheads to scale at an acceptable level, and improves the flexibility of the scheduler, (ii) the formulation and proof of an optimal tie-breaking rule for the schedulers, and (iii) the application of mainstream scheduling theory to yield a schedulability test of improved efficiency. The effectiveness of these modifications is illustrated with a small comparative study. Finally, the paper provides an overview of TTC and TTH properties in comparison to other popular scheduler architectures. These latter points should be of interest to practitioners in the field.||||
58|1||Memory access schedule minimization for embedded systems|||||
58|10|http://www.sciencedirect.com/science/journal/13837621/58/10|On the fundamentals of leakage aware real-time DVS scheduling for peak temperature minimization|As the consequence of the exponentially increased power density on integrated circuits, thermal issues are becoming critical in design of computing systems. Moreover, as both leakage and thermal issues have become more prominent in the deep sub-micron domain, a power and thermal aware design technique becomes less effective if the leakage/temperature dependency is not appropriately addressed. In this paper, we take into account the dependency among the leakage, the temperature, and the supply voltage in our theoretical analysis and explore the fundamental characteristics on how to employ dynamic voltage scaling (DVS) to reduce the peak operating temperature. We find that, for a specific interval, a real-time schedule using the lowest constant speed is not necessarily the optimal choice any more in minimizing the peak temperature. We identify the scenarios when a schedule using two different speeds can outperform the one using the lowest constant speed. In addition, we find that, when scheduling a periodic task set, the constant speed schedule is still the optimal solution for minimizing the peak temperature when the temperature is at its stable status. We formulate our conclusions into several theorems with formal proofs.||||
58|10||Perceptual control architecture for cyberâphysical systems in traffic incident management|Based on Perceptual Control Theory, we study the problem of unified modeling for incompatible approaches of Cyber–Physical Systems (CPSs). Inspired by the effective organization of living systems structure accommodating heterogeneous information processing and environmental interaction, we propose Perceptual Control Architecture of CPSs, and take Traffic Incident Management systems as the modeling research carrier. Throughout the structure of Traffic Incident Management systems, the hierarchical negative feedback is constituted by perceptual and behavioral loops to ensure a mechanism of intelligence behavior. The internal representation is categorized into two intelligent spaces: physical-reflex space and cyber-virtual space. In physical-reflex space, the sensing-actuation mapping of objective world is built, through four levels of distributed traffic infrastructure. In cyber-virtual space, subjective decision using Bayesian reasoning network is defined by three levels: principles, interrelated factors and situation assessment. Through evaluation of field operation in Dalian, the Traffic Incident Management under the developed architecture shows a considerable reduction in response time as well as assessment inaccuracy. The test results explicated the effectiveness of the architecture on integrating complex Cyber–Physical functions. Besides transportation systems, the modeling approach could be a well-defined unified architecture applied to other CPSs.||||
58|10||Adaptive response time control for metadata matching in information dissemination systems|Information dissemination is of increasing importance to our society. Existing work mainly focuses on delivering information from sources to sinks in a timely manner based on established subscriptions, with the assumption that those subscriptions are persistent. However, the bottleneck of many information dissemination systems is actually the matching process to continuously reevaluate such subscriptions between numerous sources and numerous sinks, in response to dynamically varying information attributes at runtime. In this paper, we propose an adaptive control architecture to meet the response time constraints on metadata matching in an example information dissemination system. Our adaptive controller features a rigorous design based on well-established control theory for guaranteed control accuracy and system stability. Furthermore, our controller can adapt to changes in the system model without reconfiguration and profiling. Empirical results on a physical testbed demonstrate that our controller has more accurate control and improved system quality of service than both an open-loop solution and a typical heuristic solution.||||
58|10||Randomized execution algorithms for smart cards to resist power analysis attacks|In recent years, the applications of smart cards become wider and wider. Smart cards are used in various areas and play a vital role in many security systems. At the same time, the security issue of smart cards has been more and more important. Smart cards are vulnerable to several attack approaches such as power analysis attacks. In this paper, we propose four novel algorithms to defend against power analysis attacks via randomized execution of programs. The experimental results confirm that the new approaches can even the power distribution of applications in smart cards, which makes smart cards less susceptible to power analysis attacks. Our approaches are general and not limited to a certain application.||||
58|10||Three-phase time-aware energy minimization with DVFS and unrolling for Chip Multiprocessors|Energy consumption has been one of the most critical issues in the Chip Multiprocessor (CMP). Using the Dynamic Voltage and Frequency Scaling (DVFS), a CMP system can achieve a balance between the performance and the energy-efficiency. In this paper, we propose a three-phase discrete DVFS algorithm for a CMP system dedicated to applications where the period of the applications’ task graph is smaller than the deadline of tasks. In these applications, multiple task graphs are unrolled and then concatenated together to form a new task graph. The proposed DVFS algorithm is applied to the newly formed task graph to stretch tasks’ execution time, lower operating frequencies of processors and achieve the system power efficiency. Experimental results show that the proposed algorithm reduces the energy dissipation by 25% on average, compared to previous DVFS approaches.||||
58|2|http://www.sciencedirect.com/science/journal/13837621/58/2|Memory power optimization of Java-based embedded systems exploiting garbage collection information|Nowadays, Java is used in all types of embedded devices. For these memory-constrained systems, the automatic dynamic memory manager (Garbage Collector or GC) has been always a key factor in terms of the Java Virtual Machine (JVM) performance. Moreover, in current embedded platforms, power consumption is becoming as important as performance. Thus, in this paper we present an exploration, from an energy viewpoint, of the different possibilities of memory hierarchies for high-performance embedded systems when used by state-of-the-art GCs. This is a starting point for a better understanding of the interactions between the Java applications, the memory hierarchy and the GC.||||
58|2||Hardware-assisted energy monitoring architecture for micro sensor nodes|In this paper, we present a hardware-assisted energy monitoring architecture, HEMA, which provides power information for individual system components in micro sensor nodes. HEMA gives information on how the system components of sensor nodes spend energy while applications are running. To develop a practical runtime monitoring system we used a power monitoring technique based on a battery monitor IC (Integrated Circuit) which is typically used in embedded systems. HEMA uses a software technique to monitor device usage patterns, and combines it with hardware-assisted power information in runtime. However, the battery monitor IC has problems in monitoring individual devices. In this paper, we developed a software technique to supplement its drawbacks. To evaluate the proposed system, we built a micro sensor node with battery monitor IC. We operated real applications on the sensor node and conducted a comparative analysis with a dedicated power monitor. Our experiment results show that HEMA indeed provides a suitable architecture for runtime power analysis with low overhead.||||
58|2||ARC-H: Adaptive replacement cache management for heterogeneous storage devices|||||
58|3-4|http://www.sciencedirect.com/science/journal/13837621/58/3-4|On the interfacing between QEMU and SystemC for virtual platform construction: Using DMA as a case|In this paper, we present an interface for the hardware modeled in SystemC to access those modeled in QEMU on a QEMU and SystemC-based virtual platform. By using QEMU as the instruction-accurate instruction set simulator (IA-ISS) and its capability to run a full-fledged operating system such as Linux, the virtual platform with the proposed interface can be used to facilitate the co-design of hardware models and device drivers at the early stage of Electronic System Level (ESL) design flow. In other words, by using such a virtual platform, the hardware models and associated device drivers can be cross verified while they are being developed so that malfunctions in the hardware models or the device drivers can be easily detected. Moreover, the virtual platform with the proposed interface is capable of providing statistics of instructions executed, memory accessed, and I/O performed at the instruction-accurate level—thus not only making it easy to evaluate the performance of the hardware models but also making it possible for design space exploration.||||
58|3-4||Instruction set architectural guidelines for embedded packet-processing engines|||||
58|3-4||Network-on-Chip virtualization in Chip-Multiprocessor Systems|It is expected that Chip Multiprocessor Systems (CMPs) will contain more and more cores in every new generation. However, applications for these systems do not scale at the same pace. In order to obtain a good CMP utilization several applications will need to coexist in the system and in those cases virtualization of the CMP system will become mandatory. In this paper we analyze two virtualization strategies at NoC-level aiming to isolate the traffic generated by each application to reduce or even eliminate interferences among messages belonging to different applications. The first model handles most interferences among messages with a virtual-channels (VCs) implementation reducing both execution time and network latency. However, using VCs results in area and power overhead due to the cost of control and buffer implementation. In contrast, the second model is based on the resource partitioning strategies which results in a space partitioning of the CMP chip in several regions. For this last model, Virtual-Regions (VR), we use a reconfiguration algorithm of the network that is able to dynamically adapt the network partitions in order to satisfy the application requirements. The paper shows a comparison of both models and identifies their main advantages and disadvantages. From our experimental results, we show that our proposal obtains in terms of execution time average improvements of 30% for parallel applications when compared to a baseline scenario. Moreover, when compared to a VCs implementation, our proposal improves the average execution time by 9% for parallel applications.||||
58|3-4||A dynamically reconfigurable communication architecture for multicore embedded systems|To deal with the communication bottleneck of multiprocessor systems, several communication architectures have been proposed in the last decade. Yet, none of them has demonstrated the performance of the direct connections between two communicating units. In this paper, we propose dynamically reconfigurable point-to-point (DRP2P) interconnects for setting up direct connection between two communicating units before the communication starts. DRP2P is neither point-to-point (P2P) nor Network-on-Chip (NoC); it stands between these two on-chip communication architectures. It is as fast as P2P and as scalable as NoC. Instead of using routers like in NoC, we utilize partial reconfiguration ability of FPGAs for routing data packets. Furthermore, DRP2P can work both on regular and irregular topologies. The only drawback of our approach is the reconfiguration latency. This drawback is completely hidden when the reconfiguration of the communication links is achieved during the computation times of the cores. DRP2P solves the scalability issue of P2P by setting up on-demand communication-specific links between cores. So, the occupied area and the total power consumption of communication architecture can be reduced significantly. We designed an on-chip self-reconfiguration core, c2c2PCAP so as to achieve DRP2P interconnects as fast as possible. The c2c2PCAP core is designed for Xilinx FPGAs and can partially reconfigure the FPGA at the highest rate proposed by the manufacturer (e.g. up to 400 MB/s for Virtex-4).||||
58|3-4||Reliability-aware core partitioning in chip multiprocessors|||||
58|5|http://www.sciencedirect.com/science/journal/13837621/58/5|Special issue on Model Based Engineering for Embedded Systems Design|||||
58|5||Expressing embedded systems configurations at high abstraction levels with UML MARTE profile: Advantages, limitations and alternatives|Embedded systems have become an essential aspect of our professional and personal lives. From avionics, transport and telecommunication systems to general commercial appliances such as smart phones, high definition TVs and gaming consoles; it is difficult to find a domain where these systems have not made their mark. Moreover, Systems-on-Chips (SoCs) which are considered as an integral solution for designing embedded systems, offer advantages such as run-time reconfiguration that can change system configurations during execution, depending upon Quality-of-Service (QoS) criteria such as performance and energy levels. This article deals with aspects related to modeling of these configurations, useful for describing various states of an embedded system, from both structural and operational viewpoints. Our proposal adapts a high abstraction level approach based on the principles of Model-Driven Engineering (MDE) and takes into account the UML MARTE profile for modeling of real-time and embedded systems. Elevating the design abstraction levels help to increase design productivity and achieve execution platform independence, among other advantages. The article details the current proposition of configurations in MARTE via some examples, and points out the advantages as well as some limitations, mainly concerning the semantic aspects of the defined concepts. Finally, we report our experiences on the modeling of an alternate notion of configurations and execution modes within the MARTE compliant Gaspard2 SoC Co-Design framework that has been successful for the design as well as implementation of FPGA based SoCs.||||
58|5||An aspect-oriented, model-driven approach to functional hardware verification|The cost of correcting errors in the design of an embedded system’s hardware components can be higher than for its software components, making it important to test as early as possible. Testing hardware components before they are implemented involves verifying the design through either formal or more commonly, simulation-based functional verification. Performing functional verification of a hardware design requires software-based simulators and verification testbenches. However, the increasing complexity of embedded systems is contributing to testbenches that are progressively more difficult to understand, maintain, extend and reuse across projects. This paper presents an aspect-oriented domain-specific modelling language for the e hardware verification language that can be used as part of a model-based software engineering process. The modelling language is designed to produce well modularised models from which e code can be generated, thereby improving engineers ability to develop testbenches that can be more easily maintained, adapted and reused. We demonstrate the suitability of the modelling language through its application to a representative testbench from the automotive semiconductor industry.||||
58|5||MARTE profile extension for modeling dynamic power management of embedded systems|The profile for Modeling and Analysis of Real-time and Embedded systems (MARTE) is a standard UML profile promoted by the Object Management Group (OMG). MARTE defines a framework for annotating non-functional properties of embedded systems to UML models as well as a generic package for modeling power consumption and heat dissipation of HW components. However, for modeling and analysing systems that adopt complex dynamic power management (DPM) policies and techniques additional expression power is needed. This article presents a way of modeling system-wide dynamic power management aspects of embedded systems with a UML2 profile extension. The proposed profile is compatible with the MARTE profile and can be used as its extension. The main idea of our proposal is that each HW component is associated with a state machine description that defines its time-variant power characteristics. Based on these, the system-wide power configurations are identified and modeled. Finally, application use cases or operational modes are bound to execute on certain power configurations. The models can be analysed to estimate the total energy dissipation. The MARTE and proposed DPM profile are used to model two case study platforms with different kind of DPM strategies.||||
58|6-7|http://www.sciencedirect.com/science/journal/13837621/58/6-7|An efficient method for record management in flash memory environment|Flash memory has its unique characteristics: the write operation is much more costly than the read operation, and in-place updating is not allowed. In this paper, we analyze how these characteristics affect the performance of clustering and non-clustering methods for record management, and show that non-clustering is more suitable in flash memory environment. Also, we identify the problems of the existing non-clustering method when applied to flash memory environment without any modification, and propose an effective method for record management in flash memory databases. This method, which is basically based on the non-clustering method, tries to store consecutively inserted records in the same page in order to make it possible to process them with only one write operation. In this paper, we call this method group write. Moreover, we propose two novel techniques for achieving efficient group writes: (1) dedicated buffers for group writes and (2) free space lists managed in main memory for maintaining only those pages having large free space. Our method greatly improves the write performance of database applications running in flash memory. For performance evaluation, we conduct a variety of experiments. The results show that our method achieves speed up by up to 1.67 times compared with the original non-clustering method.||||
58|6-7||Power- and time-aware buffer cache management for real-time embedded databases|Due to the explosive increases of data from both the cyber and physical worlds, the demand for database support in embedded systems is increasing. Databases for embedded systems, or embedded databases, are expected to provide timely in situ data services under various resource constraints, such as limited energy. However, traditional buffer cache management schemes, in which the primary goal is to minimize the number of I/O operations, is problematic since they do not consider the constraints of modern embedded devices such as limited energy and distinctive underlying storage. In particular, due to asymmetric read/write characteristics of flash memory-based storage of modern embedded devices, minimum buffer cache misses neither coincide with minimum power consumption nor minimum I/O deadline misses. In this paper we propose a novel power- and time-aware buffer cache management scheme for embedded databases. A novel multi-dimensional feedback control architecture is proposed and the characteristics of underlying storage of modern embedded devices is exploited for the simultaneous support of the desired I/O power consumption and the I/O deadline miss ratio. We have shown through an extensive simulation that our approach satisfies both power and timing requirements in I/O operations under a variety of workloads while consuming significantly smaller buffer space than baseline approaches.||||
58|6-7||Virtualization of reconfigurable coprocessors in HPRC systems with multicore architecture|HPRC (High-Performance Reconfigurable Computing) systems include multicore processors and reconfigurable devices acting as custom coprocessors. Due to economic constraints, the number of reconfigurable devices is usually smaller than the number of processor cores, thus preventing that a 1:1 mapping between cores and coprocessors could be achieved. This paper presents a solution to this problem, based on the virtualization of reconfigurable coprocessors. A Virtual Coprocessor Monitor (VCM) has been devised for the XtremeData XD2000i In-Socket Accelerator, and a thread-safe API is available for user applications to communicate with the VCM. Two reference applications, an IDEA cipher and an Euler CFD solver, have been implemented in order to validate the proposed architecture and execution model. Results show that the benefits arising from coprocessor virtualization outperform its overhead, specially when code has a significant software weight.||||
58|6-7||Compositional real-time models|This paper proposes a methodology for modelling the timing behaviour of hard real-time systems oriented to compositionality and reusability. When a system is built according to a modular structure, the methodology provides the system designer with capacity to build the real-time model of the system as a composition of the reusable timing models of the modules that make up the system. The modularization is applied at all levels: software, hardware and middleware. The methodology relies on a reactive modelling approach, i.e. the timing behaviour of a system is modelled by identifying and describing the timing behaviour of the activities executed in the system in response to events, coming either from the environment or from the timer. The methodology is based on the complementary concepts of model descriptor and model instance. The reusable timing model of a software or hardware module is formulated as a parameterized descriptor, which contains all the information about the internal elements of the module that is required to evaluate the behaviour of any application in which the module may be used. The analysable real-time model of a system is built by composing the model instances of the modules that form it, which are generated from their corresponding descriptors by assigning concrete values to all their parameters according to the specific configuration of the system.||||
58|8|http://www.sciencedirect.com/science/journal/13837621/58/8|FPGA-based architecture for the real-time computation of 2-D convolution with large kernel size|Bidimensional convolution is a low-level processing algorithm of interest in many areas, but its high computational cost constrains the size of the kernels, especially in real-time embedded systems. This paper presents a hardware architecture for the FPGA-based implementation of 2-D convolution with medium–large kernels. It is a multiplierless solution based on Distributed Arithmetic implemented using general purpose resources in FPGAs. Our proposal is modular and coefficient independent, so it remains fully flexible and customizable for any application. The architecture design includes a control unit to manage efficiently the operations at the borders of the input array. Results in terms of occupied resources and timing are reported for different configurations. We compare these results with other approaches in the state of the art to validate our approach.||||
58|8||An efficient model-based methodology for developing device-independent mobile applications|Current methodologies for developing mobile applications are mostly based on the application programming interfaces (APIs) offered by the native platform. Hence, most solutions are characterized by a low portability and/or reusability. In this paper, we propose a novel methodology based on a declarative and device-independent approach for developing event-driven mobile applications. The methodology relies on: (i) an abstract mobile device based on the user interface markup language; (ii) a content adaptation mechanism based on user preferences; (iii) a context adaptation mechanism based on a standardized context of delivery; (iv) a uniform set of client-side APIs based on an interface object model; (v) an efficient transformational model.||||
58|8||SINOF: A dynamic-static combined framework for dynamic binary translation|||||
58|8||Harmless, a hardware architecture description language dedicated to real-time embedded system simulation|Validation and verification of embedded systems through simulation can be conducted at many levels, from the simulation of a high-level application model to the simulation of the actual binary code using an accurate model of the processor. However, for real-time applications, the simulated execution time must be as close as possible to the execution time on the actual platform and in this case the latter gives the closest results. The main drawback of the simulation of application’s software using an accurate model of the processor resides in the development of a handwritten simulator which is a difficult and tedious task. This paper presents Harmless, a hardware architecture description language (ADL) that mainly targets real-time embedded systems. Harmless is dedicated to the generation of simulators for hardware platforms to develop and test real-time embedded applications. Compared to existing ADLs, Harmless (1) offers a more flexible description of the instruction set architecture (ISA) (2) allows to describe the microarchitecture independently of the ISA to ease its reuse and (3) compares favorably to simulators generated by the existing ADLs toolsets.||||
58|8||Retraction notice to ââSpecification and Verification of Dynamic Evolution of Software Architecturesâ [SYSARC 56 (10) (2010) 523â533]|||||
58|9|http://www.sciencedirect.com/science/journal/13837621/58/9|Exploring parallelization techniques based on OpenMP in H.264/AVC encoder for embedded multi-core processor|Recent advances in semiconductor technologies make it possible to integrate many processor cores in a small device package. The parallel execution capability of such multi-core processors can be exploited to enhance the performance of many traditional sequential applications. There have been numerous research activities to develop parallelization techniques using the OpenMp programming model, in order to speed up sequential applications such as the H.264/AVC codec, but mostly in the PC environment. Therefore, it is difficult to understand which parallelization technique fits well with the H.264/AVC encoder on an embedded multi-core architecture. In this paper, we present parallelization techniques applicable to the H.264/AVC encoder on ARM MPCore using the OpenMP programming model. Further, we propose an analytical model for the performance estimation of the H.264/AVC encoder, and we then verify the model accuracy by performing simulations using hardware/software co-verification tool. Our experimental results show that the parallelization techniques proposed in this paper for the embedded multi-core platform improve the encoder performance by up to 2.36 times, and that the parallelization technique exploiting data-level parallelism outperforms the one using task-level parallelism by 41%. It is also observed that balancing loads among processor cores is a critical parameter in achieving better scalability in the encoder.||||
58|9||k-Nearest neighbor query processing algorithm for cloaking regions towards user privacy protection in location-based services|Due to the advancement of wireless internet and mobile positioning technology, the application of location-based services (LBSs) has become popular for mobile users. Since users have to send their exact locations to obtain the service, it may lead to several privacy threats. To solve this problem, a cloaking method has been proposed to blur users’ exact locations into a cloaked spatial region with a required privacy threshold (k). With the cloaked region, an LBS server can carry out a k-nearest neighbor (k-NN) search algorithm. Some recent studies have proposed methods to search k-nearest POIs while protecting a user’s privacy. However, they have at least one major problem, such as inefficiency on query processing or low precision of retrieved result. To resolve these problems, in this paper, we propose a novel k-NN query processing algorithm for a cloaking region to satisfy both requirements of fast query processing time and high precision of the retrieved result. To achieve fast query processing time, we propose a new pruning technique based on a 2D-coodinate scheme. In addition, we make use of a Voronoi diagram for retrieving the nearest POIs efficiently. To satisfy the requirement of high precision of the retrieved result, we guarantee that our k-NN query processing algorithm always contains the exact set of k nearest neighbors. Our performance analysis shows that our algorithm achieves better performance in terms of query processing time and the number of candidate POIs compared with other algorithms.||||
58|9||Tracing and recording interrupts in embedded software|During the system development, developers often must correct wrong behavior in the software—an activity colloquially called program debugging. Debugging is a complex activity, especially in real-time embedded systems because such systems interact with the physical world and make heavy use of interrupts for timing and driving I/O devices.||||
||||||||
volume|issue|url|title|abstract||||
59|1|http://www.sciencedirect.com/science/journal/13837621/59/1|Dynamic objects: Supporting fast and easy run-time reconfiguration in FPGAs|Partial reconfiguration capabilities must be exploited to obtain the maximum benefits from dynamically reconfigurable FPGAs. Partial reconfiguration process management still faces a set of open problems that have thus far made it impossible to take full advantage of partial and dynamic reconfiguration. The work presented in this article proposes a novel architecture, development workflow, and modelling approach for dynamically reconfigurable systems management using an object model that offers a global solution. This solution is built on a system-level middleware that provides the infrastructure and tools for communication between different components in heterogeneous embedded systems. Several experiments were performed to test and evaluate each part of our proposed solution, and the obtained results are presented. These results demonstrate the excellent performance of our proposed solution.||||
59|1||A real-time embedded architecture for SIFT|SIFT has shown a great success in various computer vision applications. However, its large computational complexity has been a challenge to most embedded implementations. This paper presents a low-cost embedded system based on a new architecture that successfully integrates FPGA and DSP. It optimizes the FPGA architecture for the feature detection step of SIFT to reduce the resource utilization, and optimizes the implementation of the feature description step using a high-performance DSP. Due to this novel design, this system can detect SIFT feature and extract SIFT descriptor for detected features in real-time. Extensive experiments demonstrate its effectiveness and efficiency.||||
59|1||Embedded system for contrast enhancement in low-vision|This paper presents a real-time contrast enhancement system, implemented in FPGA and adapted to display the processed images on a Head Mounted Display (HMD). A novel visual processing scheme is proposed which combines a version of the algorithm known as Contrast Limited Adaptive Histogram Equalization (CLAHE) with a spatial filtering based on a bio-inspired retina model. The system is designed so that visually impaired people can improve their functionality in environments with non-uniform lighting or with abrupt changes in lighting conditions. The parallelism offered by FPGA devices allow to achieve real-time processing with VGA-resolution images, reaching up to 60 frames per second. This system, developed on a FPGA of reduced complexity, has been compared in performance with a parallel implementation on a portable platform based on GPU.||||
59|1||NP-SARC: Scalable network processing in the SARC multi-core FPGA platform|A multicore FPGA platform with cache-integrated network interfaces (NIs) has been developed, appropriate for scalable multicores, that combine the best of two worlds – the flexibility of caches (using implicit communication) and the efficiency of scratchpad memories (using explicit communication). Furthermore, the proposed scheme provides virtualized user-level RDAM capabilities and special hardware primitives (counter, queues) for the communication and synchronization of the cores.||||
59|1||Hierarchical neural networks based prediction and control of dynamic reconfiguration for multilevel embedded systems|Multimedia design such as video decoders are typically composed of several communicating tasks. Each task is characterized by its workload variation. The target device of this kind of application contains several processing unit. This calls for a dynamic management of hardware units to improve the QOS of the application and to optimally allocate resources. In this paper, we propose a new architecture based on hierarchical multilevel neural network to model workload variation of each task. The hierarchical structure of this neural network perfectly describes the multilevel decomposition of each hardware unit. The aim of this investigation is to build a design with a control unit that manages the architecture and resource allocation according to the neural network workload prediction.||||
59|1||A survey on application mapping strategies for Network-on-Chip design|Application mapping is one of the most important dimensions in Network-on-Chip (NoC) research. It maps the cores of the application to the routers of the NoC topology, affecting the overall performance and power requirement of the system. This paper presents a detailed survey of the work done in last one decade in the domain of application mapping. Apart from classifying the reported techniques, it also performs a quantitative comparison among them. Comparison has been carried out for larger sized test applications also, by implementing some of the prospective techniques.||||
59|PA|http://www.sciencedirect.com/science/journal/13837621/59/10/part/PA|Smart camera architecture|||||
59|PA||Contribution to the design of a CMOS image sensor with low-complexity video compression for wireless sensor networks|Demand for low-power sensing devices with integrated image processing capabilities is increasing, especially for resource-constrained systems such as WSNs. CMOS technology enables the integration of image sensing and image processing, which improves the overall system performance. The aim of this paper is to present and evaluate a smart image sensor integrating a user-driven video compression scheme designed to respect the energy constraints of image processing and transmission over WSNs. The interest of our solution is twofold. First, compression settings can be changed at run time depending on video characteristics. Second, compression is applied only on blocks that change significantly over time. This paper presents in details the internal hardware architecture of the proposed system and describes its performance, with relevant comparisons to some related works.||||
59|PA||Use of wavelet for image processing in smart cameras with low hardware resources|Images from embedded sensors need digital processing to recover high-quality images and to extract features of a scene. Depending on the properties of the sensor and on the application, the designer fits together different algorithms to process images. In the context of embedded devices, the hardware supporting those applications is very constrained in terms of power consumption and silicon area. Thus, the algorithms have to be compliant with the embedded specifications i.e. reduced computational complexity and low memory requirements. We investigate the opportunity to use the wavelet representation to perform good quality image processing algorithms at a lower computational complexity than using the spatial representation. To reproduce such conditions, demosaicing, denoising, contrast correction and classification algorithms are executed over several well known embedded cores (Leon3, Cortex A9 and DSP C6x). Wavelet-based image reconstruction shows higher image quality and lower computational complexity (3x) than usual spatial reconstruction. The use of wavelet decomposition also permits to increase the recognition rate of faces while decreasing computational complexity by a factor 25.||||
59|PA||Scene-based non-uniformity correction: From algorithm to implementation on a smart camera|Raw output data from image sensors tends to exhibit a form of bias due to slight on-die variations between photodetectors, as well as between amplifiers. The resulting bias, called fixed pattern noise (FPN), is often corrected by subtracting its value, estimated through calibration, from the sensor’s raw signal. This paper introduces an on-line scene-based technique for an improved fixed-pattern noise compensation which does not rely on calibration, and hence is more robust to the dynamic changes in the FPN which may occur slowly over time. This article first gives a quick summary of existing FPN correction methods and explains how our approach relates to them. Three different pipeline architectures for realtime implementation on a FPGA-based smart camera are then discussed. For each of them, FPGA implementations details, performance and hardware costs are provided. Experimental results on a set of seven different scenes are also depicted showing that the proposed correction chain induces little additional resource use while guarantying high quality images on a wide variety of scenes.||||
59|PA||Towards commoditized smart-camera design|||||
59|PA||Video streaming applications in wireless camera networks: A change detection based approach targeted to 6LoWPAN|Video streaming applications in wireless camera networks composed by low-end devices are attractive for enabling new pervasive high value services. When complying with the 6LoWPAN standard, the reduced amount of available bandwidth imposes the transmission of low resolution images. In this paper we present a low-complexity algorithm based on background subtraction and error resilience techniques aimed at reducing the transmission bandwidth of a video stream of uncompressed images thus permitting a higher frame rate. By means of realistic simulation studies, the performance of the presented algorithm is analyzed against state-of-the-art solutions like JPEG. These results can be considered for designing a next generation of smart cameras suited for 6LoWPAN.||||
59|PA||Efficient smart-camera accelerator: A configurable motion estimator dedicated to video codec|Smart cameras are used in a large range of applications. Usually the smart cameras transmit the video or/and extracted information from the video scene, frequently on compressed format to fit with the application requirements. An efficient hardware accelerator that can be adapted and provide the required coding performances according to the events detected in the video, the available network bandwidth or user requirements, is therefore a key element for smart camera solutions. We propose in this paper to focus on a key part of the compression system: motion estimation. We have developed a flexible hardware implementation of the motion estimator based on FPGA component, fully compatible with H.264, which enables the integer motion search, the fractional search and variable block size to be selected and adjusted. The main contributions of this paper are the definition of an architecture allowing flexibility and some new hardware optimizations of the architecture of the motion estimation allowing the improvement of the performances (computing time or hardware resources) compared to the state of the art. The paper describes the design and proposes a comparison with state-of-art architectures. The obtained FPGA based architecture can process integer motion estimation on 720×576 video streams at 67 fps using full search strategy, and sub-pel refinement up to 650 KMacroblocks/s.||||
59|PA||Efficient communication support in predictable heterogeneous MPSoC designs for streaming applications|Streaming applications are an important class of applications in emerging embedded systems such as smart camera network, unmanned vehicles, and industrial printing. These applications are usually very computationally intensive and have real-time constraints. To meet the increasing demand for performance and efficiency in these applications, the use of application specific IP cores in heterogeneous Multi-Processor System-on-Chips (MPSoCs) becomes inevitable. However, two of the key challenges in integrating these IP cores into MPSoCs are (i) how to properly handle inter-core communication; (ii) how to map streaming applications in an efficient and predictable way. In this paper, we first present a predictable high-performance communication assist (CA) that helps to tackle these design challenges. The proposed CA has zero throughput overhead, negligible latency overhead, and significantly less resource usage compared to existing CA designs. The proposed CA also provides a unified abstract interface for both processors and accelerator IP cores with flexible data access support.||||
59|PA||Low power high-performance smart camera system based on SCAMP vision sensor|Vision sensors based upon pixel-parallel cellular processor arrays offer the unique opportunity to realise high-performance, flexible, low power image processing systems. By virtue of processing on the focal-plane, the energy-demanding requirement to digitize a captured frame’s raw pixel data is reduced, with returned data constituting only that which is salient. We describe a stand-alone vision system incorporating a SCAMP-3 vision chip, an FPGA and an ARM Cortex-M3 microcontroller. SCAMP integrated circuits operate as SIMD computers; each pixel incorporating a compact but powerful analogue processor and local memory, with all operations occurring in parallel over the 128 × 128 array. Algorithms are developed to operate natively upon the focal-plane as far as possible, with additional serial and higher-level operations occurring on the microcontroller. The power consumption of the system is algorithm-dependent. An algorithm developed for loiterer detection at 8 fps has been shown to consume an average power of 5.5 mW, with a more complex object tracking and counting system consuming 29 mW.||||
59|PA||Single-lens low-disparity stereo using microlenses|In the framework of the 3-SIS project, we are designing a three-channel miniature camera using an integrated block-based processor and on-chip parallel (SIMD) image processing. This paper describes a single-lens stereo algorithm that we are planning to implement on the narrow-angle channel of the camera. Using microlenses on the sensor delivers a chessboard-like interlaced stereo pair exploiting opposite areas of the lens pupil. As the stereo base is limited by the lens aperture, disparities have very low values; choosing adequate focusing (depending on the scene) and prism deflection angles will give disparities between --1 and 1, which can be measured using local processing only involving directly neighbouring pixels, which is well adapted to the parallel image processing primitives. Subpixel correlation allows real-time, low precision but quasi-dense range information, which may be exploited as an extra clue into the image segmentation process.||||
59|PA||A hierarchical vision processing architecture oriented to 3D integration of smart camera chips|This paper introduces a vision processing architecture that is directly mappable on a 3D chip integration technology. Due to the aggregated nature of the information contained in the visual stimulus, adapted architectures are more efficient than conventional processing schemes. Given the relatively minor importance of the value of an isolated pixel, converting every one of them to digital prior to any processing is inefficient. Instead of this, our system relies on focal-plane image filtering and key point detection for feature extraction. The originally large amount of data representing the image is now reduced to a smaller number of abstracted entities, simplifying the operation of the subsequent digital processor. There are certain limitations to the implementation of such hierarchical scheme. The incorporation of processing elements close to the photo-sensing devices in a planar technology has a negative influence in the fill factor, pixel pitch and image size. It therefore affects the sensitivity and spatial resolution of the image sensor. A fundamental tradeoff needs to be solved. The larger the amount of processing conveyed to the sensor plane, the larger the pixel pitch. On the contrary, using a smaller pixel pitch sends more processing circuitry to the periphery of the sensor and tightens the data bottleneck between the sensor plane and the memory plane. 3D integration technologies with a high density of through-silicon-vias can help overcome these limitations. Vertical integration of the sensor plane and the processing and memory planes with a fully parallel connection eliminates data bottlenecks without compromising fill factor and pixel pitch. A case study is presented: a smart vision chip designed on a 3D integration technology provided by MIT Lincoln Labs, whose base process is 0.15 Î¼m FD-SOI. Simulation results advance performance improvements with respect to the state-of-the-art in smart vision chips.||||
59|PB|http://www.sciencedirect.com/science/journal/13837621/59/10/part/PB|Advanced smart vehicular communication system and applications|||||
59|PB||Design and implementation of a P2P communication infrastructure for WSN-based vehicular traffic control applications|Wireless Sensor Network (WSN) technology has recently started to be applied to Intelligent Transportation Systems (ITS). In this kind of network, sensor nodes are scattered over a sensor field and they transmit information to a sink node. In this paper, we first propose the design of a proxy functionality for the sink node, which will be responsible for exchanging information with other proxy nodes by means of a new Peer-to-Peer (P2P) communication infrastructure implemented using a Distributed Hash Table (DHT). This infrastructure will also facilitate the distributed processing of sensing data in order to perform path planning in a vehicular traffic control application. This approach will be able to distribute the computational tasks for route calculations over the different sink nodes in the network. This paper will present a detailed prototype of the proposed P2P communication infrastructure.||||
59|PB||A-GR: A novel geographical routing protocol for AANETs|With the rapid growth of general aircraft and lack of ground facilities, air traffic control and aeronautical communication system face huge pressure, especially in low-altitude airspace. Aeronautical ad hoc networks (AANETs) are large scale multi-hop wireless network of aircraft, which could provide direct air-to-air communication between aircraft without ground infrastructures. However, the features of aircraft node present a great challenge to provide efficient and reliable data packet delivery in AANETs. In this paper, we present an Automatic dependent surveillance-broadcast (ADS-B) system aided geographic routing protocol (called A-GR) for AANETs. The proposed A-GR uses the position and velocity of aircraft provided by airborne ADS-B system to eliminate the traditional routing beaconing and presents a velocity-based metric for next hop selection, which could adaptively cope with the fast moving of aircraft and dynamic changes of network topology. Many simulations are performed and the results show that the proposed A-GR can effectively decrease the routing overhead, improve the packet delivery ratio and make good use of the network resources.||||
59|PB||File downloading oriented Roadside Units deployment for vehicular networks|Vehicular Ad hoc Networks (VANETs) have been recently introduced to provide high-speed Internet access to vehicles by deploying 802.11 enhanced Roadside Units (RSUs) along roads. However, few file downloading oriented RSU deployment strategies have been proposed. In this paper, we propose a new RSU deployment strategy for file downloading in VANETs. The encounters between vehicles and RSUs are modeled as a time continuous homogeneous Markov chain. The optimal inter-meeting time between vehicles and RSUs is analyzed based on the encounter model. Then, the road network is modeled as a weighted undirected graph, and a RSU deployment algorithm is designed based on the depth-first traversal algorithm for edges of a graph. Simulation results show that the proposed RSU deployment algorithm can satisfy the file downloading service requirements with the lowest RSU deployment cost.||||
59|PB||Energy efficient path selection scheme for OFDMA-based two-hop cellular systems|In this paper, we proposed new path selection for two-hop cellular systems called energy efficient path selection (EEPS) scheme to enhance downlink system throughput and reduce transmission energy consumption. In the EEPS scheme, a base station determines either a single-hop or two-hop path that uses less energy consumption. The simulation results show that the EEPS scheme outperforms a conventional path selection scheme which uses high signal to interference and noise ratio (SINR) strength based path selection in terms of system throughput and energy consumption. Also, we suggest the appropriate relay station position and optimal resource allocation for access and relay communications.||||
59|PB||HyBR: A Hybrid Bio-inspired Bee swarm Routing protocol for safety applications in Vehicular Ad hoc NETworks (VANETs)|Increasing interests in Vehicular Ad hoc NETworks (VANETs) over the last decade have led to huge investments in technologies and research to improve road safety by providing timely and accurate information to drivers and authorities. To achieve the timely dissemination of messages, various routing protocols for VANETs have been recently proposed. We present a Hybrid Bee swarm Routing (HyBR) protocol for VANETs. HyBR is based on the continuous learning paradigm in order to take into account the dynamic environmental changes in real-time which constitute a key property of VANETs. The protocol combines the features of topology routing with those of geographic routing. HyBR is a unicast and a multipath routing protocol (aimed at both urban and rural scenarios) which guarantees road safety services by transmitting packets with minimum delays and high packet delivery. To demonstrate the effectiveness and the performance of HyBR, we conducted a performance evaluation based on several metrics such as end-to-end delay, packet delivery ratio, and normalized overhead load. We obtained better performance results with HyBR in contrast to results obtained from traditional routing algorithms such as Ad hoc On-Demand Distance Vector (AODV) topology-based routing protocol and Greedy Perimeter Stateless Routing (GPSR) geography-based protocol.||||
59|PB||Learning automata-based virtual backoff algorithm for efficient medium access in vehicular ad hoc networks|In vehicular ad hoc networks (VANETs), the effect of signal distortions due to the existence of adjoining vehicles and the high degrees of mobility of the nodes affect the reliability of transmission. This paper presents a learning automata (LA)-based solution, named LAVBA, for reliable and efficient medium access control, based on virtual backoff algorithm (VBA), in VANETs. In VBA, a counter is used at each node for fair distributed channel access. The performance of VBA medium access depends on the selection of the sequence number. We use an LA-based approach for obtaining an optimal sequence number. It is observed that the performance of the proposed scheme is improved, when compared to the legacy schemes such as distributed coordination function (DCF) and VBA.||||
59|PB||Towards a content-centric approach to crowd-sensing in vehicular clouds|Vehicular mobile cloud computing is a new research direction that will address the interactions of vehicles and other mobile devices that participate in collaborative sensing, processing and dissemination of information. Due to intermittent connectivity that can cause substantial message delivery delays or loss of messages, conventional internet protocols may not be suitable to support reliable and efficient information dissemination over vehicular networks. In this paper we address the problem of information dissemination in vehicular clouds using the new paradigm of semantic based networking of information. We show that a more intelligent and context-aware networking of information is advantageous in developing more scalable and reliable methods of routing and dissemination of information. In particular we look at the network topology beyond physical connectivity but in terms of content connectivity and how the participating nodes can be clustered based on their context and interests in some relevant information. We propose a novel selective network coding method to enhance the reliability and efficiency of information dissemination, which uses node clusters in a graph model of the information connectivity as the basis of the decision making. Analysis and numerical results show improved reliability and a lower complexity compared with the epidemic network coding based dissemination approach.||||
59|PB||WEVAN â A mechanism for evidence creation and verification in VANETs|There are traffic situations (e.g. incorrect speeding tickets) in which a given vehicle’s driving behavior at some point in time has to be proved to a third party. Vehicle-mounted sensorial devices are not suitable for this matter since they can be maliciously manipulated. However, surrounding vehicles may give their vision on another one’s behavior. Furthermore, these data may be shared with the affected vehicle through VANETs. In this paper, a VANET-enabled data exchange mechanism called WEVAN is presented. The goal of this mechanism is to build and verify evidences based on surrounding vehicles (called witnesses) testimonies. Due to the short-range nature of VANETs, the connectivity to witnesses may be reduced with time – the later their testimonies are requested, the lower the amount of witnesses may be. Simulation results show that if testimonies are ordered 5 s later, an average of 38 testimonies may be collected in highway scenarios. Other intervals and road settings are studied as well.||||
59|PB||How to build vehicular ad-hoc networks on smartphones|Vehicular ad-hoc networks have been defined in the literature as communications networks that allow disseminating information among vehicles to help to reduce traffic accidents and congestions. The practical deployment of such networks has been delayed mainly due to economic and technical issues. This paper describes a new software application to detect traffic incidents and exchange information about them, using only smartphones, without any central authority or additional equipment. Both road safety and communication security have been taken into account in the application design. On the one hand, the interface has been designed to avoid distractions while driving because it operates automatically and independently of the driver, through voice prompts. On the other hand, communication security, which is essential in critical wireless networks, is provided through the protection of attributes such as authenticity, privacy, integrity and non-repudiation. All this is achieved without increasing the price of vehicles and without requiring the integration of new devices neither in vehicles nor on roads. The only prerequisite is to have a smartphone equipped with Wi-Fi connectivity and GPS location in each vehicle. The proposed application has been successfully validated both in large-scale NS-2 simulations and in small-scale real tests to detect traffic congestions and empty parking spaces.||||
59|PB||An in-depth analysis on traffic flooding attacks detection and system using data mining techniques|Recently, as network traffic flooding attack such as DoS and DDoS have posed devastating threats on network services, rapid detection, and semantic analysis are the major concern for secure and reliable network services. In addition, in a recent issue of the safety and comfort of vehicles and communication technologies for service is required. We propose a traffic flooding attack detection and an in-depth analysis system that uses data mining techniques. In this paper we (1) designed and implemented a system that detects traffic flooding attacks. Then, it executes classification by attack type and it uses SNMP MIB information based on C4.5 algorithm; (2) conducted a semantic interpretation that extracts and analyzes the rules of execution mechanism that are additionally provided by C4.5; (3) performed an in-depth analysis on the attack patterns and useful knowledge inherent in their data by type, utilizing association rule mining. Classification by attack and attack type based on C4.5 and association rules, automatic rule extraction and semantic in-depth interpretation, which are proposed in this paper, provide a positive possibility to add momentum towards the development of new methodologies for intrusion detection systems as well as to support establishing policies for intrusion detection and response systems.||||
59|PC|http://www.sciencedirect.com/science/journal/13837621/59/10/part/PC|Embedded Systems Software Architecture|||||
59|PC||A development and verification framework for the SegBus platform|We describe the creation of a development framework for a platform-based design approach, in the context of the SegBus platform. The work intends to provide automated procedures for platform build-up and application mapping. The solution is based on a model-based process and heavily employs the UML. We develop a Domain Specific Language to support the platform modeling. An emulator is consequently introduced to allow an as much as possible accurate performance estimation of the solution, at high abstraction levels. Automated execution schedule generation is also featured. The resulting framework is applied to build actual design solutions for a MP3-decoder application.||||
59|PC||A unified execution model for multiple computation models of streaming applications on a composable MPSoC|In this paper we propose a unified model of execution that aims to fill the abstraction level gap between the primitives of models of computation and the ones of an MPSoC. This model targets a composable MPSoC platform and supports the sequential, Kahn process networks, and dataflow models. Our model comprises of (1) execution operations implementing the primitives in the models of computation, and (2) a time model of execution of streaming applications on a composable platform. We implement these models of computation with the model of execution, and discuss the trade-offs involved. Case studies on an FPGA prototype of the composable MPSoC demonstrate how the model of execution actually works on a real platform. Furthermore they indicate that multiple applications modeled in KPN and dataflow run composably on the platform.||||
59|PC||Dynamic distribution of robot control components under hard realtime constraints â Modeling, experimental results and practical considerations|||||
59|PC||Modeling and efficient solving of extra-functional properties for adaptation in networked embedded real-time systems|In this paper, we focus on modeling and efficient solving of extra-functional properties for embedded systems, in particular automotive systems. We introduce an integrated model of system constraints for efficient computation of software components being allocated to hardware platforms (ECUs), which is a prerequisite for runtime adaptation. For a set of over 126,000 constraints in a realistic automotive system, we compare SAT-solving and different heuristic search algorithms. We show that SAT-solving provides solutions in several seconds, and SAT-solving is more efficient for larger systems, whereas other heuristic search algorithms are slightly better for smaller problems.||||
59|PC||Multi-objective exploitation of pipeline parallelism using clustering, replication and duplication in embedded multi-core systems|With the popularity of mobile device, people require more computing power to run emerging applications. However, the increase in power consumption is a major problem because power is quite limited in embedded systems. Our goal is to consider power consumption along with latency and throughput. We proposed a heuristic algorithm, called Parallel Pipeline Latency Optimization for high performance embedded systems (PaPiLO), based on clustering, replication and duplication, to minimize latency under power and throughput constraints. Experimental results show our method can get 15% latency reduction and 10% improvements for random task graphs and MPEG-2 decoder, respectively.||||
59|PC||A reference architecture for cooperative driving|Cooperative driving systems enable vehicles to adapt their motion to the surrounding traffic situation by utilizing information communicated by other vehicles and infrastructure in the vicinity. How should these systems be designed and integrated into the modern automobile? What are the needed functions, key architectural elements and their relationships? We created a reference architecture that systematically answers these questions and validated it in real world usage scenarios. Key findings concern required services and enabling them via the architecture. We present the reference architecture and discuss how it can influence the design and implementation of such features in automotive systems.||||
59|2|http://www.sciencedirect.com/science/journal/13837621/59/2|Guest editorial: Workshop on Reconfigurable Computing|||||
59|2||On supporting rapid exploration of memory hierarchies onto FPGAs|This paper introduces a novel methodology for enabling fast yet accurate exploration of memory organizations onto FPGA devices. The proposed methodology is software supported by a new open-source tool framework, named NAROUTO. This framework is the only public available solution for performing architecture-level exploration, as well as application mapping onto FPGA devices with different memory organizations, under a variety of design criteria (e.g. delay improvement, power optimization, area savings, etc.). Experimental results with a number of industrial oriented kernels prove the efficiency of the proposed solution, as compared to similar approaches, since it provides better manipulation of memory blocks, leading to architectures with higher performance in terms of area, power and delay.||||
59|2||A template system for the efficient compilation of domain abstractions onto reconfigurable computers|Past research has addressed the issue of using FPGAs as accelerators for HPC systems. Such research has identified that writing low level code for the generation of an efficient, portable and scalable architecture is challenging. We propose to increase the level of abstraction in order to help developers of reconfigurable accelerators deal with these three key issues. Our approach implements domain specific abstractions for FPGA based accelerators using techniques from generic programming. In this paper we explain the main concepts behind our system to Design Accelerators by Template Expansions (DATE). The DATE system can be effectively used for expanding individual kernels of an application and also for the generation of interfaces between various kernels to implement a complete system architecture. We present evaluations for six kernels as examples of individual kernel generation using the proposed system. Our evaluations are mainly intended to provide a proof-of-concept. We also show the usage of the DATE system for integration of various kernels to build a complete system based on a Template Architecture for Reconfigurable Accelerator Designs (TARCAD).||||
59|2||Towards a multiple-ISA embedded system|In these days, every new added hardware feature must not change the underlying Instruction Set Architecture (ISA), in order to avoid adaptation or recompilation of existing code. Binary translation (BT) allows the execution of already compiled applications on different architectures. Therefore, it opens new possibilities for designers, previously tied to a specific ISA and all its legacy hardware issues. To overcome the BT inherent performance penalty, we propose a new mechanism based on a dynamic two-level binary translation system. While the first level is responsible for the BT de facto to an intermediate machine language, the second level optimizes the already translated instructions to be executed on the target architecture. The system is totally flexible: it supports the porting of radically different ISAs and the employment of different target architectures. This paper presents the first effort towards this direction: it translates code implemented in the x86 ISA to MIPS assembly (the intermediate language), which will be optimized by the target architecture: a dynamically reconfigurable array. We show that it is possible to maintain binary compatibility, with performance improvements and no energy losses, when compared to native execution.||||
59|3|http://www.sciencedirect.com/science/journal/13837621/59/3|Resource management for multimedia applications, distributed in open and heterogeneous home networks|The home network is an open, heterogeneous and distributed environment with quality of service requirement. Existing resource reservation strategies modify devices and applications and do not consider heterogeneity.||||
59|3||FPGA acceleration using high-level languages of a Monte-Carlo method for pricing complex options|In this paper we present an FPGA implementation of a Monte-Carlo method for pricing Asian options using Impulse C and floating-point arithmetic. In an Altera Stratix-V FPGA, a 149x speedup factor was obtained against an OpenMP-based solution in a 4-core Intel Core i7 processor. This speedup is comparable to that reported in the literature using a classic HDL-based methodology, but the development time is significantly reduced. Additionally, the use of a HLL-based methodology allowed us to implement a high-quality Gaussian random number generator, which produces more precise results than those obtained with the simple generators usually present in HDL-based designs.||||
59|3||Formal virtualization requirements for the ARM architecture|We present an analysis of the virtualizability of the ARMv7-A architecture carried out in the context of the seminal paper published by Popek and Goldberg 38 years ago. Because their definitions are dated, we first extend their machine model to modern architectures with paged virtual memory, IO and interrupts. We then use our new model to show that ARMv7-A is not classically virtualizable.||||
59|3||K-means clustering algorithm for multimedia applications with flexible HW/SW co-design|In this paper, we report a hardware/software (HW/SW) co-designed K-means clustering algorithm with high flexibility and high performance for machine learning, pattern recognition and multimedia applications. The contributions of this work can be attributed to two aspects. The first is the hardware architecture for nearest neighbor searching, which is used to overcome the main computational cost of a K-means clustering algorithm. The second aspect is the high flexibility for different applications which comes from not only the software but also the hardware. High flexibility with respect to the number of training data samples, the dimensionality of each sample vector, the number of clusters, and the target application, is one of the major shortcomings of dedicated hardware implementations for the K-means algorithm. In particular, the HW/SW K-means algorithm is extendable to embedded systems and mobile devices. We benchmark our multi-purpose K-means system against the application of handwritten digit recognition, face recognition and image segmentation to demonstrate its excellent performance, high flexibility, fast clustering speed, short recognition time, good recognition rate and versatile functionality.||||
59|3||Editorial Embedded Software Design for 3D Graphics Visualization|||||
59|3||gkDtree: A group-based parallel update kd-tree for interactive ray tracing|This paper proposes a new group-based acceleration data structure called gkDtree for interactive ray tracing of dynamic scenes. The main idea of the gkDtree is to construct the acceleration structure with a multi-level hierarchy, and to integrate a parallelization approach to result in a faster update and a more efficient tree traversal. A gkDtree can be viewed as a set of kd-trees, each of which is a local acceleration structure corresponding to a group. For a gkDtree, a scene is divided into several groups based on a scene graph. The local acceleration structure of each group involving only dynamic primitives is rebuilt. To achieve higher parallelization, dependencies among groups in different levels are removed before rebuilding occurs in parallel. To enhance the scalability of parallelization, a simple and fast load-balancing scheme is introduced. Furthermore, we apply a variety of accurate SAH (surface area heuristic) algorithms into tree generation for both static and dynamic groups. The experimental results show that a gkDtree has a real-time update performance. It has an update performance that is up to 166 times faster than a kd-tree for our test scenes in a six-core hardware system environment. Furthermore, the results also show that tree traversal performance of a gkDtree is competitive with that of a kd-tree.||||
59|3||Efficient hardware implementation of Ray Tracing based on an embedded software for intersection computation|Parallel implementations of Ray Tracing have been enabling real time performance, as the algorithm is embarrassingly parallel. However, in order to achieve both interactivity and real time performance, the algorithm should run at a high frame rates, i.e. at least 60 frames per second. Thus, a custom parallel design in hardware is likely to achieve high rendering performance. In this paper, we improve the GridRT architecture presented in previous work. GridRT is capable of dealing with the main desirable features of Ray Tracing, such as shadows and reflection effects, imposing low area cost and a promising rendering performance. As to this work, an application-specific instruction has been added and the underlaying computation embedded into the processor’s microprogram in order to calculate the ray–triangle intersection computations. These computations are performed in pipeline, whenever possible, yielding to a considerable reduction in terms of cycles per intersection test. The presented architecture is based on the uniform grid acceleration structure. It allows for a massive twofold parallelism: parallel ray–triangle intersection tests as well as parallel processing of many rays. A hardware implementation of the improved architecture is presented, together with the corresponding performance results and resources requirements. The rendering time is reduced by 80% using a grid configuration of eight processing elements and each intersection computation time is reduced by 50% with respect to the original GridRT implementation.||||
59|4-5|http://www.sciencedirect.com/science/journal/13837621/59/4-5|Energy-efficient stream task scheduling scheme for embedded multimedia applications on multi-issued stream architectures|With the increasing requirement of computation and data amount in embedded multimedia applications, stream architectures are exploited to accelerate the multimedia processing. The increasing arithmetic units on stream architectures make energy consumption a critical metric. In this paper, an energy-efficient stream task scheduling scheme is proposed for embedded multimedia applications on a multi-issued stream architecture. The proposed scheduling scheme takes four steps for task mapping, V/F scaling and record reordering. The target stream application is first mapped to the processor in pipeline to reduce task dependency. Then the supply voltage and frequency of tasks are scaled down for energy reduction by DVFS without violating real-time constraints. The computation intensities of tasks are taken into consideration as the priority for more efficient energy reduction. Besides, when records are dependent, more parallelism is exploited and records are reordered to improve the processing performance. The proposed scheme is evaluated with a set of benchmarks and 14% energy reduction is achieved compared with the GeneS algorithm.||||
59|4-5||ENREM: An efficient NFA-based regular expression matching engine on reconfigurable hardware for NIDS|Regular expression is a critical mechanism in modern network security and widely used in network intrusion detection system to describe malicious patterns. In order to speed up the pattern matching process, a number of studies have been investigated to implement regular expression matching on reconfigurable hardware. Several optimizations have been proposed, however the problem of sharing sub-patterns between multiple regular expressions is not solved completely. In this paper we present ENREM, an Efficient NFA-based Regular Expression Matching Engine on reconfigurable hardware. We introduce a new infix and suffix sharing architecture and employ it along with several techniques to optimize the required area of pattern matching circuits. In addition we developed tools for automatically generating the Verilog HDL source code of ENREM circuit from any given set of Perl compatible regular expression patterns. In order to evaluate proposed architecture, we exploit Snort rules and implement ENREM on Xilinx Virtex-II Pro XC2VP-50 FPGA. The system is tested on NetFPGA platform with DARPA intrusion detection as input data to verify the accuracy of circuit. The experimental results show that ENREM can reduce 42% LUTs and 32% FlipFlops compared with previous approaches while maintains high-speed matching throughput from 1.45 to 2.35 Gbps.||||
59|4-5||A systematic reordering mechanism for on-chip networks using efficient congestion-aware method|In-order delivery is a critical issue of memory parallelism in network-based MPSoCs where multiple memories can be accessed simultaneously. In addition to the in-order delivery, network congestion is another subtle point that required to be taken into account for such architectures. Therefore, a congestion-aware method is necessitated to deal with the network congestion while coping with the ordering of transactions. In this paper, we present a streamlined method, named Global Load Balancing (GLB), in order to reduce the network congestion. The ideas behind the GLB method are twofold. The first idea is to use the global congestion information as a metric for arbitration in routers to reduce the congestion level of highly congested areas. The second idea is to use an adaptive scheduler in network interfaces based on the global congestion information to avoid additional traffic to congested areas. Experimental results with synthetic test cases demonstrate that the on-chip network utilizing the GLB method considerably outperforms a conventional on-chip network.||||
59|4-5||Preface for the Special Issue of GreenCOM 2011|||||
59|4-5||Static worst-case lifetime estimation of wireless sensor networks: A case study on VigilNet|This paper proposes a feasible static analysis approach to estimate the worst-case lifetime of WSNs, with specific focus on VigilNet. We statically estimate the lifetime of each node in VigilNet with a fixed initial energy budget through a hybrid approach, which integrates an Integer Linear Programming (ILP) based method to obtain the worst-case CPU energy consumption and domain-specific scenarios analysis to compute the worst-case radio energy consumption. Our experimental results indicate that this static approach can safely and accurately estimate the worst-case lifetime for WSNs.||||
59|4-5||On-line energy-efficient real-time task scheduling for a heterogeneous dual-core system-on-a-chip|On-line energy-efficient real-time task scheduling for a heterogeneous dual-core system-on-a-chip is a challenging problem due to precedence constraints and the varied properties of the general-purpose processor core and the synergistic processor core. This study proposes an on-line heterogeneous dual-core energy-efficient scheduling framework for dynamic workloads with real-time constraints. The energy efficiency ratio is presented to manage energy consumption while considering of the varied properties of the cores, while precedence constraints among the tasks are dealt with through interaction between bandwidth servers. This framework is configurable for low energy consumption and high system utilization. The capability of the proposed methodology is evaluated by a series of experiments and the results obtained are encouraging.||||
59|4-5||Quality of service aware power management for virtualized data centers|Nowadays, one of the most important goals of data center management is to maximize their profit by minimizing power consumption and service-level agreement violations of hosted applications. In this paper, we propose an integrated management solution which takes advantages of both virtual machine resizing and server consolidation to achieve energy efficiency and quality of service in virtualized data centers. A novelty of the solution is to integrate linear programming, ant colony optimization, and control theory techniques. Empirical results show that our approach can achieve power savings of 41.3% compared to an uncontrolled system, while ensuring application performance.||||
59|4-5||Informer homed routing fault tolerance mechanism for wireless sensor networks|Sensors in a wireless sensor network (WSN) are prone to failure, due to the energy depletion, hardware failures, etc. Fault tolerance is one of the critical issues in WSNs. The existing fault tolerance mechanisms either consume significant extra energy to detect and recover from the failures or need to use additional hardware and software resource. In this paper, we propose a novel energy-aware fault tolerance mechanism for WSN, called Informer Homed Routing (IHR). In our IHR, non cluster head (NCH) nodes select a limited number of targets in the data transmission. Therefore it consumes less energy. Our experimental results show that our proposed protocol can significantly reduce energy consumption, compared to two existing protocols: Low-Energy Adaptive Clustering Hierarchy (LEACH) and Dual Homed Routing (DHR).||||
59|4-5||An effective cache scheduling scheme for improving the performance in multi-threaded processors|Although in a multi-threaded processor, the processor may execute more than one process simultaneously to maximize the overall throughput of the system, the executing processes may compete with each other in using shared caches of the processor. This can seriously affect the average performance of the processes as the probability of cache hit for each process could be lowered. In this paper, we propose a new algorithm called the sharable cache partitioning algorithm (ShaParti), for scheduling the processor caches amongst co-running processes. In ShaParti, each executing process has its own cache and a priority scheme is designed for them to share the caches belonging to other executing processes. The performance goals of ShaParti are to improve the cache hit rates of the processes and at the same time the cache miss rates of other concurrent processes will not be lowered compared with the case in which each process has its own cache. Extensive experiments have been performed to illustrate the effectiveness of ShaParti in improving the performance in accessing shared processor caches.||||
59|6|http://www.sciencedirect.com/science/journal/13837621/59/6|Reducing cache and TLB power by exploiting memory region and privilege level semantics|The L1 cache in today’s high-performance processors accesses all ways of a selected set in parallel. This constitutes a major source of energy inefficiency: at most one of the N fetched blocks can be useful in an N-way set-associative cache. The other N-1 cachelines will all be tag mismatches and subsequently discarded.||||
59|6||JSA WATERS 2011|||||
59|6||SimTrOS: A heterogenous abstraction level simulator for multicore synchronization in real-time systems|To provide a common ground for the comparison of real-time multicore synchronization protocols we developed a framework that supports heterogenous levels of abstraction for simulated functionality and simulated timing. Our intention is to make the simulator available to the real-time research community and industrial users. For the latter we initially focus on automotive real-time systems. This paper describes the simulation framework and the novel idea of heterogenous abstraction levels that lies at the heart of its design. Notwithstanding the clear focus, we believe that the simulator itself as well as the concept of heterogenous abstraction levels can be useful in a significantly broader way.||||
59|6||Grasp: Visualizing the behavior of hierarchical multiprocessor real-time systems|||||
59|6||Modelling real-time applications based on resource reservations|This paper presents a strategy for the design of real-time applications relying on the resource-reservation paradigm, based on a new modelling element that describes the schedulable entities of the applications during the whole design cycle. A virtual view of this element is used first for evaluating a set of constraints that guarantee the application schedulability independently of the execution platform. Then, the element is viewed as a thread that receives the particular scheduling parameters required for a schedulable execution of the application in a specific platform. The purpose of the design process is to make both views compatible.||||
59|6||Modeling distributed real-time systems with MAST 2|Switched networks have an increasingly important role in real-time communications. The IEEE Ethernet standards have defined prioritized traffic (802.1p) and other QoS mechanisms (802.1q). The Avionics Full-Duplex Switched Ethernet (AFDX) standard defines a hard real-time network based on switched Ethernet. Clock synchronization is also an important service in some real-time distributed systems because it allows a global notion of time for event timing and timing requirements. In the process of defining the new MAST 2 model, clock synchronization modeling capabilities have been added, and the network elements have been enhanced to include switches and routers. This paper introduces the schedulability model that will enable an automatic schedulability analysis of a distributed application using switched networks and clock synchronization mechanisms.||||
59|6||On the gap between schedulability tests and an automotive task model|In this paper, we study the adequacy of available schedulability tests for monoprocessor fixed-priority systems to enable performing scheduling analysis for automotive applications. We show that, in spite of the work carried out during the last decade to enhance these tests in order to support more realistic task model, a gap still exists between the task model considered in these tests and the usual automotive task model. However, we claim that an extension of these tests is possible to support some of the uncovered automotive features. The aim of this study is to raise discussion and make researchers involved in the development of such schedulability tests be aware of the effort needed to bridge the gap between current schedulability tests and automotive task model mostly used. The study is illustrated by showing the concrete challenges faced when applying scheduling analysis to a case study derived from a real engine control application.||||
59|7|http://www.sciencedirect.com/science/journal/13837621/59/7|Exploiting domain knowledge in system-level MPSoC design space exploration|||||
59|7||Application-Specific Network-on-Chip synthesis with flexible router Placement|Network-on-Chip (NoC) has been proposed as a possible solution to the communication problem in nanoscale System-on-Chip (SoC) design. NoC architectures with optimized application-specific topologies have been found to be superior to the regular architectures in designing Multi-Processor System-on-Chip (MPSoC) solutions. The application specific NoC design problem takes as input the chip floorplan, library of NoC components, and communication requirements between the tasks of the application. It outputs the positions of the routers in the floorplan, such that, all communication requirements of the application are satisfied. This paper presents an Integer Linear Programming formulation of the problem, followed by a heuristic technique based on Particle Swarm Optimization (PSO) for finding the router positions from the set of available positions within the chip floorplan. The goal is to minimize the communication cost between cores, satisfying both the link length and router port constraints. The results have been shown on realistic benchmarks. Comparisons have been carried out with regular mesh and custom architectures having routers positioned at (i) the corners of the cores, (ii) the centers of the cores, and (iii) the intersections of the cores. Significant reductions in communication cost have been observed over all the cases. For smaller benchmarks, the optimum results obtained via ILP matches exactly with those reported by the PSO. Many of the existing router placement policies fail even for these small benchmarks, when restrictions are imposed on permissible link length. This establishes the merit of the PSO formulation. Link and router energy consumption of the synthesized NoC have been compared with regular mesh based architectures. The results show significant reduction in communication cost, area overhead, link energy and router energy in the synthesized NoC over regular mesh topology as well.||||
59|7||Limited carry-in technique for real-time multi-core scheduling|||||
59|7||Remotely reconfigurable hardwareâsoftware platform with web service interface for automated video surveillance|This paper presents a reconfigurable hardware platform for general-purpose video processing. The proposed design is a step towards portable, web-enabled devices which, unlike most existing smart cameras, are to a large degree autonomous and have substantial intelligence embedded. The device is based on programmable logic, microcontrollers, and dedicated communication modules. It is able to acquire an input frame using a built-in camera, process the image in a massively parallel manner, and expose its functionality as a web service compliant with the Service Oriented Architecture (SOA) paradigm. Thanks to the FPGA technology used for main functionality implementation, the system’s hardware can be locally or remotely reconfigured in order to optimize it on a very low level for user-defined tasks. Moreover, the image processing core is implemented using a C-to-HDL compiler to reduce time to market when deploying new functionalities. We demonstrate the capabilities of our device on two example applications: moving object detection and face recognition.||||
59|7||Minimizing accumulative memory load cost on multi-core DSPs with multi-level memory|In multi-core Digital Signal Processing (DSP) Systems, the processor-memory gap remains the primary obstacle in improving system performance. This paper addresses this bottleneck by combining task scheduling and memory accesses so that the system architecture and memory modules of a multi-core DSP can be utilized as efficiently as possible. To improve the system and memory utilization, the key is to take advantage of locality as much as possible and integrate it into task scheduling. Two algorithms are proposed to optimize memory accesses while scheduling tasks with timing and resource constraints. The first one uses Integer Linear Programming (ILP) to produce a schedule with the most efficient memory access sequence while satisfying the constraints. The second one is a heuristic algorithm which can produce a near optimal schedule with polynomial running time. The experimental results show that the memory access cost can be reduced up to 60% while the schedule length is also shortened.||||
59|7||TSV: A novel energy efficient Memory Integrity Verification scheme for embedded systems|Embedded systems are ubiquitous in this era of portable computing. These systems are empowered to access, store and transmit abundance of critical information. Thus their security becomes a prime concern. Moreover, most of these embedded devices often have to operate under insecure environments where the adversary may acquire physical access. To provide security, cryptographic security mechanisms could be employed in embedded systems. However, these mechanisms consume excessive energy that cannot be tolerated by the embedded systems. Therefore with the focus on achieving energy efficiency in cryptographic Memory Integrity Verification (MIV) mechanism, we present a novel energy efficient approach called Timestamps Verification (TSV) to provide Memory Integrity Verification in embedded systems. This paper elaborates the proposed approach along with its theoretical evaluation, simulation results, and experimental evaluation. The results prove that the energy savings in the TSV approach are in the range of 36–81% when compared with traditional MIV mechanisms.||||
59|7||Enriching MATLAB with aspect-oriented features for developing embedded systems|This article presents an approach to enrich the MATLAB1 language with aspect-oriented modularity features, enabling developers to experiment different implementation characteristics and to acquire runtime data and traces without polluting their base MATLAB code. We propose a language through which programmers configure the low-level data representation of variables and expressions. Examples include specifically-tailored fixed-point data representations leading to more efficient support for the underlying hardware, e.g., digital signal processors and application-specific architectures, without built-in floating point units. This approach assists developers in adding handlers and monitoring features in a non-invasive way as well as configuring MATLAB functions with optimized implementations. Different aspect modules can be used to retarget common MATLAB code bases for different purposes and implementations. We validate the proposed approach with a set of representative examples where we attain a simple way to explore a number of properties. Experiment results and collected aspect-oriented software metrics lend support to the claims on its usefulness.||||
59|7||From UML specifications to mapping and scheduling of tasks into a NoC, with reliability considerations|This paper describes a technique for performing mapping and scheduling of tasks belonging to an executable application into a NoC-based MPSoC, starting from its UML specification. A toolchain is used in order to transform the high-level UML specification into a middle-level representation, which takes the form of an annotated task graph. Such an input task graph is used by an optimization engine for the sake of carrying out the design space exploration. The optimization engine relies on a Population-based Incremental Learning (PBIL) algorithm for performing mapping and scheduling of tasks into the NoC. The PBIL algorithm is also proposed for dynamic mapping of tasks in order to deal with failure events at runtime. Simulation results are promising and exhibit a good performance of the proposed solution when problem size is increased.||||
59|7||Optimal placement of vertical connections in 3D Network-on-Chip|Due to technological limitations, manufacturing yield of vertical connections (Through Silicon Vias, TSVs) in 3D Networks-on-Chip (NoC) decreases rapidly when the number of TSVs grows. The adoption of 3D NoC design depends on the performance and manufacturing cost of the chip. This article presents methods for allocating and placing a minimal number of vertical links and the corresponding vertical routers to achieve specified performance goals. A second optimization step allows to maximize redundancy in order to deal with failing TSVs. Globally optimal solutions are determined for the first time for meshes up to 17 × 17 nodes in size. A 64-core 3D NoC is modeled based on state-of-the-art 2D chips. We present benchmark results using a cycle accurate full system simulator based on realistic workloads. Experiments show that under different workloads, an optimal placement with 25% of vertical connections achieved 81.3% of average network latency and 76.5% of energy delay product, compared with full layer–layer connection. The performance with 12.5% and 6.25% of vertical connections are also evaluated. Our analysis and experiment results provide a guideline for future 3D NoC design.||||
59|7||Space optimal solution for data reordering in streaming applications on NoC based MPSoC|Many streaming applications feature coarse grain task farm or pipeline parallelism and can be modeled as a set of parallel threads. Performance requirements can often only be met by mapping the application onto a Multi Processor System-on-Chip (MPSoC). To avoid contention, hierarchical interconnection networks, where the central interconnect is a network-on-chip, are employed. In such a clustered MPSoC, the memory access latency varies strongly depending on the location of data, and is the principal cause of out-of-order arrival of data items.||||
59|7||Using task migration to improve non-contiguous processor allocation in NoC-based CMPs|In this paper, a processor allocation mechanism for NoC-based chip multiprocessors is presented. Processor allocation is a well-known problem in parallel computer systems and aims to allocate the processing nodes of a multiprocessor to different tasks of an input application at run time. The proposed mechanism targets optimizing the on-chip communication power/latency and relies on two procedures: processor allocation and task migration. Allocation is done by a fast heuristic algorithm to allocate the free processors to the tasks of an incoming application when a new application begins execution. The task-migration algorithm is activated when some application completes execution and frees up the allocated resources. Task migration uses the recently deallocated processors and tries to rearrange the current tasks in order to find a better mapping for them. The proposed method can also capture the dynamic traffic pattern of the network and perform task migration based on the current communication demands of the tasks. Consequently, task migration adapts the task mapping to the current network status. We adopt a non-contiguous processor allocation strategy in which the tasks of the input application are allowed to be mapped onto disjoint regions (groups of processors) of the network. We then use virtual point-to-point circuits, a state-of-the-art fast on-chip connection designed for network-on-chips, to virtually connect the disjoint regions and make the communication latency/power closer to the values offered by contiguous allocation schemes. The experimental results show considerable improvement over existing allocation mechanisms.||||
59|7||A fault tolerant NoC architecture using quad-spare mesh topology and dynamic reconfiguration|Network-on-Chip (NoC) is widely used as a communication scheme in modern many-core systems. To guarantee the reliability of communication, effective fault tolerant techniques are critical for an NoC. In this paper, a novel fault tolerant architecture employing redundant routers is proposed to maintain the functionality of a network in the presence of failures. This architecture consists of a mesh of 2 × 2 router blocks with a spare router placed in the center of each block. This spare router provides a viable alternative when a router fails in a block. The proposed fault-tolerant architecture is therefore referred to as a quad-spare mesh. The quad-spare mesh can be dynamically reconfigured by changing control signals without altering the underlying topology. This dynamic reconfiguration and its corresponding routing algorithm are demonstrated in detail. Since the topology after reconfiguration is consistent with the original error-free 2D mesh, the proposed design is transparent to operating systems and application software. Experimental results show that the proposed design achieves significant improvements on reliability compared with those reported in the literature. Comparing the error-free system with a single router failure case, the throughput only decreases by 5.19% and latency increases by 2.40%, with about 45.9% hardware redundancy.||||
59|7||A unified link-layer fault-tolerant architecture for network-based many-core embedded systems|Reliability is an important design concern for modern many-core embedded systems. Specifically, on-chip interconnecting systems are vulnerable to permanent channel faults and transient data transmission faults which may significantly impact the overall system performance. In this work, a Unified Link-layer Fault-tolerant NoC (ULF-NoC) architecture is proposed. ULF-NoC is developed for NoC equipped with bidirectional channels and features wormhole switching (instead of store-and-forward switching) and packet-based retransmission. An intelligent buffer controller is developed that does not require separate, dedicated buffer spaces to support packet retransmissions. Extensive simulations using both synthetic and real world data traffics demonstrated marked performance of the proposed ULF-NoC solution.||||
59|7||Silicon-aware distributed switch architecture for on-chip networks|It is well-known that current Chip MultiProcessor (CMP) and high-end MultiProcessor System-on-Chip (MPSoC) designs are growing in their number of components. Networks-on-Chip (NoC) provide the required connectivity for such CMP and MPSoC designs at reasonable costs. As technology advances, links become the critical component in the NoC due to their long delay and power consumption, becoming unacceptable for long global interconnects.||||
59|7||Fuzzy-based Adaptive Routing Algorithm for Networks-on-Chip|In this paper, we propose two adaptive routing algorithms to alleviate congestion in the network. In the first algorithm, the routing decision is assisted by the number of occupied buffer slots at the corresponding input buffer of the next router and the congestion level of that router. Although this algorithm performs better than the conventional method, DyXY, in some cases the proposed algorithm leads to non-optimal decisions. Fuzzy controllers compensate for ambiguities in the data by giving a level of confidence rather than declaring the data simply true or false. To make a better routing decision, we propose an adaptive routing algorithm based on fuzzy logic for Networks-on-chip where the routing path is determined based on the current condition of the network. The proposed algorithm avoids congestion by distributing traffic over the routers that are less congested or have a spare capacity. The output of the fuzzy controller is the congestion level, so that at each router, the neighboring router with the lowest congestion value is chosen for routing a packet. To evaluate the proposed routing method, we use two multimedia applications and two synthetic traffic profiles. The experimental results show that the fuzzy-based routing scheme improves the performance over the DyXY routing algorithm by up to 25% with a negligible hardware overhead.||||
59|7||Deadlock-free generic routing algorithms for 3-dimensional Networks-on-Chip with reduced vertical link density topologies|||||
59|7||Distributed fair DRAM scheduling in network-on-chips architecture|Memory access scheduling is an effective manner to improve performance of Chip Multi-Processors (CMPs) by taking advantage of the timing characteristics of a DRAM. A memory access scheduler can subdivide resources utilization (banks and rows) to increase throughput by accessing different DRAM banks in parallel. However, different threads running on different cores may exhibit different performance. One thread may experience starvation while the others are serviced normally. Therefore, designing a scheduler which reduces the unfairness in the DRAM system, while also improving system throughput on a variety of workloads and systems, is necessary. In this paper, a distributed fair DRAM scheduling for two-dimensional mesh network-on-chips (NoCs), called DFDS, is presented. The key design points in DFDS are: (i) assessing the total waiting cycles of a memory request in NoC and considering it as a metric in arbitration. For this purpose waiting cycles of a memory request are put in an additional flit in a packet and are updated while traversing the NoC, and (ii) proposing a semi-dynamic virtual channel allocation to provide in-order memory requests to memory controllers (MCs). Consequently, we use a simple scheduling algorithm in MCs, instead of complex algorithms. To validate our approach, we apply synthetic and real workload from Parsec benchmark suite. The results show effectiveness of our approach, as we reduce the waiting time of memory requests by up to 15%.||||
59|7||NISHA: A fault-tolerant NoC router enabling deadlock-free Interconnection of Subnets in Hierarchical Architectures|Decrease in the Integrated Circuit (IC) feature sizes leads to the increase in the susceptibility to transient and permanent errors. The growing rate of such errors in ICs intensifies the need for a wide range of solutions addressing reliability at various levels of abstractions. Network on Chip (NoC) architecture has been introduced to address the increasing demand for communication bandwidth among processing cores. The structural redundancy inherited in NoC-based system can be leveraged to improve reliability and compensate for the effects of failures. In this paper, we propose a fault-tolerant NoC router NISHA, which stands for No-deadlock Interconnection of Subnets in Hierarchical Architectures. Armed with a new flow control mechanism, as well as an enhanced Virtual Channel (VC) regulator, the proposed router can mitigate the effects of both transient and permanent errors. A Dynamic/Static virtual channel allocation with respect to the local and global traffic is supported in NISHA; thereby, it maintains a deadlock-free state in the presence of routers or link failures in hierarchical topologies. Experimental results show an enhanced operation of NoC applications as well as the decrease in the average latency and energy consumption.||||
59|8|http://www.sciencedirect.com/science/journal/13837621/59/8|Design space exploration for partially reconfigurable architectures in real-time systems|In this paper, we introduce FoRTReSS (Flow for Reconfigurable archiTectures in Real-time SystemS), a methodology for the generation of partially reconfigurable architectures with real-time constraints, enabling Design Space Exploration (DSE) at the early stages of the development. FoRTReSS can be completely integrated into existing partial reconfiguration flows to generate physical constraints describing the architecture in terms of reconfigurable regions that are used to floorplan the design, with key metrics such as partially reconfigurable area, real-time or external fragmentation. The flow is based upon our SystemC simulator for real-time systems that helps develop and validate scheduling algorithms with respect to application timing constraints and partial reconfiguration physical behaviour. We tested our approach with a video stream encryption/decryption application together with Error Correcting Code and showed that partial reconfiguration may lead to an area improvement up to 38% on some resources without compromising application performance, in a very small amount of time: less than 30 s.||||
59|8||A new self-diagnosing approach based on petri nets and correlation graphs for fault management in wireless sensor networks|In most fault-detection algorithms in wireless sensor networks (WSNs), each sensor compares its data with the data of neighboring nodes. The majority of comparative methods will not work properly if more than half of the neighboring nodes of a sensor are faulty. Moreover, such comparative methods are unable to detect common mode failures (CMFs). Hence, having noticed the deficiencies of the existing comparative methods and as a reaction against such problems, we introduced a novel self-diagnosing approach to reduce the effect of neighboring node’s data in determining the status of nodes so that a sensor’s status will be determined independently of any comparisons. A sensor will be deemed to be fault-free if its components and the inner links between the components are flawless. In this paper, the behaviors of the components of a sensor are independently analyzed by means of the proposed model based on Petri nets and the links of the sensor’s components are investigated by means of the correlation graph. In addition, the authors extended and generalized the proposed method to all the nodes of a network and evaluated their operation. Simulation results showed that the modeling implemented by the HPSim tool can cover both permanent and transient faults accurately. Moreover, using the correlation graph and Pearson correlation coefficient helped us to gain confidence in the correctness of the inner links between the components of a sensor. Evaluation of the results indicated that the statistical results from the QI Macros tool were substantially similar to those from HPSim and Matlab tools. Furthermore, simulations results demonstrated that the detection accuracy and false alarm rate of the proposed method is acceptable even when the fault probability of each sensor is generally to be high. As a result, using these mechanisms leads to the development of the self-diagnosing capability in the sensors of a WSN.||||
59|8||Energy-aware design space exploration of embedded systems|||||
59|8||An automatic energy consumption characterization of processors using ArchC|The design complexity of integrated circuits requires techniques that automate and ease common tasks, allowing developers to keep up with the rapid growth and demand of the industry. This paper presents acSynth, an integrated framework for development and synthesis based on ArchC ADL descriptions and introduces a new power characterization method at the architectural level. The acSynth is composed of a characterization tool used to extract the energy consumption behavior of processors and also of a simulation method that, after characterization, is able to estimate software energy consumption at high speeds. Our experimental results show the power figures obtained by the characterization flow of Plasma and Leon3 processors, a MIPS-I and SPARCv8 HDL descriptions, respectively, using two different synthesis tools: Xilinx Xpower and Altera PowerPlay. The acSynth simulator used these power figures to allow power analysis at more than 35 million instructions per second in a simulation with small accuracy diversion and without loss of generality. The system executes large tests in minutes, which would otherwise take years in standard HDL methodologies.||||
59|8||Compiling for power with ScalaPipe|In the world of mobile and embedded devices, most of which are battery powered, optimizing computations for low energy is becoming increasingly important. One approach to diminished energy consumption is the use of dedicated hardware logic (rather than general-purpose processors) to execute some portion of the application load. Due to the diversity of applications that one may run on the same device, field-programmable gate arrays (FPGAs) are an attractive target since they can readily be reconfigured to implement different functions and are known to provide significant energy savings in certain domains. Unfortunately, FPGAs are difficult to program, typically requiring expertise in hardware description languages. Here we analyze the potential energy benefits from offloading computations to an FPGA device when starting from a high-level language expression of an application in ScalaPipe [1], which is a domain-specific language embedded in the Scala programming language [2] for creating streaming applications on heterogeneous systems consisting of general-purpose processors and FPGAs. We explore the effect of several synthesis optimizations on improving energy usage without sacrificing application performance, concluding that it is possible to reduce energy consumption significantly for computations even when expressed in a high-level language. Here we investigate total energy consumption, which is a combination of the power use and application run time. All of the optimizations considered improve performance, but some also increase power use, which can be a net loss in energy depending on the application.||||
59|8||PASES: An energy-aware design space exploration framework for wireless sensor networks|Energy consumption is one of the most constraining requirements for the development and implementation of wireless sensor networks. Many design aspects affect energy consumption, ranging from the hardware components, operations of the sensors, the communication protocols, the application algorithms, and the application duty cycle. A full design space exploration solution is therefore required to estimate the contribution to energy consumption of all of these factors, and significantly decrease the effort and time spent to choose the right architecture that fits best to a particular application. In this paper we present a flexible and extensible simulation and design space exploration framework called “PASES” for accurate power consumption analysis of wireless sensor networks. PASES performs both performance and energy analysis, including the application, the communication and the platform layers, providing an extensible and customizable environment. The framework assists the designers in the selection of an optimal hardware solution and software implementation for the specific project of interest ranging from standalone to large scale networked systems. Experimental and simulation results demonstrate the framework accuracy and utility.||||
59|8||MELOADES: Methodology for long-term online adaptation of embedded software for heterogeneous devices|||||
59|8||Two-level caches tuning technique for energy consumption in reconfigurable embedded MPSoC|In order to meet the ever-increasing computing requirement in the embedded market, multiprocessor chips were proposed as the best way out. In this work we investigate the energy consumption in these embedded MPSoC systems. One of the efficient solutions to reduce the energy consumption is to reconfigure the cache memories. This approach was applied for one cache level/one processor architecture, but has not yet been investigated for multiprocessor architecture with two level caches. The main contribution of this paper is to explore two level caches (L1/L2) multiprocessor architecture by estimating the energy consumption. Using a simulation platform, we first built a multiprocessor architecture, and then we propose a new algorithm that tunes the two-level cache memory hierarchy (L1 and L2). The tuning caches approach is based on three parameters: cache size, line size, and associativity. To find the best cache configuration, the application is divided into several execution intervals. And then, for each interval, we generate the best cache configuration. Finally, the approach is validated using a set of open source benchmarks; Spec 2006, Splash-2, MediaBench and we discuss the performance in terms of speedup and energy reduction.||||
59|9|http://www.sciencedirect.com/science/journal/13837621/59/9|Optimizing a combined WCET-WCEC problem in instruction fetching for real-time systems|In real-time systems, time is usually so critical that other parameters such as energy consumption are often not even considered. However, optimizing the worst energy consumption case can be a key factor in systems with severe power-supply limitations. In this paper we study several memory architectures using combined time and energy optimization models for real-time multitasking systems. Each task is modeled using Lock-MS, a method to optimize the WCET of a task, with an added set of constraints to model in the same way the WCEC (worst case energy consumption). Our tested hardware components focus on instruction fetching, including a lockable cache, a line buffer and a sequential prefetch buffer. We test a variety of instruction fetch alternatives optimizing time and energy consumption. Our results show that the accuracy of the estimation of the number of context switches in the worst case may affect very much the resulting WCEC (up to 8 times in our experiments) and that optimizing the WCEC may provide similar execution times than optimizing the WCET, with up to 5 times less energy consumption Additionally optimization functions combining WCET and WCEC with different weights show very interesting WCET-WCEC trade-offs. This confirms that methodologies testing such optimizations at design time could be very helpful to provide a precise system set-up.||||
59|9||Dependable and predictable time-triggered Ethernet networks with COTS components|TTEthernet is a cross-industry communication standard that supports the integration of predictable time-triggered communication and event-triggered standard Ethernet traffic. This paper explores the ability of extending the firmware of Commercial-Off-The-Shelf (COTS) routers in order to support TTEthernet. Thereby, we can achieve a significant cost reduction, upgrade existing infrastructures and make field-failure rates of COTS devices available for certification. Based on a generic model of a COTS router, we introduce four methods for extending a COTS router with support for time-triggered and event-triggered message exchanges. The extended COTS router redirects time-triggered messages within pre-planed time intervals, while also processing event-triggered messages when no time-triggered message are scheduled. We achieve temporal predictability and low jitter by minimizing the effect of event-triggered messages onto the timing of time-triggered messages. Furthermore, experimental results from a prototype implementation provide insight into the performance differences between a COTS router and dedicated hardware.||||
59|9||Special issue on network-based many-core embedded systems|||||
59|9||Efficient multicast schemes for 3-D Networks-on-Chip|3-D Networks-on-Chip (NoCs) have been proposed as a potent solution to address both the interconnection and design complexity problems facing future System-on-Chip (SoC) designs. In this paper, two topology-aware multicast routing algorithms, Multicasting XYZ (MXYZ) and Alternative XYZ (AL + XYZ) algorithms in supporting of 3-D NoC are proposed. In essence, MXYZ is a simple dimension order multicast routing algorithm that targets 3-D NoC systems built upon regular topologies. To support multicast routing in irregular regions, AL + XYZ can be applied, where an alternative output channel is sought to forward/replicate the packets whenever the output channel determined by MXYZ is not available. To evaluate the performance of MXYZ and AL + XYZ, extensive experiments have been conducted by comparing MXYZ and AL + XYZ against a path-based multicast routing algorithm and an irregular region oriented multiple unicast routing algorithm, respectively. The experimental results confirm that the proposed MXYZ and AL + XYZ schemes, respectively, have lower latency and power consumption than the other two routing algorithms, meriting the two proposed algorithms to be more suitable for supporting multicasting in 3-D NoC systems. In addition, the hardware implementation cost of AL + XYZ is shown to be quite modest.||||
59|9||Formal approach to agent-based dynamic reconfiguration in Networks-On-Chip|A Network-On-Chip (NoC) platform is an emerging topology for large-scale applications. It provides a required number of resources for critical and excessive computations. However, the computations may be interrupted by faults occurring at run-time. Hence, reliability of computations as well as efficient resource management at run-time are crucial for such many-core NoC systems. To achieve this, we utilize an agent-based management system where agents are organized in a three-level hierarchy. We propose to incorporate reallocation and reconfiguration procedures into agents hierarchy such that fault-tolerance mechanisms can be executed at run-time. Task reallocation enables local reconfiguration of a core allowing it to be eventually reused in order to restore the original performance of communication and computations. The contributions of this paper are: (i) an algorithm for initial application mapping with spare cores, (ii) a multi-objective algorithm for efficient utilization of spare cores at run-time in order to enhance fault-tolerance while maintaining efficiency of communication and computations at an adequate level, (iii) an algorithm integrating the local reconfiguration procedure and (iv) formal modeling and verification of the dynamic agent-based NoC management architecture incorporating these algorithms within the Event-B framework.||||
59|9||Advanced technologies and theories for highly-reliable cyber physical system|||||
59|9||Service vulnerability scanning based on service-oriented architecture in Web service environments|Web services are becoming the critical components of business application, but they are often invoked with critical software and application bugs that can be explored by malicious users. Because the existing centralized vulnerability scanning systems often face performance bottleneck because of huge amount of tasks, a novel service vulnerability scanning scheme is high desirable. In this paper, we propose a service vulnerability scanning scheme based on service-oriented architecture (SoA) in Web service environments. The scanning scheme contains three components, i.e., domain-oriented distributed architecture, service providing mode based on SoA and hierarchical strategy scheduling model. The hierarchical strategy scheduling model is the key of the scanning scheme, which is used to solve the problems of distributed scheduling management in vulnerability scanning process for Web service environments. We conduct a centralized scanner to compare our scheme with other schemes by the implement of prototype system. Experimental results show that our proposed scheme outperforms other schemes with respect to time cost, accuracy and load.||||
59|9||Applications IO profiling and analysis for smart devices|Nowadays, Android has been widely installed in mobile devices, including smart-phones, tablet PCs, and PMPs, that are equipped with a wealth of standard capabilities, such as touchpad display, wireless communications, GPS, and memory-based storage. In Android systems, most users perform many activities not only phone calls, but also internet browsing, emailing, playing games, taking and sharing pictures, downloading and reading mobile e-books, and using multimedia applications. These activities and user responsiveness are mainly affected by Android IO subsystems since the Android framework uses many IO operations with framework components. In Android-based systems, IO subsystems are organized with both internal system storage and external data storage. We analyze storage traffic of three android killer applications, Gallery, Web browsing, and SNS services, by profiling storage IO for various user activities. Based on this analysis, we summarize the statistical pattern of various Android applications, and show the effect of the proposed IO write buffering scheme which reduces the number of writes for the storage system and increases latency.||||
59|9||Evaluating the impact of proactivity in the user experience of a context-aware restaurant recommender for Android smartphones|Proactivity has recently gained much attention in the area of cyber physical systems as sensing the users’ real world environment is needed to achieve it. Proactive recommenders are a good example because they push recommendations to the user when the current situation seems appropriate, without explicit user request. Evaluating whether users would accept proactive recommendations, how to properly notify them and how to present recommended items are important research questions. In this article, we present the scenario related to a context-aware restaurant recommender for Android smartphones. Two options to achieve proactivity have been designed: a widget- and a notification-based solution. In addition, our mobile user interface includes a visualization of recommended items and allows for user feedback. The approach was evaluated in a survey among users with good results regarding usefulness and effectiveness. The results also showed that test users preferred the widget-based solution in terms of user experience.||||
59|9||JCOOLS: A toolkit for generating context-aware applications with JCAF and DROOLS|We present a toolkit named JCOOLS that effectively generates context-aware applications in a ubiquitous environment. With JCOOLS, developers can define contexts and actions as context rules according to the change of context information. Based on the predefined context rules and the underlying DROOLS inference engine, JCOOLS generates responding actions that would execute in the associated end-user applications. In addition, to facilitate the development and deployment of context-aware applications, JCOOLS generates abstract program codes based on the context information for JCAF.||||
59|9||A stability-considered density-adaptive routing protocol in MANETs|Due attention has been paid to design stable and efficient routing protocols in mobile ad hoc networks by both researchers and manufacturers. However, few of those proposed routing protocols concern about the impact of network density on the quality of routing. In this paper, we propose a stability considered density adaptive routing protocol not only to support the stable routing, but to guarantee the efficiency of routing process in order to reduce the overhead caused by control messages generated in the routing process. According to different density values of vicinity, our proposal adopts the corresponding routing tactics to guarantee the stability and efficiency of routing process. The performance simulation analysis confirms the availability and superiority of our routing protocol.||||
59|9||Context-aware service roaming for heterogeneous embedded devices over cloud|Cloud computing advocates a promising paradigm that facilitates the access within heterogeneous services, platforms, and end users. However, platforms (or host servers) have confined to devices which require a considerable computing resources. In this case, solutions concerning the efficient use of pervasive devices with constrained resources become an open issue. This study investigates the seamless connection between embedded devices and cloud resources to enhance the capability of computing and furthermore provide context-aware services. A method for wireless program dissemination and boot loading is proposed to transfer necessary information and resources between service and target device(s). The experiment results on time delay and energy cost demonstrate the feasibility and performance.||||
59|9||Parallelized sub-resource loading for web rendering engine|High-performance web browsers would be more emphasized in the commercial electronic devices including smart phones, tablet PCs, netbooks, laptops, and smart TVs. On the other hand, the web browsers still experience performance degradation due to the increase of resources provided on a web page. In fact, web pages with a large number of images require a very complex rendering operations. In this paper, we propose a parallel web browser for rapid web rendering operations with exploiting thread-level parallelism. The proposed architecture parallelizes the sub-resource loading operation on various platforms including the conventional PC system and the mobile embedded system. The proposed parallel sub-resource loading operation achieves a maximum speedup of 1.87 in a quad-core system. For the dual-core embedded system, a maximum speed-up is as large as 1.45.||||
59|9||Virtual Battery: A testing tool for power-aware software|Virtualization is an inexpensive and convenient method for setting up software test environments. Thus it is being widely used as a test tool for software products requiring high reliability such as mission critical cyber-physical systems. However, existing virtualization platforms do not fully virtualize the battery subsystem. Therefore, it is difficult to test battery-related features of guest systems. In this paper, we propose Virtual Battery, a battery virtualization scheme for type II full virtualization platforms. Virtual Battery takes the form of an ACPI-compatible battery device driver dedicated to each virtual machine, which virtualizes a target system. Through Virtual Battery, developers can easily manipulate the charging and battery status of each virtual machine (VM), regardless of the existence or current status of the host system’s battery. In addition, Virtual Battery emulates the behavior of batteries by discharging the virtual batteries according to the resource usages of their VMs. This feature enables VMs to act as battery resource containers. Three case studies demonstrate the effectiveness of the proposed scheme.||||
59|9||Large scale wireless sensor networks with multi-level dynamic key management scheme|Cyber-Physical Systems (CPSs) have emerged as a promising approach to facilitate the integration of the cyber and physical worlds in highly interconnected and complex ways. CPSs consist of several components, such as sensors, actuators, controllers, etc., and their structures are being complicated, and their scales are increasing day by day. Therefore, the data reliability and security have emerged as critical challenges between physical and virtual components of these systems. Wireless Sensor Networks (WSNs) are accepted as one of the most crucial technologies for building future CPSs. Because of their wireless and dynamic nature, WSNs are more vulnerable to security attacks than wired networks. The main solution for this problem is the usage of signed messages with symmetric or asymmetric key cryptography. Although, asymmetric key cryptography increases network security, it also causes severe computational, memory, and energy overhead for sensor nodes. On the other hand, symmetric key cryptography has the difficulty of providing high-level security and efficient key management scheme; however, it is better in terms of speed and low energy cost. In this paper, it is aimed to build a multi-level dynamic key management system for WSNs with the aid of an Unmanned Aerial Vehicle (UAV), which is a key distribution and coordination center for asymmetric keys. After that, each sensor node constructs different symmetric keys with its neighbors, and communication security is achieved by data encryption and mutual authentication with these keys. Evaluation results show the proposed system is scalable, and its performance is significantly better than asymmetric key management systems.||||
59|9||Automatic elimination of unnecessary packets for smart terminals in Wireless LAN environments|This paper presents the problems when Linux-based smart terminals receive unnecessary packets from Wireless LAN environments. Thus, we propose an algorithm to improve these problems. Smart terminals that receive unnecessary packets store them into a socket buffer instead of dropping them immediately. Then the smart terminals transfer the packets to an upper layer and the packets are eliminated at the matched protocol layer. If the smart terminals receive a lot of ineffective packets, the processing time for these packets are unnecessary overhead. In order to reduce the unnecessary overhead, this paper proposes an automatic elimination algorithm for ineffective packets. In this algorithm, we create a new table which contains information of unnecessary packets in kernels, and the ineffective packets are filtered according to the tabled information in the early stage of the MAC layer. Our designed algorithms are implemented by modification of the Linux kernel and the WLAN device driver source code, and the test results of our designed algorithms are presented.||||
||||||||
volume|issue|url|title|abstract||||
60|1|http://www.sciencedirect.com/science/journal/13837621/60/1|Efficient decomposition of strongly connected components on GPUs|The GPU (Graphics Processing Unit) has recently become one of the most power efficient processors in embedded and many other environments, and has been integrated into more and more SoCs (System on Chip). Thus modern GPUs play a very important role in power aware computing. Strongly Connected Component (SCC) decomposition is a fundamental graph algorithm which has wide applications in model checking, electronic design automation, social network analysis and other fields. GPUs have been shown to have great potential in accelerating many types of computations including graph algorithms. Recent work have demonstrated the plausibility of GPU SCC decomposition, but the implementation is inefficient due to insufficient consideration of the distinguishing GPU programming model, which leads to poor performance on irregular and sparse graphs.||||
60|1||Hardware security platform for multicast communications|Secure multicast applications of multimedia contents, such as Internet TV, pay per view, satellite TV, etc., need to maintain a high number of keys. In these applications, a user contracts a group of channels or even specific content (films, sports, etc.) which do not have to coincide with the services contracted by other users, so different keys are needed to encrypt the contents. These keys must be recalculated, encrypted and redistributed when a user joins or unjoins a specific group in order to prevent users who do not belong to a group from being able to access the contents. Original algorithms generate only one group key for all users, so this key must be recalculated and resent when a user joins or unjoins in the user group. This is an important problem, because a group key could be changed even when one content is performing. This paper presents a high performance implementation of one of the most employed algorithms of group key maintenance, the LKH algorithm, using reconfigurable hardware and a very high and realistic number of users (8,388,609). The performance obtained by this study improves a lot other results found in the literature in terms of both performance and number of users.||||
60|1||Evaluation of stereo correspondence algorithms and their implementation on FPGA|The accuracy of stereo vision has been considerably improved in the last decade, but real-time stereo matching is still a challenge for embedded systems where the limited resources do not permit fast operation of sophisticated approaches. This work presents an evaluation of area-based algorithms used for calculating distance in stereoscopic vision systems, their hardware architectures for implementation on FPGA and the cost of their accuracies in terms of FPGA hardware resources. The results show the trade-off between the quality of such maps and the hardware resources which each solution demands, so they serve as a guide for implementing stereo correspondence algorithms in real-time processing systems.||||
60|1||A novel 3-D FPGA architecture targeting communication intensive applications|The interconnection structures in FPGA devices increasingly contribute more to the delay, power consumption and area overhead. The demand for even higher clock frequencies makes this problem even more important. Three-dimensional (3-D) chip stacking is touted as the silver bullet technology that can keep Moores momentum and fuel the next wave of consumer electronics products. However, the benefits of such a new integration paradigm have not been sufficiently explored yet. In this paper, a novel 3-D architecture, as well as the software supporting tools for exploring and evaluating application implementation, are introduced. More specifically, by assigning to different layers logic and I/O resources, we achieve mentionable wire-length reduction. Experimental results prove the effectiveness of such a selection, since target architectures outperform the conventional 2-D FPGAs.||||
60|1||MobileFBP: Designing portable reconfigurable applications for heterogeneous systems|Power-efficiency has been a key issue for today’s application and system design, ranging from embedded systems to data centers. While application-specific designs and optimizations may improve the power efficiency, it requires significant efforts to co-design the hardware and software, which are difficult to re-use. On the hardware front, the trend of heterogeneous computing enables custom designs for specific applications by integrating different types of processors and reconfigurable hardware to handle compute-intensive tasks. However, what is still missing is an elegant application framework, i.e., a programming environment and a runtime system, to develop portable applications which can offload tasks or be reconfigured dynamically to run on a variety of systems efficiently.||||
60|1||Methodologies and tools for the design space exploration of embedded systems|||||
60|1||The COMPLEX methodology for UML/MARTE Modeling and design space exploration of embedded systems|The design of embedded systems is being challenged by their growing complexity and tight performance requirements. This paper presents the COMPLEX UML/MARTE Design Space Exploration methodology, an approach based on a novel combination of Model Driven Engineering (MDE), Electronic System Level (ESL) and design exploration technologies. The proposed framework enables capturing the set of possible design solutions, that is, the design space, in an abstract, standard and graphical way by relying on UML and the standard MARTE profile. From that UML/MARTE based model, the automated generation framework proposed produces an executable, configurable and fast performance model which includes functional code of the application components. This generated model integrates an XML-based interface for communication with the tool which steers the exploration. This way, the DSE loop iterations are efficiently performed, without user intervention, avoiding slow manual editions, or regeneration of the performance model. The novel DSE suited modelling features of the methodology are shown in detail. The paper also presents the performance model generation framework, including the enhancements with regard the previous simulation and estimation technology, and the exploration technology. The paper uses an EFR vocoder system example for showing the methodology and for demonstrative results.||||
60|1||Fast and standalone Design Space Exploration for High-Level Synthesis under resource constraints|The very high computing capacity available in the latest Field Programmable Gate Array (FPGA) components allows to extend their application fields, in High-Performance Computing (HPC) as well as in embedded applications. This paper presents a new methodology for Design Space Exploration (DSE) in the context of High-Level Synthesis (HLS) for HPC and embedded systems targeting FPGAs.||||
60|1||On the design space exploration through the Hellfire Framework|Embedded systems have faced dramatic and extensive changes throughout the past years leading to each more complex designs. Thus, this article presents the Hellfire Framework, which implements a design space exploration tool based on two basic steps: explore and refine. The tool leads the designer through three main different levels of abstraction: (i) application level; (ii) OS level, and; (iii) hardware architecture level. In the application level, the initial input is a task graph that represents the application’s behavior. The resulting application (divided in tasks) uses the OS we provide (and its system calls) to perform varied operations. The OS itself can be mainly configured in terms of real-time scheduling and memory occupation. Finally, the hardware architecture level allows to choose parameters regarding the processor frequency and communication infrastructure. The framework guides the designer through these levels in an explore and refine fashion so that, from a high level description of the application, the entire platform can be assembled with proper design exploration. Results show the exploration and refinement steps in the three levels we propose in different applications for MPSoC-based systems.||||
60|1||ASP-based optimized mapping in a simulink-to-MPSoC design flow|This paper presents an approach to the automated identification of optimal mapping choices in a Simulink-to-MPSoC design flow. The mapping process relies on an appropriately chosen model of computation, capturing the high-level structure of the Simulink application as well as enabling formal checking of several relevant properties, such as boundedness, liveness, as well as throughput and latency formulas. The optimization approach exploits an emerging logic programming language, Answer Set Programming (ASP), for design space exploration. The proposed ASP-based solution can be used in the context of Simulink-to-MPSoC translation as it provides a technique to automate the optimization of design choices aimed at resource utilization and execution time. A case-study and the related experimental results, presented at the end of the paper, demonstrate the effectiveness of the proposed approach.||||
60|1||Multi-objective module partitioning design for dynamic and partial reconfigurable system-on-chip using genetic algorithm|This paper proposes a novel architecture for module partitioning problems in the process of dynamic and partial reconfigurable computing in VLSI design automation. This partitioning issue is deemed as Hypergraph replica. This can be treated by a probabilistic algorithm like the Markov chain through the transition probability matrices due to non-deterministic polynomial complete problems. This proposed technique has two levels of implementation methodology. In the first level, the combination of parallel processing of design elements and efficient pipelining techniques are used. The second level is based on the genetic algorithm optimization system architecture. This proposed methodology uses the hardware/software co-design and co-verification techniques. This architecture was verified by implementation within the MOLEN reconfigurable processor and tested on a Xilinx Virtex-5 based development board. This proposed multi-objective module partitioning design was experimentally evaluated using an ISPD’98 circuit partitioning benchmark suite. The efficiency and throughput were compared with that of the hMETIS recursive bisection partitioning approach. The results indicate that the proposed method can improve throughput and efficiency up to 39 times with only a small amount of increased design space. The proposed architecture style is sketched out and concisely discussed in this manuscript, and the existing results are compared and analyzed.||||
60|1||MPSoC based on Transport Triggered Architecture for baseband processing of an LTE receiver|||||
60|10|http://www.sciencedirect.com/science/journal/13837621/60/10|MoNoC: A monitored network on chip with path adaptation mechanism|Complex systems on chip containing dozens of processing resources with critical communication requirements usually rely on the use of networks on chip (NoCs) as communication infrastructure. NoCs provide significant advantages over simpler infrastructures such as shared busses or point to point communication, including higher scalability, more efficient energy management, higher bandwidth and lower average latency. Applications running on NoCs with more than 10% of bandwidth usage attest that the most significant portion of message latencies refers to buffered packets waiting to enter the NoC, whereas the latency portion that depends on the packet traversing the NoC is sometimes negligible. This work presents an adaptive routing architecture, named Monitored NoC (MoNoC), which is based on a traffic monitoring mechanism and the exchange of high priority control packets. This method enables to adapt paths by choosing less congested routes. Practical experiments show that the proposed path adaptation is a fast process, enabling to transmit packets with smaller latencies, up to 9 times smaller, by using non-congested NoC regions.||||
60|10||Race-to-halt energy saving strategies|Energy consumption is one of the major issues for modern embedded systems. Early, power saving approaches mainly focused on dynamic power dissipation, while neglecting the static (leakage) energy consumption. However, technology improvements resulted in a case where static power dissipation increasingly dominates. Addressing this issue, hardware vendors have equipped modern processors with several sleep states. We propose a set of leakage-aware energy management approaches that reduce the energy consumption of embedded real-time systems while respecting the real-time constraints. Our algorithms are based on the race-to-halt strategy that tends to run the system at top speed with an aim to create long idle intervals, which are used to deploy a sleep state. The effectiveness of our algorithms is illustrated with an extensive set of simulations that show an improvement of up to 8% reduction in energy consumption over existing work at high utilization. The complexity of our algorithms is smaller when compared to state-of-the-art algorithms. We also eliminate assumptions made in the related work that restrict the practical application of the respective algorithms. Moreover, a novel study about the relation between the use of sleep intervals and the number of pre-emptions is also presented utilizing a large set of simulation results, where our algorithms reduce the experienced number of pre-emptions in all cases. Our results show that sleep states in general can save up to 30% of the overall number of pre-emptions when compared to the sleep-agnostic earliest-deadline-first algorithm.||||
60|10||A metaprogrammed C++ framework for hardware/software component integration and communication|With the ever growing complexity of System-on-Chip design, a considerable effort has been made to introduce higher levels of abstraction and to integrate high-level synthesis solutions to the design flow. In such design flows, a uniform communication interface is needed to enable high-level implementations of SoC components regardless of whether they are compiled as software running on a processor or synthesized to dedicated hardware IPs. This paper addresses this issue and proposes a component communication framework that defines an object-oriented remote call mechanism which allows transparent communication across hardware/software boundaries. The proposed framework relies on C++ static metaprogramming techniques to efficiently abstract communication between components implemented using high-level C++. We also define a portability layer that enables the migration of designs throughout different hardware platforms, operating systems, and tools. We assessed the performance and area footprint of our communication infrastructure through the implementation of a voice processing pipeline on top of a Network-on-Chip based architecture. Our results, when compared to previous related works with the same set of capabilities, show that our mechanisms yield small overhead in terms of software memory (up to 64% smaller), FPGA resources (up to 40% smaller), and hardware/software communication latency (up to 51% smaller).||||
60|10||MPS-CAN analyzer: Integrated implementation of response-time analyses for Controller Area Network|We present a new response-time analyzer for Controller Area Network (CAN) that integrates and implements a number of response-time analyses which address various transmission modes and practical limitations in the CAN controllers. The existing tools for the response-time analysis of CAN support only periodic and sporadic messages. They do not analyze mixed messages which are partly periodic and partly sporadic. These messages are implemented by several higher-level protocols based on CAN that are used in the automotive industry. The new analyzer supports periodic, sporadic as well as mixed messages. It can analyze the systems where periodic and mixed messages are scheduled with offsets. It also supports the analysis of all types of messages while taking into account several queueing policies and buffer limitations in the CAN controllers such as abortable or non-abortable transmit buffers. Moreover, the tool supports the analysis of mixed, periodic and sporadic messages in the heterogeneous systems where Electronic Control Units (ECUs) implement different types of queueing policies and have different types of buffer limitations in the CAN controllers. We conduct a case study of a heterogeneous application from the automotive domain to show the usability of the tool. Moreover, we perform a detailed evaluation of the implemented analyses.||||
60|2|http://www.sciencedirect.com/science/journal/13837621/60/2|Design and optimization for embedded and real-time computing systems and applications|||||
60|2||Supporting soft real-time parallel applications on multiprocessors|The prevalence of multicore processors has resulted in the wider applicability of parallel programming models such as OpenMP and MapReduce. A common goal of running parallel applications implemented under such models is to guarantee bounded response times while maximizing system utilization. Unfortunately, little previous work has been done that can provide such performance guarantees. In this paper, this problem is addressed by applying soft real-time scheduling analysis techniques. Analysis and conditions are presented for guaranteeing bounded response times for parallel applications under global EDF multiprocessor scheduling.||||
60|2||Memory reservation and shared page management for real-time systems|Memory reservations are used to provide real-time tasks with guaranteed memory access to a specified amount of physical memory. However, previous work on memory reservation primarily focused on private pages, and did not pay attention to shared pages, which are widely used in current operating systems. With previous schemes, a real-time task may experience unexpected timing delays from other tasks through shared pages that are shared by another process, even though the task has enough free pages in its own reservation. In this paper, we first describe the problems that arise when real-time tasks share pages. We then propose a shared-page management framework which enhances the temporal isolation provided by memory reservations in resource kernels that use the resource reservation approach. Our proposed solution consists of two schemes, Shared-Page Conservation (SPC) and Shared-Page Eviction Lock (SPEL), each of which prevents timing penalties caused by the seemingly arbitrary eviction of shared pages. The framework can manage shared data for inter-process communication and shared libraries, as well as pages shared by the kernel’s copy-on-write technique and file caches. We have implemented and evaluated our schemes on the Linux/RK platform, but it can also be applied to other operating systems with paged virtual memory.||||
60|2||Studying the code compression design space â A synthesis approach|Embedded domain has witnessed the application of different code compression methodologies on different architectures to bridge the gap between ever-increasing application size and scarce memory resources. Selection of a code compression technique for a target architecture requires a detailed study and analysis of the code compression design space. There are multiple design parameters affecting the space, time, cost and power dimensions. Standard approaches of exploring the code compression design space are tedious, time consuming, and almost impractical with the increasing number of proposed compression algorithms. This is one of the biggest challenges faced by an architect trying to adopt a code compression methodology for a target architecture. We propose a novel synthesis based tool-chain for fast and effective exploration of the code compression design space and for evaluation of the tradeoffs. The tool-chain consists of a frontend framework that works with different compression/decompression schemes and a backend with high-level-synthesis, logic-synthesis, and power estimation tools to output the critical design parameters. We use the tool-chain to effectively analyze different code compression/decompression schemes of varying complexities.||||
60|2||DTS: Dynamic TDMA scheduling for Networked Control Systems|Networked Control Systems (NCSs) are pervasively applied in modern industry. With increasing functionalities, modern NCSs tend to have dynamic workload by holding a variety of applications via a shared network. To handle workload variations and provide performance guarantees, dynamic network scheduling scheme is highly desired in NCSs. In this paper, we propose a network scheduling scheme, referred to as DTS, that can make on-the-fly decisions to schedule the applications in NCSs. DTS aims at NCSs that use time-triggered network as shared medium and Time division multiple access (TDMA) as network access method. DTS dynamically changes the network accessing sequence of the applications in a way to provide optimal system performance and maintain control stability in NCSs. DTS adopts a decentralized schedule mechanism where each application can make its local schedule decision, enhancing the scalability of NCSs. Simulation results demonstrate the effectiveness of the proposed scheme by improving the network bandwidth and providing better system performance in NCS comparing with the existing time-triggered scheduling schemes.||||
60|2||Communications-oriented development of component-based vehicular distributed real-time embedded systems|We propose a novel model- and component-based technique to support communications-oriented development of software for vehicular distributed real-time embedded systems. The proposed technique supports modeling of legacy nodes and communication protocols by encapsulating and abstracting the internal implementation details and protocols. It also allows modeling and performing timing analysis of the applications that contain network traffic originating from outside of the system such as vehicle-to-vehicle, vehicle-to-infrastructure, and cloud-based applications. Furthermore, we present a method to extract end-to-end timing models to support end-to-end timing analysis. We also discuss and solve the issues involved during the extraction of these models. As a proof of concept, we implement our technique in the Rubus Component Model which is used for the development of software for vehicular embedded systems by several international companies. We also conduct an application-case study to validate our approach.||||
60|2||Comparative analysis of two different middleware approaches for reconfiguration of distributed real-time systems|Software-based reconfiguration of distributed real-time systems is a complex problem with many sides to it ranging from system-wide concerns down to the intrinsic non-robust nature of the specific middleware layer and the used programming techniques. In a completely open distributed system, mixing reconfiguration and real-time is not possible; the set of possible target states can be very large threatening the temporal predictability of the reconfiguration process. Over the last years, middle ware solutions have appeared mainly for general purpose systems where efficient state transitions are sought for, but real-time properties are not considered. One of the few contributions to run-time software reconfiguration in distributed real-time environments has been the iLAND middleware, where the germ of a solution with high potential has been conceived and delivered in practice.1 The key idea has been the fact that a set of bounds and limitations to the structure of systems and to their open nature needs to be imposed in order to come up with practical solutions. In this paper, the authors present the different sides of the problem of software reconfiguration from two complementary middleware perspectives comparing two strategies built inside distribution middleware. We highlight the lessons learned in the iLAND project aimed at service-based reconfiguration and compare it to our experience in the development of distributed real-time Java reconfiguration based on distributed tasks rescheduling. Authors also provide a language view of both solutions. Lastly, empirical results are shown that validate these solutions and compare them on the basis of different programming language realizations.||||
60|2||MORM: A Multi-objective Optimized Replication Management strategy for cloud storage cluster|Effective data management is an important issue for a large-scale distributed environment such as data cloud. This can be achieved by using file replication, which efficiently reduces file service time and access latency, increases file availability and improves system load balancing. However, replication entails various costs such as storage and energy consumption for holding replicas. This article proposes a multi-objective offline optimization approach for replica management, in which we view the various factors influencing replication decisions such as mean file unavailability, mean service time, load variance, energy consumption and mean access latency as five objectives. It makes decisions of replication factor and replication layout with an improved artificial immune algorithm that evolves a set of solution candidates through clone, mutation and selection processes. The proposed algorithm named Multi-objective Optimized Replication Management (MORM) seeks the near optimal solutions by balancing the trade-offs among the five optimization objectives. The article reports a series of experiments that show the effectiveness of the MORM. Experimental results conclusively demonstrate that our MORM is energy effective and outperforms default replication management of HDFS (Hadoop Distributed File System) and MOE (Multi-objective Evolutionary) algorithm in terms of performance and load balancing for large-scale cloud storage cluster.||||
60|3|http://www.sciencedirect.com/science/journal/13837621/60/3|Real-time embedded software for multi-core platforms|||||
60|3||Deadline and activation time assignment for partitioned real-time application on multiprocessor reservations|Providing temporal isolation between critical activities has been an important design criterion in real-time open systems, which can be achieved using resource reservation techniques. As an abstraction of reservation servers, virtual processor is often used to represent a portion of computing power available on a physical platform while hiding the implementation details. In this paper, we present a general framework of partitioning an application comprised of hard real-time tasks with precedence constraints onto multiple virtual processors in consideration of communication latencies between tasks. A novel method is proposed for assigning deadlines and activation times to tasks such that tasks partitioned onto different virtual processors can be analyzed separately using well-established theories for uniprocessor. Extensive simulations have been performed and the results have shown that, compared to existing algorithms, the proposed method achieves better performance in terms of minimizing both total bandwidth and the maximum individual bandwidth.||||
60|3||Exploring the design space of multiprocessor synchronization protocols for real-time systems|||||
60|3||A parallel Bees Algorithm implementation on GPU|Bees Algorithm is a population-based method that is a computational bound algorithm whose inspired by the natural behavior of honey bees to finds a near-optimal solution for the search problem. Recently, many parallel swarm based algorithms have been developed for running on GPU (Graphic Processing Unit). Since nowadays developing a parallel Bee Algorithm running on the GPU becomes very important. In this paper, we extend the Bees Algorithm (CUBA (i.e. CUDA based Bees Algorithm)) in order to be run on the CUDA (Compute Unified Device Architecture). CUBA (CUDA based Bees Algorithm). We evaluate the performance of CUBA by conducting some experiments based on numerous famous optimization problems. Results show that CUBA significantly outperforms standard Bees Algorithm in numerous different optimization problems.||||
60|3||CuSora: Real-time software radio using multi-core graphics processing unit|The Sora platform, which is a fully programmable, high-performance software radio platform based on a commodity general purpose PC, has recently received significant attention. However, acceleration techniques used in Sora are too complicated for developers, which can prevent researchers from modifying physical layer (PHY) processing. This paper presents the CuSora platform, which integrates the Sora platform with a popular multi-core graphics processing unit (GPU) as the modem processor to achieve high-speed PHY signal processing. CuSora also exploits software techniques to fulfill requirements for real-time communication. A software controller is presented to achieve multi-mode communication. The features of the single-instruction multiple data parallel computation of the GPU are also employed to accelerate PHY processing. Several wireless protocols, such as WiFi (802.11a) or WiMAX (802.16), are demonstrated on the CuSora platform for verification. CuSora meets the requirement of real-time communication and has an excellent bit error ratio performance. CuSora has a higher performance, shorter development cycle, and better coding flexibility than the Sora platform.||||
60|3||Colored Petri Net model with automatic parallelization on real-time multicore architectures|This paper proposes a novel Colored Petri Net (CPN) based dynamic scheduling scheme, which aims at scheduling real-time tasks on multiprocessor system-on-chip (MPSoC) platforms. Our CPN based scheme addresses two key issues on task scheduling problems, dependence detecting and task dispatching. We model inter-task dependences using CPN, including true-dependences, output-dependences, anti-dependences and structural dependences. The dependences can be detected automatically during model execution. Additionally, the proposed model takes the checking of real-time constraints into consideration. We evaluated the scheduling scheme on the state-of-art FPGA based multiprocessor hardware system and modeled the system behavior using CPN tools. Simulations and state space analyses are conducted on the model. Experimental results demonstrate that our scheme can achieve 98.9% of the ideal speedup on a real FPGA based hardware prototype.||||
60|3||An efficient and comprehensive scheduler on Asymmetric Multicore Architecture systems|Several studies have shown that Asymmetric Multicore Processors (AMPs) systems, which are composed of processors with different hardware characteristics, present better performance and power when compared to homogeneous systems. With Moore’s law behavior still lasting, core-count growth creates typical non-uniform memory accesses (NUMA). Existing schedulers assume that the underlying architecture is homogeneous, and as consequence, they may not be well suited for AMP and NUMA systems, since they, respectively, do not properly explore hardware elements asymmetry, while improving memory utilization by avoid multi-processes data starvation. In this paper we propose a new scheduler, namely NUMA-aware Scheduler, to accommodate the next generation of AMP architectures in terms of architecture asymmetry and processes starvation. Experimental results show that the average speedup is 1.36 times faster than default Linux scheduler through evaluation using PARSEC benchmarks, demonstrating that the proposed technique is promising when compared to other prior studies.||||
60|4|http://www.sciencedirect.com/science/journal/13837621/60/4|Optimized implementation of synchronous models on industrial LTTA systems|Synchronous models are used to specify embedded systems functions in a clear and unambiguous way and allow verification of properties using formal methods. The implementation of a synchronous specification on a distributed architecture must preserve the model semantics to retain the verification results. Globally synchronized time-triggered architectures offer the simplest implementation path, but can be inefficient or simply unavailable. In past work, we defined a mapping of synchronous models on a general class of distributed asynchronous architectures, for which the only requirement is a lower bound on the rate of activation of tasks. In this paper, we set tighter requirements on task execution rates, and we include a realistic modeling of communication delays, task scheduling delays and schedulability conditions, discussing the timing characteristics of an implementation on a system with a Controller Area Network (CAN). Next, the semantics preservation conditions are formulated as constraints in an architecture optimization problem that defines a feasible task model with respect to timing constraints. An automotive case study shows the applicability of the approach and provides insight on the software design elements that are critical for a feasible implementation.||||
60|4||Integrated write buffer management for solid state drives|NAND flash memory-based Solid State Drives (SSD) have many merits, in comparison to the traditional hard disk drives (HDD). However, random write within SSD is still far slower than sequential read/write and random read. There are two independent approaches for resolving this problem as follows: (1) using overprovisioning so that reserved portion of the physical memory space can be used as, for example, log blocks, for performance enhancement, and (2) using internal write buffer (DRAM or Non-Volatile RAM) within SSD. While log blocks are managed by the Flash Translation Layer (FTL), write buffer management has been treated separately from the FTL. Write buffer management schemes did not use the exact status of log blocks, and log block management schemes in FTL did not consider the behavior of the write buffer management scheme. This paper first demonstrates that log blocks and write buffers maintain a tight relationship, which necessitates integrated management to both of them. Since log blocks can also be viewed as another type of write buffer, we can manage both of them as an integrated write buffer. Then we propose an Integrated Write buffer Management scheme (IWM), which collectively manages both the write buffer and log blocks. The proposed scheme greatly outperforms previous schemes in terms of write amplification, block erase count, and execution time.||||
60|4||Reducing SSD access latency via NAND flash program and erase suspension|In NAND flash memory, once a page program or block erase (P/E) command is issued to a NAND flash chip, the subsequent read requests have to wait until the time-consuming P/E operation to complete. Preliminary results show that the lengthy P/E operations increase the read latency by 2× on average. This increased read latency caused by the contention may significantly degrade the overall system performance. Inspired by the internal mechanism of NAND flash P/E algorithms, we propose in this paper a low-overhead P/E suspension scheme, which suspends the on-going P/E to service pending reads and resumes the suspended P/E afterwards. Having reads enjoy the highest priority, we further extend our approach by making writes be able to preempt the erase operations in order to improve the write latency performance. In our experiments, we simulate a realistic SSD model that adopts multi-chip/channel and evaluate both SLC and MLC NAND flash as storage materials of diverse performance. Experimental results show the proposed technique achieves a near-optimal performance on servicing read requests. The write latency is significantly reduced as well. Specifically, the read latency is reduced on average by 46.5% compared to RPS (Read Priority Scheduling) and when using write–suspend–erase the write latency is reduced by 13.6% relative to FIFO.||||
60|4||Memory efficient and scalable address mapping for flash storage devices|Flash memory devices commonly rely upon traditional address mapping schemes such as page mapping, block mapping or a hybrid of the two. Page mapping is more flexible than block or hybrid mapping without being restricted by block boundaries. However, its mapping table tends to grow large quickly as the capacity of flash memory devices does. To overcome this limitation, we propose novel mapping schemes that are fundamentally different from the existing mapping strategies. We call these new schemes Virtual Extent Trie (VET) and Extent Mapping Tree (EMT), as they manage mapping information by treating each I/O request as an extent   and by using extents as basic mapping units rather than pages or blocks. By storing extents instead of individual addresses, our extent mapping schemes consume much less memory to store mapping information and still remain as flexible as page mapping. We observed in our experiments that our schemes reduced memory consumption by up to an order of magnitude in comparison with the traditional mapping schemes for several real world workloads. Our extent mapping schemes also scaled well with increasing address spaces by synthetic workloads. Even though the asymptotic mapping cost of VET and EMT is higher than the OO(1) time of a page mapping scheme, the amount of increased overhead was almost negligible or low enough to be hidden by an accompanying I/O operation.||||
60|4||CaRINA Intelligent Robotic Car: Architectural design and applications|This paper presents the development of two outdoor intelligent vehicles platforms named CaRINA I and CaRINA II, their system architecture, simulation tools, and control modules. It also describes the development of the intelligent control system modules allowing the mobile robots and vehicles to navigate autonomously in controlled urban environments. Research work has been carried out on tele-operation, driver assistance systems, and autonomous navigation using the vehicles as platforms to experiments and validation. Our robotic platforms include mechanical adaptations and the development of an embedded software architecture. This paper addresses the design, sensing, decision making, and acting infrastructure and several experimental tests that have been carried out to evaluate both platforms and proposed algorithms. The main contributions of this work is the proposed architecture, that is modular and flexible, allowing it to be instantiated into different robotic platforms and applications. The communication and security aspects are also investigated.||||
60|4||The use of unmanned aerial vehicles and wireless sensor networks for spraying pesticides|The application of pesticides and fertilizers in agricultural areas is of crucial importance for crop yields. The use of aircrafts is becoming increasingly common in carrying out this task mainly because of their speed and effectiveness in the spraying operation. However, some factors may reduce the yield, or even cause damage (e.g., crop areas not covered in the spraying process, overlapping spraying of crop areas, applying pesticides on the outer edge of the crop). Weather conditions, such as the intensity and direction of the wind while spraying, add further complexity to the problem of maintaining control. In this paper, we describe an architecture to address the problem of self-adjustment of the UAV routes when spraying chemicals in a crop field. We propose and evaluate an algorithm to adjust the UAV route to changes in wind intensity and direction. The algorithm to adapt the path runs in the UAV and its input is the feedback obtained from the wireless sensor network (WSN) deployed in the crop field. Moreover, we evaluate the impact of the number of communication messages between the UAV and the WSN. The results show that the use of the feedback information from the sensors to make adjustments to the routes could significantly reduce the waste of pesticides and fertilizers.||||
60|4||In-vehicle monitoring and management for military vehiclesâ integrated vetronics architectures|Both the rapid uptake of electronics utilisation for military vehicles (vetronics) and the trend in designing vehicles based on open standards and architectures (such as the UK MoD Generic Vehicle Architecture – Def Stan 23-09 GVA) have resulted in modern and future vehicles becoming highly complex platforms. Although most modern vehicles provide advanced diagnostics and health monitoring, most of these systems focus on end systems and applications and there has been little work on monitoring the infrastructure on which these systems operate. Monitoring the operational integrity of the vetronics that enable integration of vehicle subsystems and providing feedback to the crew offers safety and reliability benefits as it allows the crew to assess in real-time, the vehicle’s ability to complete its mission. This paper presents a novel approach to achieve in-vehicle Vetronics Integrity Monitoring and Management (VIMM). The paper highlights the requirements for a generic modular system, and presents both an architecture and proof-of-concept implementation.||||
60|5|http://www.sciencedirect.com/science/journal/13837621/60/5|Introduction to special issue on embedded systems architecture and applications|||||
60|5||Improving branch divergence performance on GPGPU with a new PDOM stack and multi-level warp scheduling|General-purpose graphics processing unit (GPGPU) plays an important role in massive parallel computing nowadays. A GPGPU core typically holds thousands of threads, where hardware threads are organized into warps. With the single instruction multiple thread (SIMT) pipeline, GPGPU can achieve high performance. But threads taking different branches in the same warp violate SIMD style and cause branch divergence. To support this, a hardware stack is used to sequentially execute all branches. Hence branch divergence leads to performance degradation. This article represents the PDOM (post dominator) stack as a binary tree, and each leaf corresponds to a branch target. We propose a new PDOM stack called PDOM-ASI, which can schedule all the tree leaves. The new stack can hide more long operation latencies with more schedulable warps without the problem of warp over-subdivision. Besides, a multi-level warp scheduling policy is proposed, which lets part of the warps run ahead and creates more opportunities to hide the latencies. The simulation results show that our policies achieve 10.5% performance improvements over baseline policies with only 1.33% hardware area overhead.||||
60|5||An efficient parallel-network packet pattern-matching approach using GPUs|||||
60|5||Improving the computational efficiency of modular operations for embedded systems|Security protocols such as IPSec, SSL and VPNs used in many communication systems employ various cryptographic algorithms in order to protect the data from malicious attacks. Thanks to public-key cryptography, a public channel which is exposed to security risks can be used for secure communication in such protocols without needing to agree on a shared key at the beginning of the communication. Public-key cryptosystems such as RSA, Rabin and ElGamal cryptosystems are used for various security services such as key exchange and key distribution between communicating nodes and many authentication protocols. Such public-key cryptosystems usually depend on modular arithmetic operations including modular multiplication and exponentiation. These mathematical operations are computationally intensive and fundamental arithmetic operations which are intensively used in many fields including cryptography, number theory, finite field arithmetic, and so on. This paper is devoted to the analysis of modular arithmetic operations and the improvement of the computation of modular multiplication and exponentiation from hardware design perspective based on FPGA. Two of the well-known algorithms namely Montgomery modular multiplication and Karatsuba algorithms are exploited together within our high-speed pipelined hardware architecture. Our proposed design presents an efficient solution for a range of applications where area and performance are both important. The proposed coprocessor offers scalability which means that it supports different security levels with a cost of performance. We also build a system-on-chip design using Xilinx’s latest Zynq-7000 family extensible processing platform to show how our proposed design improve the processing time of modular arithmetic operations for embedded systems.||||
60|5||A comparison of instruction memories from the WCET perspective|||||
60|5||Applying link stability estimation mechanism to multicast routing in MANETs|Mobile ad hoc networks are self-organizing network architectures of mobile nodes. Node mobility causes network topologies to change dynamically over time, which complicated important tasks such as routing. In this paper, a novel link stability estimation model based on received signal strength indication is proposed. We have integrated this model into MAODV and present a stability-based multicast routing protocol termed as SMR. SMR can discover more available stable routes and better adapt to network topology changes. Simulation results show the superiority of SMR over the existing methods in terms of packet delivery ratio, average end-to-end delay and routing packet overhead.||||
60|6|http://www.sciencedirect.com/science/journal/13837621/60/6|FPGA prototyping of emerging manycore architectures for parallel programming research using Formic boards|Performance evaluation of parallel software and architectural exploration of innovative hardware support face a common challenge with emerging manycore platforms: they are limited by the slow running time and the low accuracy of software simulators. Manycore FPGA prototypes are difficult to build, but they offer great rewards. Software running on such prototypes runs orders of magnitude faster than current simulators. Moreover, researchers gain significant architectural insight during the modeling process. We use the Formic FPGA prototyping board [1], which specifically targets scalable and cost-efficient multi-board prototyping, to build and test a 64-board model of a 512-core, MicroBlaze-based, non-coherent hardware prototype with a full network-on-chip in a 3D-mesh topology. We expand the hardware architecture to include the ARM Versatile Express platforms and build a 520-core heterogeneous prototype of 8 Cortex-A9 cores and 512 MicroBlaze cores. We then develop an MPI library for the prototype and evaluate it extensively using several bare-metal and MPI benchmarks. We find that our processor prototype is highly scalable, models faithfully single-chip multicore architectures, and is a very efficient platform for parallel programming research, being 50,000 times faster than software simulation.||||
60|6||Analysis of worst-case backlog bounds for Networks-on-Chip|In networks-on-chips (NoCs), analyzing the worst-case backlog bounds of routers is very important to identify network congestions and improve network performance. In this paper, we propose a method called DiGB (DIrected-contention-Graph-based Backlog bound derivation) to analyze worst-case backlog bounds. For primitive scenarios, we propose analytical models for backlog bound derivation. For complex scenarios, we first construct a directed-contention-graph (DCG) to analyze the relationships among traffic flows. Then, we use the Breadth-First-Search strategy to traverse the DCG so that complex scenarios can be divided into primitive scenarios. Finally we compute the worst-case backlog bounds of each router. To illustrate this method, we present the derivation of closed-form formulas to compute the worst-case backlog bounds under all-to-one gather communication. The experimental results show that our method can achieve correct and tight worst-case backlog bounds.||||
60|6||Resolving priority inversions in composable conveyor systems|The well known problem of priority inversions that occurs in classical real-time systems also manifests in decentralized cyber-physical systems. Using a specific example of composable conveyor systems, we show how priority inversions hinder the transport of entities through the conveyor systems. We present a novel adaptation of the classical priority inheritance protocol for resolving these cyber-physical priority inversions. While the approach resolves cyber-physical priority inversions, the structure and constraints of the conveyor systems cause the jitter associated with the end-to-end latency of the highest priority parts to increase. Further, these constraints also limit the applicability of the classical priority ceiling protocol in this class of cyber-physical systems. Simulation results demonstrate the correctness and reasonable communication overhead of the approach.||||
60|6||DreamCam: A modular FPGA-based smart camera architecture|DreamCam is a modular smart camera constructed with the use of an FPGA like main processing board. The core of the camera is an Altera Cyclone-III associated with a CMOS imager and six private Ram blocks. The main novel feature of our work consists in proposing a new smart camera architecture and several modules (IP) to efficiently extract and sort the visual features in real time. In this paper, extraction is performed by a Harris and Stephen filtering associated with customized modules. These modules extract, select and sort visual features in real-time. As a result, DreamCam (with such a configuration) provides a description of each visual feature in the form of its position and the grey-level template around it.||||
60|7|http://www.sciencedirect.com/science/journal/13837621/60/7|Cache-based high-level simulation of microthreaded many-core architectures|The accuracy of simulated cycles in high-level simulators is generally less than the accuracy in detailed simulators for a single-core systems, because high-level simulators simulate the behaviour of components rather than the components themselves as in detailed simulators. The simulation problem becomes more challenging when simulating many-core systems, where many cores are executing instructions concurrently. In these systems data may be accessed from multiple caches and the abstraction of the instruction execution has to consider the dynamic resource sharing on the whole chip. The problem becomes even more challenging in microthreaded many-core systems, because there may exist concurrent hardware threads. Which means that the latency of long latency operations can be tolerated from many cycles to just few cycles. We have previously presented a simulation technique to improve the accuracy in high-level simulation of microthreaded many-core systems, known as Signature-based high- level simulator, which adapts the throughput of the program based on the type of instructions, number of instructions and number of active threads in the pipeline. However, it disregards the access to different levels of the caches on the many-core system. Accessing L1-cache has far less latency than accessing off-chip memory and if the core is not able to tolerate latency, different levels of caches can not be treated equally. The distributed cache network along with the synchronization-aware coherency protocol in the Microgrid is a complicated memory architecture and it is difficult to simulate its behaviour at a high-level. In this article we present a high-level cache model, which aims to improve the accuracy in high-level simulators for general-purpose many-core systems by adding little complexity to the simulator and without affecting the simulation speed.||||
60|7||End-to-end schedulability tests for multiprocessor embedded systems based on networks-on-chip with priority-preemptive arbitration|Simulation-based techniques can be used to evaluate whether a particular NoC-based platform configuration is able to meet the timing constraints of an application, but they can only evaluate a finite set of scenarios. In safety-critical applications with hard real-time constraints, this is clearly not sufficient because there is an expectation that the application should be schedulable on that platform in all possible scenarios. This paper presents a particular NoC-based multiprocessor architecture, as well as a number of analytical methods that can be derived from that architecture, aiming to allow designers to check, for a given platform configuration, whether all application tasks and communication messages always meet their hard real-time constraints in every possible scenario. Experiments are presented, showing the use of the proposed methods when evaluating different task mapping and platform topologies.||||
60|7||Extending KernighanâLin partitioning heuristic for application mapping onto Network-on-Chip|This paper extends the basic Kernighan–Lin graph bi-partitioning algorithm for partitioning core graphs of applications to be designed using Network-on-Chip (NoC) concept. Mapping techniques have been developed for three different types of NoC topologies – Mesh, Mesh-of-Tree (MoT), and Butterfly-Fat-Tree (BFT). Suitable post-processing schemes have been developed to improve upon the basic solution produced by the partitioning algorithm. Significant improvement in both static and dynamic performances could be observed, compared to many existing approaches reported in the literature.||||
60|7||Memory optimization in FPGA-accelerated scientific codes based on unstructured meshes|This paper approaches the memory bottleneck problem in FPGA-accelerated codes processing unstructured meshes. A methodology to reduce the required memory bandwidth is presented and evaluated, based on the combined application of data sorting, coding and compression techniques. Sorting allows efficient streaming between the memory and the FPGA, improving data locality and avoiding redundant data requests. Coding achieves a compact representation of the mesh connectivity. Differential compression reduces the size of the mesh data. We propose a hardware implementation with low resource requirements, tailored to accelerators based on reconfigurable devices. The combination of techniques reduces the memory traffic of two computational problems down to an average 34% and 19% of their original sizes, respectively.||||
60|7||A framework for post-silicon realization of arbitrary instruction extensions on reconfigurable data-paths|In this paper we present a framework for realizing arbitrary instruction set extensions (IE) that are identified post-silicon. The proposed framework has two components viz., an IE synthesis methodology and the architecture of a reconfigurable data-path for realization of the such IEs. The IE synthesis methodology ensures maximal utilization of resources on the reconfigurable data-path. In this context we present the techniques used to realize IEs for applications that demand high throughput or those that must process data streams. The reconfigurable hardware called HyperCell comprises a reconfigurable execution fabric. The fabric is a collection of interconnected compute units. A typical use case of HyperCell is where it acts as a co-processor with a host and accelerates execution of IEs that are defined post-silicon. We demonstrate the effectiveness of our approach by evaluating the performance of some well-known integer kernels that are realized as IEs on HyperCell. Our methodology for realizing IEs through HyperCells permits overlapping of potentially all memory transactions with computations. We show significant improvement in performance for streaming applications over general purpose processor based solutions, by fully pipelining the data-path.||||
60|8|http://www.sciencedirect.com/science/journal/13837621/60/8|Publisherâs Note|||||
60|8||Sensor virtualization for underwater event detection|Distributed event detection is a popular application in Underwater Wireless Sensor Networks (UWSNs). The Base Station (BS) collects the measurements from multiple sensor nodes, and makes a decision based on the sensors’ reports. However, due to the unpredictable moving of underwater sensor nodes and interference among multiple events, it is difficult to guarantee the accuracy of event detection. In this paper, we propose a sensor virtualization approach to deal with the event detection problem in UWSNs. The final decision making at the BS will be implemented with the reports of multiple virtual sensors. Although the events may happen in a large scale, the locations where the events happen are relatively sparse in the underwater environment. Consider the sparse property of events, we employ the technique of compressive sensing to recover the original signal from the correlated sensors’ measurements. Through a proper signal reconstruction, the accurate event detection can be reached with a remarkable low sensing overhead. We implement the sensor virtualization based on the compressive sensing technique. Our approach is suitable for the high dynamic topology of UWSN, and it can improve the accuracy of event detection and reduce energy consumption in UWSNs.||||
60|8||Garbage collection of multi-version indexed data on flash memory|Maintaining a multi-version index on flash memory could generate a lot of updates and invalid pages. It is important to have an efficient garbage collection mechanism to ensure the flash memory has sufficient number of free blocks for storing new data versions and their index structures. In this paper, we study the important performance issues in using the purging-range query to reclaim the blocks, which are storing old data versions and invalid index entries, to be free blocks. To reduce the cost for processing the purging-range query, we propose the physical block labeling (PBL) scheme to provide a better estimation on the purging version number to be used for purging old data versions. To further enhance the performance of the garbage collection process, and at the same time to maximize the deadspans of data versions and balance the wear levels of the blocks, we propose two schemes called, the sequential placement (SQ) and frequency-based placement (FBP), for placing new data versions into free pages. As illustrated in the performance studies, both SQ and FBP can effectively balance the wear levels of the blocks. The deadspans of data versions are longer under FBP than both SQ and RR, and the page reallocation cost is also lower under FBP especially when the size of flash memory allocated for the database is limited. The experimental results also illustrate that PBL can effectively minimize the number of invocations of the purging-range query to be one to reclaim the required number of blocks in each garbage collection.||||
60|8||A reconfigurable point target detection system based on morphological clutter elimination|The point target detection application is widely applied to the field of medicine, military, and astronautics. To support good detectability of point targets and high-performance computing, this work proposes a point target detection system (PTDS) and a reconfigurable point target detection system (RePTDS). In the PTDS and the RePTDS, we propose a pipelined morphological clutter elimination (PMCE) hardware design that supports the point target detection application. Further, we also propose a hardware/software interface component that provides seamless data transfers between the PMCE hardware design and the microprocessor. To reduce the effects of the noise in the source images, different median filter functions are also designed in the PTDS and the RePTDS individually. Both the PTDS and the RePTDS can dynamically adapt their median filter functions to reduce the noise of the source images. Besides providing good detectability of point targets, according to our experiments, the PTDS and the RePTDS have been demonstrated that they can accelerate up to 165 times the processing time required by using the conventional point target detection method. Further, compared to the PTDS, our experiments have also demonstrated the RePTDS can provide more complete support of system adaptability for hardware functions, performance, and power consumption.||||
60|8||A space allocation and reuse strategy for PCM-based embedded systems|Phase change memory (PCM) has emerged as a promising candidate to replace DRAM in embedded systems, due to its appealing properties, such as zero leakage power, scalability, shock-resistivity and high density. However, it can only sustain a limited number of write operations. On the other hand, as a program in embedded systems usually distributes write traffic in an extremely unbalanced way, which could further decrease PCM lifetime.||||
60|8||Smart devices and spaces for pervasive computing|Applications and services for pervasive computing have been dramatically grown and have contributed extensively to our daily experiences in recent years. Smart systems, devices, and spaces are proactive for ubiquitous and pervasive computing. Smart information technology (IT) is also an outcome of the state of the art and novel mobile and ubiquitous computing technologies that include highly capable handheld device, pervasive and personal device, etc. This special issue will be a trigger for further related research and technology improvements in pervasive and ubiquitous computing using smart devices and services. This special issue called for original papers describing the latest developments, trends, and solutions of smart devices and spaces for pervasive computing including real-time operating systems (OS), tiny OS and middleware supports, mobile system performance, trustworthy Internet and communications, agents and mobile and pervasive services, among others. In particular, this special issue focuses on a remote control and media-sharing system, flash storage-based smart system, heterogeneous mobile OS, and prediction and auto-execution system for pervasive computing.||||
60|8||A remote control and media-sharing system using smart devices|The remote control and media sharing of electronic devices are key services in smart homes. The incorporation of mobile smart devices in these services has become a popular trend. Existing services require that these devices are located in the same local network. This paper presents the design and implementation of an integrated service architecture that supports the remote control of home appliances and the sharing of digital media between indoor and outdoor devices. The proposed design follows standards related to digital homes, and this study presents the details of its hardware and software components.||||
60|8||Journaling deduplication with invalidation scheme for flash storage-based smart systems|Transaction support for filesystems has become a common feature in modern operating systems where data atomicity is achieved by writing transactions to the log region in advance. The logging mechanism is appropriate for flash storage devices due to the inherent nature of flash memory. However, the logging schemes inherently create multiple copies of data, leading to a decrease in the bandwidth of storage systems. In this paper, we present a simple and efficient invalidation scheme for multiple copies of data in a common journaling module. We identify two types of duplications, one in which there is an explicit duplication of the journal region and original region with the same data, and the other in which there is an implicit duplication of transaction commit operations. The invalidation of duplicated data reduces internal write and erase operations and garbage collection overhead for flash devices, which would otherwise increases external I/O bandwidth. Experimental results show that the overall performance improves roughly from 5% to 35% with the invalidation scheme for journal transactions.||||
60|8||Design and implementation of the WIPI-to-Android automatic mobile game converter for the contents compatibility in the heterogeneous mobile OS|Given that mobile communication companies choose different mobile platforms, game developers must create content designed specifically for each platform or use a conversion process to provide game content to consumers. In this paper, to resolve this problem, the WIPI-to-Android automatic mobile game converter was designed to automatically translate game contents from the WIPI (Wireless Internet Platform for Interoperability) for feature phones to the Android platform for smart phones. Through the WIPI-to-Android converter, resources such as images and sounds can be converted, APIs can be converted using a platform mapping engine, and source code can be translated using the source translator. These and all other content conversion functions were examined. Test results indicate that the graphics, image output, sound output, and other functions of converted Android games were equivalent to those of the WIPI games before conversion. Furthermore, most converted Android game content responded more quickly than the WIPI game content.||||
60|8||A prediction and auto-execution system of smartphone application services based on user context-awareness|Smartphones have emerged as suitable environments for user context-awareness and intelligent service provision due to the high penetration rate, the high usability, various embedded sensors, and so on. In particular, its most unique characteristic is the usage of various applications. However, the most of existing studies through the three steps process (log collection, context inference, and service provision) did not consider smartphone applications (Apps) as the target service. Smartphone users still have to use Apps with manual controls by own decision. Therefore, in this paper, we propose a system to predict smartphone applications based on inferring user context. We define a mobile context model with a new level of context (Situation) and its inference method to perceive a user’s intention or purpose related to the App usage. Based on the Situation context, the system predicts Apps which can be useful and helpful for a user and automatically executes it on his/her smartphone. With the proposed system, it will be possible to autonomously provide and manage smartphone application services without users’ perception or intervention.||||
60|9|http://www.sciencedirect.com/science/journal/13837621/60/9|Radio propagation modeling and real test of ZigBee based indoor wireless sensor networks|The deployment of nodes in Wireless Sensor Networks (WSNs) arises as one of the biggest challenges of this field, which involves in distributing a large number of embedded systems to fulfill a specific application. The connectivity of WSNs is one of the main issues to assure the efficiency of the system implementation and the quality of the service of the deployment, which is difficult to estimate due to the diversity and irregularity of the applied environment and it affects the WSN designers’ decision on deploying sensor nodes. Therefore, in this paper, a new method is proposed to enhance the efficiency and accuracy on ZigBee propagation modeling and simulation in indoor environments. The method consists of two steps: automatic 3D indoor reconstruction and 3D ray-tracing based radio simulation. The automatic 3D indoor reconstruction employs unattended image classification algorithm and image vectorization algorithm to accurately build the environment database, which also significantly reduces time and efforts spent on non-radio propagation issues. The 3D ray tracing is developed by using a kd-tree space division algorithm and a modified polar sweep algorithm, which accelerates the searching of rays over the entire space. A ZigBee signal propagation model is proposed for the ray-tracing engine by considering both the materials of obstacles and the impact of positions along the ray path of the radio. Three different WSN deployments are realized in the indoor environment of an office and the simulation results are verified to be accurate. Experimental results also indicate that the proposed method is efficient in the pre-simulation strategy and the 3D ray searching scheme, and it is robust for different indoor environments.||||
60|9||Challenges in real-time virtualization and predictable cloud computing|Cloud computing and virtualization technology have revolutionized general-purpose computing applications in the past decade. The cloud paradigm offers advantages through reduction of operation costs, server consolidation, flexible system configuration and elastic resource provisioning. However, despite the success of cloud computing for general-purpose computing, existing cloud computing and virtualization technology face tremendous challenges in supporting emerging soft real-time applications such as online video streaming, cloud-based gaming, and telecommunication management. These applications demand real-time performance in open, shared and virtualized computing environments. This paper identifies the technical challenges in supporting real-time applications in the cloud, surveys recent advancement in real-time virtualization and cloud computing technology, and offers research directions to enable cloud-based real-time applications in the future.||||
60|9||Remote service discovery and binding architecture for soft real-time QoS in indoor location-based service|Indoor location-based service (LBS) is generally distinguished from web services that have no physical location and user context. In particular, various resources have dynamic and frequent mobility in indoor environments. In addition, an indoor LBS includes numerous service lookups being requested concurrently and frequently from several locations, even through a network infrastructure requiring high scalability in indoor environments. The traditional centralized LBS approach needs to maintain a geographical map of the entire building or complex in its central server, which can cause low scalability and traffic congestion. This paper presents a self-organizing and fully distributed indoor LBS platform with regional cooperation among devices. A service lookup algorithm based on the proposed distributed architecture searches for the shortest physical path to the nearest service resource. A continuous service binding mechanism guarantees a probabilistic real-time QoS regardless of dynamic and frequent mobility in a soft real-time system such as an indoor LBS. Performance evaluation of the proposed algorithm and platform is compared to the traditional centralized architecture in the experimental evaluation of scalability and real test bed environments.||||
60|9||A cloud middleware for assuring performance and high availability of soft real-time applications|Applications are increasingly being deployed in the cloud due to benefits stemming from economy of scale, scalability, flexibility and utility-based pricing model. Although most cloud-based applications have hitherto been enterprise-style, there is an emerging need for hosting real-time streaming applications in the cloud that demand both high availability and low latency. Contemporary cloud computing research has seldom focused on solutions that provide both high availability and real-time assurance to these applications in a way that also optimizes resource consumption in data centers, which is a key consideration for cloud providers. This paper makes three contributions to address this dual challenge. First, it describes an architecture for a fault-tolerant framework that can be used to automatically deploy replicas of virtual machines in data centers in a way that optimizes resources while assuring availability and responsiveness. Second, it describes the design of a pluggable framework within the fault-tolerant architecture that enables plugging in different placement algorithms for VM replica deployment. Third, it illustrates the design of a framework for real-time dissemination of resource utilization information using a real-time publish/subscribe framework, which is required by the replica selection and placement framework. Experimental results using a case study that involves a specific replica placement algorithm are presented to evaluate the effectiveness of our architecture.||||
60|9||An architectural approach with separation of concerns to address extra-functional requirements in the development of embedded real-time software systems|A large proportion of the requirements on embedded real-time systems stems from the extra-functional dimensions of time and space determinism, dependability, safety and security, and it is addressed at the software level. The adoption of a sound software architecture provides crucial aid in conveniently apportioning the relevant development concerns. This paper takes a software-centered interpretation of the ISO 42010 notion of architecture, enhancing it with a component model that attributes separate concerns to distinct design views. The component boundary becomes the border between functional and extra-functional concerns. The latter are treated as decorations placed on the outside of components, satisfied by implementation artifacts separate from and composable with the implementation of the component internals. The approach was evaluated by industrial users from several domains, with remarkably positive results.||||
||||||||
volume|issue|url|title|abstract||||
61|1|http://www.sciencedirect.com/science/journal/13837621/61/1|Area-performance trade-off in floorplan generation of Application-Specific Network-on-Chip with soft cores|Application-Specific Network-on-Chip (ASNoC) synthesis has found increasing significance in developing System-on-Chip (SoC) solutions for applications. This paper integrates various issues in the ASNoC synthesis process – availability of soft cores (with area and aspect ratio regulations), floorplanning for the whole NoC and determining router locations. Apart from attaching primary routers to the cores, it also introduces necessary extra secondary routers, so that the inter-router link length can be kept within a specified upper bound. A Particle Swarm Optimization (PSO) based formulation has been made to solve this integrated problem with a trade-off between the overall chip-area and the network communication overhead. Experimentation has been carried out with a set of well-known benchmarks. Apart from static communication cost, throughput, latency and energy consumption of the approach have been computed. The approach compares favourably with some recent approaches reported in the literature.||||
61|1||Data loss recovery for power failure in flash memory storage systems|Due to the rapid development of flash memory technology, NAND flash has been widely used as a storage device in portable embedded systems, personal computers, and enterprise systems. However, flash memory is prone to performance degradation due to the long latency in flash program operations and flash erasure operations. One common technique for hiding long program latency is to use a temporal buffer to hold write data. Although DRAM is often used to implement the buffer because of its high performance and low bit cost, it is volatile; thus, that the data may be lost on power failure in the storage system. As a solution to this issue, recent operating systems frequently issue flush commands to force storage devices to permanently move data from the buffer into the non-volatile area. However, the excessive use of flush commands may worsen the write performance of the storage systems. In this paper, we propose two data loss recovery techniques that require fewer write operations to flash memory. These techniques remove unnecessary flash writes by storing storage metadata along with user data simultaneously by utilizing the spare area associated with each data page.||||
61|1||Using Chaos Theory based workload analysis to perform Dynamic Frequency Scaling on MPSoCs|Embedded systems sometimes experience transient overloads due to workload bursts. Such systems have to be designed to take timely reactions at the occurrences of unexpected situations. The development of smart techniques that focus the available computing power on these urgent events and at the same time, slow down the processing frequency during inactive periods could be the key for preserving energy. In the context of this study, we present a Dynamic Frequency Scaling technique based on the workload trend of a dynamic wireless application. The system can adjust the operation frequency by analyzing the workload fluctuations without degrading the final performance or violating any deadlines. In this direction, we employ an abstract model of workload analysis that combines mathematical tools from the Chaos Theory field, allowing the dynamic handling of data streams with complex behavior. To evaluate the efficiency of the proposed approach we applied it using a real application workload on a cycle-accurate Network-on-Chip simulation framework. The simulation results showed that the proposed technique could achieve remarkable improvements at the final power consumption, between 17.5% and 37.8%, depending on the system constraints.||||
61|1||Optimization of a Time-to-Digital Converter and a coincidence map algorithm for TOF-PET applications|This contribution describes the optimization of a multichannel high resolution Time-to-Digital Converter (TDC) in a Field-Programmable Gate Array (FPGA) initially capable of obtaining time resolutions below 100 ps for multiple channels. Due to its fast propagation capability it has taken advantage of the FPGA internal carry logic for accurate time measurements. Furthermore, the implementation of the TDC has been performed in different clock regions and tested with different frequencies as well, achieving improvements of up to 50% for a pair of channels. Moreover, since the TDC is potentially going to be used in a trigger system for Positron Emission Tomography (PET), the algorithm for coincidence identification has been subjected to tests in order to estimate the impact on occupied resources and the execution time. This time has been optimized, resulting in speed improvements of up to 20% while preserving occupied resources.||||
61|1||A methodology for performance/energy consumption characterization and modeling of video decoding on heterogeneous SoC and its applications|To meet the increasing complexity of mobile multimedia applications, SoCs equipping modern mobile devices integrate powerful heterogeneous processing elements among which Digital Signal Processors (DSP) and General Purpose Processors (GPP) are the most common ones. Due to the ever-growing gap between battery lifetime and hardware/software complexity in addition to application’s computing power needs, the energy saving issue becomes crucial in the design of such architectures. In this context, we propose in this paper an end-to-end study of video decoding on both GPP and DSP. The study was achieved thanks to a two steps methodology: (1) a comprehensive characterization and evaluation of the performance and the energy consumption of video decoding, (2) an accurate high level energy model is extracted based on the characterization step.||||
61|2|http://www.sciencedirect.com/science/journal/13837621/61/2|Energy minimization for reliability-guaranteed real-time applications using DVFS and checkpointing techniques|This paper addresses the energy minimization issue when executing real-time applications that have stringent reliability and deadline requirements. To guarantee the satisfaction of the application’s reliability and deadline requirements, checkpointing, Dynamic Voltage Frequency Scaling (DVFS) and backward fault recovery techniques are used. We formally prove that if using backward fault recovery, executing an application with a uniform frequency or neighboring frequencies if the desired frequency is not available, not only consumes the minimal energy but also results in the highest system reliability. Based on this theoretical conclusion, we develop a strategy that utilizes DVFS and checkpointing techniques to execute real-time applications so that not only the applications reliability and deadline requirements are guaranteed, but also the energy consumption for executing the applications is minimized. The developed strategy needs at most one execution frequency change during the execution of an application, hence, the execution overhead caused by frequency switching is small, which makes the strategy particularly useful for processors with a large frequency switching overhead. We empirically compare the developed real-time application execution strategy with recently published work. The experimental results show that, without sacrificing reliability and deadline satisfaction guarantees, the proposed approach can save up to 12% more energy when compared with other approaches.||||
61|2||FASA: A software architecture and runtime framework for flexible distributed automation systems|Modern automation systems have to cope with large amounts of sensor data to be processed, stricter security requirements, heterogeneous hardware, and an increasing need for flexibility. The challenges for tomorrow’s automation systems need software architectures of today’s real-time controllers to evolve.||||
61|2||Minimizing write operation for multi-dimensional DSP applications via a two-level partition technique with complete memory latency hiding|Most scientific and digital signal processing (DSP) applications are recursive or iterative. The execution of these applications on a chip multiprocessor (CMP) encounters two challenges. First, as most of the digital signal processing applications are both computation intensive and data intensive, an inefficient scheduling scheme may generate huge amount of write operation, cost a lot of time, and consume significant amount of energy. Second, because CPU speed has been increased dramatically compared with memory speed, the slowness of memory hinders the overall system performance. In this paper, we develop a Two-Level Partition (TLP) algorithm that can minimize write operation while achieving full parallelism for multi-dimensional DSP applications running on CMPs which employ scratchpad memory (SPM) as on-chip memory (e.g., the IBM Cell processor). Experiments on DSP benchmarks demonstrate the effectiveness and efficiency of the TLP algorithm, namely, the TLP algorithm can completely hide memory latencies to achieve full parallelism and generate the least amount of write operation to main memory compared with previous approaches. Experimental results show that our proposed algorithm is superior to all known methods, including the list scheduling, rotation scheduling, Partition Scheduling with Prefetching (PSP), and Iterational Retiming with Partitioning (IRP) algorithms. Furthermore, the TLP scheduling algorithm can reduce write operation to main memory by 45.35% and reduce the schedule length by 23.7% on average compared with the IRP scheduling algorithm, the best known algorithm.||||
61|2||Preference-oriented real-time scheduling and its application in fault-tolerant systems|In this paper, we consider a set of real-time periodic tasks where some tasks are preferably executed as soon as possible (ASAP) and others as late as possible (ALAP) while still meeting their deadlines. After introducing the idea of preference-oriented (PO) execution, we formally define the concept of PO-optimality. For fully-loaded systems (with 100% utilization), we first propose a PO-optimal scheduler, namely ASAP-Ensured Earliest Deadline (SEED), by focusing on ASAP tasks where the optimality of ALAP tasks’ preference is achieved implicitly due to the harmonicity of the PO-optimal schedules for such systems. Then, for under-utilized systems (with less than 100% utilization), we show the discrepancies between different PO-optimal schedules. By extending SEED, we propose a generalized Preference-Oriented Earliest Deadline (POED) scheduler that can obtain a PO-optimal schedule for any schedulable task set. The application of the POED scheduler in a dual-processor fault-tolerant system is further illustrated. We evaluate the proposed PO-optimal schedulers through extensive simulations. The results show that, comparing to that of the well-known EDF scheduler, the scheduling overheads of SEED and POED are higher (but still manageable) due to the additional consideration of tasks’ preferences. However, SEED and POED can achieve the preference-oriented execution objectives in a more successful way than EDF.||||
61|3-4|http://www.sciencedirect.com/science/journal/13837621/61/3-4|Exploring AADL verification tool through model transformation|Architecture Analysis and Design Language (AADL) is often used to model safety-critical real-time systems. Model transformation is widely used to extract a formal specification so that AADL models can be verified and analyzed by existing tools. Timed Abstract State Machine (TASM) is a formalism not only able to specify behavior and communication but also timing and resource aspects of the system. To verify functional and nonfunctional properties of AADL models, this paper presents a methodology for translating AADL to TASM. Our main contribution is to formally define the translation rules from an adequate subset of AADL (including thread component, port communication, behavior annex and mode change) into TASM. Based on these rules, a tool called AADL2TASM is implemented using Atlas Transformation Language (ATL). Finally, a case study from an actual data processing unit of a satellite is provided to validate the transformation and illustrate the practicality of the approach.||||
61|3-4||MaCACH: An adaptive cache-aware hybrid FTL mapping scheme using feedback control for efficient page-mapped space management|Flash memories based storage systems have some specific constraints leading designers to encapsulate some management services into a hardware/software layer called the Flash Translation Layer (FTL). The performance of flash based storage systems such as Solid State Drives (SSDs) are strongly driven by the FTL intricacies and also by a cache system placed on top of the FTL. Those systems are generally developed independently. In order to accelerate I/O request processing, FTLs use some space of the flash memory called the over-provisioning space. The over-provisioning space is thus not dedicated to data storage and should be small and of fixed size. This paper presents MaCACH, a maximum page-mapped region usage, cache-aware, and configurable hybrid mapping scheme. MaCACH design is based on two motivations: (1) the FTL should make full profit of the fixed size over-provisioning space to accelerate I/O processing, (2) as in most cases cache systems are put on top of FTLs, the latter should use information about the former in order to optimize data management. MaCACH is mainly based on two solutions: (1) it uses a proportional–integral–derivative (PID) feedback control system to keep the over-provisioning space fully used whatever the I/O workload characteristics, making it more efficient, (2) it is cache-aware as it uses a common feature of flash specific caches in order to route evicted data toward a page-mapped or block-mapped area which helps in optimizing the write operation costs. The performance evaluation shows very good behavior of MaCACH as compared to state-of-the-art FTLs in addition to a high flexibility as MaCACH has a large configuration space.||||
61|3-4||High-level design space exploration for adaptive applications on multiprocessor systems-on-chip|This paper presents an abstract design and analysis framework for applications on multiprocessor systems-on-chip (MPSoCs). The aim is to allow for faster and cost-effective implementation decisions. This framework is intended as an intermediate reasoning support to deal with important design decisions in the early design stage. Our framework enables design space exploration in order to identify adequate parallelism levels for hardware/software mappings in presence of adaptive system behaviors. For this purpose, we use an activation clock-based encoding in which an application is represented according to its event occurrences with their precedence relations. Then, different mapping and scheduling scenarios of applications on MPSoC are analyzed via clock traces representing system simulations. Among properties of interest are functional behavioral correctness, temporal performance and energy consumption. A number of experiments are performed to compare with state-of-the-art techniques based on synchronous dataflow graphs. Experimental results show that our framework provides a similar level of precision and efficiency. In addition, our framework is more flexible to deal with both application and platform adaptive behaviors, while allowing customized communication and computation behavior simulations.||||
61|3-4||FPGA acceleration of semantic tree reasoning algorithms|Semantic trees are a particular type of trees widely used in the representation of the concepts and their relations. Therefore, a computational model of the reality can be built and processed by Artificial Intelligence algorithms to infer knowledge, make decisions, etc. In this work, the design of a hardware component to accelerate reasoning operations on semantic trees by means of an FPGA based platform is presented. The target application is common-sense reasoning where marker-passing algorithms work on semantic tree structures; the core of the Scone Knowledge-Based system.||||
61|5-6|http://www.sciencedirect.com/science/journal/13837621/61/5-6|A control-based methodology for power-performance optimization in NoCs exploiting DVFS|Networks-on-Chip (NoCs) are considered a viable solution to fully exploit the computational power of multi- and many-cores, but their non negligible power consumption requires ad hoc power-performance design methodologies. In this perspective, several proposals exploited the possibility to dynamically tune voltage and frequency for the interconnect, taking steps from traditional CPU-based power management solutions. However, the impact of the actuators, i.e. the limited range of frequencies for a PLL (Phase Locked Loop) or the time to increase voltage and frequency for a Dynamic Voltage and Frequency Scaling (DVFS) modules, are often not carefully accounted for, thus overestimating the benefits. This paper presents a control-based methodology for the NoC power-performance optimization exploiting the Dynamic Frequency Scaling (DFS). Both timing and power overheads of the actuators are considered, thanks to an ad hoc simulation framework. Moreover the proposed methodology eventually allows for user and/or OS interactions to change between different high level power-performance modes, i.e. to trigger performance oriented or power saving system behaviors. Experimental validation considered a 16-core architecture comparing our proposal with different settings of threshold-based policies. We achieved a speedup up to 3 for the timing and a reduction up to 33.17% of the power ∗ time product against the best threshold-based policy. Moreover, our best control-based scheme provides an averaged power-performance product improvement of 16.50% and 34.79% against the best and the second considered threshold-based policy setting.||||
61|5-6||Improving schedulability and energy efficiency for window-constrained real-time systems with reliability requirement|For real-time embedded systems, schedulability, energy efficiency, Quality of Service (QoS), and reliability are four highly co-related important design concerns. In this paper, we explore combining these four dimensions of design issues to achieve better schedulability and energy efficiency for real-time systems while satisfying the QoS and reliability requirements. The QoS requirements are deterministically quantified with the window-constraints, which require that at least m out of each non-overlapped window of k consecutive jobs of a real-time task meet their deadlines. Our contributions mainly consists of two parts: in the first part, we propose two off-line approaches for the management of appropriate mandatory/optional job partitioning with the purpose of improving the schedulability for real-time systems with window-constraints; based on it, in the second part, we proposed advanced techniques to reduce the energy consumption of the real-time systems while satisfying the reliability requirements. The results of extensive experiments demonstrate that our proposed techniques significantly outperform previous approaches in both schedulability and energy efficiency for real-time embedded systems while satisfying the window-constraints and reliability requirements.||||
61|5-6||A predictable hardware to exploit temporal reuse in real-time and embedded systems|In this paper we propose a new hardware data cache (FAFB, fully-associative FIFO tagged buffers) to complement the data cache in processors. It provides predictability when exploiting temporal reuse in array data structures, i.e. it allows an accurate WCET analysis, which is required in real-time systems. With our hardware proposal, compiler transformations that exploit such reuse (essentially tiling) can be safely applied. Moreover, our proposal has other features of particular interest to embedded systems, where a set of well-tuned applications run in a hardware platform which may be constrained in size, complexity and energy consumption. In order to test the most uncommon features of the FAFBs (predictability and effectiveness with a small size), we perform a worst-case analysis on several kernel algorithms for embedded and real-time computing, showing the interaction between tiling and our hardware architecture. Our results show that the number of data cache misses is reduced between 1.3 and 19 times on such algorithms.||||
61|5-6||A library for developing real-time and embedded applications in C|Next generation applications will demand more cost-effective programming abstractions to reduce increasing maintenance and development costs. In this context, the article explores the integration of an efficient programming language and high-level real-time programming abstractions. The resulting abstraction is called Embedded Cyber Physical C (ECP-C) and it is useful for designing real-time applications directly on C. The abstraction has its roots on the real-time Java: one of the most modern programming languages, which benefited from mature programming patterns previously developed for other languages. It also targets embedded processors running on limited hardware. ECP-C takes the programming abstractions described in real-time Java and reflects them into a C application system, providing extensions for multi-threading, resource sharing, memory management, external event, signaling, and memory access. It also reports on the performance results obtained in a set of infrastructures used to check ECP-C, providing clues on the overhead introduced by these mechanisms on limited infrastructures.||||
61|5-6||Distributed storage protection in wireless sensor networks|With reference to a distributed architecture consisting of sensor nodes connected in a wireless network, we present a model of a protection system based on segments and applications. An application is the result of the joint activities of a set of cooperating nodes. A given node can access a segment stored in the primary memory of a different node only by presenting a gate for that segment. A gate is a form of pointer protected cryptographically, which references a segment and specifies a set of access rights for this segment. Gates can be freely transmitted between nodes, thereby granting the corresponding access permissions. Two special node functionalities are considered, segment servers and application servers. Segment servers are used for inter-application communication and information gathering. An application server is used in each application to support key management and rekeying. The rekey mechanism takes advantage of key naming to cope with losses of rekey messages. The total memory requirements for key and gate storage result to be a negligible fraction of the overall memory resources of the generic network node.||||
61|5-6||Real-time and distributed computing in emerging applications. Foreword by the general chairs of Reaction 2012|||||
61|7|http://www.sciencedirect.com/science/journal/13837621/61/7|Compositional power-aware real-time scheduling with discrete frequency levels|Power consumption remains a hot issue in all areas of computing ranging from embedded systems that rely on batteries to large scale data centers where reducing the power consumption of computing devices directly affects not only the management cost, but also contributes to a greener computing environment. The power-aware real-time scheduling problem has recently been addressed for a compositional framework with periodic task model under the assumption that a processor can continuously vary its operating frequency and voltage. However, in practice, this technique is only suboptimal and still produce the waste of computational resources. This paper introduces new frequency scaling schemes that statically determine optimal processor speeds at system, component, and task levels with the objective of minimizing the total energy consumption of the entire framework. Since real-world processors support only a finite set of operating frequencies, our algorithms also consider only discrete speed levels and guarantee still that each task meets its deadline. We implemented and evaluated the performance of a prototype framework that incorporates our algorithms on top of the RT-Xen hypervisor in order to provide power-aware compositional real-time scheduling framework to virtual machines.||||
61|7||Energy Optimization of Security-Critical Real-Time Applications with Guaranteed Security Protection|Designing energy-efficient applications has become of critical importance for embedded systems, especially for battery-powered systems. Additionally, the emerging requirements on both security and real-time make it much more difficult to produce ideal solutions. In this work, we address the emerging scheduling problem existed in the design of secure and energy-efficient real-time embedded systems. The objective is to minimize the system energy consumption subject to security and schedulability constraints. Due to the complexity of the problem, we propose a dynamic programming based approximation approach to find efficient solutions under given constraints. The proposed technique has polynomial time complexity which is half of existing approximation approaches. The efficiency of our algorithm is validated by extensive experiments and a real-life case study. Comparing with other approaches, the proposed approach achieves energy-saving up to 37.6% without violating the real-time and security constraints of the system.||||
